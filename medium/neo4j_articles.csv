url;text;date;tags
https://medium.com/neo4j/neo4j-considerations-in-orchestration-environments-584db747dca5;David AllenFollowMar 5, 2019·7 min readNeo4j Considerations in Orchestration EnvironmentsAs more workloads are being run in Docker containers, using orchestration environments like Kubernetes has spiked in popularity.Neo4j already provides public docker containers, and has published a helm chart that makes installing Neo4j easy.UPDATE: June 2020 added references to the helm chart, and corrected some details.Databases in Container OrchestratorsOrchestration environments though introduce some special challenges for databases, and Neo4j in particular. This article aims to cover some of those and the extra considerations you might want to keep in mind. Most of the examples here will talk about Kubernetes specifics, but architecturally, the considerations are the same whether it’s Kubernetes or any other orchestration manager.We’ll take a look at the architecture of a Neo4j deployment, both from inside of Kubernetes” and outside” perspectives, and talk about how that impacts querying.Questions? Comments? Come discuss on the Neo4j Community Site on this thread.Looking at Neo4j inside of KubernetesThe diagram below shows how the internal deploy of a typical Neo4j cluster in Kubernetes works. Each pod is placed in a StatefulSet depending on whether it is a core or read replica node. Each pod has a persistent volume claim which maps to an underlying disk which stores the data.Neo4j on Google Kubernetes Engine (GKE) architecture diagramLooking at Neo4j from outside of KubernetesWithin Kubernetes, everything is straightforward the pods can be connected to directly via internal DNS. The app2” in the box below has no trouble dealing with the cluster because the auto-assigned DNS routes for it.App1 talking to Neo4j inside of KubernetesThe outside app1 though has a different set of issues. The Kubernetes cluster manager has some public ingress IP address, and we need a way of routing traffic into the right cluster node, and this is where things become a bit tricky.TipThis section assumes some familiarity with how bolt+routing works in Neo4j. If you need a brush up on any of the ideas of how Neo4j query routing works, please have a look at Querying Neo4j Clusters. In that article, we described how the public advertised address” of a node interacts with the Routing Table shown in the diagram above.In Kubernetes setups, there is a specific problem: internal DNS in the table above cannot be routed by app1 outside of the cluster. So even if there is an IP ingress, attempts to use bolt+routing Neo4j drivers will fail. App1 will get a routing table with un-routable information, and will fail to connect.SolutionThe external exposure instructions posted in the Neo4j Helm repository chart a way of solving this.Request / allocate static IP addresses for each memberSend custom configuration to the pods to tell them to advertise themselves according to that static IP addressConfigure a load balancer for each pod that exposes that pod by the static IP addressOnce we can get this far, we can also register DNS and associate the static IP addresses with that DNS, and do things like HTTPS/SSL if we need.Now that we’ve covered Neo4j networking and bolt+routing specifically, let’s get into other things that make databases different and what to look out for.Orchestration Managers and DatabasesAs orchestration managers were born and grew up, they typically handled workloads that had a lot of stateless microservices. Developers would create small containers that held a single service. Typically that service did not store any data (if it did, it talked to a database deployed outside). These services, because they were stateless, could be deployed in fleets, and willy nilly killed and restarted. Typical deployments might have many replica containers, fronted by a load balancer. Clients would talk to the load balancer, and end up getting forwarded to any of the microservice instances, it didn’t matter which.Orchestration manager features play directly to this sweet spot: Kubernetes with the concepts of ReplicaSets(a set of replicas of a given container) and LoadBalancers(allowing access to any replica through a single network ingress).Databases are DifferentDatabases (including Neo4j) are different in three key ways here!They’re Stateful: They must store data, that’s their whole reason for being. So in orchestration managers we need to think about concepts like disks, persistent volume claims, and so on.Database pods use Persistent Volume Claims to keep long-term storage of stateTheir Pods are Not Interchangeable — They either host different services, or have different capabilities. In the Neo4j architecture, for example, there are leaders and followers, where leaders need to process the writes.Different nodes play different roles in clustered database architecturesIf you need to talk to a particular cluster member though, this limits the usefulness of abstractions like load balancers, because they don’t know or care about any differences. In several databases, this necessitates the whole idea of a routing driver” that handles this logic at the application or driver level. Which in turn complicates the orchestration networking configuration.They scale differently — with micro services, suppose I have a function hello which returns hello world”. I can scale this service up and down very quickly. Launching the container takes seconds, and there’s no stored data to synchronize. With databases, when you scale up” you may need to copy most or all of the database contents to the new node before it can fully participate in the cluster. Doing that scale up” may also place extra load on the other cluster members while they replicate data and check in with their new peer. When you scale down, your cluster finds that a member has gone missing, and it will recover but each of the other instances needs to update its member list.None of that is happening with your typical micro service. As a result, up-front capacity planning for databases is more important, as it isn’t going to be a good idea to rapidly add/remove 10 instances of a 5TB database.Noisy Neighbors, Co-Residency, and HAIn an orchestrator it can also be important to think about Affinity and Anti-Affinity rules to spread cluster members out across physical hardware. With microservices with many copies this may matter less — but if you’re running a 3-node clustered database for high availability, it is very important to ensure that not all 3 pods are running on the same physical server. If they are, and the physical server dies, then the database is completely down despite your attempts to guarantee HA!If all three nodes of your cluster live on the same physical server, and that server burns, you have no high availability! (HA)Spreading things out (and avoiding co-residency) also helps to insulate you from so-called Noisy Neighbors” or other workloads on the same machine that are soaking up shared resources.Disks: Thin vs. Thick ProvisioningIn virtualized environments, whether in the cloud or on-prem, when you create a persistent volume claim you take for granted that you get a disk and it just works”. But how does that actually work?Under the covers, your virtual disk is usually allocated from some kind of storage solution, or SAN. Many of those tend to use thin provisioning by default.Over-allocation or over-subscription is a mechanism that allows a server to view more storage capacity than has been physically reserved on the storage array itself. This allows flexibility in growth of storage volumes, without having to predict accurately how much a volume will grow. Instead, block growth becomes sequential. Physical storage capacity on the array is only dedicated when data is actually written by the application, not when the storage volume is initially allocated. The servers, and by extension the applications that reside on them, view a full size volume from the storage but the storage itself only allocates the blocks of data when they are written.Thin provisioningThis is great for a wide range of applications but very bad for databases. It’s possible in some storage environments for your database to try to write data and to fail, because the underlying storage isn’t available. This can occur when the storage layer becomes over-subscribed and can be a catastrophic problem for the database, because its core underlying assumption is that it has a disk that it can use — which turns out not to be true. If you have a clustered Neo4j setup and this is only happening to one node, you probably won’t lose data (because the HA capabilities of the database are keeping the data safe on the other two nodes) but you’re going to see lots of strange errors which make no sense, and you’ll have operational issues.The fix is to ensure that when you allocate disk, you’re doing it in a thick” provisioning way, meaning that when the Persistent Volume is created, you’re guaranteed to have an exclusive hold on it. How to do this will differ between cloud environments and storage solutions and so depending on your kubernetes provider or method of hosting, this may or may not apply to you, but it’s something to keep a careful eye on.Other ReferencesNeo4j Helm Chart User GuideNeo4j on GKE Marketplace User GuideWebinar: Why You Need Graph Technology on GKE;Mar 5, 2019;[]
https://medium.com/neo4j/modeling-patient-journeys-with-neo4j-d0785fbbf5a2;Matt HolfordFollowJun 3, 2020·10 min readModeling Patient Journeys with Neo4jThis article is co-written by Ravi Anthapu and Matt Holford. We are both engineers at Neo4j, inc.We do a lot of what we call event-based” modelling at Neo4j. This kind of modeling is something for which graph databases are especially well suited. It enables our customers to quickly draw new and meaningful conclusions from massive amounts of data. But what is event-based modeling” and why are graph databases so good at it? To answer these questions, we will walk through modeling events in the healthcare domain and see what sorts of insights we can make from our model.JourneysWhen we model events, we look at the actions of individuals. Over the course of time, an individual participates in a number of events. The events themselves may vary according to the domain we are looking at they may range in importance from the mundane to the earth-shaking. The point is that these events when taken in sequence, tell us a story” about that individual in our domain. This connected series of events is also commonly called a journey”, as the individual travels step by step from one event to the next.Although the story of one individual can be interesting in its own right, the true power of the event-based model is revealed when we start to aggregate the journeys of many individuals. For example, we can start to detect common patterns of behavior. This in turn makes it easier for us to pick out outliers”, individuals exhibiting unusual behavior. We can also begin to group individuals based upon having similar behavior patterns. These groups, or communities, are useful in predicting future behavior, as it is likely that an individual will behave as other members of her community did when confronted with a similar situation.Patient JourneysIn the medical domain, we can think of a person’s life as a Patient Journey”, filled with medical events, starting with birth and culminating, eventually, in death. All interactions with the medical establishment during a person’s lifetime form steps along the path. These can include doctor visits, hospitalizations, prescriptions, diagnoses and medical procedures. If we aggregate a large number of patient’s journeys, we can begin to observe trends in disease and patterns in treatment. For example, we could see what conditions emerge as side-effects to taking a medication or what medical procedures are followed after diagnosis with a disease, and how effective these procedures are.Sankey diagram showing procedures within 90 days of Pre-eclampsia diagnosisHow can we get data for our model? Unless you are an insider at a biomedical organization, like a hospital or research facility, it will be quite difficult to get hold of real patient data. While anonymization of patient data is possible, even this data can require significant clearance and expense. Luckily, there is a third option — synthetic patient data. The Synthea project is able to generate medical records on demand based upon statistical distributions. Modular in design, it is quite customizable as far as disease prevalence, treatment options and patient demographics. Though not perfect (we notice various discrepancies from time to time), the generated patient data is remarkably plausible. Synthea is widely used and actively maintained. For example, as I write this, several efforts are underway to incorporate COVID-19 records. You can find instructions on how to generate Synthea data for the model on our GitHub repository here.Data Model — First VersionPeople and PlacesLet’s start modelling this domain. First off, we can create a Patient node. Properties should include demographic information such as: first name, last name, race, gender, birth date and death date (when applicable). For capturing the Patient’s location, it makes sense to model a distinct Address node, and link it to the Patient via a HAS_ADDRESS relationship. This design gives us a couple of advantages. For one, some patients (e.g. family members) may live at the same address. Secondly, other entities such as doctors and hospitals have addresses as well. The Address node would allow us to readily match entities with the same addresses. Additionally, the Address node can exploit Neo4j’s built-in geo-spatial capabilities by representing the latitude and longitude of the Address using the Point type. Then we can quickly compute the distances between Addresses and thus find Patients close to each other or to other landmarks.Providers (doctors) and Organizations (hospitals, clinics, etc) can be modeled the same way. We keep things like name and speciality as properties on Provider and Organization nodes and move location out into a separate node via a HAS_ADDRESS relationship. There is also a connection between Providers and the Organizations for whom they work. So, we create a BELONGS_TO relationship between Providers and Organizations.Let’s also model Payers- insurance providers or others who foot the bill for medical encounters. A patient’s insurance can change over the years and we will want to capture that in our graph. Thus, we create INSURANCE_START and INSURANCE_END relationships between the Patient and Payer nodes. A relationship property can be used to indicate the times at which coverage started or ended.Encounters and EventsWe can treat each interaction with the medical community as a separate Encounter. Each Encounter is given a date property and is associated with a Provider and a Payer. So, for each Encounter node, we create HAS_PAYER and HAS_PROVIDER relationships connecting it to the relevant parties. We also create a link from Patient via the HAS_ENCOUNTER relationship. The Encounter node should also be linked to the events that occurred or commenced within its span. We model six types of event nodes (Condition, Drug, Procedure, CarePlan, Allergy and Observation), each of which can materialize zero to many times as part of an Encounter. Properties of these nodes would include a name and a formal identification code (e.g. from SNOMED or ICD9). The relationships connecting them from Encounter would be called, naturally enough, HAS_CONDITION, HAS_DRUG, etc.Let’s consider one more subtle point. Some medical events can have start dates as well as end dates. For example, a condition may be cured or disappear after a while a patient may stop taking a drug or following a care plan. Other medical events have only a performed date. For example, a medical procedure is performed at a particular time and not over a span of dates. For all events, we use a startDate property to capture the start or perform (if applicable) date. For those events with an end date, we do something a bit different. Whilst we could also model endDate as a property, what if we wanted to capture patient journeys following the end of something? We may want to see what happens after patients stop taking a medication or after an illness goes away. For this reason, we treat the end as a separate Encounter and we connect it to the original Encounter via a HAS_END relationship. The end Encounter will also be connected to the event e.g. by HAS_CONDITION etc.There. Now we have a complete first pass at a Patient Journey model.Using the Data ModelTo follow a single patient’s journey, we could find that Patient by their ID and traverse the HAS_ENCOUNTER relationships. From there, we can hop to whatever event node by navigating the HAS_CONDITION (or HAS_DRUG, etc) relationship. If we wanted to capture the journeys of all patients after being diagnosed with a disease, we could start from the Condition node representing the disease, get all Encounters where the disease was diagnosed, get the Patients attached to those Encounters and proceed with the journey for each patient as above.To form a journey, we want to present the patient’s Encounters in date order but here is where we start to run into problems. To find the patient’s first Encounter, we need to sort all of the Encounters connected to the Patient by date. Since we have created an index on the date property for Encounters, this does not seem so bad. However, we must do a similar sort to obtain the next Encounter and again for the Encounter after that. This may not have too negative an effect when finding the journey of a single patient, barring one who is especially sickly and/or long-lived, but the impact accumulates when we aggregate across thousands or even millions of patients. We could prevent multiple sorts by sorting once and keeping the full list in memory, but here again we run into resource problems when we scale up the number of patients.Data Model — ImprovementsHmm, how can we do better? Can graph technology lead us to a better solution? Let’s start by thinking about the notion of a journey”. If we were walking along a clearly-marked path, a yellow brick road for example, it would be immediately obvious what our next step should be — the next brick. We wouldn’t need to consult a map to see where to go the direction to go is right before our eyes. Graph databases store data in a manner compatible with this metaphor. Connected relationships are stored directly in the data structure of the nodes themselves by way of a linked list.The NEXT RelationshipWe can exploit this powerful data structure by connecting each Encounter to the next chronological Encounter for that Patient. The relationship we call, naturally enough, NEXT, and here is where the magic of Graph databases comes into play. We can compute the whole patient journey by simply hopping along the NEXT relationship from Encounter to Encounter. From each Encounter, it’s still just a quick hop to find whatever Condition, Drug, etc. occurred during that Encounter.With the NEXT relationship, the model looks like this. Now let us see if our thought process is correct.Performance ComparisonWe can run a few Cypher queries to indicate the performance characteristics of the two models. In the first model, the query to get the journey of a single patient would look something like this:MATCH (p:Patient {id:$patientId})-[:HAS_ENCOUNTER]->(e)WITH e ORDER BY e.dateMATCH (e)-[:HAS_CONDITION|:HAS_DRUG|:HAS_CARE_PLAN|:HAS_ALLERGY|:HAS_PROCEDURE]->(x)OPTIONAL MATCH (e)-[:HAS_END]->(end)RETURN labels(x)[0] AS eventType, x.description AS name,      e.date AS startDate, coalesce(end.date, ‘NA’) AS endDateProfiling this query on our test instance composed of a million patients, we found the query took 3ms and 334 db hits. Not bad. In the second model, the query would resemble:MATCH (p:Patient {id:$patientId})- [:HAS_ENCOUNTER]->(e)WHERE apoc.node.degree.in(e, ‘NEXT’) = 0WITH eMATCH (e)-[:NEXT*]->(e2)-[:HAS_CONDITION|:HAS_DRUG|:HAS_CARE_PLAN|:HAS_ALLERGY|:HAS_PROCEDURE]->(x)OPTIONAL MATCH (e2)-[:HAS_END]->(end)RETURN labels(x)[0] AS eventType, x.description AS name,     e2.date AS startDate,coalesce(end.date, ‘NA’) AS endDateThis query also takes just a couple milliseconds and spent 390 db hits under profile.Hmm. This suggests that using the NEXT relationship comes with a small cost. This is because the Patient node is connected to all event nodes, making sort look like an efficient option.Let’s take a look at this data from a different perspective. When we are trying to analyze a Condition, we would not start from a single Patient node. We want to look at the journeys of all patients after diagnosis with a Condition. Let’s look at such a query in the first model:MATCH (c:Condition {code:$code}) <-[:HAS_CONDITION]-(encounter)WITH encounter LIMIT 1MATCH (encounter)<-[:HAS_ENCOUNTER]-(patient)WITH patient, encounterMATCH (patient)-[:HAS_ENCOUNTER]->(e)WHERE encounter.date <= e.date < (encounter.date + duration(‘P90D’))WITH e ORDER BY e.dateMATCH (e)-[:HAS_CONDITION|:HAS_DRUG|:HAS_CARE_PLAN|:HAS_ALLERGY|:HAS_PROCEDURE]->(x)OPTIONAL MATCH (e)-[:HAS_END]->(end)RETURN labels(x)[0] AS eventType, x.description AS name,      e.date AS startDate,coalesce(end.date, ‘NA’) AS endDateNote that for these purposes we have reduced the number of patients to just one. On our test server, this completes in 2 ms and consumes 428 db hits. Adapting this query to the second model, we end up with something like this:MATCH (c:Condition {code:‘271737000’}) <-[:HAS_CONDITION]-(e)WITH e LIMIT 1MATCH (e)-[:NEXT*]->(e2)-[:HAS_CONDITION|:HAS_DRUG|:HAS_CARE_PLAN|:HAS_ALLERGY|:HAS_PROCEDURE]->(x) WHERE e2.date < ( e.date + duration(‘P90D’) )OPTIONAL MATCH (e2)-[:HAS_END]->(end)RETURN labels(x)[0] AS eventType, x.description AS name,      e2.date AS startDate,coalesce(end.date, ‘NA’) AS endDateThis query also takes 2 ms, but spends only 412 db hits, so it seems our thought process is a valid one. The query using NEXT is not only easier to understand- it performs better. The savings using the NEXT technique instead of sorting may appear small with a patient population of just one, but when multiplied across thousands or even millions of patients it makes a huge difference.Using the NEXT relationship also allows us to form queries that require back and forth traversal between Encounters.MATCH (a:Allergy)<-[:HAS_ALLERGY]-(e)-[:HAS_END]->(e2),     (e)<-[:HAS_ENCOUNTER]-(patient)WITH patient, a, e, e2 LIMIT 10MATCH p=(e)-[:NEXT*]->(e2)WITH patient, a, nodes(p) AS nodesUNWIND nodes AS tempeMATCH (tempe)-[:HAS_DRUG]->(d)RETURN patient.firstName AS firstName, patient.lastName AS lastName,     a.description AS Allergy, collect(d.description) AS drugsIn this query we find Encounters in which an Allergy ceased to be present. We then collect the drugs taken by patients prior to relief from the allergy. This shows what kind of drugs can successfully treat an allergy across the patient population. The query above takes 4527 db hits. If we attempted such a query with our sort-reliant model, it would look like:MATCH (a:Allergy)<-[:HAS_ALLERGY]-(e)-[:HAS_END]->(e2),      (e)<-[:HAS_ENCOUNTER]-(patient)WITH patient, a, e, e2 LIMIT 10MATCH (patient)-[:HAS_ENCOUNTER]->(encounter)WHERE e.date <= encounter.date <= e2.dateWITH patient, a, encounter ORDER BY encounter.dateWITH patient, a, collect(encounter) AS nodesUNWIND nodes AS tempeMATCH (tempe)-[:HAS_DRUG]->(d)RETURN patient.firstName AS firstName, patient.lastName AS lastName,      a.description AS Allergy, collect(d.description) AS drugsThis query consumes 7509 db hits, over 50% worse! As you can see, the more we need to traverse along Encounter nodes, the more performance benefit we gain from the NEXT relationship.Diagram of an individual patient journeyNext StepsNow we have our complete model for patient journeys! On our GitHub, you can find Cypher scripts to load data into Neo4j from Synthea. For ingestion, we use this python utility. Pyingest leverages batching and optimizations from the Pandas library to enable rapid ingestion to the graph. We have found that it outperforms Neo4j’s built-in CSV loader while consuming less server resources.In our next blog, we will look at how we can use Neo4j’s Java API to quickly calculate patient journeys starting from events of interest.;Jun 3, 2020;[]
https://medium.com/neo4j/from-kinesis-via-spark-to-neo4j-97d564562b61;Davide FantuzziFollowJul 25, 2021·9 min read(from:Kinesis)-[:VIA_SPARK]->(to:Neo4j)Kinesis is a powerful AWS suite for managing video and data streams. In this article we will see how to read a Kinesis Data Stream from Apache Spark into Neo4j, leveraging the Spark Structured Streaming API and the Neo4j Connector for Apache Spark.Our GoalLet’s say we are event organizers, and we have devices tracking attendees check-ins time at each event and sending them to Kinesis. We then want to save this check-in stream to Neo4j for applying some GDS algorithms in a later stage.Let’s Get Ready!We will get into the code later, we need to first setup our environment. Let’s see which steps we have to take in order to get to our goal. Note that if you have any of these already, you can skip to the next one:Signup for an AWS accountSetup a IAM UserSetup AWS Kinesis Data StreamSetup Kinesis Data Generator (requires a Cognito User Pool)Clone the Neo4j Spark Connector Notebooks repositoryConnect the code with your IAM UserWatch the magic happen!Depending on your experience with AWS, the setup part could be more or less tricky for me it was extremely tricky.Note: if you already have a Kinesis stream setup, you can jump straight to the Neo4j and Spark: the missing pieces” section!AWS AccountWell I think we first need an AWS Account. You can register here. It will probably ask you to add a payment method you might be billed depending on how many data you stream and read from Kinesis. Here’s the billing policies for Kinesis. Just to give you an idea, I heavily streamed data for 3 days straight for preparing the code and I got billed less than 2 dollars.A more detailed guide on how to setup an AWS account can be found here.IAM User for the ConsumerWe need to create an IAM User for the Spark consumer we are going to use to read from the stream. Here’s what I did:I created an IAM Group called Kinesis with these policies.This is probably too much but I always panic when dealing with AWS policies. Better safe than sorry.Please be sure that your group matches my exact policies, or you might encounter some issues in later stages.After this, proceed to create a IAM User with programmatic access only:In the next step, link the user to the Kinesis group you just created.The following step asks for any tags to add to the user, I put none, feel free to add some if you want, it doesn’t matter for this demo.Once the user is created you’ll be shown the Access key ID and the Secret access key. Save them now! You’ll need them later.AWS Kinesis Data StreamKinesis offers a few products, what we’ll be focusing on today is Kinesis Data Stream go to your AWS Console and search for kinesis” in the search bar, or go to this link.One important note before moving on: always remember to check for the correct AWS Region, or you might have some difficulties. For this demo I’ll be using the eu-central-1, but you are free to use whichever you prefer.From here you should see a Create Data Stream” button, click on that.To set up our stream we only need two things: the stream name, and the number of shards. We’ll use Kinesis2Neo4j as stream name and 1 as number of shards.For for creating a new Kinesis Data StreamMore info on creating a data stream can be found here.Amazon Kinesis Data GeneratorThe Kinesis Data Generator (KDG from now on) is a random data generator for Kinesis, that will help us send dummy data to the stream. The random data are generated by the Faker.js library and it can be configured to send any text format (XML, CSV, Json, etc…).Let’s go to the KDG homepage. To be able to use the KDG we need to sign in with a Cognito user. If you already have a Cognito user, skip this section.If you don’t have a Cognito Pool set up, click the Help link on the KDG homepage you can read the initial paragraphs if you want to understand what’s about to happen, but what you really need to do is clicking on the big blue button Create a Cognito User with CloudFormation” you can find scrolling down a bit.Click it and follow the instructions. The first part of this article describes the process in more detail. Again, be sure to use the same AWS Region!At the end of the process, you should end up with something like this:Check the Region, and click on the Outputs tab in your CloudFormation stack.Click on the link in the Value column, you’ll get redirected to the KDG homepage (I suggest to bookmark this link), login with the user and password you just created, and you should be seeing this:You might have different values here, don’t worry, we will configure this later.Select the region where you created the Kinesis Data Stream, and the stream name will appear in the select box below.Set as many records per second as you want, for now I will set 3, but feel free to increase it. This number tells the KDG how many records will be sent to the stream each second. Remember that the more data you stream, the higher chance of getting billed by AWS.Down, in the Record Template section, paste this template:{     user_name :  {{name.firstName}} {{name.lastName}} ,     event_name :  {{random.arrayElement(      [ My Fairy Fair ,  Euro 2020 Final ,  Nodes2022 ]    )}} ,     user_checkin_time : {{date.now( X )}}}What you see between double curly braces is a Faker template, the content will be replaced with the value returned by the function if you click on Test Template” you can see it in action:Example of random generated record to send to Kinesis stream.Later on we will click on the Send data” button to start streaming the data on Kinesis, but for now just leave this browser tab open.So, it looks like we are done with the setup!Neo4j and Spark: The Missing PiecesOk, now some serious business. First of all, clone this repository that contains some Apache Zeppelin notebooks you can use to play with Spark and Neo4j. Everything runs on Docker so if you don’t have it on your machine please look here.To start the environment just run docker-compose up and wait for everything to be ready:When everything is up and running, open https://localhost:8080 for the Zeppelin interface, and open the notebook located in Streaming / From Kinesis to Neo4j (http://localhost:8080/#/notebook/2GAXPN2BH).At http://localhost:7474 you have access to Neo4j Browser, you can login using neo4j as user and password as password.SparkNow let’s see the different cells and comment on them.A quick but interesting note: the following bits of code are applicable to basically any streaming service you are using as source. The only critical information here are the Kinesis specific options (endpointUrl, AWS access keys) and the Dataframe transformation we will see in a later cell. The pattern would stay the same.val kinesisStream = spark.readStream  .format( kinesis )  .option( streamName , z.textbox( StreamName ).toString)  .option( endpointUrl ,  z.textbox( KinesisEndpoint ).toString)  .option( awsAccessKeyId , z.textbox( AwsAccessKeyId ).toString)  .option( awsSecretKey , z.textbox( AwsSecretAccessKey ).toString)  .option( startingPosition ,  LATEST )  .loadz.textbox is a Zeppelin utility that creates a text box under the cell where you can put your parameters, like this:z.textbox in actionThe required parameters are:streamName is the name of the Kinesis Stream you set when creating itThe endpointUrl can be found hereThe awsAccessKeyId and the awsSecretKey we saved earlier when creating the IAM UserThe startingPosition option tells Kinesis which data we want to start reading from, in our case we want just the data streamed from when we started reading.We are ready to read the Kinesis Data Stream from Apache Spark, to do so we use the Kinesis Connector for Structured Streaming, a package that allow us to easily leverage the Apache Spark Structured Streaming API .We now have a Streaming DataFrame that contains all the data coming from Kinesis, and it have this schema:kinesisStream.printSchemaroot |-- data: binary (nullable = true) |-- streamName: string (nullable = true) |-- partitionKey: string (nullable = true) |-- sequenceNumber: string (nullable = true) |-- approximateArrivalTimestamp: timestamp (nullable = true)We just need the data field that contains our randomly generated JSON from the KDG, but since it comes in binary format we have to convert it to string first, and then decode it to JSON using the from_json Spark function. This function requires a string to be decoded as first parameter and a StructType as second parameter, that represents the Schema of the JSON being created.This is our schema:val kinesisStreamDataSchema = StructType(Seq(  StructField( user_name , DataTypes.StringType, nullable = false),  StructField( user_checkin_time , DataTypes.TimestampType, nullable = false),  StructField( event_name , DataTypes.StringType, nullable = false),))And this is our Spark DataFrame transformation:val kinesisData = kinesisStream  .selectExpr( CAST(data AS STRING) ).as[String]  .withColumn(     jsonData ,     from_json(col( data ), kinesisStreamDataSchema)  )  .select( jsonData.* )We now end up with a Streaming DataFrame with 3 columns:kinesisData.printSchemaroot |-- user_name: string (nullable = true) |-- user_checkin_time: timestamp (nullable = true) |-- event_name: string (nullable = true)What do we want to do with this? We want to write these 3 columns to Neo4j in a way that allows us to have nodes of type Attendee connected with a CHECKED_IN relationship to nodes of type Event. This would be the schema:The attendee will have a property name, the event will have a property name as well, and the relationship will have a property at containing the timestamp the attendee checked in at the event.The time has come, brace yourself and prepare to dive into the code that will make the magic happen! To write the Kinesis Stream in Neo4j we will use the Neo4j Connector for Apache Spark, the official package to leverage the powerful features of Neo4j directly from Apache Spark.First the code, that can be a bit scary first but don’t worry because I will explain it in detail:val kinesisQuery = kinesisData  .writeStream  .format( org.neo4j.spark.DataSource )  // connection options  .option( url ,  bolt://localhost:7687 )  .option( authentication.type ,  basic )  .option( authentication.basic.username ,  neo4j )  .option( authentication.basic.password ,  password )  .option( checkpointLocation ,  /tmp/kinesis2Neo4jCheckpoint )  // end connection options  .option( save.mode ,  Append )  .option( relationship ,  CHECKED_IN )  .option( relationship.save.strategy ,  keys )  .option( relationship.properties ,  user_checkin_time:at )  .option( relationship.source.labels ,  :Attendee )  .option( relationship.source.save.mode ,  Overwrite )  .option( relationship.source.node.keys ,  user_name:name )  .option( relationship.target.labels ,  :Event )  .option( relationship.target.save.mode ,  Overwrite )  .option( relationship.target.node.keys ,  event_name:name )  .start()The first part is nothing more than connection options. We also need to specify the checkpoint files location.What’s happening in the second part? First of all, we will write using the relationship mode, and via the save.mode = Append option we are telling the connector to use the MERGE keyword when writing the relationship. relationship.save.strategy = keys means we want to specify the merging keys manually.That’s what we are doing with relationship.source.node.keys and relationship.target.node.keys where we specify which column of our DataFrame should be mapped to which property on the node. The labels of the nodes to be written are specified via the relationship.source.labels and relationship.target.labels. We use Overwrite save mode for both source and target nodes, in this way we will be merging Attendee and Event nodes based on their name property.That code will generate a Cypher query similar to this:MERGE (a:Attendee {name: df.user_name})MERGE (e:Event {name: df.event_name})MERGE (a)-[rel:CHECKED_IN]->(e)SET rel.at = df.user_checkin_timeLet’s Give It a Shot!Now that we have the KDG ready to start streaming and our Docker Neo4j instance up and running, we just need to execute the code cells in the notebook, go back to the KDG tab, and press the Send data button after a few seconds you can stop the generator (so we don’t waste AWS resources), go to Neo4j Browser and run MATCH (n) RETURN n to see what happened!The graph representation of the Kinesis Data Stream in Neo4j Browser.ConclusionApart from the initial AWS configuration, we saw how easy it is to write your Kinesis stream into Neo4j using just a few lines of Spark code. This is obviously an easy example, but what could change it’s really just how you want to write the data in Neo4j.I hope you enjoyed the walkthrough, if you have any questions you can open an issue on the repository or reach out to me via Twitter.If you want to know more on how to write any Spark DataFrame into Neo4j check out the Neo4j Connector for Apache Spark official documentation.;Jul 25, 2021;[]
https://medium.com/neo4j/rock-n-roll-traffic-routing-with-neo4j-part-2-f2a74fe7d7f;Jasper BluesFollowJul 24, 2020·11 min readROCK ’N’ ROLL TRAFFIC ROUTING, WITH NEO4J, PART 2Originally posted on November 28, 2018 by Jasper Blues in Saxeburg Series.This post is the second of my #saxeburg-series, for young entrepreneurs (or the young at heart), in which we’ll learn about Neo4j and have some fun at the same time. How? By building our own Neo4j-powered mobile game.Originally at Liberation Data, the series will be continued on Neo4j’s own developer relations space — thanks so much to Ljubica Lazarevic and my friends at Neo4j for editing & hosting.THE STORY CONTINUESI did plan on each of the Saxeburg Series having an accompanying short story, with Moongirl, and some new folks, who you will meet soon. Just like we had in the first installment.Because, let’s face it, what could be more awesome than exploring graph algorithms, aided by a menagerie of colorful albeit possibly criminally minded side-kicks? Especially if we place our beloved fictional friends into a city that is so dysfunctional, chaotic and downright dangerous that their very survival (wait, he survived!?) depends on the formulas we employ on their behalf.However, I’ve been working on a new open-source framework for you. Therefore I didn’t have the time to prepare a new chapter this week. Tell you what though: You take the leading role for this installment, and I’ll have a new chapter for you soon, okay?LET’S BEGINIn the last post we learned how to perform traffic routing with Neo4j.We discovered that Neo4j is accessible. With nothing more than a graph model, Cypher and a little creative thinking we were able to solve a real-world problem.We also observed that Neo4j provides us with a directed property graph model. This gave us a lot of flexibility. Having solved the question What is the most expedient route between two points in a city?” we immediately asked a new one. (By the way, don’t you find that this is exactly what happens in the real world? Business requirements evolve). With a few tweaks to our model, we were able to re-route traffic in the event of a road closure.Today we’ll optimize our solution with the help of Neo4j Graph Data Science Library. This library, as the name suggests, provides a range of Graph Data Science algorithms. It’s maintained and officially supported by Neo4j, with participation from the community.We’ll be accessing the libraries using a feature of Neo4j called stored procedures. Stored procedures give us a way to call, from Cypher, routines that run on the graph. There’s a plethora of interesting open-source routines. It is also fun and easy to write your own. I will show you how to do that in a future post.Just one of those libraries above would in fact, be enough to perform optimization today. However we’ll be making good use of both in subsequent posts. They’re both pretty interesting too, so let’s read on.GETTING SET UPYou’ve got a number of options for getting Neo4j set up for this post. You can:Use Neo4j Sandbox Graph Data Science use case. It already has GDS installed, just remove the existing data set before you continue. This is the no download, no install route.Download and install Neo4j Desktop and use the Plugin helper to install GDS.Get set up on Neo4j using the server directly. If going down this route, you will need to manually install the GDS plugin. Make sure you’ve downloaded the right versions for the database you have installed. You will also need to grant access to the plugins in the neo4j.conf file.You can test out what procedures you’ve got available to you with the following command:call dbms.procedures()WELCOME BACK TO SAXEBURGIts good to have you back!You awaken after a fitful night’s sleep in some nondescript hotel in Pigalle. The driver wanted to take you to Red Light, but you you insisted on the more civilized North-east quarter.Nonetheless, it seems every neighbourhood in Saxeburg hoists their red lights come nightfall. In fact, one shone, like a tainted moonbeam, from across the street through your hotel-room window. Bereft of curtains, as it was. Barely after your eyes had closed it crept like tendrils through shuttered lids and sparked the technicolors of your mind. Lurid dreams followed. Such fantasies leave you nary rested, thus, the morning sun came as a shock, seemingly moments later. Truth be told, you wouldn’t have minded staying in your dream a little longer, and yet now daylight beckons.COFFEECoffee fixes everything. And the coffee is good, surprisingly good!Outside, eleven floors below, the world’s most unlikely urban eco-system hums, grinds and teems with life. The sidewalks are full. People spill onto the road. Traffic overflows onto the crumbling, tree-lined pavement. No space is wasted. Hawkers have set up what seems to be the world’s most popular street-food stall on a traffic-island. The smoke from their grills wafts and mixes with that belched by a packed ten peso passenger jeep. Said jeep hurtles past, then slams on the brakes, with a squeal.Meanwhile, at the demolition site next door, stray dogs are playing a game of chase, leaping and tumbling among piles of concrete debris. Atop one: a life-sized, smiling statue of Buddha in pock-marked golden plastic. Suddenly, two dogs and Buddha collide, then all three are tumbling down the rubble-side. Buddha’s face is alight, as though laughing, but the dogs don’t seem to notice. They too are absorbed in the present moment, in their game.STOP PEERING OUT THE WINDOWHey! Come on, finish your coffee. We’ve got work to do!Showered, and dressed for the day, you’re greeted out of your hotel-room door by dim, flickering lights, in a windowless elevator well. The floor is scuffed. An ancient lounge that sits on the opposite side of the elevator doors is flanked by two pot plants. Although plastic, they display drooping, thirsty leaves that seem to be pushing the limits of survival.You wonder at the special kind of neglect it must take to kill a plastic pot plant! Probably the kind that is comfortable to scrawl Out of Service” using what appears to be red lipstick on a piece scrap paper. Undoubtedly, the type that would then stick it to the elevator door with spent chewing gum.At least the air-con is still working. It is attacking the fecund heat, keeping it at bay with an acrid smelling air that is two parts squalid, and one part you-don’t-want-to-know.You reach the lobby after an eleven floor stair descent. Behind the counter, a beautiful human with remarkably androgynous features and a dashing red uniform greets you with a heartfelt Good Morning!”. That sing-song intonation. Damn, you love the Saxeburgian accent.You feel a little awkward though, when you ask How do you prefer to be addressed?”.There is no sign of offense taken in the answer. I prefer she”, she laughs warmly, and hands you a brochure (Moongirl and the Bullhorns Tour Dates) along with a map of Saxeburg.You’re not wearing your mixed-reality goggles, and so Actual paper!”, you think to yourself, How quaint!”ORIENTEERINGLet’s reorient ourselves and take a little squiz at our map:Saxeburg, c. 1986Sheesh, it seems this map is from 1986! But we’ve got all the information we need: What I’m seeing is that:There are neighbourhoods. We are in Pigalle.There are routes between each of them, neighbourhoods, that is.Each route has an associated travel time or cost.What we are dealing with here is a weighted graph.WHAT IS A WEIGHTED GRAPH?As we learned previously, graph structures have been considered since antiquity, with path-finding being a classical application thereof.Not long after Euler came up with the first known graph, folks started to develop variants applicable to different avenues of exploration. For example, previously we saw that a graph can be directed or undirected.A weighted graph, then, is a graph in which each edge is given a numerical value. These numbers are usually taken to be positive. The value that is attached to an edge is what gives the edge its weight. The weight can represent cost, distance, throughput or some other property.A weighted graph.PROPERTY GRAPH MODEL VS WEIGHTED GRAPHOnce again, we see that Neo4j’s property graph model is pretty versatile. Previously we explored how we can model graphs in which the relationships are bidirectional or undirected, even though, under the hood, all Neo4j relationship are directed. Do you remember?Neo4j can happily serve as a weighted graph model too. There are various algorithms that deal with weighted graphs, probably the most famous of which is Djikstra’s. That’s what we’ll use for routing today.DJIKSTRA’S ALGORITHMDijkstra came up with his algorithm in 1956, although it wasn’t published until three years later. At the time, he was trying to come up with something to show off the new ARMAC mainframes. Bear in mind that this was the dawn of the electronic computing age. One of the challenges was to find a problem and solution that people not familiar with computing would be able to understand. Thus he designed a solution for finding the most expedient route between two points in a road network.Perhaps it is a little ironic, then. Although he wanted the algorithm to be accessible for the layperson, budding computer scientists find it one of the more intimidating algorithms to grasp.Here’s what the man himself had to say.IN HIS OWN WORDSWhat is the shortest way to travel from Rotterdam to Groningen, in general: from given city to given city? It is the algorithm for the shortest path, which I designed in about twenty minutes. One morning I was shopping in Amsterdam with my young fiancée, and tired, we sat down on the café terrace to drink a cup of coffee and I was just thinking about whether I could do this, and I then designed the algorithm for the shortest path. As I said, it was a twenty-minute invention. In fact, it was published in ’59, three years late. The publication is still readable, it is, in fact, quite nice.”– Edsger Dijkstra, in an interview with Philip L. Frana, Communications of the ACM, 2001MY TAKE ON THATThere were only a handful of working programmers in the 1950s, however at least one of them was as enthusiastic about coffee as some of our modern comrades.Djikstra was a rock star developer. Nothing pleases a rock-star-dev more than his or her own mad skillz.LET’S BEGIN, THIS TIME FOR REALWe’re using the same graph as last time. Open your browser (either click ‘Open Browser’ on Neo4j Sandbox, ‘Open’ on Neo4j Desktop, or put localhost:7474 into your favorite browser )and run this statement.To recap where we left off:We expressed our requirements (find the fastest route) in Cypher. It was a handful of lines of code.We found candidate routes between two required destinations, trusting that Neo4j’s Cypher would choose the most appropriate path finding algorithm for us.Using aggregation functions and ordering, we reduced that down to the best route.This got the job done. When the requirements evolved we were able to update our Cypher to match too. Pretty handy.MOST EXPEDIENT ROUTEThere are graph models and corresponding algorithms for specific classes of problems. One of which is Djikstra’s Algorithm, as discussed. It is an optimal solution, for finding the most expedient route on a weighted graph. The algorithm has a time complexity of O(|E| + |V| log|V|) where V is the number of nodes and E is the number of edges (relationships).Let’s get down to applying Djikstra’s algorithm with the Graph Data Science Library. By the way, if you would like to read about how the algorithm works, there are some excellent tutorials on that topic. We’ll see you back here when you’re done.The brochure that Charm, from hotel reception, gave you this morning says that Moongirl and the Bullhorns are headlining at the Ruins tonight — an unplugged tribute to Doctor Cerulean. The concert starts at sunset. It is 5pm now. Let’s see if we can get you there on time:MATCH (a:Metro {name: Pigalle})MATCH (b:Metro {name: The Ruins})CALL gds.alpha.shortestPath.stream({   nodeProjection: Metro,   relationshipProjection: {     HAS_ROUTE: {       type: HAS_ROUTE,   properties: travelTime,   orientation: UNDIRECTED      }    },   startNode: a,   endNode: b,   relationshipWeightProperty: travelTime})YIELD nodeId, costWITH gds.util.asNode(nodeId) AS nodes, costRETURN nodes, cost AS journeyTimeTHE RESULTS:Notice how our query returned two columns. The first is a path, containing nodes and relationships, so we have a visualization available:We can travel to The Ruins via China Town, Intramuros, Uptown and Brooklyn.We returned weight, as journeyTime. Switching to the table tab and looking at our final stop, The Ruins, we can see that journeyTime is 29.5 minutes.MOST EXPEDIENT ROUTE IF THERE ARE CLOSURESLet’s solve the road closure problem. Previously, we entered all the Nodes we had to run Djikstra’s algorithm. This doesn’t account for road closures. We have to project a specific graph to be able to address these closures.It turns out, we’ve already done a graph projection when we ran the last query, except that we projected the whole graph. What we can do is project a specific part of the graph we want to run the algorithm on, and to do that we’re going to use Cypher to help us.First let’s ensure that each HAS_ROUTE relationship has a status property, and reset that status to ACTIVE.MATCH ()-[r:HAS_ROUTE]-() SET r.status = ACTIVENow, we’ll close the route between Pigalle and China Town:MATCH (n:Metro {name: Pigalle})-[r:HAS_ROUTE]-(m:Metro {name:China Town})SET r.status = CLOSEDFinally we can use Cypher in our GDS algorithm to account for open roads when using shortest path:MATCH (a:Metro {name: Pigalle})MATCH (b:Metro {name: The Ruins})CALL gds.alpha.shortestPath.stream({  nodeQuery: MATCH (n:Metro) RETURN id(n) as id,  relationshipQuery:MATCH (n:Metro)-[r:HAS_ROUTE]-(m:Metro) WHERE r.status <>  CLOSED  RETURN id(n) AS source, id(m) AS target, r.travelTime AS travelTime,  startNode: a,  endNode: b,  relationshipWeightProperty: travelTime})YIELD nodeId, costRETURN gds.util.asNode(nodeId).name, costTHE RESULTS:A road closure between Pigalle and China Town would increase the journey time to 35 minutes.CONCLUSIONToday we explored weighted graphs and Djikstra’s Algorithm with Neo4j. We discovered that Neo4j puts the power of graph algorithms at our fingertips. We also learned that we can project the property graph model to other graph models.Please stay tuned for the next installment. I hope that you enjoyed this one! If you did, why not tell your friends about the fun that can be had when #doctor-cerulean-liberates-your-data.Thanks a lot!;Jul 24, 2020;[]
https://medium.com/neo4j/election-2016-debate-three-on-twitter-4fc5723a3872;John SwainFollowOct 26, 2016·7 min readElection 2016 — Debate Three on Twitter.Overall conversation map showing users and the connections between them based on retweets.IntroductionIn a previous analysis, we looked at the difference in structure between Clinton and Trump supporters in the Twitter conversation. In particular, we examined how the most influential users were distributed and how this indicated that the mainstream media and establishment users were aligned with the Clinton side of the debate.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comA recent article published by The BBC, based on research by @pnhoward, indicated that there was a significant number of bots creating twitter traffic.Trumps Twitter debate lead was swelled by bots - BBC NewsMore than four times as many tweets were made by automated accounts in favour of Donald Trump around the first US…www.bbc.comDuring the third debate, we also collected 2.5 million tweets from 780k users and decided to analyse these phenomena in a little more detail.Out data set was slightly different from the set used by Professor Howard. We collected a wider range of tweets about the election and used machine learning community detection to identify the sides rather than relying on hashtags. Here is an article with some more detail about our methodology.Election 2016 — Debate OneStructure of conversation on Twittermedium.comIdentifying Influential UsersWe use two basic measures for influence within a particular conversation.1. Voice — within a given conversation (in this case debate three of the Presidential Election) the influence of a user is called their voice. This is measured using several graph algorithms which provides us with an overall rank score for voice.2. Authority — at Right Relevance we measure the influence of users on social media in over 50k Topics. The measure of a user’s influence within a given topic is called their Authority.We can illustrate these measures in conversation maps which show the connections between users representing communication (retweets between users) and the importance of the user by the size of each node.Twitter conversation map showing important users (size determined by page rank) and connections indicating communication between users (retweets)The first thing to notice about this map is that the number of users associated with Trump appears to be much bigger than the Clinton side. This would (prima facie) support the theory that there are a large number of bots tweeting pro-Trump messages. Inspection of the map would also indicate that there are relatively fewer important (indicated by size) users in the Trump side.Close up of the Clinton side of the map.Close up of the Trump side of the map.Ratios of Influencers to Non InfluencersIf we define influence in this network using the definition of Authority above, we can filter the two groups of users to show just those users with influence scores above a specified threshold and compare those with the number of users below the threshold (non influencers).If we set the score for Authority at 70 the ratios are as follows:This shows that there are almost double the number of Influencers (pro rata) on the Clinton side than on the Trump side.We are not making any judgement about in which Topics the user has Authority in terms of worthy or valuable Topics. We are just measuring the minimum Authority score a user has within any Topic. For example, in this context, a high score in a Topic like TCOT has the same value than a high score in Journalism. You can find a full list of Topics we found in this conversation in our Tableau dashboard.However, if we just look at those users with the highest Voice in the conversation (by removing all uses with a low Voice score) we can make a qualitative assessment of the types of users that are associated with each side by a visual inspection of the users on both sides.Conversation map showing only users with a high Voice score — the most influential users in the conversation.Looking at the visualisation it is clear that there is a much higher proportion of mainstream media and establishment users on the Clinton side. The Trump side has a much higher proportion of users who are specifically Trump supporting accounts.Right Relevance Insights usually focuses on these ‘important’ users within a conversation, however, for this post we are interested in the users identified as less ‘important’ but who generate a large volume of Tweets.Filtering BotsIf we use the Prof. Howard’s definition of a Bot (more than 50 Tweets in the day) we can observe the ratio of Bots to non Bots in the two sides of the conversation in our analysis.Note: our data set includes a total of 780k Twitter users. Before our initial analysis we filter out small users and those we identify as bots/spammers with a very high probability. This reduces the number of users to 550k. We use a combination of different techniques to identify these simple bots at this initial stage. We then reduce the size of the data set for visualisation to a much smaller 17k these are the maps shown in this post. The 17 contains just the most important users in the network by various measures including pagerank & betweeness centrality.What is interesting, therefore is that there are still a significant number of bots, by Prof. Howards’s definition which remain in our data set. We would characterise these as more ‘sophisticated’ bots.Contrast of conversation maps with bots included (left) and bots removed (right).The ratios of bots to all users are as follows.In the tweets we analysed there are 3x as many bots on the Trump side as the Clinton side.Therefore, our analysis also provides evidence in support of Professor Howard’s assertion that there are a much higher proportion of Bots supporting the Trump campaign than those supporting Clinton.ConclusionRight Relevance Insights is an application designed to find the valuable and relevant information in social media conversations. The noise generated by Bots is one of the elements that pollutes conversations and makes it hard to find relevant information.In this short example we have shown how the noise from Bots can be isolated. Once isolated it is possible to examine both parts of the conversation:1. Noise — examining the noise created by the Bots is useful for identifying malevolent agents who are attempting to exert influence in support of a specific cause.2. Non Noise — by removing the noise it is possible to identify who is actually having the most effective influence within the conversation.These are the Top Influencers within the conversation identified by Right Relevance Insights:Tables of top users by various measures of influence.Brexit and the Michael Buble EffectThe Brexit vote in the UK had a very similar conversation structure. There was a very clear concentration of major media and establishment users within the Remain side of the conversation. We identified this prior to the vote.We coined the phrase ‘The Michael Buble Effect’ to describe the way in which influence is important within a particular interest group.If you are Michael Buble, there are a finite number of people on earth who will buy your latest album. To maximise the revenue from album sales it is only important to enthuse these people — it is totally pointless spending any effort trying to convince people who are not in this group.Crucially it is unimportant how much you alienate the people who are not in your supporters group. No matter how much a person hates Michael Buble they cannot buy less than zero records, so their opinion is irrelevant. In fact the more that non supporters hate, the more determined the supporters become. This phenomenon also applies in a two horse race election — no one can express a negative opinion.In the Brexit debate it was clear that the Leave campaign were running a campaign aimed at enthusing a very narrow set of interests and completely alienating those with opposing views. The gamble, of course, is that there are enough people within the core supporting group to win the vote. A gamble which paid off in the Brexit vote but has left very bitter divisions within the UK as a result.Social media (including Twitter) is a useful reflection of society as a whole. However, it is only a reflection and contains very significant skews and biases.The key issue is that we don’t know the exact size of the people who will vote for Trump who can be energised by strong rhetoric. Brexit shows that this strategy can be successful and flies in the face of the conventional wisdom of appealing to floating voters with a reasoned argument based on persuasion.There is a common perception that Hillary Clinton is winning the election comfortably. The assertion that there are a large number of bots ‘supporting’ Donald Trump plays to this perception by suggesting that the noisy support for Trump is not real. Based on what we observed in the Brexit election where there is a larger ‘hidden’ support for one side over the other, we would advise some caution over thinking the election is already won by Hillary Clinton.Free download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Oct 26, 2016;[]
https://medium.com/neo4j/introducing-the-neo4j-stackoverflow-sandbox-69e3729d01f3;CristinaFollowNov 10, 2021·3 min readIntroducing the Neo4j Stack Overflow SandboxAnalyze Stack Overflow questions, answers, tags, and users with Neo4j APOC and Graph Data SciencePhoto by Lagos Techie on UnsplashNeeding no introduction, Stack Overflow has been a vital part of not just the Neo4j community, but of the global programming community, with millions of questions asked and answered. A long-time Neo4j example dataset, it is very exciting to see Stack Overflow in the Neo4j Sandbox collection.What Does That Mean For You?You can dive into the basics of social network analysis of a developer social network without having to download, install, and configure a Neo4j environment, or perform a cumbersome ETL process to use the data. Not only that, but after you open the Stack Overflow project in Neo4j Sandbox, you’ll have an interactive browser guide waiting for you that will help you get started.The Stack Overflow Graph GuideA sandbox lasts for three days but can be extended to a maximum of 10 days. As mentioned, once you open the Neo4j Browser, you’ll have the browser guide waiting for you to take you through the data model, a proposed additional import step (in case you need more data), and social network and tag similarity analysis using the optimized Node Similarity algorithm.The Stack Overflow DatasetBy walking through the browser guide, you’ll learn how to use the Cypher query language to explore the data and evaluate overall network information, use Load JSON to explore additional data, use APOC to create virtual graphs of the network, and explore the Graph Data Science algorithms, specifically the Node Similarity algorithm and the Jaccard Similarity algorithms, to compare tags.If you have some ideas on how to analyze the data that are not included in the guide, you can experiment using the built-in Neo4j Bloom tool to experiment and come up with new insights.Neo4j Bloom in ActionGet started with your Stack Overflow Sandbox directly.More on Neo4j and Stack OverflowEvery developer has a tab open to Stack OverflowBuild a StackOverflow GraphQL API & Demo App in 10 MinutesUsing the GRANDstack starter project in 5 stepsmedium.comTagOverflow - Correlating Tags in Stackoverflow (Neo4j Online Meetup #52)(We had an issue with Chrome screen sharing so skip forward to 5:50 where we properly get started …neo4j.comExploring StackOverflow data with Michael Hunger - Twitch streamIn this Twitch stream, Michael explores the developers favourite Q&A hotspot - StackOverflow, and shows you how to…neo4j.comAdditional ResourcesNode Similarity - Neo4j Graph Data ScienceThis section describes the Node Similarity algorithm in the Neo4j Graph Data Science library. The algorithm is based on…neo4j.comLoad JSON - APOC DocumentationWeb APIs are a huge opportunity to access and integrate data from any sources with your graph. Most of them provide the…neo4j.comJaccard Similarity - Neo4j Graph Data ScienceThis section describes the Jaccard Similarity algorithm in the Neo4j Graph Data Science library. Jaccard Similarity…neo4j.comVirtual Graph - APOC DocumentationThe procedure apoc.graph.fromDocument transforms a JSON into a graph structure. It takes two arguments: json, type…neo4j.com;Nov 10, 2021;[]
https://medium.com/neo4j/enhancing-word-embedding-with-graph-neural-networks-c26d8e54fe4a;Tomaz BratanicFollowMar 21·15 min readEnhancing Word Embedding With Graph Neural NetworksUse GraphSAGE algorithm in combination with OpenAI word embeddings to increase downstream document classification accuracyNatural Language Processing (NLP) has seen rapid advancements in recent years. One important aspect of this progress has been the use of embeddings, which are numerical representations of words or phrases that capture their meaning and relationships to other words in a language. Embeddings can be used in a wide range of NLP tasks, such as document classification, machine translation, sentiment analysis, and named entity recognition. Furthermore, with the availability of large pre-trained language models like GPT-3, embeddings have become even more critical for enabling transfer learning across a range of language tasks and domains. As such, embeddings are closely tied to the rapid advancements in NLP.On the other hand, recent advancements in graphs and graph neural networks have led to improved performance on a wide range of tasks, including image recognition, drug discovery, and recommender systems. In particular, graph neural networks have shown great promise in learning representations of graph-structured data, where the relationships between data points provide a signal that improves the accuracy of downstream machine-learning tasks.In this blog post, you’ll discover how to harness the power of graph neural networks to capture and encode the relationships between data points and enhance document classification accuracy. Specifically, you will train two models to predict a medium article’s tags.Example tags from my previous blog post. Image by the author.Most medium articles have relevant tags assigned to them by the author for easier discoverability and search performance. Additionally, you can think of these tags as a categorization of articles. Each article can have up to 5 tags or categories it belongs to, as shown in the above image. Therefore you will train two classification models to perform a multi-label classification, where each article can have one or more tags assigned to them.Multi-label classification of medium article tags. Image by the author.The first classification model will use OpenAI’s latest embeddings (text-embedding-ada-002) of the article’s title and subtitle as the input features. This model will provide a baseline accuracy you will try to improve using a graph neural network algorithm called GraphSAGE. Interestingly, word embeddings can be and will be used in this example as input to GraphSAGE. During training, the GraphSAGE algorithm then leverages these word embeddings to iteratively aggregate information from neighboring nodes, resulting in powerful node-level representations that can improve the accuracy of downstream machine-learning tasks like document classification.In short, this blog post explores the use of graph neural networks to improve word embeddings by taking into account the relationships between data points. When the relationships between data points are relevant and predictive, graph neural networks can learn more meaningful and accurate representations of text data and, consequently, increase the accuracy of downstream machine learning models.Medium DatasetThere are a couple of medium article datasets available on Kaggle. However, none of them contain any relationships between articles. What type of relationships between articles would even be predictive for predicting their tags? Medium has added the ability for users to create lists that can help them bookmark and curate the content they have or intend to read.Curated lists of articles by users on medium. Image by the author.This image presents an example where the user created four lists of articles based on their topics. For example, most articles were grouped under the Data Science list, while other articles were added to the Communication, Maths, and Design lists. The idea is that if two articles are in the same list, they are somewhat more similar than if they don’t have any common lists. You can think of medium lists as human-annotated relationships between articles that can help you find and potentially recommend similar articles.There is one exception to this assumption. Some users create vast reading lists that contain all sorts of articles.An example of a reading list that contains 2551 articles. Image by the author.Interestingly, most of these lists with a vast amount of articles have an identical Reading list title. So it has to be some sort of default value by Medium or something, as I have noticed the reading list title with a couple of users.Unfortunately, there are no publicly available datasets with information about the Medium articles as well as the user lists they belong to. Therefore, I had to spend an afternoon parsing the data. I retrieved information about 55 thousand medium articles from 4000 user lists.Preparing Neo4j EnvironmentThe graph construction and GraphSAGE training will be executed in Neo4j. I like Neo4j as it offers a nicely designed graph query language called Cypher as well as a Graph Data Science plugin that contains more than 50 graph algorithms that cover most of graph analytics workflow. Therefore, there is no need to use multiple tools to create and analyze the graph.The graph schema of the Medium dataset if the following:Graph schema. Image by the author.The schema revolves around Medium articles. We know the url, title, and date of the article. Additionally, I have calculated the OpenAI’s embeddings using the text-embedding-ada-002 model based on the article title and subtitle and stored them as openaiEmbedding property. Additionally, we know who wrote the article, which user’s lists it belongs to, and its tags.I have prepared two options for you to import the medium dataset into Neo4j database. You can execute the following Jupyter notebook and import the dataset from Python. This option also works with Neo4j Sandbox environment (use blank graph data science project).blogs/Import.ipynb at master · tomasonjo/blogsYou cant perform that action at this time. You signed in with another tab or window. You signed out in another tab or…github.comThe other option is to restore the Neo4j database dump I have prepared.medium-dump-v55.dumpEdit descriptiondrive.google.comThe dump has been created with Neo4j version 5.5.0, so make sure to use that version or later. The easiest way to restore the database dump is to use the Neo4j Desktop environment. Additionally, you will need to install the APOC and GDS libraries if you are using the Neo4j Desktop environment.After the database import is finished, you can run the following Cypher statement in Neo4j Browser to verify that the import was successful.MATCH p=(n:Author)-[:WROTE]->(d)-[:IN_LIST]->(), p1=(d)-[:HAS_TAG]->()WHERE n.name =  Tomaz Bratanic RETURN p,p1 LIMIT 25The result will contain a couple of articles I have written along with their lists and tags.A small subset of the Medium graph. Image by the author.Now it is time for the practical part of this blog post. All the analysis code is available as a Jupyter Notebook.blogs/Classification with GraphSAGE.ipynb at master · tomasonjo/blogsYou cant perform that action at this time. You signed in with another tab or window. You signed out in another tab or…github.comExploratory AnalysisWe will be using the Graph Data Science Python Client to interface with Neo4j and its Graph Data Science plugin. It is an excellent addition to the Neo4j ecosystem, allowing us to execute graph algorithms using pure Python code. Check out my introductory blog post for more information.First, we will evaluate the distribution of tags per medium article.dist_df = gds.run_cypher(   MATCH (a:Article)RETURN count{(a)-[:HAS_TAG]->()} AS count   )sns.displot(dist_df[count], height=6, aspect=1.5)Distribution of tags per article. Image by the author.Around 50% of articles have no tags present. There are two reasons for that. Either the author did not use any, or the scrapping process failed to retrieve them for various reasons, like medium publications having custom HTML structures. However, it is not a big deal as we still have more than 25 thousand articles with their tags present, allowing us to train and evaluate the multi-label classification model of article tags. Most authors choose to use five tags per article, which is also the upper limit that the Medium platform allows.Next, we will evaluate if any articles are not part of any user lists.gds.run_cypher(       MATCH (a:Article)RETURN exists {(a)-[:IN_LIST]-()} AS in_list,       count(*) AS countORDER BY count DESC   )The results show that all articles belong to at least one list. Identifying isolated nodes (nodes with no connection) is a critical part of any graph analytics workflow, as we have to pay special attention to them while calculating node embeddings. Luckily, this dataset contains no isolated nodes, so we don’t have to worry about that.In the last part of the exploratory analysis, we will examine the most frequent tags. Here, we will construct a word cloud of tags present in at least 100 articles.tags = gds.run_cypher(       MATCH (t:Tag)WITH t, count {(t)<--()} AS sizeWHERE size > 100RETURN t.name AS tag, sizeORDER BY size DESC   )d = {}for i, row in tags.iterrows():    d[row[ tag ]] = row[ size ]wordcloud = WordCloud(    background_color= white , colormap= tab20c , min_font_size=1).generate_from_frequencies(d)plt.figure()plt.imshow(wordcloud)plt.axis( off )plt.show()Word cloud of the most frequent tags. Image by the author.The most frequent tags are data science, artificial intelligence, programming, and machine learning.Multi-Label ClassificationAs mentioned, we will train a multi-label classification model to predict tags of a Medium article. Therefore, we will use the scikit-multilearn library to help with data splitting and model training.I noticed that the dataset split with scikit-multilearn library does not provide a random seed parameter, and therefore, the dataset split is not deterministic. For a proper comparison of the baseline model trained on OpenAI’s word embedding and a model based on GraphSAGE embeddings, we will perform a single dataset split so that both model versions use the same training and test examples. Otherwise, there could be some differences between the models’ accuracy based solely on the dataset split.The word embeddings are already stored in the graph, so we only need to calculate the node embeddings using the GraphSAGE algorithm before we can train the classification models.GraphSAGEGraphSAGE is a convolutional graph neural network algorithm. The key idea behind the algorithm is that we learn a function that generates node embeddings by sampling and aggregating feature information from a node’s local neighborhood. As the GraphSAGE algorithm learns a function that can induce the embedding of a node, it can also be used to induce embeddings of a new node that wasn’t observed during the training phase. This is called inductive learning.Neighborhood exploration and information sharing in GraphSAGE. [1]If you want to learn more about the training process and the math behind the GraphSAGE algorithm, I suggest you take a look at the An Intuitive Explanation of GraphSAGE blog post by Rıza Özçelik or the official GraphSAGE site.Monopartite Projection With Node Similarity AlgorithmGraphSAGE supports graphs with multiple types of nodes, where each type of node has different features representing it. In our example, we have Article and List nodes. However, I have decided to simplify the workflow by performing a monopartite projection.Monopartite projection of articles. There is a relationship between articles if they share a list. Image by the author.Monopartite projection is a frequent step in graph analysis. The idea is to take a bipartite graph (graph with two node types) and output a monopartite graph (graph with only one node type). In this specific example, we can create a relationship between two articles if they are part of the same list. Additionally, the number of shared lists or a normalized value like the Jaccard coefficient can be stored as a relationship property.Since the monopartite projection is a common step in graph analysis, the Neo4j Graph Data Science library offers a Node Similarity algorithm to help us with it.First, we need to project an in-memory graph. We will include the Article and List nodes along with the IN_LIST relationships. Additionally, we will include the openaiEmbedding node properties.G, metadata = gds.graph.project(     articles ,     [ Article ,  List ],     IN_LIST ,     nodeProperties=[ openaiEmbedding ])Now we can perform the monopartite projection using the Node Similarity algorithm. One thing to note is that the default value of the topK parameter is 10, meaning that each node will be connected to only its ten most similar nodes. However, in this example, we want to create a relationship between all articles in the user list. Therefore, we will use a relatively high value of the topK parameter.gds.nodeSimilarity.mutate(    G, topK=2000, mutateProperty= score , mutateRelationshipType= SIMILAR )We have used the mutate mode of the algorithm which stores the results back to the in-memory projected graph. The SIMILAR relationship has been created between all pairs of articles that share at least a single user list.Training the GraphSAGE ModelThe GraphSAGE algorithm is inductive, meaning that it can be used to generate embeddings for nodes that were previously unseen during training. The inductive nature allows us to train the GraphSAGE model only on a subset of the graph and then generate the embeddings for all the nodes. Training the GraphSAGE model only on a subset of the graph saves us time and compute power, which is useful when dealing with large graphs. While our graph is not that large, we can use this example to demonstrate how to sample the training subset of the graph efficiently.Random walk with restarts samplingThe idea behind random walk with restarts sampling is quite simple. The algorithm takes random walks from a set of predefined start nodes. At each step of the walk, there is a probability that the current random walk stops and a new one starts from the set of start nodes. The user can define the start nodes. If no start nodes are defined, the algorithm chooses them uniformly at random.I thought it would be interesting to show you an example of choosing a start node manually. So we will begin by executing the Weakly Connected Components algorithm to evaluate how connected the graph of articles is. A weakly connected component is a set of nodes within the graph where a path exists between all nodes in the set if the direction of relationships is ignored.A weakly connected component can be considered an island that nodes from other components cannot reach.While the algorithm identifies connected sets of nodes, its output can help you evaluate how disconnected the overall graph is.wcc = gds.wcc.stream(G)wcc_grouped = (    wcc.groupby( componentId )    .size()    .to_frame( componentSize )    .reset_index()    .sort_values( componentSize , ascending=False)    .reset_index())print(wcc_grouped)There is a total of 604 connected components in our graph. The largest component contains 98% of all nodes, while the other ones are smaller, with many containing only two nodes. If a component contains only two nodes, it means that we have a medium user list that has only two articles in it, and those two articles are not part of any other lists.We executed the Weakly Connected Component algorithm to identify a node that belongs to a large connected component and, therefore, can be used as a starting node of the sampling algorithm. For example, if we used a node with only one neighbor, the sampling algorithm couldn’t perform longer walks to subsample the graph efficiently.Fortunately, the sampling algorithm is implemented to automatically expand the set of start nodes if the random walks do not visit any new nodes. However, as we have used a start node from the largest connected component with 98% of all nodes, the algorithm won’t have to expand the set of start nodes automatically.largest_component = wcc_grouped[ componentId ][0]start_node = wcc[wcc[ componentId ] == largest_component][ nodeId ][0]trainG, metadata = gds.alpha.graph.sample.rwr(     trainGraph ,    G,    samplingRatio=0.20,    startNodes=[int(start_node)],    nodeLabels=[ Article ],    relationshipTypes=[ SIMILAR ],)The sampling ratio parameter defines the fraction of nodes in the original graph to be sampled. For example, when using the value 0.20 for the sampling ratio, the sampled subgraph will be 20% the size of the original graph. Additionally, we need to define that the random walks can only visit Article nodes through SIMILAR relationships by using the nodeLabels and relationshipTypes parameters.GraphSAGE TrainingFinally, we can go ahead and train the GraphSAGE model on the sampled subgraph.gds.beta.graphSage.train(    trainG,    modelName= articleModel ,    embeddingDimension=256,    sampleSizes=[10, 10],    searchDepth=15,    epochs=20,    learningRate=0.0001,    activationFunction= RELU ,    aggregator= MEAN ,    featureProperties=[ openaiEmbedding ],    batchSize=10,)The GraphSAGE algorithm will use the openaiEmbedding node property as input features. The GraphSAGE embeddings will have a dimension of 256 (vector size). While I have played around with hyper-parameter optimization for this blog, I have noticed that the learning rate and activation function are the most impactful parameters.Generate EmbeddingsAfter the GraphSAGE model has been trained, we can use it to calculate the node embeddings for all the Article nodes in the original larger projected graph and consider only the SIMILAR relationships.gds.beta.graphSage.write(    G,    modelName= articleModel ,    nodeLabels=[ Article ],    writeProperty= graphSAGE ,    relationshipTypes=[ SIMILAR ],)This time, we used the write mode to store the GraphSAGE embeddings as node properties in the database.Classification modelWe have prepared both the OpenAI and GraphSAGE embeddings. The only thing left is to train the models and compare their performance.First, we will label the article tags we want to predict. I arbitrarily decided to only include tags that are present in at least 100 articles. The target tags will be labeled with a secondary Target label.gds.run_cypher(       MATCH (t:Tag)WHERE count{(t)<--()} > 100SET t:TargetRETURN count(*) AS count   )We have labeled 161 tags we want to predict. Remember, the word cloud visualization above took the same 161 tags and visualized them according to their frequencies.As we will use the scikit-multilearn library, we need to export the relevant information from Neo4j.data = gds.run_cypher(       MATCH (a:Article)-[:HAS_TAG]->(tag:Target)RETURN a.url AS article,        a.openaiEmbedding AS openai,        a.graphSAGE AS graphSAGE,        collect(tag.name) AS tags   )Next, we need to construct a binary matrix that indicates the presence of tags for a given article. Essentially, you can think of it as one-hot-encoding of tags per article. So, we can utilize the MultiLabelBinarizer procedure to achieve this.mlb = MultiLabelBinarizer()tags_mlb = mlb.fit_transform(data[ tags ])data[ target ] = list(tags_mlb)The scikit-multilearn library offers an improved dataset split for multi-label prediction tasks. However, it does not allow a deterministic approach with a random seed parameter. Therefore, we will perform the dataset split only once for both the word and GraphSAGE embeddings and then train the two models accordingly.The following function takes in a data frame and the columns that should be separately used as input features to a multi-label classification model and returns the best-performing model while printing the weighted macro and weighted precisions. Here, we use the LabelPowerset approach to multi-label classification.def train_and_evaluate(df, input_columns):    max_weighted_precision = 0    best_input =       # Single split data    X = data[input_columns].values    y = np.array(data[ target ].to_list())    x_train_all, y_train, x_test_all, y_test = iterative_train_test_split(        X, y, test_size=0.2    )    # Train a model for each input option    for i, input_column in enumerate(input_columns):        print(f Training a model based on {input_column} column )        x_train = np.array([x[i] for x in x_train_all])        x_test = np.array([x[i] for x in x_test_all])        # train        classifier = LabelPowerset(LogisticRegression())        classifier.fit(x_train, y_train)        # predict        predictions = classifier.predict(x_test)        print( Test accuracy is {} .format(accuracy_score(y_test, predictions)))        print(             Macro Precision: {:.2f} .format(                get_macro_precision(mlb.classes_, y_test, predictions)            )        )        weighted_precision = get_weighted_precision(mlb.classes_, y_test, predictions)        print( Weighted Precision: {:.2f} .format(weighted_precision))        if weighted_precision > max_weighted_precision:            max_weighted_precision = weighted_precision            best_classifier = classifier            best_input = input_column    return best_classifier, best_inputWith everything prepared, we can go ahead and train the models based on word and graphSAGE embeddings and compare their performance.p.s. If you are using Google Colab, you might run into OOM problems using the openai embeddingsclassifier, best_input = train_and_evaluate(data, [ openai ,  graphSAGE ])The results are the following:Training a model based on openai columnTest accuracy is 0.055443548387096774Macro Precision: 0.20Weighted Precision: 0.36Training a model based on graphSAGE columnTest accuracy is 0.05584677419354839Macro Precision: 0.30Weighted Precision: 0.41Although the embeddings of the title and subtitle provide some information about their tags, they may not be the most efficient. This could be due to clickbait-style titles that prioritize grabbing attention over accurately describing the content. Furthermore, authors may have different preferences for tagging identical content with varying labels. Despite these challenges, our model predicts 161 labels, many of which have few examples, yielding acceptable results. To further improve accuracy, we can embed the entire article text and evaluate its performance.Interestingly, using GraphSAGE embeddings enhances classification precision by considering the relationships between articles. Our model’s macro precision improves by ten percentage points, while the weighted precision improves by five. These outcomes demonstrate that GraphSAGE embeddings help identify infrequent tags more effectively. Unlike standard word embedding models, graph neural networks enable us to encode additional relationships between data points, thereby enhancing downstream machine learning models. We have also performed a dimensionality reduction from 1536 to 256 while increasing the performance, which is a great outcome.Test PredictionsThere are almost 50% of articles without any tags in our database. We can test the model on several and manually evaluate the results.example = gds.run_cypher(       MATCH (a:Article)WHERE NOT EXISTS {(a)-[:HAS_TAG]->()}RETURN a.title AS title,       a.openaiEmbedding AS openai,       a.graphSAGE AS graphSAGELIMIT 15   )tags_predicted = classifier.predict(np.array(example[best_input].to_list()))example[ tags ] = [list(mlb.inverse_transform(x)[0]) for x in tags_predicted]example[[ title ,  tags ]]ResultsInterestingly, the model mostly assigns one or two labels per article, when most real-world articles have five tags. This is probably one cause for the values of precision scores. Other than that, the results look promising judging by this small sample.SummaryTraditional word embedding models like word2vec focus on encoding the co-occurrence statistics of words. However, they entirely ignore any other relationships that can be found between data points. For instance, we had users annotate similar articles by placing them in various reading lists. Luckily, graph neural networks offer a bridge between traditional word embeddings and graph embeddings as they allow us to build on top of word embeddings and encode additional information derived from relationships between data points. Therefore, the graph neural networks do not have to start from scratch but can be used to enhance state-of-the-art word or document embeddings.References[1] Hamilton, Will, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.” Advances in Neural Information Processing Systems. 2017.;Mar 21, 2023;[]
https://medium.com/neo4j/welcome-sdn-%EF%B8%8Frx-22c8fe6cd955;Gerrit MeierFollowAug 28, 2019·7 min readWelcome SDN⚡️RXPlease note that Spring Data Neo4j RX replaces Spring Data Neo4j in the future. More information can be found here.If you have followed the development of Neo4j you may have noticed that there is a milestone release of the upcoming 4.0 version. Besides all the other great features, the post mentions SDN/RX, the experimental new version of Spring Data Neo4j.SDN/RX will be completely based on the Spring Framework and utilizing the support of the Spring Data commons project. It will not have an intermediate mapping layer like Neo4j-OGM between Spring Data and the database.Introducing SDN⚡️RXWhen we started to think about a new implementation of Spring Data Neo4j, we wanted to use the approach that Spring Data JDBC took as well:Create a simple, understandable library and be opinionated in the way it should get used.Knowing that reactive capability was coming to Neo4j, we started to base our approach around it. Adding full immutable object support to our must-have list gave us the constraints to get a picture in our heads how the library should work. Although we talk about reactive and the project’s name has a RX suffix, SDN/️RX supports everything it does for reactive also for an imperative synchronous programming model.Photo by Ezekiel Elin on UnsplashGetting startedTo try out all the feature SDN/RX has right now, the best choice is to start a Neo4j 4.0 milestone release instance. If you have not yet downloaded it, you can get it from the Neo4j download page.The current version 1.0.0-beta01 is available in Maven central under following coordinates:<dependency>    <groupId>org.neo4j.springframework.data</groupId>    <artifactId>spring-data-neo4j-rx</artifactId>    <version>1.0.0-beta01</version></dependency>The easiest way to get started is to create a Spring Boot application with the Spring Boot Starter. Choose the latest available milestone release of the 2.2.0 series as the base of the application. From our point of view it is the most common way to get started. The following parts assume that you created a blank Spring Boot 2.2.0 application.Spring Boot starterIf you set up your application with Spring Boot, you can also benefit from a second project we created for SDN/RX: the SDN/RX Spring Boot starter.Instead of defining a direct dependency to SDN/RX, you can define the dependency to the starter which also brings you support for the application configuration.<dependency>    <groupId>org.neo4j.springframework.data</groupId>    <artifactId>spring-data-neo4j-rx-spring-boot-starter     </artifactId>    <version>1.0.0-beta01</version></dependency>The starter will also pull in the dependencies for the official Spring Boot auto-configuration for the Neo4j Java Driver that lets you define the connection parameters etc. for the driver.You can also use this starter as a stand-alone, driver-only solution in combination with your Spring Boot projects where an object mapping library is not needed.Having the starter dependency in your application, you can now add the mandatory properties to the existing application.properties in your resources folder.org.neo4j.driver.uri=neo4j://localhost:7687org.neo4j.driver.authentication.username=neo4jorg.neo4j.driver.authentication.password=secretAs you can see the namespaces for the properties are driver centric and not related to SDN/RX or Spring Data in general. The setup steps are now done and we can focus on modeling our domain.Data modelingThe example project we will create is based on the Neo4j’s favorite domain: Movies. We are so into movies that you can even create a data set to play with right through the Neo4j Browser interface. Simply type :play movies and follow along the guide to the second step where you will get provided with the statements to create the graph.The domain we are looking at is pretty minimal: We have a Movie and a Person entity. Let’s start with the Person entity.@Node // Ipublic class Person {    @Id // II    private String name    private int born    public Person(String name, int born) {	this.name = name	this.born = born    }    // skipping getter}As you can see the definition of the entity class is pretty straight forward:I: We define it as a for mapping eligible class by annotating it with @Node.II: Additionally we provide the information which is the property that represents the identifier with @Id and if it should get generated, in this case we define the name of the person (actor) as our business identifier. If we would decide to use the internal id mechanism of Neo4j, we could define@GeneratedValue on a Long typed property.Of course it is possible to define an own IdGenerator when using the @GeneratedValue annotation.Next up is the Movie entity.@Nodepublic class Movie {    @Id    private final String title    @Property( tagline ) // I    private final String description    @Relationship(type =  ACTED_IN , direction = INCOMING) // II    private Set<Person> actors    @Relationship(type =  DIRECTED , direction = INCOMING)    private Set<Person> directors    public Movie(String title, String description) {        this.title = title        this.description = description    }    // skipping getter}This looks pretty much the same as the other entity but has additional declarations of the relationship to Person entities. I: If a property in our entity class does not represent the same property in the graph, we can tell SDN/RX to respect this during the mapping phases by using @Property.II: All relationships need to be defined by using the @Relationship annotation. They can either point to an entity class or a List or Set type of other entity classes. As you can see, it is possible to declare different relationship types for the same entity types within one class.In this example the relationship are defined as INCOMING to match the graph model. The default direction of the annotation would be OUTGOING.Of course we now want to work with the domain we just created. In this post we focus on using of Spring Data’s repositories. There is another way to interact with the database in SDN/RX but this will get covered in one of our next blog posts about SDN/RX.If you are familiar with Spring Data in general or Spring Data Neo4j in particular, the repository definition will look the same as expected. To create a repository for our Person entity, we just need to declare an interface named PersonRepository and extend the ReactiveNeo4jRepository with the entity-type and primary-key type as generic parameters.public interface PersonRepository                  extends ReactiveNeo4jRepository<Person, String> {}This repository offers a lot of built-in CRUD methods that cover all of the basic functionality you need to interact with the database.In case you want to use imperative types (in which the repository returns List and Optional instead of Flux and Mono, you must extend from Neo4jRepository instead of ReactiveNeo4jRepository.public interface PersonRepository                  extends Neo4jRepository<Person, String> {}Now we are mostly done with the example but still need to interact with the repository. To keep things simple here, we create a test class annotated with @SpringBootTest to benefit from Spring Boot’s test support that will take care of the bean dependency management beside other things.@SpringBootTestclass DemoApplicationIT {    @Autowired    private PersonRepository repository    @Test    void loadAllPeopleFromGraph() {	int expectedPersonCount = 133	        StepVerifier.create(repository.findAll())            .expectNextCount(expectedPersonCount)            .verifyComplete()            }}Here we use one of the many data access methods findAll that the repository infrastructure provides out of the box. The test uses Project Reactor’s test dependency. Because of the nature of the reactive programming model we need a subscriber, in this case this is realized by the StepVerifier that waits for the signals (data) and can act/verify on arrival.A test on the imperative version of that repository would likely use AssertJ assertThat(repository.findAll()).hasSize(expectedPersonCount) or similar.While both ReactiveNeo4jRepositoryand Neo4jRepository offer the complete CRUD functionality for a domain type, we might need sometimes more data access options than those methods offer. To achieve this you have two choices:Use derived finder methodsDefine your own custom queryDerived finder methodsYou can easily define you own methods within the repository interface by following a structured syntax. e.g. defining a method like Mono<Person> findOneByName(String name) orOptional<Person> findOneByName(String name) depending whether you used the reactive or the imperative version. This will create the query you need under the hood and it will exactly do what you expect: search and return the Person node from the graph with the given name, roughlyMATCH (p:Person) WHERE p.name = $name RETURN pA test can prove this functionality and also shows the right mapping of the previously defined relationships. This would be the test for the imperative version:@Testvoid findPersonByName() {    Optional<Person> person = repository.findOneByName( Tom Hanks )    assertThat(person)        .map(PersonEntity::getBorn)        .isPresent().hasValue(1956)}For the reactive part this test is a little bit more complex:@Testvoid findPersonByName() {   StepVerifier.create(repository.findByName( Tom Hanks ))       .assertNext(personEntity -> {            assertThat(personEntity.getBorn()).isEqualTo(1956)       })       .verifyComplete()}These are of course just simple examples of what you can do with derived finder methods. There are plenty of features we support like find in ranges, concatenated conditions, etc. Those will all be listed in the upcoming documentation.Custom queriesWhen derived finder methods cannot express what you want to achieve, it is a good idea to make use of custom queries. To do this you just need to define a method and annotate it with @Query.@Query( MATCH (p:Person) WHERE (p)-[:ACTED_IN]->() AND (p)-[:DIRECTED]->() RETURN p )Flux<Person> getPeopleWhoActedAndDirected()Also this functionality can be proven working in a test case.@Testvoid findsPeopleWhoActedAndDirected() {    int expectedActorAndDirectorCount = 5    StepVerifier.create(repository.getPersonsWhoActAndDirect())       .expectNextCount(expectedActorAndDirectorCount)       .verifyComplete()}Please keep in mind if we do not return the related entities in the custom queries, the relationship-fields in the entities won’t get populated.Continue exploringIf you want to peek in the sources of SDN/RX, you can do this on Github. There are also complete examples of the topics touched in this post.The reactive example can be used with VSCode remote containers. To learn more about this, read and watch the introduction from Michael Simons’ blog post.(near) Future plansA lot of work besides code will be the documentation and — this is very important for us — a migration path document.Of course we would love your feedback on SDN/RX, so please head over to our Neo4j community site if you have questions or create issues on the repository for things you ran into.Although we will keep supporting Spring Data Neo4j and Neo4j-OGM for a long time along with SDN/RX, we would be happy to bring as many of you over to our (brave) new world.;Aug 28, 2019;[]
https://medium.com/neo4j/league-of-legends-with-neo4j-champions-diversity-in-worlds-2019-16999b21c457;Jimmy CrequerFollowOct 29, 2019·5 min readLeague of Legends with Neo4j : champions diversity in Worlds 2019League of Legends biggest event Worlds 2019” is being held right now in Europe. As we have reached the semi-finals, I wanted to do a small breakdown of the champions played so far.In League of Legends, players assume the role of an unseen summoner” that controls a champion” with unique abilities and battle against a team of other players. The goal is usually to destroy the opposing team’s Nexus”, a structure that lies at the heart of a base protected by defensive structures. Each League of Legends match is discrete, with all champions starting off fairly weak but increases in strength by accumulating items and experience over the course of the game. (Wikipedia)This article is part of a series around League of Legends and Neo4j, if you are interested you can also check my previous article about European casters.The GraphI used data from the groups stage and the quarterfinals to build the graph. You can find the full CSV data below but here is what a row looks like.G2 Esports,DAMWON Gaming,Akali,Qiyana,Renekton,Alistar,Leona,Pantheon,Syndra,Rakan,Elise,LeBlanc,Ryze,Gragas,Yasuo,Xayah,Nautilus,Irelia,Taliyah,Kayle,KaiSa,OrnnThe first 2columns are the 2 teams that faced each other. In the example row, G2 Esports and DAMWON Gaming.The next 5 columns are the champions banned by the first team. Here, G2 Esports banned Akali, Qiyana, Renekton, Alistar and Leona.The following 5 columns are the champions banned by the second team. DAMWON Gaming banned Pantheon, Syndra, Rakan, Elise and LeBlanc.The next 5 columns are the champions chosen (picked) by the first team, ordered by role. Here, G2 Esports picked Ryze (Top Lane), Gragas (Jungle), Yasuo (Mid Lane), Xayah (Bot Lane) and Nautilus (Support).The last 5 columns are the champions chosen by the second team. In the example, DAMWON Gaming picked Irelia (Top Lane), Taliyah (Jungle),Kayle (Mid Lane), Kai’Sa (Bot Lane) and Ornn (Support).This is how our DB schema looks like. As you can see, it is pretty straight-forward.DB Schema for this articleThe dataset is available here, while the import instructions can be found there.Champions DiversityAn indicator of how balanced is the game is the champions diversity in pro play. We can easily check the number of unique champions played so far throughout the tournament with the following Cypher query.MATCH (c:Champion)RETURN COUNT(c)So far 91 unique champions have been used (picked or banned), among the 145 available. This is quite good if we compare to the precedent editions : 76 in 2018 and 2017. Next, let’s look at the popular picks and bans for this year.MATCH (champ:Champion)<-[:PICKED|:BANNED]-()WITH champ, COUNT(*) AS cntRETURN champ.name AS champion, (100 * cnt / 66) +  %  AS presenceORDER BY cnt DESCLIMIT 10(There were 66 games played so far, that’s why I used 66 to get the presence percentage)Most popular championsPantheon is the only champion to be present every game (banned almost every time). Next come Qiyana, Akali and Renekton followed by Xayah and Kai’Sa. Let’s look a bit closer at the champions diversity for each lane to see if it can explain this result.MATCH ()-[r:PICKED]->(champ:Champion)WITH r.role AS role, collect(DISTINCT champ.name) AS uniqueChampsRETURN role, size(uniqueChamps) AS numberOfUniqueChamps, uniqueChampsORDER BY size(uniqueChamps) DESCChampions diversity per roleLooking at these results, we can infer that the current meta (trend) is around a stable Jungle and Bot Lane/Support duo, enabling diversity and counter picks in the Top and Mid Lanes, where it is important to get an advantage early.You may have noticed that some champions (Ryze, Qiyana, …) appear in multiple roles. When drafting a composition, champions flexibility (the ability to be played at multiple roles) is crucial to hide your strategy until the end or to avoid bad lane matchups. Those champions are highly regarded and can sometimes decide the outcome of the game when properly used. Let’s look at which champions are flex picks”.MATCH ()-[r:PICKED]->(champ:Champion)WITH champ, collect(DISTINCT r.role) AS rolesWHERE size(roles) > 1RETURN champ.name AS champion, rolesORDER BY size(roles) DESCChampions flexibility8 champions have been played into 3 different positions, and I think this is what makes this year’s Worlds very interesting because of all strategies that it allows. It also requires players to go out of their comfort zone and learn to play new champions at a competitive level.Towards the semi-finalsLet’s look at the upcoming matchups. We can use Neo4j to visualize the relations between teams and the champions they played, enabling us to foresee which champions might be contested during the upcoming games.Invictus Gaming vs FunPlus PhoenixMATCH p = (t:Team)--(c:Champion)WHERE t.name IN [ Invictus Gaming ,  FunPlus Phoenix ]RETURN pChampions Graph for Invictus Gaming vs FunPlus PhoenixIf we put aside the most popular picks (Qiyana, Kayle, Nautilus, Kai’sa, Xayah, Gragas, Lee Sin, etc…), champions like Rek’Sai, Camille and Gangplank might be contested during the draft phase. If we look at specific champions for each team, Jayce and Orianna are high valued for Invictus Gaming while FunPlus Phoenix might rely on the Varus Tahm Kench duo.SK Telecom T1 vs G2 EsportsMATCH p = (t:Team)--(c:Champion)WHERE t.name IN [ SK Telecom T1 ,  G2 Esports ]RETURN pChampions Graph for SK Telecom T1 vs G2 EsportsFor the second match, Alistar, Rakan and Yasuo are definitely the picks to watch, since both teams like to them play. Regarding each team specific champions, G2 Esports can always go for Rek’Sai in the jungle or Jayce, while SK Telecom T1 may use Draven and Thresh to get an advantage in the Bot Lane.Definitely, more analysis can be done on this topic and I am open to any comments or suggestions! This time I didn’t include ban and pick order, which I think can be very interesting to try to understand more how teams decide how and when to play a specific champion, for instance :Which champion is the most first picked?Which role’s pick is saved until the last pick?Which champions are banned in the first banning phase? And in the second one?…;Oct 29, 2019;[]
https://medium.com/neo4j/discover-auradb-free-week-14-consumer-complaints-database-1b1770f17ef5;Michael HungerFollowNov 24, 2021·8 min readDiscover AuraDB Free: Week 14 — Consumer Complaints DatabaseIn 2015 our data scientist Nicole White ran a Webinar LOAD CSV in the Real World,” which is really popular. It has more than 56k views and people are still referring to it frequently because it is so well done.That’s why we decided this time in our live-stream to re-enact Nicole’s webinar with Neo4j 4.3 and AuraDB Free to give people today’s equivalent of that session.Here is this week’s video:ReferencesConsumer Complaints Database: https://catalog.data.gov/dataset/consumer-complaint-databaseNicole White’s webinar LOAD CVS in the real World” and GitHub repositoryOur GitHub RepositoryDatasetThe consumer complaints database is available with data since 2011 from data.gov, with more than 2.3 million complaints about products from different companies totalling 1.4GB (380MB compressed).We’re using xsv a really neat and fast CSV processing tool written in Rust for some pre-processing.xsv frequency -s Company complaints.csvTo fit the data into the LIMITs of AuraDB free, we need to sub-filter to a certain range — the complaints received in November 2021 are 12295, which is a good number for us.xsv search for filteringxsv search -s Date received 2021-11- complaints.csv | wc -l   12295xsv search -s Date received 2021-11- complaints.csv > complaints-nov-2021.csvThe file is also available here.To load the data into Neo4j, we need it on a publicly accessible URL.We can put it into a GitHub Gist or a Google Spreadsheet.GitHub GistShort URL: https://git.io/J1m7eGoogle Sheet (File → Publish to Web → Publish Sheet)Let’s try to load all rows?LOAD CSV WITH HEADERS FROM  https://git.io/J1m7e  AS rowRETURN count(*)// 12295What does the data look like in Cypher?LOAD CSV WITH HEADERS FROM  https://git.io/J1m7e  AS rowRETURN row LIMIT 1When loading the data via Cypher we get each row RETURNed as a map/dict/hash object that we then can use to create our graph.{   Company :  EQUIFAX, INC. ,   Date received :  2021-11-03 ,   Issue :  Incorrect information on your report ,   Timely response? :  Yes ,   Sub-product :  Credit reporting ,   Consumer complaint narrative : null,   ZIP code :  45458 ,   Consumer consent provided? : null,   Product :  Credit reporting, credit repair services, or ... ,   Consumer disputed? :  N/A ,   Company public response : null,   Sub-issue :  Information belongs to someone else ,   Date sent to company :  2021-11-03 ,   State :  OH ,   Complaint ID :  4874335 ,   Submitted via :  Web ,   Tags : null,   Company response to consumer :  In progress }Data ModelWe closely follow Nicole’s modeling, with the exception of new datatypes for dates. All names are capitalized to ensure uniqueness.We have the following Nodes in our models that are connected with appropriate relationships.Complaint - core entity, also holds user informationCompany - the company the complaint is againstProduct - the product that is complained aboutSubProduct - optional subclassification of productIssue - the categorized issue with the productSubIssue - optional subclassification of the issueResponse - Response of the companyTag - there are a few tagged valuesHere is our data model, which is very similar to the original.Data ImportFirst we create constraints for our key-properties, the complaint-id, and the names for the other entities.CREATE CONSTRAINT ON (c:Complaint) ASSERT c.id IS UNIQUECREATE CONSTRAINT ON (c:Company) ASSERT c.name IS UNIQUECREATE CONSTRAINT ON (c:Response) ASSERT c.name IS UNIQUECREATE CONSTRAINT ON (c:Product) ASSERT c.name IS UNIQUECREATE CONSTRAINT ON (c:SubProduct) ASSERT c.name IS UNIQUECREATE CONSTRAINT ON (c:Issue) ASSERT c.name IS UNIQUECREATE CONSTRAINT ON (c:SubIssue) ASSERT c.name IS UNIQUECREATE CONSTRAINT ON (c:Tag) ASSERT c.name IS UNIQUEWe can run :schema to see if our constraints were correctly created.Then we import the data step by step, starting with the Complaints. And then we run additional passes to load and connect the other entities.We need to deal with a few optional elements, like SubIssue, SubProduct, and Tag. Tags also form a comma separated list.For complaints, we MERGE (get-or-create) each by the key-id and set the date, zip-code, and state as properties. Note the backticked field names if they are not just alphanumeric characters.Importing complaintLOAD CSV WITH HEADERS FROM  https://git.io/J1m7e  AS rowMERGE (c:Complaint {id:row.`Complaint ID`})SET c.dateReceived = date(row.`Date received`)SET c.zip = row.`ZIP code`SET c.state = row.StateFor companies, we do our second pass over the data. We find the Complaint by id and MERGE the Company and then MERGE the AGAINST relationship (so only one relationship can exist between each complaint and a company). Then we set the date properties for when the complaint was sent to this company.Importing companyLOAD CSV WITH HEADERS FROM  https://git.io/J1m7e  AS rowMATCH (c:Complaint {id:row.`Complaint ID`})MERGE (co:Company {name:toUpper(row.Company)})MERGE (c)-[rel:AGAINST]->(co)SET rel.date = date(row.`Date sent to company`)Most frequently companies complained againstMATCH (n:Company)<-[:AGAINST]-()RETURN n.name, count(*) AS cORDER BY c DESC LIMIT 10╒════════════════════════════════════════╤════╕│ n.name                                 │ c  │╞════════════════════════════════════════╪════╡│ EQUIFAX, INC.                          │5858│├────────────────────────────────────────┼────┤│ TRANSUNION INTERMEDIATE HOLDINGS, INC. │3583│├────────────────────────────────────────┼────┤│ EXPERIAN INFORMATION SOLUTIONS INC.    │707 │├────────────────────────────────────────┼────┤│ NAVY FEDERAL CREDIT UNION              │114 │├────────────────────────────────────────┼────┤│ ALLY FINANCIAL INC.                    │96  │└────────────────────────────────────────┴────┘For adding Products, we need to first connect the complaint to the product as before, but then filter out rows where the Sub-product cell contains an empty string. For all others we merge and connect the SubProduct both to the Product and the Complaint.Importing product and sub-productLOAD CSV WITH HEADERS FROM  https://git.io/J1m7e  as rowMATCH (c:Complaint {id:row.`Complaint ID`})MERGE (p:Product {name:toUpper(row.Product)})MERGE (c)-[:ABOUT]->(p)WITH * WHERE trim(row.`Sub-product`) <>   MERGE (sp:SubProduct {name:toUpper(row.`Sub-product`)})MERGE (c)-[:ABOUT]->(sp)MERGE (sp)-[:IN_CATEGORY]->(p)We add issues and sub-issues the same way.Importing issue and sub-issueLOAD CSV WITH HEADERS FROM  https://git.io/J1m7e  as rowMATCH (c:Complaint {id:row.`Complaint ID`})MERGE (iss:Issue {name:toUpper(row.Issue)})MERGE (c)-[:WITH]->(iss)WITH * WHERE trim(row.`Sub-issue`) <>   MERGE (si:SubIssue {name:toUpper(row.`Sub-issue`)})MERGE (c)-[:WITH]->(si)MERGE (si)-[:IN_CATEGORY]->(iss)Now we can have a bit of fun with the data.Complaint counts by state and issueMATCH (n:Complaint)-[:WITH]->(iss:Issue)RETURN n.state, iss.name, count(*) as cORDER BY c DESC LIMIT 5╒═════════╤═══════════════════════════════════════════╤═══╕│ n.state │ iss.name                                  │ c │╞═════════╪═══════════════════════════════════════════╪═══╡│ TX      │ PROBLEM WITH A CREDIT REPORTING COMPANYS │847││         │INVESTIGATION INTO AN EXISTING PROBLEM     │   │├─────────┼───────────────────────────────────────────┼───┤│ PA      │ INCORRECT INFORMATION ON YOUR REPORT      │596│├─────────┼───────────────────────────────────────────┼───┤│ FL      │ INCORRECT INFORMATION ON YOUR REPORT      │542│├─────────┼───────────────────────────────────────────┼───┤│ CA      │ PROBLEM WITH A CREDIT REPORTING COMPANYS │455││         │INVESTIGATION INTO AN EXISTING PROBLEM     │   │├─────────┼───────────────────────────────────────────┼───┤│ TX      │ INCORRECT INFORMATION ON YOUR REPORT      │425│└─────────┴───────────────────────────────────────────┴───┘If there is a Response, it is coming from the Company pointing to the Complaint. So we need to match both in our pass and then connect the response to the complaint and to the company. Additionally, we store some properties on the relationship. Yes/No/Blank values are converted to boolean just with a boolean expression (value=text). For other conditional expressions we can use CASE.Importing responseLOAD CSV WITH HEADERS FROM  https://git.io/J1m7e  as rowMATCH (c:Complaint {id:row.`Complaint ID`})MATCH (co:Company {name:toUpper(row.Company)})WITH * WHERE trim(row.`Company response to consumer`) <>   MERGE (res:Response {name:toUpper(row.`Company response to consumer`)})MERGE (c)<-[rel:TO]-(res)SET rel.disputed = (row.`Consumer disputed?` =  Yes )SET rel.timely = (row.`Timely response?` =  Yes )SET rel.text = CASE row.`Company public response`                WHEN    THEN null                ELSE row.`Company public response` ENDMERGE (co)-[:ANSWERED]->(res)For Tags, we need to do two things. First,split the comma-separated name into its constituent parts. Then taking that list, turn it into rows with UNWIND and connect the newly merged Tag nodes to the previously found Complaint.The WITH * WHERE …​ is there to filter arbitrary data with a WHERE clause.Importing tagLOAD CSV WITH HEADERS FROM  https://git.io/J1m7e  as rowMATCH (c:Complaint {id:row.`Complaint ID`})WITH * WHERE trim(row.`Tags`) <>   WITH distinct row.Tags as tagsName,cUNWIND split(tagsName, ,  ) as tagNameMERGE (t:Tag {name:toUpper(tagName)})MERGE (c)-[:TAGGED]->(t)Now we can look at our imported graph model with call db.schema.visualization() or better call apoc.meta.graph() to see the schema of the data we have imported. It’s great to see how well it fits our model.ExplorationThe queries here were taken and adjusted from Nicole’s GitHub repository.Sub-issues with communication tacticsMATCH (i:Issue {name:COMMUNICATION TACTICS})MATCH (sub:SubIssue)-[:IN_CATEGORY]->(i)RETURN sub.name AS subissueORDER BY subissue╒════════════════════════════════════════════════════════════╕│ subissue                                                   │╞════════════════════════════════════════════════════════════╡│ CALLED BEFORE 8AM OR AFTER 9PM                             │├────────────────────────────────────────────────────────────┤│ FREQUENT OR REPEATED CALLS                                 │├────────────────────────────────────────────────────────────┤│ USED OBSCENE, PROFANE, OR OTHER ABUSIVE LANGUAGE           │├────────────────────────────────────────────────────────────┤│ YOU TOLD THEM TO STOP CONTACTING YOU, BUT THEY KEEP TRYING │└────────────────────────────────────────────────────────────┘Common responsesMATCH (r:Response)-[:TO]->(:Complaint)RETURN r.name AS response, COUNT(*) AS countORDER BY count DESC╒═════════════════════════════════╤═══════╕│ response                        │ count │╞═════════════════════════════════╪═══════╡│ IN PROGRESS                     │6097   │├─────────────────────────────────┼───────┤│ CLOSED WITH EXPLANATION         │5970   │├─────────────────────────────────┼───────┤│ CLOSED WITH NON-MONETARY RELIEF │192    │├─────────────────────────────────┼───────┤│ CLOSED WITH MONETARY RELIEF     │35     │└─────────────────────────────────┴───────┘Sub-issues in multiple different issuesMATCH (sub:SubIssue)-[:IN_CATEGORY]->(i:Issue)WITH sub, COLLECT(i.name) AS issuesWHERE size(issues) > 1RETURN sub.name, issues LIMIT 2╒══════════════════════════════╤══════════════════════════════╕│ sub.name                     │ issues                       │╞══════════════════════════════╪══════════════════════════════╡│ DIFFICULTY SUBMITTING A DISPU│[ PROBLEM WITH A CREDIT REPORT││TE OR GETTING INFORMATION ABOU│ING COMPANYS INVESTIGATION IN││T A DISPUTE OVER THE PHONE    │TO AN EXISTING PROBLEM , PROBL││                              │EM WITH A COMPANYS INVESTIGAT││                              │ION INTO AN EXISTING ISSUE ]  │├──────────────────────────────┼──────────────────────────────┤│ THEIR INVESTIGATION DID NOT F│[ PROBLEM WITH A CREDIT REPORT││IX AN ERROR ON YOUR REPORT    │ING COMPANYS INVESTIGATION IN││                              │TO AN EXISTING PROBLEM , PROBL││                              │EM WITH A COMPANYS INVESTIGAT││                              │ION INTO AN EXISTING ISSUE ]  │└──────────────────────────────┴──────────────────────────────┘Product and issues with ‘EQUIFAX, INC.’MATCH (ef:Company {name:EQUIFAX, INC.})MATCH (complaint:Complaint)-[:AGAINST]->(ef)MATCH (:Response)-[:TO]->(complaint)MATCH (complaint)-[:ABOUT]->(p:Product)MATCH (complaint)-[:WITH]->(i:Issue)RETURN p.name AS product, i.name AS issue, COUNT(*) AS countORDER BY count DESC LIMIT 2╒═══════════════════════╤═══════════════════════╤═══════╕│ product               │ issue                 │ count │╞═══════════════════════╪═══════════════════════╪═══════╡│ CREDIT REPORTING, CRED│ PROBLEM WITH A CREDIT │2394   ││IT REPAIR SERVICES, OR │REPORTING COMPANYS INV│       ││OTHER PERSONAL CONSUMER│ESTIGATION INTO AN EXIS│       ││ REPORTS               │TING PROBLEM           │       │├───────────────────────┼───────────────────────┼───────┤│ CREDIT REPORTING, CRED│ INCORRECT INFORMATION │2262   ││IT REPAIR SERVICES, OR │ON YOUR REPORT         │       ││OTHER PERSONAL CONSUMER│                       │       ││ REPORTS               │                       │       │└───────────────────────┴───────────────────────┴───────┘Which (sub-)products have sub-issues about obscene languageMATCH (subIssue:SubIssue)WHERE subIssue.name contains OBSCENEMATCH (complaint:Complaint)-[:WITH]->(subIssue)MATCH (complaint)-[:ABOUT]->(p:Product)OPTIONAL MATCH (complaint)-[:ABOUT]->(sub:SubProduct)RETURN p.name AS product, sub.name AS subproduct, COUNT(*) AS countORDER BY count DESC╒═════════════════╤══════════════════╤═══════╕│ product         │ subproduct       │ count │╞═════════════════╪══════════════════╪═══════╡│ DEBT COLLECTION │ OTHER DEBT       │3      │├─────────────────┼──────────────────┼───────┤│ DEBT COLLECTION │ CREDIT CARD DEBT │2      │├─────────────────┼──────────────────┼───────┤│ DEBT COLLECTION │ PAYDAY LOAN DEBT │2      │├─────────────────┼──────────────────┼───────┤│ DEBT COLLECTION │ MEDICAL DEBT     │1      │└─────────────────┴──────────────────┴───────┘Typical response percentages per productMATCH ()<--(r:Response) with r, count(*) as rCountMATCH (p:Product)<--(:Complaint)<--(r)RETURN p.name, r.name, count(*) as c, (count(*)*100)/rCount as percent ORDER BY percent DESC LIMIT 10╒══════════════════════╤══════════════════════╤════╤═════════╕│ p.name               │ r.name               │ c  │ percent │╞══════════════════════╪══════════════════════╪════╪═════════╡│ CREDIT REPORTING, CRE│ IN PROGRESS          │5672│93       ││DIT REPAIR SERVICES, O│                      │    │         ││R OTHER PERSONAL CONSU│                      │    │         ││MER REPORTS           │                      │    │         │├──────────────────────┼──────────────────────┼────┼─────────┤│ CREDIT REPORTING, CRE│ CLOSED WITH EXPLANATI│4752│79       ││DIT REPAIR SERVICES, O│ON                    │    │         ││R OTHER PERSONAL CONSU│                      │    │         ││MER REPORTS           │                      │    │         │├──────────────────────┼──────────────────────┼────┼─────────┤│ CREDIT REPORTING, CRE│ CLOSED WITH NON-MONET│135 │70       ││DIT REPAIR SERVICES, O│ARY RELIEF            │    │         ││R OTHER PERSONAL CONSU│                      │    │         ││MER REPORTS           │                      │    │         │├──────────────────────┼──────────────────────┼────┼─────────┤ConclusionThis was a fun dataset and a bit more challenging to import. There are lots of possibilities to build analytics and visualizations with the data now that we have it in the graph. Let us know if you found it helpful to explore with us here and if you built anything on top of the data.;Nov 24, 2021;[]
https://medium.com/neo4j/how-to-have-a-cybersecurity-graph-database-on-your-pc-366884ac6a08;Adamantios - Marios BerzovitisFollowOct 26, 2021·5 min readHow to Have a Cybersecurity Graph Database on Your PCGraphKer represents every public record of CVE, CWE, CAPEC and CPE provided by MITRE and NIST in a connected graph using Neo4j.Let’s talk about GraphKer a combination of the words Graph and Hacker. You see where it is going. GraphKer is a free and open-source tool, providing a detailed and updated cybersecurity graph database using Neo4j.Nowadays, when everything is translated into data and is being used for knowledge extraction, every scientist, researcher, industry worker etc., tries to find connectivity between these huge amounts of data. Connectivity and knowledge extraction from data are two highly connected concepts and not just a methodology for scientists. The common use of these concepts has a very important impact on our lives even though we don’t notice it.In cybersecurity we also have this need. We work in a constantly changing environment and must be ahead of unwanted surprises as much as possible. Every try for a successful analysis of this vast amount of data that we deal with, every day, contributes to cyber defense and protection of our colleagues, clients, data, and organizations. In 2021 we have plenty of great software, hardware, and techniques to enrich our cyber defense. However, we can admit that is not enough, it’s not even close to stop the advanced threats unleashed by malicious users.We need to educate our colleagues and clients (and users in general) into cybersecurity policies and procedures, and possible ways that a dangerous user could exploit to break in our data or organization. That’s an ongoing process. As long as we stay up to date into cybersecurity techniques, procedures, policies, regulations etc. we can maintain a -relatively- safe environment both virtual and physical. Even this, is not enough. How can we think out of the box? How can we search connections between cyber security incidents, threat actors, hardware, software, skills required, attack paths, etc. to update our cybersecurity level, and operational readiness?With Neo4j and GraphKer Tool, we demonstrate an example on how we can manage big cybersecurity data, and more specifically every public record of CVE, CWE, CAPEC and CPE provided by MITRE and NIST to find new connections between them and their attributes, and extract knowledge from connected graphs.GraphKer uses a new ontology approach for data feeds, to reinforce the knowledge extraction ways that graph databases provide. Using Neo4j, we can search these graphs, in a fast and super user-friendly way, and discuss them with users that don’t have any idea about cybersecurity and graph databases. GraphKer makes good use of Neo4j as a native graph database platform and APOC Library Procedures to maintain an efficient way on data storage and retrieval.To run GraphKer you need to do 3 + 1 things.Download and install Neo4j.Create and configure the database.Install Python requirements for GraphKer.Install and use applications created for Neo4j such as Neo4j Bloom, Graphlytic, Neo4j Database Analyzer etc.Then go to your Linux or Windows terminal and run GraphKer! Depends on your hardware you will have the most recent data feeds of CVEs, CWEs, CAPECs, CPEs in about 6 to 15 minutes.Run GraphKer on Windows — YouTubeNow you are ready to explore your data in Neo4j graphs!Check out a DBMS Dump File with 2021 CVEs and relevant CPEs, and all CWEs and CAPECs: amberzovitis/GraphKer-DBMS-Dump (github.com)GraphKer Repository: amberzovitis/GraphKer: Open Source Tool — Cybersecurity Graph Database in Neo4j (github.com)GraphKer — Examples on Neo4j Browser — YouTubeLet’s go deep into the data.Which are the most appearing CWEs during the first six months with covid-19?As you can see, there are 1772 vulnerabilities not related to any known weakness yet. In every record in our data, there will be several modifications by the numbering authorities. When we re-run GraphKer for example after 1 month, these number will probably be different even though, these CVEs have been published in 2020. That’s why we need to have the most recent update of our data!Let’s find out more about CWE-79, Improper Neutralization of Input During Web Page Generation (‘Cross-site Scripting’) with Neo4j Bloom’s help.1095 CVEs exploited the XSS — Cross Site Scripting during the first six months with covid-19.Let’s go deeper into our data. We increase the amount of nodes that Neo4j Bloom can print to our screen and use the maximum of our RAM.How an attacker could exploit this weakness ? We need to find out which attack patterns an attacker can use.There are six attack patterns that an attacker can use to exploit CWE-79 (Cross-Site Scripting). The interesting part is that 5 of them (CAPEC-85, CAPEC-591, CAPEC-63, CAPEC-588 and CAPEC-592) need Low and Medium skills to be accomplished.With the expansion of the graph we can get even more data and extract more knowledge. With the above graph — data exploration, we can explain (from a specific point of view) the trends in hacking during the ages of COVID-19. For example, many businesses, during lockdowns all over the world, tried to create e-shops and several web-applications. These web-apps were created so quickly and security issues were ignored. That’s a simple explanation we can give. However, it’s an abstract explanation.The deeper we dive into our data, the most accurate explanations we can give. That’s the way of using data. That’s the way of using GraphKer.Don’t forget to think out of the box. The attackers will certainly do it.For any questions feel free to send me a message on LinkedIn.I implemented GraphKer during my diploma research of the MSc Distributed Systems, Security and Emerging Information Technologies in University Of Piraeus. A huge thank you to the team of Cyber Security Research Lab | University Of Piraeus.;Oct 26, 2021;[]
https://medium.com/neo4j/cypher-code-golf-hackathon-completion-5b43877d2904;Michael HungerFollowApr 6·8 min readCypher Code-Golf Completion: Hackathon Winners AnnouncedWith spring in full swing and summer just right around the corner, I wanted to take a moment to reflect on the Neo4j Hackathon we ran last year, where we encouraged you all to compete in your Cypher skills.Similar to the classic code-golf where the shortest solution to a problem wins, we wanted to see who can write the most efficient and shortest Cypher query to solve one of available three challenges in Cypher Code-Golf,” where the level of difficulty ranges from Beginner, Intermediate, and Advanced Cypher skill levels.The efficiency was measured as Database Hits,” a metric returned from using PROFILE with your query, and the query length in characters counted for the shortest. For ties on both metrics, we used the first submission.By the numbers, more than 1000 people registered, and over 5,000 queries were submitted. At the conclusion of the hackathon, 18 winners were selected to take home hard-earned cash prizes, and we couldn’t be more thankful to have so many participants in this challenge. More on those 18 winners below!Our first-ever hackathon would only be complete with some learnings and challenges. We quickly realized that not having a result verification tool led to a lot of invalid queries with an unusually high number of database hits that simply didn’t make sense. Zero db-hits was an obvious one, and we got plenty of those! Next time we’ll apply the approach from Advent of Code that executes the statement with parameters for another set of input parameters and validates the correctness of the results that way.Here’s a quick rundown of the user interface of Code-Golf:After signing up and picking an avatar, you could start swinging.We provided three difficulty levels to accommodate different skills, one of which you could compete in.Stack Overflow DatasetBeing developers, we felt that a Stack Overflow dataset would be easy to understand and fun to query for everyone. Based on our previous work, we used the import of the whole Stack Overflow dump from 2018 (55M nodes, 123M relationships), which provided us with this data model:So effectively, the elements we are looking at are:Labels: Tag, Question:Post, Answer:Post, UserPatterns(:User)-[:POSTED]->(:Post), (Tag)-[:SIMILAR]->(Tag)(:Question)-[:TAGGED]->(:Tag)(:Question)-[:ACCEPTED]->(:Answer), (:Question)<-[:ANSWERED]-(:Answer)For the 3 levels, we had a question each.Level 1: BeginnerQuestion: How many tags do questions in the Cypher area have at most?This question could be solved in these stepsFind the cypher” tagFind questions tagged with that tagReturn the max degree (relationship count) of those questionsWe got the most submissions in this category, 1197 in total.There were a number of cheats e.g. just returning the number or sneakily creating a statement that just hit 1000 db-hits and then returning the fixed results or similar.An efficient statement that meets the criteria with 12719 db-hits is:MATCH (t:Tag {name:cypher})<-[:TAGGED]-(q)RETURN max(size( (q)-[:TAGGED]->())) as mostWith Neo4j 5.x you would replace the size(pattern) with a count { pattern } expression instead.WinnersOur 6 winners in this category areChris ZirkelPaweł GronowskiErtan KabakcıBenjamin MalburgJoren Van de VondelPaul Billing-RossHere’s an example query of one of our winners that’s different from the minimal query:MATCH (:Tag {name:  cypher })<-[:TAGGED]-(q)RETURN max(apoc.node.degree(q, TAGGED ))MATCH (t:Tag)<-[:TAGGED]-(q)WHERE t.name =  cypher WITH q, size((q)-[:TAGGED]->()) as tsRETURN max(ts)Level 2: IntermediateQuestion: What is the title of the most highly voted question posted on April 1st in the Perl category?This question was also pretty straightforward, you can access the components like month or day of date(-time) property individually.Find the perl” tagFind questions with that tagFilter those questions by day 1 and month 4Sort by score DESCendingReturn the titleAgain in the 932 submissions, we got a lot of cheats that just looked for the question on that single date, or with the question-id or only within that single year and not all years, or just blatantly returned the expected title.A reasonable solution would be:MATCH (t:Tag {name:perl})<-[:TAGGED]-(q)WHERE q.createdAt.month = 4 and q.createdAt.day = 1RETURN qORDER BY q.score DESC LIMIT 1An alternative solution could be iterating over the years and then using a range (between) for using the index on the date field and a hash-join between the tag and the question.But that one had higher db-hit costs (353k) as it had to filter all questions for that date, and not just the ones within the perl tag.UNWIND range(2008,2019) as yearWITH datetime({year:year,month:4,day:1}) as dateMATCH (t:Tag {name:perl})<-[:TAGGED]-(q:Question)USING JOIN on tWHERE date <= q.createdAt < date + duration( P1D )RETURN q.title, q.scoreORDER BY q.score DESC LIMIT 1WinnersThe winners for this category are:Justin BBrian LeeBelinda DhamersCarmi RazVenkatesh PrasannaCamille CaulierSome of their different solutions:CYPHER runtime=interpretedMATCH (t:Tag{name:perl})<-[:TAGGED]-(q:Question)USING INDEX q:Question(createdAt)USING JOIN ON tWHERE datetime({year:2008, month:4, day:1}) <= q.createdAt < datetime({year:2008, month:4, day:2})OR datetime({year:2009, month:4, day:1}) <= q.createdAt < datetime({year:2009, month:4, day:2})OR datetime({year:2010, month:4, day:1}) <= q.createdAt < datetime({year:2010, month:4, day:2})…day:2})OR datetime({year:2022, month:4, day:1}) <= q.createdAt < datetime({year:2022, month:4, day:2})RETURN q.title ORDER BY q.score DESC LIMIT 1Or with a regular expression:MATCH (:Tag {name: perl})<-[:TAGGED]-(q)WHERE apoc.convert.toString(q.createdAt) =~ .*04\-01T.*RETURN q.titleORDER BY q.score DESCLIMIT 1Or via date components:MATCH (:Tag {name: perl })<-[:TAGGED]-(q) WHERE EXISTS {MATCH(q) WHERE q.createdAt.month=4 AND q.createdAt.day=1} RETURN q.id ORDER BY q.score DESC LIMIT 1Level 3: AdvancedQuestion: Given the top 10 tags in the Lua community (except Lua), which other tags most frequently co-occur with them. For those tags, find the 25 people who most frequently answered those questions and see what are the top 10 question tag names they had that were not in the original 10-element list.This question was a bit more involved but you can just follow the steps one at a time to build it up.Find the lua” tagFind questions tagged with that tag, and other tags for these questionsAggregate the other tags by their frequency, sort descending and select the top 10Turn the 10 top other tags into a listFind the users who posted answers to questions with those top 10 tagsAggregate by frequency, sort descending, and collect the top 25 usersFor those users find what the tags were for questions they answeredFilter out the tags that were in our top 10 tags listReturn the remaining tags, ordered by frequency descending, and pick the top 10An example query on how it could be solved is:MATCH (:Tag {name:lua})<-[:TAGGED]-()-[:TAGGED]->(o)WITH o, count(*) as c ORDER BY c DESC LIMIT 10CALL { with o return collect(o) as all }MATCH (o)<-[:TAGGED]-()<-[:ANSWERED]-()<-[:POSTED]-(u)WITH u, all, count(*) as c ORDER BY c LIMIT 25MATCH (o)<-[:TAGGED]-()<-[:ANSWERED]-()<-[:POSTED]-(u)WHERE not o in allRETURN o, count(*) as c ORDER BY c DESC LIMIT 10This category was as expected the hardest, many folks also had issues understanding the challenge. Unfortunately, we didn’t have the capacity to provide explanations for all your questions, apologies for that.WinnersBut we still got 111 submissions, of which we could pick the 6 winners.Ron van WeverwijkLaura ArdittiHüseyin ÇötelĐức Lê TựNiclas Kjäll-OhlssonRajendra KadamSome different solutions from the one we’ve shown:MATCH (:Tag{name:  lua })<-[:TAGGED]-(p)-[:TAGGED]->(o)WITH o, count(p) as npORDER BY np DESCLIMIT 10WITH collect(o) as osCALL {WITH osUNWIND os as oMATCH (o)<-[:TAGGED]-()<-[:ANSWERED]-(a)<-[:POSTED]-(u)RETURN u, count(DISTINCT a) AS naORDER BY na DESCLIMIT 25}WITH os, collect(u) as usCALL {WITH us, osUNWIND us as uMATCH (u) →(:Question)-[t:TAGGED]->(ot)WHERE (NOT ot IN os)RETURN ot, count(t) as ntORDER BY nt DESCLIMIT 100}return us, collect(ot)Using APOC:MATCH l = (t:Tag {name:  lua })-[:SIMILAR]-(s:Tag)<-[:TAGGED]-(p:Post)-[:TAGGED]->(c:Tag)WITH c, count(l) AS n, collect(p) AS pORDER BY n DESC LIMIT 10WITH apoc.coll.flatten(collect(p)) AS p, collect(c) AS oMATCH r = (u:User)-[:POSTED]->(:Answer)-[:ANSWERED]->(q:Question)WHERE q in pWITH count(r) AS a, u, o, collect(q) AS qORDER BY a DESC LIMIT 25WITH apoc.coll.flatten(collect(q)) AS q, oMATCH (v:Question)-[:TAGGED]->(t:TAG)WHERE NOT (t IN o) AND v IN qWITH t, count(v) AS cRETURN t ORDER by c DESC LIMIT 100Using a single query:MATCH (tf:Tag)<-[tr:TAGGED]-(q:Question)-[:TAGGED]-(t:Tag {name: lua }) with tf, count(tr) as cnt order by cnt desc limit 10MATCH (tf)<-[:TAGGED]-(q:Question)<-[:ANSWERED]-(:Post)<-[rp:POSTED]-(u:User) with u, count(rp) as cntrp, collect(tf.name) as tf_list order by cntrp desc limit 25MATCH (tt:Tag)<-[rtt:TAGGED]-(:Question)<-[:ANSWERED]-(:Post)<-[:POSTED]-(u) where not tt.name in tf_list with tt, count(rtt) as crtt order by count(rtt) desc limit 100 return ttA smaller Stack Overflow dataset is also available on Neo4j AuraDB, the full dataset is on the Demo server demo.neo4jlabs.com with username/password/database stackoverflow”.Please make sure you join our user forums or hang out in our Discord to learn from each other.Neo4j Online CommunityDiscussion forums for Neo4j.community.neo4j.comIf you want to learn more, we have published additional Cypher, Graph Data Science, and Application Development courses on GraphAcademy.Free, Self-Paced, Hands-on Online TrainingExpert Training All courses have been developed by seasoned Neo4j Professionals with years of experience.graphacademy.neo4j.comAnd we’re running regular live streams, Meetups, GraphSummits, and other events, all of which you can find on our Events Page.Events ArchiveJoin Neo4j at the next graph technology event to uncover the possibilities of connected data. Register for upcoming…neo4j.com;Apr 6, 2023;[]
https://medium.com/neo4j/time-travel-with-neo4j-bloom-2-1-55db7c8a7a82;Jeff GagnonFollowFeb 22, 2022·7 min readTime Travel with Neo4j Bloom 2.1This week we welcome the release of Neo4j Bloom 2.1, and in this post I’ll showcase a useful new feature — filtering and rule-based styling based on times and dates, which was frequently asked for.Photo by Mohamed Osama on UnsplashLet’s imagine we wish to analyze some summarized network flow traffic for a global organization with offices in:San Francisco (where the timezone offset is GMT-08:00),Ottawa (GMT-05:00),London (GMT or Z), andMalmö (GMT+01:00).Each office has a network monitoring device, and we’ve loaded data from all of them into our Neo4j database. Our nodes represent internal IP Addresses in the various global offices, and the relationships are traffic flows. The traffic flows have properties indicating the number of packets transferred, to and from ports, the date the sample was collected, and the time the sample was collected — including one of the aforementioned time offsets depending on where the sample was collected.Before we dig any further into our analyses, let’s take a moment to think about the implications of the different time offsets. Consider the table below — we can see what the local time is in each office at certain points in the day — start of business (09:00), lunch (12:00), and close of business (17:00).While we all understand the concept of time zones, sometimes their impact on visual data analysis is not highly intuitive. An analyst may choose to normalize all times in a dataset to a common time offset like UTC prior to analysis, but this can take extra time and in some cases may not be the ideal solution.In our case, the time property of a traffic_flow relationship was logged by our network monitoring device and is stored as a time type with the appropriate time offset, depending on the location of the monitoring device.So, a traffic flow collected at lunchtime in San Francisco (GMT-08:00) would have occurred at 8pm in London (Z). The time property on that relationship would be expressed as 12:00–08:00, to represent that it is 12:00 noon in time zone GMT-08:00 (not 12:00 minus 8 hours).If data is stored in Neo4j as a temporal value (date, time, localTime, datetime or localDatetime), Neo4j Bloom now offers the ability to filter and apply rule-based styling to nodes and relationships, including when time zones are present.There are two ways (default and translated) to apply filters and rule-based styling to properties with time zones (time and datetime values). The features described here also work on temporal values without time zones, albeit without the ability to translate time zones to an offset of choice (more on that later).Now, let’s go back to our network data. Here’s a view of IPAddress nodes across our imaginary organization’s four offices, connected by summarized traffic_flow relationships.A rule-based style has already been applied to the relationships that increases their width according to the number of packets forwarded.As you can see by the histogram in the Filter pane, the traffic_flow relationships in this scene were all captured between 05:01:45 and 11:42:37. These times do not take the time zones into account — which is to say, they’re local to the office where they were logged by our monitoring devices.You can see that at the top of the histogram, Bloom shows the earliest time on the left, and the latest time on the right, with LocalTime underneath to show that these are the local times.You can also see, in the Inspector, details for one of the traffic_flow relationships, where the time value is 11:28:17+01:00 (indicating this flow was logged by our monitoring device in Malmö, based on the time offset of GMT+01:00). By default, the filter and rule-based style features will ignore the time offset +01:00 for time-zoned data types.Default Histogram and Inspector view of a time propertyThis is useful for certain enquiries. For example, we might want to look at only traffic that occurs before the normal start of business, or 09:00, regardless of where it was collected. Put another way, we want to look at network flows before nine o’clock local time relative to the place where it was logged. Lets simply apply the filter with those criteria:Network data filtered based on local timesWe have significantly reduced the number of traffic_flow relationships visible by applying this filter. We can zoom in and get a closer look at flows of interest.Now, let’s imagine a different type of analytic question. Suppose something anomalous happens with a server in our London office (time offset Z) — maybe it goes offline unexpectedly at around 10:30 local time on a particular day. We can now make use of the Translate timezones to option in Bloom. When selecting this option, a dropdown appears with all possible time offsets.Neo4j supports only time offsets for time values, but time offsets as well as time zones for datetime values — while the terms are often used interchangeably, including in this post, the difference is simply that time offsets are expressed as GMT + or GMT - some number of hours, whereas time zones are expressed as geographic region names. Bloom will show both in the list for datetime values, but only time offsets for time values.Translate timezones dropdownSince the event we’re concerned with happened around 10:30 in London, we’ll select Z to translate all of the time values in our scene to London time, and look for relationships occurring between 10:25 and 10:35. This will not change the data in the database, nor will it change how that data is displayed in captions or the inspector.It will only adjust the histogram so that when we select a time, it’s applied to the scene from the perspective of the time offset chosen. You’ll notice that the earliest and latest times are now shown as 10:00:56 and 11:00:56 respectively, because when we translate all of the four different time zones on the scene to London time, everything happened within about the 10:00 to 11:00 window from the perspective of London local time.What?! Although initially it looked like our data spanned almost seven hours (from about 5 a.m. until almost noon), it really doesn’t — it just appeared that way because, before, we were only looking at the local component of the time values spanning four distant geographic locations (not so intuitive, is it?).Why don’t the earliest and latest times end with the same minute and second values as before we translated timezones, since we only have four time zones in full-hour increments? That’s because Bloom’s histogram adjusts intelligently to optimize the number of bins and the range for each, based on the data — so changes in the number of bins and their max/min values can be expected when we translate everything to one time zone. Looking at our table above, we see that there are 12 possible time values if we ignore the time zone (default), with 03:00 being the earliest and 22:00 being the latest. If we normalize all values to, say, GMT-08:00 using ‘Translate timezones to’, then we will only have three values left — 09:00, 12:00, and 17:00. The histogram will adjust accordingly.Histogram translated to London time, with a filter applied between 10:25 and 10:35We can now see the flows that happened in this 10-minute interval, regardless of where they were logged. If we zoom in and take a closer look, we see that the time values are still stored and displayed the same way they were before, but we can do the math in our heads to prove that everything is working.Inspector view of a traffic_flow relationship showing stored time valueLooking at this particular relationship in the inspector, we see that it was logged at 11:31:23 in GMT+01:00 (based on the time offset, we know it must’ve been logged by the monitoring device in our Malmö office). That means it was 10:31:23 in London at the time (Z) — so it fits within our selected time bounds.All of these same principles apply in the same way to Bloom’s Rule-based styling, offering a simple way to travel through time in your data analysis. You can even apply some styles and filters with timezone translation, and others without in the same scene, depending on your needs.If you’re not already a Neo4j Bloom user, feel free to head over to our website and get started by downloading Neo4j Desktop for free, which includes the Basic Access version of Bloom, or set up a free database on AuraDB !BloomThe fastest Path to Graph Data Visualization A beautiful and expressive data visualization tool to quickly explore and…neo4j.com;Feb 22, 2022;[]
https://medium.com/neo4j/speak-at-conferences-around-the-world-let-neo4j-pay-your-way-21f0002c807c;Karin WolokFollowMar 22, 2019·2 min readSpeak at Conferences Around the World! Let Neo4j Pay your Way!Hilary Mason of Cloudera and Fast Forward Labs @ GraphConnect 2018If you’ve already been exposed to graphs and graph databases, you may be aware that you’re a little ahead of the game. There are a whole slew of engineers, data scientists, architects, and researchers that still don’t really know about graph databases and all their benefits. Crazy, right?On the flip side, we also have a vast amount of graph enthusiasts and experts that are willing and eager to share their knowledge and experiences.We wanted to create a program that allowed our thought leaders to thrive! So, we’ve launched the Neo4j Speaker Program!What is it?For select conferences around the world, we are offering accepted-speakers on Neo4j-topics up to $1,000 in travel-stipend! (in other words, we will be covering the cost of your travel, hotel accommodations, and meals around your conference adventure!)Why be a speaker?Being a conference speaker sets you up to build a reputation as a thought-leader… but why is that valuable?You will be recognized as an industry-leader amongst both technologists and others in your industry.It opens up both personal and professional career opportunities.You will be empowering the world by providing insight and knowledge.You will learn and improve your skills through teaching!Ashley Sun from LendingClub talking about microservices ops with Neo4jSound Enticing? :DEvery couple weeks, we will be sending out lists of conferences with open CFPs (call-for-papers” — conferences that are currently accepting talk submissions).Included in the qualifying-conferences are leading events like Strata Data Conference, OSCON, GraphQL Summit, QCon, LondonConnect, Devoxx, Code Motion, VoxxedDays, JavaZone, Gartner Data Analytics, PyConf, and more!Get Notified!To receive emails of qualifying conferences with open CFPs, submit your name and email on this form. If you’re interested in receiving specific conferences specific to your industry or use case, please fill out the following fields with your topic title, abstract, locations, etc.Ready to Submit?Here’s how:Submit your talk to the conferenceEmail us to let us know you submitted (speakers@neo4j.com)If your talk gets accepted, email us to let us know! We can either loop you in with a dedicated travel agent that can help book your flight and hotel or reimburse your costs.Please note: Conferences that qualify for the program are pre-selected. If there’s a conference that you’re interested in submitting to and want to see if it qualifies for the program, run it by us at speakers@neo4j.com.;Mar 22, 2019;[]
https://medium.com/neo4j/neo4j-and-graphql-in-one-tool-box-3a3653a50572;Thomas WissFollowJul 11, 2022·7 min readNeo4j and GraphQL in One ToolboxCC-BY 2.0 Toolbox from Flickr by veryusefulAt Neo4j we’ve been providing a flexible, low-code, open-source GraphQL library for several years now. It’s enjoying a growing user base and gets a lot of positive feedback and contributions.GraphQL API MiddlewareWe recently made it even easier to get started with GraphQL for your Neo4j database. It just takes a few keystrokes and clicks, you don’t need to know any programming language and let alone know your way around the command line.The Neo4j GraphQL Toolbox is a new user interface that allows you, with very little effort, to write and execute GraphQL queries and mutations against your Neo4j database, with absolutely no setup required. Intrigued to see more? Let’s try it out together!An example workflow using the Neo4j GraphQL ToolboxPsst, are you no stranger to a cli or know your code? We have you covered too! See the server setup and everything else you need to know here.Getting Started - Neo4j GraphQL LibraryNow its time to add some data to your Neo4j database using your GraphQL API! Figure 1. Apollo Server Landing Page…neo4j.comNew to GraphQL or Neo4j? Check these sources if you want to learn more about GraphQL or Neo4j before we get started.PreppingOne important piece we need to use the Neo4j GraphQL Toolbox to its full extent is a set of type definitions describing the shape of your data, commonly called the type definitions. Don’t have one ready yourself? No worries, we can use a simple one for Movies and Actors to get you started.The second essential part that we need in place is a Neo4j database. You can easily get a free online AuraDB instance, just sign up there and create a free blank database.What we will need to continue is your login credentials to the database and the connection URI. The connection URI for an AuraDB database follows this pattern: neo4j+s://<db-id>.databases.neo4j.io.Ready for Launch!Alright, all the pieces come together here. The Neo4j GraphQL Toolbox is available under the following link: https://graphql-toolbox.neo4j.io.Let’s head over there now, fill in your credentials, paste the connection URI of your Neo4j database, and hit Connect.On the Definition view which you look at now, you can add your type definitions in the editor in the center. Feel free to enter your definitions on the blank page straight away, we assist with autocomplete hints and linting on the fly. Or paste in the type definitions for the Movie domain we provided above.The Definition” view page of the Neo4j GraphQL ToolboxCheck the list of keybindings for some more information on how to trigger the hints, you find them in the Help and learn sidebar (click the (?) button on the top right).Our library has a number of custom types, directives and more. In the Help sidebar, you’ll also find a lot of resources and documentation around the @neo4j/graphql library which is powering this tool. It also contains a link to a Neo4j Graph Academy course to turn you into a Neo4j GraphQL professional in no time!Start your no-code and zero setup Neo4j GraphQL journey right here at https://graphql-toolbox.neo4j.ioYou can star (☆) your type definitions as a favourite, that lets you revisit them easily once you make some changes to the current type definitions.When you’re done providing your type definitions, you can hit the Build schema” button, this will transition us to the Editor view. You can always come back to the Definition view, to update your typedefs. We’ll come back later to show another cool feature.Time to Write Some GraphQL Queries!In the Editor view, we can write our GraphQL queries and mutations.But fear not, if know the syntax by heart, we have you covered! The Explorer on the left-hand side lets you, with just a few clicks, construct a GraphQL query or mutation. It also highlights all the different options, filters, aggregations and more.The second aid we can offer is available in the Help and learn drawer, there you can find the Schema documentation in full detail and length.The editor view page of the Neo4j GraphQL ToolboxHave you already populated your Neo4j database in the meantime? No? Then let me offer you a suggestion. Let’s start with creating some nodes and relationships with a single GraphQL mutation:This GraphQL mutation (arguably a bit of a hefty one) creates a The Matrix movie and also connects a newly added Carrie-Anne Moss as an actress. In the same execution context, it also creates an entry for Laurence Fishburne and Joe Pantoliano and connects them as actors to the The Matrix movie.Let’s have a look at the resulting graph in the Neo4j database after running the GraphQL mutation:The resulting graph after executing the GraphQL mutation, viewed in Neo4j BrowserNow, I’m curious to know who acted in the movie The Matrix” and was born before 1970. We can easily do that with a GraphQL query. Check out the results yourself.This should be enough introduction for you to keep on exploring on your own.In case you need a bit of inspiration:How can you create a new movie?Or how can you connect another actor to The Matrix”?Can you delete one of the actors?An example workflow using the Neo4j GraphQL ToolboxThere is lots more you can do! See for instance here for a few example queries.And keep in mind, you can always go back to the schema view and update or extend your type definitions, just hit Build schema again and keep on interacting with your data!Magic Happens Here — Autogenerate the Type DefinitionsOn that note, we included another very handy feature in the Neo4j GraphQL Toolbox: A way to in(tro)spect your Neo4j graph database and generate type definitions based on the data currently stored in your database, all with just a single button click!We call it introspection, not to confuse it with the GraphQL introspection though. The introspector tool is part of the tool suite that the Team GraphQL at Neo4j has built over the years and is used inside the Neo4j GraphQL Toolbox.Just go to the Definition page, save your current type definition as a favorite (to be able to find them again later), then hit Generate type definitions and you’ll see the results in the editor soon after.Handy right? You can now edit those type definitions to your liking and reword the auto-generated field names to something more suitable for your needs. When done, hit Build schema again and write more GraphQL queries!Curious About Some Under the Hood Details?The Neo4j GraphQL Toolbox is built to fully leverage the @neo4j/graphql library and is written in Typescript and uses React. Some React components are from Neo4j’s very own (and new!) design system NDL.We use CodeMirror for all our editors but we’re looking into using Monaco instead to deliver you even better autocompleting, linting and other tooling.The incredible Explorer component in the editor view on the left-hand side is from OneGraph, thank you for that!But hey, why not see all the details for yourself? The Neo4j GraphQL Toolbox is open-source and therefore just a click away: GitHub — @neo4j/graphql-toolbox.graphql/packages/graphql-toolbox at dev · neo4j/graphqlExperiment with your Neo4j GraphQL API. Connect to the database with your credentials Define (or Introspect) Typedefs…github.comThe Neo4j GraphQL Toolbox is still in development and we will add more functionality as we go. If you have any feedback or feature requests, drop them here, thank you!GraphQL | Neo4jfeedback.neo4j.comConclusionAs said above, we have a lot of documentation and a free online course that you can utilize to build an amazing GraphQL API for Neo4j.The Neo4j GraphQL Toolbox will help you explore and try out things quickly and when you’re ready you can set out to build a powerful server with your very own GraphQL API.With all that said, we are happy to have you around and want to encourage you to have a look at our GitHub repo, chat with us on Discord while building your next amazing application with Neo4j as your database and @neo4j/graphql as the base for your API to give you the full power of GraphQL and Neo4j!Cheerio Thomas for the Neo4j GraphQL Team!;Jul 11, 2022;[]
https://medium.com/neo4j/analyzing-the-firemcmaster-twitter-data-7cb49729d417;Michael HungerFollowAug 7, 2017·6 min readAnalyzing the FireMcMaster” Twitter DataMy interest was triggered by this tweet, which points out that the #FireMcMaster” hashtag suddenly trended and that a network of bots was driving it up.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comHere is the quote from the New York Times Article: Trump Defends McMaster as Conservatives Seek His Dismissal”The #FireMcMaster hashtag was tweeted more than 50,000 times since Wednesday. Echoing the drumbeat were social media organs tied to the Russian government. According to the Alliance for Securing Democracy, a bipartisan group created to focus attention on Russian interference in the West, the top hashtag among 600 Twitter accounts linked to Russian influence operations at one point on Thursday was #FireMcMaster.DataNeo4j Database with 90k imported TweetsURL: http://52.86.50.254:34023/browser/Readonly Username: twitterPassword: twitterOr use this:Database + plugins + config neo4j 3.2.3 (60MB Zip)Twitter importI used the Python script to import user and their tweets from our Community Graph Initiative with a blank Neo4j SandboxThe import data is pulled from the twitter search API: 900 pages with 100 tweets eachmatch (n) return labels(n), count(*) labels(n)                   │ count(*) [ Tweet , Content , Retweet ]│79689     [ Tweet , Content , Reply ]  │2522      [ Tweet , Content ]          │5933      [ Tag ]                      │1102      [ User ]                     │32885     [ Tweet ]                    │1378      [ Link ]                     │4009Most of the tweets are retweets though, with only 6k being original content, issued by a total of 32k users.Tweets per dayMATCH (t:Tweet) WHERE exists(t.created)RETURN apoc.date.format(t.created,s,yyyy/MM/dd) AS date, count(*)ORDER BY date ASC date       │ count  2017-07-27 │13      2017-07-28 │10      2017-07-29 │64      2017-07-30 │26      2017-07-31 │26      2017-08-01 │14      2017-08-02 │8754    2017-08-03 │33031   2017-08-04 │32874   2017-08-05 │13030   2017-08-06 │201You clearly see a sharp rise in tweets with these hashtags since August 2nd but going down again on the 5th.Top tags, correlated tagsTop Tagsmatch (t:Tag)return t.name, size( (t)<-[:TAGGED]-() ) as degorder by deg desc limit 20 t.name            │ deg  firemcmaster      │47581 mcmasterfacts     │16114 mcmaster          │3668  maga              │2407  muslimbrotherhood │2085  draintheswamp     │1582  deepstate         │860   firemueller       │653   drainthesewer     │575   trump             │544   leakerstatus      │533   trumptrain        │462   trumprally        │421   leaks             │388   firemcmasters     │345   traitor           │343   americafirst      │343   thursdaythoughts  │329   susanrice         │288   leaker            │286Most frequently correlated TagsMATCH (t:Tag) WHERE toLower(t.name) =  firemcmaster MATCH (t)<-[:TAGGED]-()-[:TAGGED]->(t2:Tag)WHERE t2 <> tRETURN t2.name, count(*) AS freqORDER BY freq DESC LIMIT 20 t2.name             │ freq  mcmaster            │2407   maga                │2087   muslimbrotherhood   │2030   draintheswamp       │1216   mcmasterfacts       │836    firemueller         │591    deepstate           │575    drainthesewer       │572    leakerstatus        │531    trumprally          │418    trumptrain          │391    leaks               │376    trump               │346    traitor             │320    americafirst        │309    leaker              │283    mcleaker            │275    iftwitterdidntexist │234    rednationrising     │217    susanrice           │208Tags in Tweets that replied or retweeted this tagMATCH (t:Tag) WHERE toLower(t.name) =  firemcmaster MATCH (t)<-[:TAGGED]-()<-[:REPLIED_TO|RETWEETED]-()-[:TAGGED]->(t2:Tag)WHERE t2 <> tRETURN t2.name, count(*) AS freqORDER BY freq DESC LIMIT 20 t2.name             │ freq  mcmaster            │2347   muslimbrotherhood   │2021   maga                │1807   draintheswamp       │1053   mcmasterfacts       │754    leakerstatus        │530    drainthesewer       │529    deepstate           │448    trumprally          │407    trump               │384    leaks               │367    firemueller         │338    trumptrain          │337    traitor             │286    thursdaythoughts    │281    americafirst        │276    mcleaker            │274    leaker              │266    iftwitterdidntexist │233    rednationrising     │215Top-MentionsMATCH (t:Tag) WHERE toLower(t.name) =  firemcmaster MATCH (t)<-[:TAGGED]-()-[:MENTIONED]->(u:User)RETURN u.screen_name, u.name, count(*) as freqORDER BY freq DESC LIMIT 20 u.screen_name   │ u.name               │ freq  realDonaldTrump │ Donald J. Trump      │7853   POTUS           │ President Trump      │6847   stranahan       │ Lee Stranahan        │3778   NatashaBertrand │ Natasha Bertrand     │3686   DarrenKaplan    │ Darren Kaplan        │3685   WayneDupreeShow │ Wayne Dupree         │2299   PeeSparkle      │ PSparkleMAGA         │2088   LVNancy         │ ɳαɳ૮ყ                │1991   PrisonPlanet    │ Paul Joseph Watson   │1789   RedNationRising │ Red Nation Rising    │1593   Cernovich       │ Mike Cernovich       │1519   pnehlen         │ Paul Nehlen          │1474   StefanMolyneux  │ Stefan Molyneux      │1410   TrumpTrain45Pac │ Patriot 24/7         │1143   alozrasT        │ Amy T                │992    Pamela_Moore13  │ Pamela Moore         │936    StockMonsterUSA │ STOCK MONSTER        │817    ReaganBattalion │ The Reagan Battalion │787    _Makada_        │ Makada               │725    ThePatriot143   │  Cris                │575Most RetweetedMATCH (u:User)-[:POSTED]->(t:Tweet)<-[:RETWEETED]-(o:Tweet)RETURN u.screen_name, u.name, count(distinct t) AS tweets, count(*) AS freqORDER BY freq DESC LIMIT 10 u.screen_name   │ u.name              │ tweets │ freq  Cernovich       │ Mike Cernovich      │21      │8792   StefanMolyneux  │ Stefan Molyneux     │6       │3895   DarrenKaplan    │ Darren Kaplan       │1       │3737   stranahan       │ Lee Stranahan       │17      │3721   passionatechica │ ᗷᗩᔕᗴᗪ ᑭᖇᎥᔕᑕᎥᒪᒪᗩ    │3       │3683   AmyMek          │ Withheld account    │2       │3117   LVNancy         │ ɳαɳ૮ყ               │5       │2882   JackPosobiec    │ Jack Posobiec       │1       │2711   WayneDupreeShow │ Wayne Dupree        │7       │2607   StockMonsterUSA │ STOCK MONSTER       │3       │2572Interestingly Darren Kaplan shows up here. Why? Because his tweet about the hashtag driving bot net” got retweeted and favorited so oftenMost Active Accountsmatch (n:User)return n.screen_name, n.name, n.location, size( (n)-[:POSTED]->() ) as activityorder by activity desc limit 20 n.screen_name   │ n.name               │ n.location      │ activity  IRISHHEAVYT     │ HEAVY T              │                 │179        thatgirlsandra5 │ Trump 2020           │ Florida, USA    │157        TheGoodGuy2017b │ Anti-Globalist       │                 │155        BoycottHRC      │ John Durrant         │                 │133        MarieMa49685063 │ I ️Winning            │ TrumpTrain, USA │126        unablogger      │ Una Blogger          │ Los Angeles     │120        MagaNavajo      │ MAGA                 │ Main St. USA    │118        GeneralDefense  │ General Defense      │ United States   │105        uliw315         │ Anonymous Source     │                 │100        clint4usa       │ BringBackFlynn       │                 │98If we look at the distribution of activity across users, i.e. how many tweets were posted / retweeted / replied to by a user. We see that there is a common power-law distribution of activity.MATCH (u:User)RETURN size((u)-->(:Tweet)) as activity, count(*)ORDER BY activity ASC LIMIT 100We can see that most (31k) users only had a few interactions (<10) and only 1400 had more than that.MATCH (u:User)RETURN size((u)-->(:Tweet)) < 10 as lessThan10, count(*)Something we don’t have is followship between these accounts and which other interactions (outside of the tags we looked for) there were. We could pull the tweets of all these accounts and start to look into this, but I leave that for a later time.Algorithms — Centrality / PageRankcall algo.pageRank(MATCH (u:User) return id(u) as id,MATCH (u:User)-[:POSTED]->()<-[:RETWEETED|REPLIED_TO]-()<-[:POSTED]-(u2:User) return id(u) as source,id(u2) as target,{graph:cypher})MATCH (u:User)RETURN u.name, u.screen_name, u.pagerankORDER BY u.pagerank DESC LIMIT 10│ u.name               │ u.screen_name   │ u.pagerank       │ HEAVY T              │ IRISHHEAVYT     │12.1623955        │ Anti-Globalist       │ TheGoodGuy2017b │5.187134          │ King Eric            │ ericsuniverse   │4.2865675         │ Dani Pereira         │ weblollipop1    │2.7041904999999997│ John Durrant         │ BoycottHRC      │2.5428434999999996│ deborah sidener      │ debbiesidener2  │2.1846535         │ Based chris          │ CJTUCKERTRUPAT  │1.9179149999999998│ Bobby                │ slowbob         │1.8188475         │ ZillaStevenson       │ ZillaStevenson  │1.772446          │ Trump 2020 ‼         │ thatgirlsandra5 │1.7422285PageRank on a mention” Networkcall algo.pageRank(MATCH (u:User) return id(u) as id,MATCH (u:User)-[:POSTED]->()-[:MENTIONED]->(u2:User) return id(u) as source,id(u2) as target,{graph:cypher,writeProperty:mentionRank})MATCH (u:User)RETURN u.name, u.screen_name, u.mentionRank, u.pagerankORDER BY u.mentionRank desc LIMIT 10 u.name              │ u.mentionRank    │ u.pagerank         Natasha Bertrand    │1273.3055510000002│0.15003400000000003 Darren Kaplan       │690.7605669999999 │0.15000000000000002 Donald J. Trump     │416.37035         │0.15000000000000002 Mike Cernovich      │352.29304899999994│0.1650705           Paul Joseph Watson  │220.54896499999998│0.15013600000000002 The Columbia Bugle  │217.4538345       │0.15034850000000002 President Trump     │215.79957299999998│0.15000000000000002 STOCK MONSTER       │192.0740075       │0.15008500000000002 ɳαɳ૮ყ               │156.2747785       │0.15512550000000003 Stefan Molyneux     │116.32824650000002│0.15006800000000003Clustering AlgorithmsWe run the clustering on top of the interaction” network, i.e. people interacting with each others tweets.call algo.unionFind(MATCH (u:User) RETURN id(u) as id,MATCH (u:User)-[:POSTED]->()<-[:RETWEETED|REPLIED_TO]-()<-[:POSTED]-(u2:User) RETURN id(u) as source,id(u2) as target{graph:cypher})Results in 1946 partiions, but almost all people are in the first partition.MATCH (u:User)RETURN u.partition, count(*) as c ORDER BY c DESCLIMIT 10 u.partition │ c   70           │304043657         │27   21563        │6    2721         │4    738          │3    9403         │3    1415         │3    22819        │3    14646        │3    3659         │3So let’s try label propagation instead.call algo.labelPropagation(MATCH (u:User) return id(u) as id,MATCH (u:User)-[:POSTED]->()<-[:RETWEETED|REPLIED_TO]-()<-[:POSTED]-(u2:User) return id(u) as source,id(u2) as target,OUTGOING,{graph:cypher})match (u:User)return count(distinct u.partition) as partitionsResulting in 9786 partitions with a long tail.match (u:User)return u.partition, count(*) as c order by c desclimit 30 u.partition │ c   70           │21368103537       │397  79479        │338  17685        │209  50206        │82   30059        │79   54271        │71   23203        │46   103520       │45   23026        │42We can mark the largest partitions and give them the name of the member with the highest pagerank.match (u:User)with u order by u.pagerank descwith u.partition as p, count(*) as c, collect(u) as users,head(collect(u.screen_name)) as partitionName order by c desclimit 50foreach (u in users |      set u:Group set u.partitionName = partitionName)And render them as a summary” visualization.call apoc.nodes.group([Group],[partitionName]) yield nodes, relationshipsunwind nodes as nreturn n,relationshipsThere is much more possible with this data (esp. if we pull in the followships and the other tweets of the users).This should just give you an idea what is in here.Sorry for the ugly tables but adding proper HTML tables to Medium seems to be no meant to be easy.Free download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Aug 7, 2017;[]
https://medium.com/neo4j/deploying-neo4j-on-kubernetes-with-helm-charts-f08a53cdc2a6;Stu MooreFollowAug 11, 2021·12 min readDeploying Neo4j on Kubernetes with Helm ChartsIt is time for Part 3 of the Neo4j 4.3 technical blog series, where we explore deploying Neo4j with the new neo4j-standalone Helm Chart on Minikube Kubernetes running on a Windows 10 laptop with Docker Desktop.This is our series so far:Relationship / Relationship Property IndexesRelationship Chain Locks: Don’t Block the RockDeploying Neo4j on Kubernetes with Helm Charts (this blog)Server Side RoutingRead Scaling for AnalyticsPhoto by Andrey Sharpilo on UnsplashWhy Minikube on Windows 10?Well the Neo4j documentation is packed full of quick start guides for deploying on the major cloud providers AWS, Azure and GKE and on Docker Desktop on Mac and I wanted to demonstrate how accessible Neo4j is to Windows users outside of the usual options.Don’t worry, if you don’t want to use Windows then you can follow along with the instructions below and those provided in the product docs for your chosen environment. Minikube is optional too, you can skip it and still run with the Kubernetes provided by Docker Desktop.What Are Helm Charts and What Do They Do for Neo4j?For those of you who are unfamiliar with Helm Charts - they provide a prescriptive way of running products like Neo4j on Kubernetes. The author of a Helm Chart has figured out the configuration (and the best way to configure) for the software to run on Kubernetes, exposing the right services for administrators and clients to connect. Essentially it is a best practice” package and configuration management system that will pull down the required image, configure and deploy Neo4j on Kubernetes.Photo by Frank Eiffert on UnsplashNeo4j-standalone Helm Chart, What About Clusters?Please note that this first Chart to be released is for deploying Neo4j on a Kubernetes Cluster running a single Pod, it does not configure and deploy a Causal Cluster.If you are looking to run Neo4j on a Cluster then you will need to continue to use the Chart provided by Neo4j Labs.Installation - Neo4j-Helm User GuideStandalone instances installed in this way cannot be scaled into clusters. If you attempt to scale a standalone system…neo4j.comThe standalone Helm Chart is for use in development / test environments and production deployments when fault tolerance isn’t required. Productized Helm Charts that support cluster deployments are still work in progress.What’s the Difference Between neo4j-standalone and the Chart in Neo4j Labs?On the surface there is little functional difference between the recently released Helm Chart and the lab version — both will deploy and run Neo4j on K8s. However, there are some very important differences when it comes to support as Helm Charts have matured from a Labs incubated innovation project to a fully supported product backed by Neo4j Support team’s SLAs.Product wise there have been extensive changes under the hood too as there has been a substantial investment in making Neo4j more K8s friendly, and embedding best practices from deployments on the major cloud providers plus some great new documentation.Kubernetes - Operations ManualCurrently, the Neo4j product supports Helm charts for a standalone server. If you are interested in working with Neo4j…neo4j.comWhat About an Kubernetes Operator for Running Neo4j on K8s?A Kubernetes operator is a controller that extends the Kubernetes API to provide a control interface that is specific to that application it knows how to create, configure, and manage instances of complex applications on behalf of a Kubernetes.There are no immediate plans to make an operator available for users who want to run self-managed instances of Neo4j.Get Ready to Use Helm Charts to Deploy Neo4j on KubernetesWindows x64 systems: Version 1903 or higher, with Build 18362 or higher (required on Windows)WSL — Windows Subsystem for Linux 2.0 (required on Windows)Docker for Desktop with Kubernetes integration enabled (required all for all operating systems)Minikube (optional)I have introduced Minikube into the mix as well because the distro includes a great dashboard which is a handy way to explore Kubernetes if you are brand new to it.Note: Neo4j doesn’t officially support Minikube as a platform.First up, before you install Docker for Desktop, check that you are running the latest Windows Subsystem for Linux (WSL) 2.0 — if you have already installed Docker for Desktop dont worry the order doesn’t matter you will just need to restart Docker after you have upgraded WSL.Install/Update WSLLaunch a PowerShell command prompt as Administrator (required for all the command line software installs), and enable WSLdism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /allUsing Dsim to enable WSLRun the wsl.exe -l -v command to check the version, in the screenshot below you can see that I am running version 2 for docker-desktop.wsl version commandIf it says version 1 (one) then you will need to upgrade, before you attempt to do this check the version of Windows 10 that you are running because WSL 2.0 only runs on the following versionsFor x64 systems: Version 1903 or higher, with Build 18362 or higher.For builds prior to 18362 you must use the Windows Update Assistant to update your version of Windows first because WSL 2.0 isn’t supported.Full details to upgrade to v2.0, links to downloads and commands to set WSL 2.0 as the default are available here.Install DockerTo install Docker Desktop jump over here to download it. It is pretty straightforward, it has the usual UI based installer… just double click and run from your Downloads folder and accept all the defaults and start Docker Desktop.Next you will need to configure WSL integration in Docker Desktop. Select Settings | Resources | WSL INTEGRATION to proceed.Docker Desktop Resource Settings for WSL integrationAgain dont worry you won’t see Ubuntu displayed, that is only present because I have a specific distro installed, and it isn’t required for the blog.Install MinikubeTo install Minikube (optional if you want to use the dashboard to become more familiar with Kubernetes setup — more on this later) and Helm I used Chocolatey (choco) — a really handy package manager for Windows — feel free to use whatever one you prefer.Chocolatey - The package manager for WindowsChocolatey has the largest online registry of Windows packages. Chocolatey packages encapsulate everything required to…chocolatey.orgTo install Minikube and Helm make sure you are still running the PowerShell command prompt as Admin.Installing Minikube with ChocoInstall HelmSame steps for installing kubernetes-helm.Installing Helm with ChocoYou can now exit from the Admin PowerShell, everything else should be run as a standard user.Getting StartedStart Minikube so that it configures Kubernetes’ kubectl to use the minikube cluster and the default namespacestart minikubeMinikube starting up and configuring kubectlLaunch the minikube dashboard to check out your new kubernetes environment, where you will find the default service for kubernetes, and the default config map.minikube dashboardMinikube’s Dashboard showing Service & Config Map configurationGet Set and Deploy Neo4j Using Helm Charts on KubernetesFirst, add the Neo4j Helm charts repository.helm repo add neo4j https://helm.neo4j.com/neo4jGet the latest charts from the chart repository:helm repo updateUpdating the Helm repo with Neo4j’s chartsView the available charts:helm search repo neo4j --versionsVersions of Neo4j’s Helm ChartsEach individual Neo4j instance in Helm is called a release” and the objects created in Kubernetes are assigned a release name.If Helm needs to create any other objects within the Kubernetes environment then it will derive the name from this release name. The release name must consist of lower case alphanumeric characters, — or ., and must start and end with an alphanumeric character.Set the release name as an environment variable, so that it is available in the rest of the session:set RELEASE_NAME=nebuchadnezzarIf you have an Enterprise license and care about performance then you will need to override the defaults provided in the Helm Chart to switch to Enterprise Edition with some custom settings specified in a YAML file (as expected).Overriding defaults in the neo4j.conf using a custom YAML file is an important technique to be familiar with, so even if you don’t switch to Enterprise Edition, I recommend you try specifying your own password, or tune the memory / CPUs.Caution! Watch the formatting in the YAML file, like Python it relies on the use spaces for each section.Copy and paste the following into a file called custom-values.yamlneo4j: password: yourchosenpassword” resources:  cpu: 2”  memory: 5Gi”  # Neo4j Edition to use (community|enterprise) the default is   # community so only change this if you have a license  edition: enterprise”  # set edition: enterprise” to use Neo4j Enterprise Edition  # To use Neo4j Enterprise Edition you must have a Neo4j license   # agreement.  #  # More information is also available at: neo4j.com/licensing  # Inquiries can be directed to: https://neo4j.com/contact-us/  #  # Set acceptLicenseAgreement: yes” to confirm that you have a   # Neo4j license agreement  acceptLicenseAgreement: yes”volumes: data: mode: defaultStorageClass” # Neo4j configuration (yaml format) config:  dbms.default_database: neo4j”  dbms.config.strict_validation: true”Then install the release with helm and provide your confighelm install %RELEASE_NAME% neo4j/neo4j-standalone --set volumes.data.mode=defaultStorageClass -f custom-values.yamlThe set volumes.data.node configures the persistent volume claim to use the default storage specification in the yaml — allocates 10GB of persistent storage.Pod rollout using the Helm ChartCaution! If you don’t specify your own password in the custom-values.yaml then the Helm Chart will create a secure random password for you. If it does please make a note of the password in your favourite password manager, or change it to something more memorable when you login for the first time.That’s it! You are up and running on a single Kubernetes Pod. It will take a couple of minutes before you can log in so run the rollout status command suggested in the console outputkubectl rollout status --watch —-timeout=600s statefulset/nebuchadnezzarThis command will report back when the rollout is complete.Neo4j Pod Deployment in Minikube DashboardIn the meantime switch back to the minikube dashboard to see that you have a workload status for pods and your stateful sets. Under Pods it will show your release name is running in the default namespace and that the stateful sets are managed by Helm.Minikube Dashboard showing the Pod up and runningScrolling down you can see a full list of the services (described here) show Neo4j running on a Kubernetes Pod that the Helm Chart has created for Neo4j some of them are available only from inside the Kubernetes cluster, others are accessible from outside the cluster.Services created on KubernetesIf you select the Config and Storage | Config Maps and the default-config & env you can see the configuration that has been passed in to set up Neo4j.Minikube displaying the Neo4j Config MapsAccess Neo4j on KubernetesI am going to show you a couple of different ways of accessing Neo4j, the first is from inside the Kubernetes cluster, the second is from outside the cluster.From inside the cluster, running on another pod by running the cypher shell — execute the command shown in the output when Helm rolled out the pod running Neo4jkubectl run -rm -it -image neo4j:4.3.2-enterprise” cypher-shell --cypher-shell -a neo4j://nebuchadnezzar.default.svc.cluster.local:7687” -u neo4j -p xxxxxxxx”Cypher-shell running on a Kubernetes PodNote! the use of nebuchadnezzar.default.svc.cluster.local, this is because the Pod is running inside the cluster.If we want to access the Neo4j service from outside of the cluster i.e. using localhost on your laptop you will need to set up port forwardingkubectl port-forward svc/nebuchadnezzar tcp-bolt tcp-http tcp-httpsNote! Remember to replace nebuchadnezzar with your chosen releasename.Now you can launch Neo4j Browser on http://127.0.0.1:7474/browser/Unfortunately we are unable to access the Neo4j service from outside of the cluster using a static IP on your home network this is because neither Docker Desktop or Minikube provide a load balancer that supports this — you will need to do this on Azure, AWS or GCP if you want to test out external access.Loading up Neo4jYou can copy files from a local directory on your laptop into the import volume on the Pod running Neo4j using kubectl cp. The following example shows how to copy a backup dump file of the Neo4j database called network-management-4.3.dump available publicly here to the /import directory on a Neo4j instance with the release name nebuchadnezzar in the default namespace.Note! You can copy in CSV files for use with neo4j-admin import, or with Cypher’s LOAD CSV as well.Caution! The syntax for the file path isn’t the usual backslashes on Windows but forward slashes like in Linux and you need to append -0 (zero) on the release name to access the pod.kubectl cp Documents/data/network-management-43.dump default/nebuchadnezzar-0:/importYou can check they are there withkubectl exec nebuchadnezzar-0 -- ls /import/Now load up the dump into a database called networkkubectl exec nebuchadnezzar-0 --namespace default -- neo4j-admin load -from=/import/network-management-43.dump -database=networkLoading the dump of the network databaseIf you are new to Neo4j it is worth pointing out that even though you have done the import and the files exist on disk the logical database does not technically exist yet.You need to create the database with the create database network command in the system db using cypher-shell. To do this, create a pod using the enterprise image to run the cypher-shell (it gets thrown away when you exit)kubectl run --rm -it --image  neo4j:4.3.2-enterprise  cypher-shell -- cypher-shell -a  neo4j://nebuchadnezzar.default.svc.cluster.local:7687  -u neo4j -p xxxxxxxxx” -d systemshow databasesShow database command highlights the network database doesnt exist yetNow create the network database, which associates it with all the files that are on diskcreate database networkNetwork database existsIf we switch over to the browser you can run :use network and thenMATCH (n) RETURN n LIMIT 100 to see the data exists in your new database.Browser displaying some of the network in the Network databaseThanks for sticking around to the end to load up your database, look out for the server side routing blog in a couple of weeks.Debugging and Troubleshooting TipsThere are some great troubleshooting tips and tricks in the documentation including lots of commands for displaying the contents of the log.Troubleshooting - Operations ManualTroubleshooting information that can help you diagnose and correct a problem. The rollout of the Neo4j Helm chart in…neo4j.comIf you get errors while installing or setting up the environment then I would recommend just uninstalling the following and trying again making sure you have completed all the intermediate stepsDocker for DesktopHelmMinikubeDon’t worry it really doesn’t take long… before you start reinstalling make sure you have WSL 2.0 installed and enabled at the OS level, after Docker Desktop is installed make sure it is selected within the Docker for Desktop configuration settings.If you get as far as installing the Helm chart and you want to clean up and rebuild Neo4j on Kubernetes either because something went wrong or you just want to try a different Neo4j configuration then you need to uninstall the release AND delete the persistent volume which is storing the database.helm uninstall nebuchadnezzarThen you need to delete the persistent volumekubectl delete pvc data-nebuchadnezzar-0Unable to login with password?If you receive the The client is unauthorized due to authentication failure.” Then you have probably forgotten to delete the persistent volume claim (pvc) and the old database is hanging around.;Aug 11, 2021;[]
https://medium.com/neo4j/release-your-inner-graph-nerd-at-nodes-2020-c497ae002b42;Karin WolokFollowSep 30, 2020·3 min readRelease Your Inner Graph-Nerd at NODES 2020!Yes, it’s true. Graph-nerds from around the globe come together once again!The Neo4j Online Developer Expo & Summit (NODES 2020) is the largest virtual graph conference ever. With 9-hours of content, across 6-tracks (50+ hours of total content!), you’re sure to get your yearly dose of graph-y content (well…maybe, as some of us have serioussss addictions and can never get enough graph)!We don’t need to tell you how awesome graphs are because chances are, you already know. So, we’ll just give you a run down of what you can expect.Technical TalksStrengthen your skills as a subject-matter expert by tuning into talks from 70+ graph experts from around the globe.Tracks include use cases, visualization, knowledge graphs, graph data science, building applications, as well as a deep dive track with some of our Neo4j experts.The agenda has already been announced and you can start building your schedule now! :DSource: https://neo4j.com/blog/dark-side-neo4j-worst-practices/Connect the Global Graph Community!Neo4j has the largest graph community, so you’ll be amongst thousands of fellow graph enthusiasts from around the globe.And, it doesn’t end there!A bunch of technical communities with varied focuses and interests areas (data science, Javascript, Python, analytics, AI, cloud, etc.) have signed up to participate as well!Whatever technical topics you’re into, chances are, you will have a lot of other friends with similar interests! :DVirtual Expo HallChat with some of the most innovative technology companies doing cool things with graphs in the virtual expo booth. See live demos and chat live with industry experts!Certified Professionals TrainingIf attending the conference isn’t enough for you, and you can access hands-on, live, advanced training sessions on graph-topics.Between October 6–8 and 13–15, Neo4j will run five 4-hour virtual training sessions available totally free to Neo4j Certified Professionals!If you’re not yet certified in Neo4j, you can take the 1-hour exam today (the exam is also free!) and get access to the training sessions.We look forward to seeing you all!If you have any questions, please email us at events@neo4j.com;Sep 30, 2020;[]
https://medium.com/neo4j/angular-and-neo4j-graphql-the-nest-js-backend-a20ffb304698;Konrad KalicińskiFollowMar 2·7 min readAngular With @neo4j/graphql and the Nest.js BackendSetting up your stack ready for work can be time-consuming and relatively boring. In this tutorial, I’ll try to show how simple it can be with the @neo4j/graphql library.neo4j cypher query showing movies and actorsIn my previous article, you could learn how to set up @neo4j/graphql with the NestJS backend framework. So now let’s create an Angular frontend on top of that and organize everything with NX monorepo.I will try to show how quickly you can start building an entire app with @neo4j/graphql used on the backend.InitializationYou can start by cloning the repo and following step by step along this tutorial. Before installing, just make sure you create your .env file and set up USE_AUTH to 0 to skip AWS Cognito authentication.The starter app you can find in the repository, on the branch called blank”. When you clone the project and hit npm install, you’ll have:NestJS backend with movieNeo4j database (docker)Angular front-end appQuestions to AnswerWe can use one of the datasets that Neo4j provides, for example, a movie database. Let’s think for a second: What problems do we want to resolve with our system? Well, we would like to inform our users about some movies, by providing data for the pages below:A page with a list of all available movies with a search (filter list of movies by actors and/or year)A movie page with all its data ( producer, director, and all the actors)A page to list a person’s data (movies she/he acted in, directed, and the movies producedPrepare the BackendTo populate the database with sample movie data, just go to Neo4j browser (you can either open localhost:7474 if you are running a local database, or try our online hosted version at browser.neo4j.io and run the :play movie-graph command.GraphQL ModelsAs you may already know, Neo4j is built with nodes (data pieces) and relationships (how data pieces are related to each other). Nodes can have labels (to describe the data type). Both nodes and relationships can have properties to provide more information. That’s it.In our example, we have nodes described as Person and Movie. The relations describe how they are connected.# Nodes and Relationshipsnode:Person node:Movie relation:ACTED_INrelation:DIRECTEDrelation:PRODUCED# Relationships directionsPerson -DIRECTED-> Movie Person -PRODUCED-> Movie Person -ACTED_IN-> MovieAlong with some properties describing movies and people, we have basic tools to describe our data.You can find the model definition in the file libs/gql/src/lib/type-defs.ts . If you know how to read GraphQL, you probably understand what’s there. Otherwise, let’s go through it.There are two types defined, Movie and Person. Each type contains some basic fields — Person: name, born and Movie: title, released, and tagline. Additionally, they have fields described with a @relationship directive — we use it for fields defined by relationships. For example, actors are defined by ACTED_IN relationship, and we can create a field that will represent all Person nodes with a relationship of type ACTED_IN. Additionally, our relationship has its own properties defined in the ActedIn interface. It contains a list of Person roles in the given movie. As in the snippet below:// type-defs.tsimport { gql } from apollo-server-expressexport const typeDefs = gql`    type Person {        name: String!        born: Int        actedIn: [Movie!]! @relationship(type:  ACTED_IN , properties:  ActedIn , direction: OUT)        directed: [Movie!]! @relationship(type:  DIRECTED , direction: OUT)        produced: [Movie!]! @relationship(type:  PRODUCED , direction: OUT)    }    type Movie {        title: String!        released: Int!        tagline: String        actors: [Person!]! @relationship(type:  ACTED_IN , properties:  ActedIn , direction: IN)        directedBy: [Person!]! @relationship(type:  DIRECTED , direction: IN)        producedBy: [Person!]! @relationship(type:  PRODUCED , direction: IN)    }    interface ActedIn @relationshipProperties {        roles: [String!]    }`Building Angular ApplicationSince we are building our application in the NX monorepo, let’s put our functionality in the movie library.nx generate @nrwl/angular:library --name=movies --style=scss --addModuleSpec --changeDetection=OnPush --importPath=movies --lazy --routing --simpleModuleName --standaloneConfigInside the created library, we will create a MovieModule that will contain three pages, plus a few components, routing and services. We will focus only on the GraphQL part, and we skip the entire components creation part. You can check in the repo how they are implemented. I just quickly describe what is where:movies/    components/    // presentational components    pages/         // this contains page components    queries/       // graphql queries and mutations    services/      // business logic    types/         // types used in our module, hopefully self-describing    lib.routes.ts  // routes definitionsThe entire codebase for this blog post can be found on the branch movies-graphql (link). All links can be found at the bottom of the article.So the database is populated, and components are ready, but if you click through, you’ll quickly see that the data is static. So, let’s jump into the code to connect the real data. We need to build queries and use them in our service to fetch the data and we’re good.Create QueriesWe need to create three queries for each page: movie list, movie details, and person details. Let’s start with the easiest one:We want to display a movie title and its tagline. Here is the query we can try in our sandbox in localhost:3333/graphql. It will return a list of all node movies with properties title and tagline from the database.query {  movies {    title    tagline  }}Let’s move that in a typescript file so we will be able to import it whenever we need it:import { gql } from  @apollo/client/core  export const GET_MOVIES = gql(` query {   movies {     title     tagline   } } `)The last thing is to replace the getAllMovies() method in the MovieService:@Injectable({providedIn:  root }) export class MovieService {     constructor(private readonly apollo: Apollo) {   }     getAllMovies(): Observable<MovieListItem[]> {     return this.apollo       .watchQuery<any>({         query: GET_MOVIES,       })       .valueChanges.pipe(         map(response => {           return response.data.movies         })       )   }That was easy. Now, if we click on the persons name (either actor, director, or producer), we will need to make a query based on the person’s name. Assuming we get the name from the URL, the GraphQL query would look like this:query {       people(where: {name:  Tom Hanks }) {           name           born           actedIn {               title           }           directed {               title           }           produced {               title           }       }   }Take a look at the related items. There are relationships mapped to properties and combined into arrays: actedIn, directed, produced — as we defined in the type definition file.Now, let’s transform that into a format that will work in the application. We should also update the query to provide the name parameter, making the query more flexible.import { gql } from  @apollo/client/core export const GET_PERSON_DETAILS = gql`    query PersonDetails($name: String) {        people(where: {name: $name}) {            name            born            actedIn{                title            }            directed{                title            }            produced{                title            }        }    }`This should go in the service MovieServive under the getPersonByName method:getPersonByName(name: string) {    return this.apollo      .watchQuery<any>({        query: GET_PERSON_DETAILS,        variables: {          name: name        },      })      .valueChanges.pipe(        map(response => {          const {data } = response          return data.people[0]        }),      )  }Since our result is an array, we want to take only the first item — that is handled in the mapping function. This service method is called in the page component.For the last query, we want to get the full movie info, including the actors and roles they played in the given movie. Since a role is described as a relationship property, the query will look a little bit different.movies(where: {title: $title}) {      title      tagline      released      directedBy {        name        born      }      producedBy {        name        born      }      actorsConnection {        edges {          node {            name          }          roles        }      }}If we would want to display only actors, then we could choose the actors property. However, we’d like to display the roles that actors play, and the role is described in the properties of the ACTED_IN relationship.The example below shows it’s the best:(p:Person)-[a:ACTED_IN]->(m:Movie) WHERE m.title = ‘Cloud Atlas’With our query, we display all (:Person) nodes that are connected with the (:Movie) node with a particular title, and in the right sidebar, we can see the properties of the selected [:ACTED_IN] relationship.So we have four actors that acted in the movie, but each actor performed different roles.Our GraphQL library allows for creating such queries that have relation properties.We could get just actors, but that would not give us full information. This can be easily compared in the query below — in the actors property we want to get all actor names, but to get more, we have to reach actorsConnection field and define the edges (relationships in Neo4j terms), with its properties (roles) and additionally to have the context of the (:Actor) node property.Take a look at the query below:As you can see, you can get more context from the query.ConclusionNeo4j provides a flexible tool that can be set up for use in no time.I have shown you only a tiny part of it, but it’s definitely worth taking a closer look at it since it gives you much more than just that.With proper type configuration, you may create JWT authentication, role-based authorization, subscriptions, custom resolvers, and much more.LinksThe @neo4j/graphql documentation.Repository: https://github.com/akkonrad/nx-gann-starterkit;Mar 2, 2023;[]
https://medium.com/neo4j/the-power-of-subqueries-in-neo4j-4-x-4f1888739bec;Michael HungerFollowJul 16, 2020·8 min readMatryoshkas by Iza Gawrych on UnsplashThe Power of Subqueries in Neo4j 4.xNest your Cypher statements with subqueries!IntroductionNeo4j 4.1 came out in June with the final piece required to make subqueries work well in Neo4j — So I wanted to use the opportunity for a deep dive into the topic.For those of you who prefer to watch than read, I did a Twitch session on this topic last week. We have regular schedule of live streaming, do check out the calendar. Follow us there or YouTube to be notified when we go live!Twitch Session on Subqueries in Neo4j 4.1I have to start with a disclaimer: In all the years writing SQL with a variety of databases, I have always disliked subqueries. I felt that they made queries harder to read, understand, and to reason about.UpdateMy colleague Andrew Bowman created a series of very helpful knowledge base articles for this topic.HistoryThat’s why I always vetoed subqueries in Neo4j’s Cypher, and rather, pushed for a query pipeline that passes data from one query part to the next. Much like a Unix shell pipe that connects small tools, each one that does one job well with each other, like Lego blocks to form a more complex process.In Cypher you achieve this with the WITH statement, allowing you to chain query parts together. WITH has the same semantics as the final RETURN, you can select columns, rename, aggregate, filter, paginate and sort your data. And all that WITHIN each query multiple times.That is also the reason we never needed a HAVING keyword in Cypher, as it is just glorified post-filter after your result selection. You can have as many of those intermediate result-processing-passing steps in your Cypher query as you want.Here is a quick example. Get me the cast of the movies with the highest ratings in our dataset.// this is the first query partMATCH (m:Movie)<-[r:RATED]-()// this is the inline processing + passing of dataWITH m, avg(r.rating) AS ratingORDER BY rating DESC LIMIT 5// this is the next query part afterMATCH (m)<-[:ACTED_IN]-(a:Person)RETURN m.title, collect(a.name) AS cast, ratingThat’s also the approach you can use for handling many of the traditional” subqueries. Built-in list processing, and an exists( (n)-[:REL]->(m) ) expression for patterns manages other use-cases.SubqueriesHowever, subqueries support two use-cases that were not so easy to handle before.One was a long-time requested feature called post-union-processing. A UNION (ALL) construct combines multiple independent queries into one result.MATCH (:Person) RETURN person AS type, count(*) AS countUNION ALLMATCH (:Movie)  RETURN movie AS type, count(*) AS countBut UNION itself is not a full query, i.e. you can’t do anything else except combine those queries, and have their (identically named) columns returned to the caller in any order. So in the past, you had to do any post-processing client-side. (or with apoc.cypher.run()).Neo4j 4.0 introduced subqueries that addressed this issue. With the new CALL {} syntax, you can wrap a statement part as a subquery, and its result and cardinality will be available in the subsequent parts.MATCH (q:Question)CALL { RETURN 42 AS answer }RETURN q.text AS question, answerThis calls the subquery which returns 42 as an answer, which is then available in the outer statement, for each question found.To apply this to the UNION in question, you would wrap that statement as a subquery, and then can filter, sort, paginate (or do whatever) with the results.//UNION wrapped in subqueryCALL {  MATCH (:Person) RETURN person AS type, count(*) AS count  UNION ALL  MATCH (:Movie)  RETURN movie AS type, count(*) AS count}// post union processingWITH * WHERE count > 100RETURN type, count ORDER BY count DESC LIMIT 5NOTEPlease note that the subquery functionality in Neo4j 4.0 was not able to access variables defined outside of their scope (non-correlated) . That has only changed in 4.1, see below.My colleague Mark wrote a great developer guide on subqueries and Luanne Misquitta from our partner GraphAware dove into this in a full blog post on post-union-processing.Existential SubqueriesExistential subqueries are a special construct, a partial subquery to test the existence of certain graph patterns. Before, you could express patterns with the existsfunction/predicate.Now with existential subqueries the syntax is one of these three:WHERE exists { (node)-[:REL]->…​(:Label) }, which is a shorthand forWHERE exists { MATCH (node)-[:REL]->…​(:Label) }, which can be extended with WHEREWHERE exists { MATCH (node)-[:REL]->…​(:Label) WHERE expression …​ }It cannot contain RETURN, WITH, aggregations or other operations.NOTECurrently the existential subquery cannot be used as an expression, I think that’s a bug and will be fixed.Navigating along sub-treesWhen fetching data that contained many disparate sub-trees, the WITH and aggregation approach became a bit cumbersome, as your query was not just a pipe for processing, but a tree of nested sub-parts.For example, if you want to get the co-actors of an actor in their shared movies, but also at the same time, the directors and their movies in a single query, you’re navigating two very distinct subtrees of your data. This was possible with multiple (OPTIONAL) MATCH expressions and incremental aggregation of map data structures, but not trivial.(actor)-->(movie)<--(co-actors)(actor)-->(movie)<--(director)-->(other-movies)Actress Helen Hunt with the two subtrees of co-actors and directorsIn Neo4j 3.1 Andres Taylor, the father of Cypher, sneaked in two really cool features that were inspired by GraphQL and made this kind of querying so much easier.First there are pattern comprehensions. Those are like list comprehensions, but allow an expression to use a graph pattern (with new identifiers) to be filtered, and then have an expression applied to each element [ (pattern…​) WHERE filter | expression].These two return equivalent results:MATCH (a:Person)-[:ACTED_IN]->(m:Movie)OPTIONAL MATCH (m)<-[:ACTED_IN]-(coActor:Person)WHERE coActor.name contains TRETURN a.name, collect(coActor.name) AS coActorsandMATCH (a:Person)-[:ACTED_IN]->(m:Movie)RETURN a.name,  [(m)<-[:ACTED_IN]-(coActor:Person)   WHERE coActor.name contains T | coActor.name] AS coActorsWhich is very cool as it doesn’t change the cardinality of your query like aMATCH would, counteracting with an aggregation like collect or count.And then map projections. These can take a map-like element (map, node, relationship), and extract attributes in a concise syntax into a map again: elem {.foo, .bar, .* , answer: 42}.So you can sub-select the parts you are interested in, and then with the regular key:expression syntax you can start nesting the two together.So our second tree” can be expressed as:[ (m)<-[:DIRECTED]-(d:Person)  | d { .name, movies:      [ (d)-[:DIRECTED]->(m2) | m2 {.title} ]] AS directorsThese features are especially useful if you want to regularly query nested structures with defined sub-tree selections, like in any object-graph-mapping tool, such as our GraphQL integration or Spring Data Neo4j.So I wrote up this concept as a dedicated blog post a while ago:Loading Graph Data for An Object Graph Mapper or GraphQLAn interesting way to load complex data structures from a graph into objects is using nested pattern comprehensions in…medium.comThe drawback is that those features are not optimized well by the query planner, and also don’t offer support for sorting and pagination. You can paginate by just applying a slice [0..10] after the fact, but that is only applied after the full comprehension has been computed. Also, sorting has to be simulated with a user defined function in apoc apoc.coll.sortMaps().Full SubqueriesThis is where full (or correlated) subqueries come in, which were added in Neo4j 4.1. These subqueries can now also access identifiers/variables from the outer scope, but those need to be declared explicitly at the beginning of the subquery (i.e. using WITH a,b,c).I personally think that syntax is not optimal — it would have been nicer to align subquery and procedure call syntax, and treat them like parameters. Such a subquery can contain a full Cypher statement with all clauses and operations.Here is our full example:MATCH (a:Person)-[:ACTED_IN]->(m)CALL {    WITH a,m    MATCH (m)<-[:ACTED_IN]-(co:Person)    WHERE a <> co AND co.name contains T    WITH distinct co LIMIT 10    RETURN collect(co.name) AS coactors}CALL {    WITH m    MATCH (m)<-[:DIRECTED]-(d:Person)-[:DIRECTED]->(m2)    WITH  d, collect(m2.title) AS movies    RETURN collect(d {.name, movies:movies}) AS directors}RETURN a.name, m.title, coactors, directorsHere is one row where both subqueries return a result, unfortunately a not great movie.WARNINGThe cardinality of a subquery can affect the outer query. If it doesn’t return any rows, the outer query will not return any rows. The opposite is also true, if your subquery returns multiple rows, then that will multiply the cardinality of your outer query. That’s why you should either use OPTIONAL MATCH in your subquery and/or a pure (single) aggregation with collect, so you always get one row from a subquery (except if you want more).You cannot shadow existing identifiers. If you want to return something that was passed in, you have to rename it. Identifiers returned from a subquery are named as is. If you have an expression, the name of the identifier will be the same, e.g. n.value + 5, so make sure to always alias them properly, even if it’s not enforced (which is, in my humble opinion, an oversight, in WITH we enforced it back then).Subqueries can be nested, and are planned better (like regular query parts) by the query analyzer. Within subqueries, you can now also use sorting and pagination, so our workarounds for GraphQL and Spring Data Neo4j will not be necessary any longer, starting with this version.MATCH (m:Movie)RETURN m { .title, actors: apoc.coll.sortMaps(              [(m)<-[:ACTED_IN]-(a:Person) | a {.name}], [ name ]            )[0..10]} AS moviecan become:MATCH (m:Movie)CALL {    WITH m    MATCH (m)<-[:ACTED_IN]-(a:Person)    WITH a ORDER BY a.name DESC LIMIT 10    RETURN collect(a {.name}) AS actors}RETURN m { .title, actors: actors }Personally, I still like the pattern comprehension syntax much more. Perhaps at some point it can be extended with ORDER BY and LIMIT semantics as syntactic sugar for subqueries.UpdatesYou can also do updates in a subquery. Remember that the subquery is executed each time for an outer row (with the same cardinality), so watch out for that (e.g. use MERGE instead of CREATE).If you want to use subqueries for conditional updates, you should always return an aggregation, to make sure to not stop the outer query.MATCH (m:Movie)CALL {   WITH m    // conditional execution   WITH m WHERE exists(m.genres)   UNWIND m.genres AS genre   MERGE (g:Genre {name: genre})   MERGE (m)-[:IN_GENRE]->(g)   RETURN count(*) AS c}RETURN m, cSubqueries in Neo4j FabricFinally, subqueries are also used in Neo4j’s sharding and federation approach — Neo4j Fabric” — for delineating query parts that are meant to execute on different databases.Here you can use subqueries together with the USE keyword to direct a query part to a certain database.For example (from the Fabric documentation):CALL {  USE movies.moviesUSA  MATCH (movie:Movie)  RETURN max(movie.released) AS usLatest}CALL {  USE movies.moviesEU  WITH usLatest  MATCH (movie:Movie)  WHERE movie.released = usLatest  RETURN movie}RETURN movieThis uses a fabric database named movie with two mounted shards/databases called moviesUSA and moviesEU.ReferencesPlease ask your question in our community forum or SlackRead the developer guide on subqueriesOr the reference documentationHappy querying!;Jul 16, 2020;[]
https://medium.com/neo4j/tailor-made-neo4j-connectivity-with-spring-boot-2-4-281876ef5f46;Michael SimonsFollowFeb 11, 2022·3 min readTailor-Made Neo4j Connectivity With Spring Boot 2.4+Spring Boot has support for a broad range of properties to configure the Neo4j Java Driver since Spring Boot 2.4. The Neo4j team worked together with VMWare engineers to make this as integrated as possible.What does that mean? You can either go to start.spring.io and add Neo4j” as a dependency which will give you the driver plus Spring Data Neo4j.Spring InitializrInitializr generates spring boot project with just what you need to start quickly!start.spring.ioor you can add the driver dependency manually like this:Please note that the version number should be omitted when you are using Spring Boots provided parentorg.springframework.boot:spring-boot-starter-parent .Why? Because Spring Boot manages dependencies for your project, among them, the Neo4j Java driver. Thus, you always have the latest, compatible version.In the rare cases you need to change, use the following property: <neo4j-java-driver.version>4.4.3</neo4j-java-driver.version> .This does not only apply to our dependency, but in general: In case a dependency comes with more than one artefact, Spring Boots dependency management makes sure you catch them all. If you are using Gradle, the same principles apply.You don’t need to write any code to configure the Neo4j connection in the vast majority of use cases. For all the basic needs, just enter three essential properties (URL, username, and password) and you’re good to go:Spring Boot Properties for Neo4jYou don’t have to write a single line of config code yourself to configure any of this. This is important, as we often see a lot of code written to just do something that is already supported out of the box.If you roll out your own configuration, most of the time you will lose benefits such as: Validation of properties and properties that are agnostic to the sources (the configuration shown above can originate from property files, YAML files, environment variables, command line parameters or config servers, just to name a few).There are only a small number of things you can’t configure from properties. How to deal with those situations? One way is to inject org.springframework.boot.autoconfigure.neo4j.Neo4jProperties into an@Bean annotated method on a@Configurationclass and first construct the org.neo4j.driver.Config and then the database driver yourself and return the latter as a bean.This way you keep Spring Boot’s awesome configuration sources and can reuse the standard properties. However, that’s a lot of work to do and you are in danger of forgetting one essential property to copy over to that manual config.Tailor-Made All the ThingsPhoto by Beatriz Moraes on UnsplashThe ConfigBuilderCustomizer is the correct way to go. It’s an official part of Spring Boot. Provide it as Spring Bean and it will receive a builder for configuration required by the Neo4j Java driver with all properties applied that still allows adding new ones or even overwriting some. In a contrived example, that would look like this, showing a couple of additional options for which we don’t provide Spring configuration properties out of the box:Happy hacking and make sure you try out Neo4j AuraDB, our cloud offering which works from a broad range of ecosystems, including Java and Spring Boot.;Feb 11, 2022;[]
https://medium.com/neo4j/building-a-graph-with-neo4j-http-api-powershell-464c75568312;Sinister China PenguinFollowJun 22, 2018·3 min readBuilding a Graph with neo4j HTTP API & PowerShellIntroductionI’m quite new to neo4j & one of the things immediately wanted to achieve was to create a Graph based on one of my relational DB’s.I work in a .Net Dev environment, so I wanted to use the neo4j HTTP API with PowerShell as a bridge between the worlds of SQL Server & Graph.(There is also the neo4j BOLT protocol — I’m planning on exploring that next)This is a really simple example of how I’ve used PowerShell to access the HTTP API. I’ve purposely left out error trapping to keep the script short.T-SQL\ADO code to read in from a relational source can be added later.The CodeIf you want a quick example to hack it to your own ends, here it is….Create a Neo Node with PowerShell….Detailsneo4j HTTP EndpointOn a basic level the neo4j HTTP API allows us to pass a JSON payload containing CYPHER commands to neo4j for execution.It’s fully documented here.There are just 4 basic steps to using the HTTP API with PowerShell:· Create credentials to authenticate with the API· Build JSON payload containing CYPHER commands· POST JSON payload to the neo HTTP Endpoint· Check ResultsCredentialsWe set the neo4j ID & Password at the top of the script, unless your password happens to be EatNotCheese” the you will need to change this!PowerShell uses a credential object to store authentication details, we simply instantiate a credential object & use the ConvertTo-SecureString cmdlet to store the password as a secure string.$secPasswd = ConvertTo-SecureString $pw -AsPlainText -ForceFig 1. EatNotCheese” as a secure stringClearly a plain text password in code is a bad thing, PowerShell documentation details how to perform these operations more securely with encryption.Next we create a PSCredential Object passing in our neo4j ID & secure string” password.$neo4jCreds = New-Object System.Management.Automation.PSCredential (‘neo4j’, $secPasswd)Fig 2. Credentials good to go!Build JSON PayloadI’ve built the JSON in a text string for this simple example. I would steer away from the ConvertTo-Json Cmdlet, it seemed to do a good job of mangling my JSON payload into gibberish.$query=@ { statements  : [ {                statement  :  CREATE (n:test {props}) RETURN n ,             parameters  : {                    props  : {                     Name : Mike ,                     Occupation : LifeCoach                    }               }                } ]           } @One of my PowerShell expert friends recommends the NewtonSoft Json.NET instead — if you want to go this route check out https://www.newtonsoft.com/jsonAgain, the neo4j documentation explains the JSON structure in detail but this simple example should work once you add your own ID & Password.I’ve used a PowerShell here string, as it allows you to easily use a lot of special characters & code a string over multiple lines easily.NOTE we reference a props” variable within the JSON for the attributes of our new node. We then define the values for the props” variable later in the JSON document.POST JSONNow we use PowerShell’s Invoke-WebRequest cmdlet to POST our XML to the local neo4j instance, passing in the URL of the neo4j HTTP API as well as our credentials & JSON payload from above.$response = Invoke-WebRequest -Uri $serverURL -Method POST -Body $query -credential $neo4jCreds -ContentType application/json”Fig 3. Post to neo4j!Check ResultsWe now have a nice JSON response, we can check out the properties to understand if our CYPHER command has succeeded or not. PowerShell makes a pretty good go over displaying the returned text on screen.Note the errors” array, I spent a lot of time there :-)but it should be empty if we are successful.Woot — It Worked!SummaryHopefully this script is useful as core” code to access neo4j from PowerShell & maybe quickly create a graph from SQL Server queries or simply to query a neo4j graph from PowerShell.BOLT is probably the way to go for production code but the HTTP API is a nice, simple starting point.;Jun 22, 2018;[]
https://medium.com/neo4j/cypher-sleuthing-dealing-with-dates-part-3-eccd90206dbf;Jennifer ReifFollowJul 1, 2021·10 min readCypher Sleuthing: Dealing with Dates, Part 3*Latest version with Neo4j Browser duration format changes is available at https://jmhreif.com/blog/cypher-sleuthing-dates-part3/My previous part 1 and part 2 posts on this topic introduced Cypher dates, translated formats to the Cypher-supported ISO 8601 format, calculated durations, and measured lengths of time.Cypher Sleuthing: Dealing with Dates, Part 1No matter what database, programming language, or webpage you might be using, dates always seem to cause headaches…neo4j.comCypher Sleuthing: Dealing with Dates, Part 2My previous part 1 post on this topic introduced Cypher dates and translated a few other date formats to the…neo4j.comIf you read those, then this post is the next step with Cypher dates on date components and component translations. If you haven’t read parts 1 or 2, feel free to catch up — though this post doesn’t require the previous ones. :)We will take a brief detour back to components of temporal instants to see some additional use cases. Then, for the bulk of this post, we will cover translating duration values into specific measurements and accessing duration components.Accessing Date and Time ComponentsWe briefly mentioned temporal components in part 1 of this series (examples 6, 7, and 8), but I wanted to come back and add a couple more use cases that came to mind.In the part 1 post examples, we:used date components as a way to set a property to a specific piece of a date (year of current date stamp),did a general search (blog posts for month of March),and returned a specific piece of a longer date (day of the week).Accessing parts of a full date or time could also be helpful for searches that aren’t a good fit for date ranges or extracting part of a value for UIs. We can see some examples below.Dataset:MERGE (o:Order {orderId: 8272629462, orderDate: date(‘2020–05–27’)})MERGE (o2:Order {orderId: 8197274027, orderDate: date(‘2021–05–09’)})MERGE (o3:Order {orderId: 1749174018, orderDate: date(‘2020–06–01’)})MERGE (o4:Order {orderId: 6193472917, orderDate: date(‘2019–10–16’)})MERGE (o5:Order {orderId: 8174937104, orderDate: date(‘2019–05–27’)})MERGE (o6:Order {orderId: 3921746719, orderDate: date(‘2020–05–04’)})MERGE (o7:Order {orderId: 3918375629, orderDate: date(‘2021–05–27’)})MERGE (o8:Order {orderId: 2847209447, orderDate: date(‘2019–05–13’)})MERGE (o9:Order {orderId: 2846203472, orderDate: date(‘2020–05–01’)})MERGE (o10:Order {orderId: 6481749274, orderDate: date(‘2019–05–17’)})Example 1: Find sales in a certain monthMATCH (o:Order)WHERE o.orderDate.month = 5RETURN o.orderDateNOTE: I switched to the text view (tab on left of the result pane) so I could see all the values without scrolling.The example above works well for finding dates in any year and on any day, but within a certain month. This type of search wouldn’t work so well if you were trying to use ranges. With date ranges, you would end up with a query something like this:MATCH (o:Order)WHERE date(‘2019–05–01’) < o.orderDate <date(‘2019–05–30’)   OR date(‘2020–05–01’) < o.orderDate <date(‘2020–05–30’)   OR date(‘2021–05–01’) < o.orderDate <date(‘2021–05–30’)RETURN o.orderDateThere may be better ways to write the ugly query above, but a Cypher truncate wouldn’t work in this case, since it defaults only to smaller values where we couldn’t default the year without defaulting the month and day as well.Let’s continue with our example above to see which purchases were made on a specific day of the month or day of the week.Example 2: Orders for a particular day of the monthMATCH (o:Order)WHERE o.orderDate.day = 27RETURN o.orderDateExample 3: Find most popular day of the week for ordersMATCH (o:Order)RETURN o.orderDate.dayOfWeek as dayOfWeek,   count(o.orderDate) as orderCount, collect(o.orderDate) as datesORDER BY orderCount DESCAccording to our results above, the first day of the week is the most popular (Monday, in the ISO8601 standard). This could help us determine when to run a social campaign, publish content around products, or maybe when to run promotions or deals.Now that we have seen some extra examples of how we could use component values of temporal instants, we can dig into converting durations from one measurement to another.Translating Duration Values to Different PrecisionsIn the previous blog post in this series, we saw how to specify and calculate a variety of durations with Cypher.One particular example of this was for dosing medicine and determining how long before a person could take another dose. The duration returned from that example wasn’t very meaningful (48,600 seconds after midnight), and I promised to return in another post to show how to translate this value into something more readable.There are a couple of steps we need to take in order to convert values. First, we can calculate durations with the default process (a mix of months, days, and seconds), or we can specify a certain unit to convert the duration. Here are the options we can have Cypher use:inMonths(from,to)inDays(from, to)inSeconds(from, to)These units will calculate into whole values only, and remainders will be truncated. For instance, a duration calculation like below simply discards anything less than a whole month.Example 4: Translate duration to monthsRETURN duration.between(date(‘2021–05–01’),date(‘2021–06–08’)) as preciseDuration, duration.inMonths(date(‘2021–05–01’),date(‘2021–06–08’)) as monthsDurationNote that our first calculation preserves the full value at 1 month 7 days, while the second calculation only shows 1 month because it takes complete months and discards the remaining days.With that, let’s dive into a couple of use case examples.Example 6: Calculate the number of days a blog post has been publishedMATCH (b:BlogPost)RETURN duration.between(b.publishedDatetime, datetime()) as publishedDuration, duration.inDays(b.publishedDatetime, datetime()) as publishedDaysAbove, we can determine how many days a blog post has been published. On the left, we see the precise duration calculation (2 months 15 days, plus thousands of seconds). On the right, we see that duration translated to days (76). With some other information, we could calculate an average of how much traffic per day we have seen, or compare against other posts to track trends over time.Example 7: Translate time until vacation starts :)MATCH (v:Vacation) SET v.startDate = date(‘2021–09–10’)RETURN duration.between(date(),v.startDate) as preciseDuration, duration.inMonths(date(),v.startDate) as monthsThis could help us plan for when we should book reservations for lodging and activities or set a goal for content published — or track inches to lose from the waistline. We could also change the month calculation to inDays for a countdown.Photo by Cece B on UnsplashUsing Duration ComponentsJust like with temporal instant types, we can also access components (or parts) of the duration amount. There are a couple of rules I have discovered to help me avoid some pitfalls.You can only convert among units in a component grouping, not across groups. (explanation coming)Reminder: there must be whole values in order to convert to larger values. Most components do not retain fractions of larger units (i.e. 36 hours -> 1.5 days).We have already discussed the second item above, so let’s dive into the first item a bit more.After finding that certain components don’t return data and others do for different durations, I finally figured out that there are component groupings, and components don’t convert across them. Here are the component groups, as shown in the Cypher manual section:4.2. Accessing components of durationsThe column on the right is the key — values in one column can be converted to any other unit in that same cell, but not one in another cell. For instance, I can convert a duration from quarters to years and months, but not to weeks or hours.Also, I could convert a duration from days to weeks, but not to months or minutes. Notice, also, that these categories correspond to our duration functions of inMonths(), inDays(), and inSeconds(). That is for a specific purpose that we’ll cover in our next post.Example 8: Access components of duration in secondsMATCH (c:Conference)RETURN c.length, c.length.minutes, c.length.hours, c.length.daysIn the example above, I have a duration of 10,800 seconds, which sits in the Seconds category of the component groups. I can easily convert those seconds to minutes and hours, but I cannot convert to days without a translation function, even though 30 hours is well above 1 day (30 > 24 hours).NOTE: I cannot go up to the .weeks component for two reasons — weeks is in the next category (Days row in the table), and I do not have a whole week in hours (168hrs = 1week). Even if you put in 168+ hours for the duration, we cannot convert to weeks, because it’s in another conversion category.Let’s look at another example.Example 2: Access components of duration in daysMATCH (v:Vacation)RETURN v.length, v.length.weeks, v.length.days, v.length.hoursWow, none of my conversions worked here. Why is that?For both of our ruleswe do not have a whole week (only 5 days),we cannot convert to values outside our category (days/weeks).That leaves us stuck with our lonely 5 days. So is it possible to convert to something in another category? YES! We can do this by combining our duration functions (inMonths, inDays, inSeconds), and then using components to get to the desired conversion.We will walk through this thoroughly in the next post.Remember our medicine dose example from Part 2 of this blog series? Let’s look at that conversion!Example 3: Convert medicine dose seconds to hoursMATCH (d:Dose)RETURN d.frequency, d.frequency.hoursOK, here we have converted our lovely precise dose frequency into something more understandable. Instead of our medicine instructions to take a dose every 14,400 seconds, it can say to take a dose every 4 hours. Much better!Now, what about converting our dose times to something more meaningful? In our last post, we left them as durations (09:30:00 as P0M0DT34200S and 13:30:00 as P0M0DT48600S), which are not very pretty to read or understand.While the simpler (and probably more logical) method would be to store the dose times as temporal instants and calculate the time by adding the temporal value and frequency duration, I’ll demonstrate how we can take our existing durations and calculate them back into readable durations.I mentioned above that the components would only convert to whole values, but there are a few components where you can display remainders in smaller units (i.e. 9 hours 30 minutes). I’ll show a screenshot of the section in the documentation of those below.Components of Duration values and how they are truncated within their component groupLet’s use our dose time example to demonstrate this!Example 4: Translate dose time from seconds duration to hours/minutesMATCH (d:Dose)RETURN d.dose1Time, d.dose1Time.hours, d.dose1Time.minutesOfHourIf we simply translate the duration P0M0DT34200S with the .minutes component, we get 570 minutes, which is the entire duration (9.5hrs) converted to minutes. However, if we use the .hours and .minutesOfHour components, it preserves the partial hour and displays the remainder after we remove whole hours (9) from the amount.We could do the same with the dose2Time, but I’ll let you tackle that on your own. Next example!Example 4: Conversions with values in multiple categoriesMATCH (:Employee)-[rel:ASSIGNED]-(p:Project)WITH duration.between(rel.startDate, date()) as currentLengthRETURN currentLength, currentLength.quarters, currentLength.months, currentLength.weeks, currentLength.days, currentLength.hoursThis query is a bit more complicated because we now have duration amounts in different categories we can convert, but it helps us understand our rules even better. Here, we have measured how long someone has been on a project by calculating the duration between the date an employee was assigned to the current date, returning P3M25DT0S.Our result means that the 3 months can be converted to years, months, and quarters (months group), and the 25 days can be converted to weeks (days group). Since we don’t have any amount in the time group, we cannot use any components for hours, seconds, etc. And this is what we see — 3 months converted to quarters (1), 25 days converted to weeks (3), and no hours.If you’d like to try a couple more examples to help solidify this information, let me leave you with a couple to play with on these duration components.Example 5: Variety of durations to test with componentsWITH duration(P3D) as durationRETURN duration.weeks, duration.days, duration.hours, duration.minutes, duration.secondsWITH duration(PT95M) as durationRETURN duration.days, duration.hours, duration.minutes, duration.secondsWITH duration(PT95M) as durationRETURN duration.hours, duration.minutesOfHourWITH duration(PT42H) as durationRETURN duration.days, duration.hours, duration.minutes, duration.secondsWITH duration(P10D) as durationRETURN duration.days, duration.weeks, duration.daysOfWeekWrapping up!In this third post, we took a deep dive into durations with components and duration functions. We saw how to convert durations to different values by understanding the categories into which duration values are divided (months, days, seconds). With that understanding, we could then convert our durations into other temporal units within the same category.To wrap up our series on Cypher dates and times, our next (and final) post in this series will pick up any remnants we haven’t covered on temporals. We will see how to translate across categories and specific units by combining duration functions and components. We would also be remiss not to mention APOC, so we will take a brief look at some procedures and functions in the APOC library that might provide extra flexibility or that may be obsolete with the Cypher functionality. Tune in next time and happy coding!ResourcesCypher manual: Components of Temporal InstantsCypher manual: Truncating Temporal ValuesCypher manual: Duration FunctionsCypher manual: Duration ComponentsBlog post: Part 1 of Cypher Sleuthing with DatesBlog post: Part 2 of Cypher Sleuthing with Dates;Jul 1, 2021;[]
https://medium.com/neo4j/whats-cooking-part-4-similarities-d4443d89556a;Mark NeedhamFollowApr 23, 2019·8 min readWhat’s cooking? Part 4: SimilaritiesIt’s time for part 4 of the BBC Good Food Series. In this post we’ll learn how to use the Jaccard Similarity Algorithm to compute recipe to recipe similarities, and more.You can read the previous parts of the series to get up to date:Part 1: Importing BBC goodfood information into Neo4jPart 2: What can I make with these ingredients?Part 3: A segue into graph modellingSimilarity Algorithms in Neo4j Graph Algorithms LibraryIn this post we’re going to use the similarity algorithms in the Neo4j Graph Algorithms, but let’s first remind ourselves about the graph model that we created in the earlier posts.We can view the model by running CALL db.schema()in the Neo4j Browser:BBC Good Food Graph ModelRecipes are at the centre of our world, and they’re connected to Collections, Keywords, Authors, DietTypes, and Ingredients. We could compute similarities between any of this items, but why would we want to do that?Why compute similarities?We want to compute similarities between items so that we can make recommendations to users about other things that they might like. For example, if a user likes a recipe, can we suggest other recipes that they could try?Recipe to RecipeRecipes sit at the centre of this graph, so that seems like a good place to start. We can compute the similarity of a pair of recipes based on the ingredients that they have in common.We can use the Jaccard Similarity Algorithm. Jaccard Similarity is defined as the size of the intersection divided by the size of the union of two sets, best seen in the diagram below:Jaccard SimilarityThis algorithm returns a score between 0 and 1 that indicates similarity between items. 0 means that they are dissimilar. 1 means that they are identical.Let’s have a look how it works with an example. The diagram below shows two bolognese recipes and the ingredients that they contain:Ingredients used in Bolognese RecipesWe want to compute the similarity of the recipes based on their ingredients. From visual inspection we can see that there are 4 overlapping ingredients: spaghetti, carrot, basil, and olive oil. The combined number of ingredients is 23, which means our Jaccard Similarity score should be 4/23 = 0.17Jaccard Similarity FunctionLet’s use the Jaccard Similarity function to compute the similarity of these two recipes and check that it matches our quick calculation:// Find the 1st recipe and collect the node ids into a listMATCH (r1:Recipe)-[:CONTAINS_INGREDIENT]->(i)WHERE r1.name =  So-simple spaghetti Bolognese WITH r1, collect(id(i)) AS r1Ingredients// Find the 2nd recipe and collect the node ids into a listMATCH (r2:Recipe)-[:CONTAINS_INGREDIENT]->(i)WHERE r1.name =  The best spaghetti Bolognese recipe WITH r1, r1Ingredients, r2, collect(id(i)) AS r2Ingredients// Compute similarity based on the lists of node idsRETURN algo.similarity.jaccard(         r1Ingredients, r2Ingredients) AS scoreCool! The result is as we expected, and the similarity function is great for computing the similarity of pairs of nodes. But what if we want to compute the similarity between all pairs of recipes?Jaccard Similarity ProcedureIt will take a long time to do this one function call at a time, which is where the similarity procedure comes into play. The procedure parallelises the computation of similarities, and is therefore well suited for computing the similarity of lots of pairs of items.The library contains two procedures for Jaccard Similarity: a streaming procedure and a write procedure.Streaming procedureThe following query will compute the similarity of all pairs of recipes based on their ingredients, and stream the top 20 pairs of most similar recipes:// Construct lists of node ids of ingredients, // grouped by node ids of recipes MATCH (r:Recipe)-[:CONTAINS_INGREDIENT]->(ingredient)WITH {item:id(r), categories: collect(id(ingredient))} as userDataWITH collect(userData) as dataCALL algo.similarity.jaccard.stream(data)YIELD item1, item2, count1, count2, intersection, similarity// Look up nodes by node idWITH algo.asNode(item1) AS from,      algo.asNode(item2) AS to,      similarity, intersection RETURN from.name, from.id, to.name, to.id, intersection, similarityORDER BY similarity DESCLIMIT 20Similar recipesAll the pairs here have identical ingredients, which in several cases looks like it’s because they are duplicates! Getting rid of those duplicates is an interesting problem, but we’ll leave that for another blog post.Write procedureThis query only shows us the top 20 similar pairs of recipes. It’d be good to know the distribution of similarity scores, which we can do by calling the write version of the algorithm. We’ll set write:false so that we don’t write any relationships to the database.The following query returns the number of similarity pairs, along with 25th, 50th, 75th, 99th, 99.9th, and 100th percentile values:MATCH (r:Recipe)-[:CONTAINS_INGREDIENT]->(ingredient)WITH {item:id(r), categories: collect(id(ingredient))} as userDataWITH collect(userData) as dataCALL algo.similarity.jaccard(data, {write: false})YIELD nodes, similarityPairs, min, max, mean,       p25, p50, p75, p99, p999, p100RETURN nodes, similarityPairs, min, max, mean,        p25, p50, p75, p99, p999, p100Jaccard Similarity PercentilesFrom this output we learn that 50% of the similarity scores being returned are 0, and 99% of them are less than 0.2. We can get rid of those pairs of items by passing in the similarityCutoff config parameter.Filtering with similarityCutoffLet’s re-run the write version of the procedure, but this time we’ll return pairs of recipes that have a Jaccard Similarity of 0.2 or higher:MATCH (r:Recipe)-[:CONTAINS_INGREDIENT]->(ingredient)WITH {item:id(r), categories: collect(id(ingredient))} as userDataWITH collect(userData) as dataCALL algo.similarity.jaccard(data, {  write: false, similarityCutoff: 0.2})YIELD nodes, similarityPairs, min, max, mean,       p25, p50, p75, p99, p999, p100RETURN nodes, similarityPairs, min, max, mean,        p25, p50, p75, p99, p999, p100Jaccard Similarities above 0.2The number of similarity pairs has reduced from 676 million to 762,000. We have 11,000 recipes nodes, which is an average of 70 similar values per recipe. For our recipe recommendation engine we won’t show that many similar recipes, a maximum of 10 similar recipes is sufficient. similarityCutoff: 0.3 returns 89,600 similarity pairs, an average of 8 similar values per recipe, so we’ll use that cut off value.topKThe similarity algorithms all support the topK config parameter. If we set this parameter the procedure will return a maximum of k similar values per node.We’ll set topK: 5 , which will find the 5 most similar values per recipe. We’ll also set write:true so that relationships will be created between for each of the similar values. The following query will do this:MATCH (r:Recipe)-[:CONTAINS_INGREDIENT]->(ingredient)WITH {item:id(r), categories: collect(id(ingredient))} as userDataWITH collect(userData) as dataCALL algo.similarity.jaccard(data, {  write: true,  writeRelationshipType:  SIMILAR ,   writeProperty:  score ,  similarityCutoff: 0.3})YIELD nodes, similarityPairs, min, max, mean,       p25, p50, p75, p99, p999, p100RETURN nodes, similarityPairs, min, max, mean,        p25, p50, p75, p99, p999, p100Exploring the similarity graphNow let’s have a look at the similarity graph that’s been created by running this procedure:Similarity GraphIf you like potatoes, there are lots of recipes to keep you happy! And let’s see if we can find some other recipes for our bolognese loving friends:MATCH path = (r1:Recipe)-[:SIMILAR]->(r2)-[:CONTAINS_INGREDIENT]-(i)WHERE r1.name =  So-simple spaghetti Bolognese RETURN pathOther bolognese recipesWe could use the results of this query as recommendations for users who want to try a different bolognese recipe.So we’ve now learnt how to compare recipes against recipes, but what if we want to compare items that have different labels?Collection to IngredientAn example of this is computing similarities between collections and ingredients. These types of nodes intersect on recipes, via the following graph pattern:(collection)<-[:COLLECTION]-(rec)-[:HAS_INGREDIENT]->(ingredient)We can can build up lists of the node ids of recipes for all collection and ingredient nodes and compare them to each other. If we used the approach we’ve seen so far, we’d have to compare all ingredients and collections to each other, which isn’t what we want. We want to compare collections to ingredients, not collections to collections or ingredients to ingredients.The similarity algorithms allows us to pass in the sourceIds and targetIds config parameters to handle this.The following query computes Jaccard similarity between Collection and Ingredients, and returns the top 100 most similar pairs:// Create list of nodes ids of recipes for collectionsMATCH (recipe:Recipe)-[:COLLECTION]->(collection)WITH {item:id(collection), categories: collect(id(recipe))} as dataWITH collect(data) AS collectionRecipes// Create list of nodes ids of recipes for ingredientsMATCH (recipe:Recipe)-[:CONTAINS_INGREDIENT]->(ingredient)WITH collectionRecipes,      {item:id(ingredient), categories: collect(id(recipe))} as dataWITH collectionRecipes, collect(data) AS ingredientRecipes// Extract node ids of collections to use as sourceIds// Extract node ids of recipes to use as targetidsWITH collectionRecipes + ingredientRecipes AS allRecipes,      [value in collectionRecipes | value.item] AS sourceIds,     [value in ingredientRecipes | value.item] AS targetIdsCALL algo.similarity.jaccard.stream(allRecipes, {  similarityCutoff: 0.0,   sourceIds: sourceIds,   targetIds: targetIds})YIELD item1, item2, count1, count2, intersection, similarityWITH algo.getNodeById(item1) AS from,      algo.getNodeById(item2) AS to,      similarityRETURN labels(from) AS fromLabels, from.name AS from,        labels(to) AS toLabels,  to.name AS to, similarityORDER BY similarity DESCLIMIT 100Collection -> Ingredient SimilaritiesUnsurprisingly, ingredients with the same name as the collection have high similarity scores.We can also use this approach to compare a single collection to all ingredients. The query below finds the top 5 ingredients (topK:5) used by the ‘Apple’ collection:MATCH (recipe:Recipe)-[:COLLECTION]->(collection:Collection)WHERE collection.name =  Apple WITH {item:id(collection), categories: collect(id(recipe))} as dataWITH collect(data) AS collectionRecipesMATCH (recipe:Recipe)-[:CONTAINS_INGREDIENT]->(ingredient)WITH collectionRecipes,      {item:id(ingredient), categories: collect(id(recipe))} as dataWITH collectionRecipes, collect(data) AS ingredientRecipesWITH collectionRecipes + ingredientRecipes AS allRecipes,      [value in collectionRecipes | value.item] AS sourceIds,     [value in ingredientRecipes | value.item] AS targetIdsCALL algo.similarity.jaccard.stream(allRecipes, {  similarityCutoff: 0.0,   sourceIds: sourceIds,   targetIds: targetIds,  topK: 5})YIELD item2,  intersection, similarityWITH algo.getNodeById(item2) AS to, similarity RETURN to.name AS to, similarityORDER BY similarity DESCIngredients similar to the Apple CollectionSummaryIn this post we’ve learnt how to use the Jaccard Similarity Algorithm in the Neo4j Graph Algorithms Library, and we’ve explored what we can do with the various config parameters that it supports.If you enjoyed learning how to apply graph algorithms to make sense of data, you might like the O’Reilly Graph Algorithms Book that Amy Hodler and I wrote.You can download a free copy from neo4j.com/graph-algorithms-bookDownload the O’Reilly Graph Algorithms Book;Apr 23, 2019;[]
https://medium.com/neo4j/brightening-up-your-development-experience-with-neo4j-devtools-2bfc93fb6cb3;GregFollowOct 27, 2021·6 min readBrightening Up Your Development Experience with Neo4j DevToolsPhoto by Jonatan Pie on UnsplashThe DevTools blog has been a little quiet since my last installment back when summer still seemed like a distinct possibility.New and Noteworthy: Neo4j Developer ToolsNeo4j Browser melds new with old and Neo4j Desktop gets more capable on Windowsmedium.comWith the winter nights drawing in for those of us in the Northern hemisphere, we’re back once more to brighten up your development experience. First off, here are our latest updates from Neo4j Browser.New Neo4j Browser Properties PanelFor quite some time, properties in the Neo4j Browser graph visualization have been neatly tucked away in a slither of space at the bottom of the graph visualization. Great for keeping out of your way, but at the same time, not the easiest thing to work with when dealing with more than a handful of properties.Previous Overview (1) and Properties (2) UITo help you inspect property-rich nodes or relationships more easily, we’re now presenting them in their own dedicated side-panel. We’ve also moved the Overview and Styling information, which was previously split between the top and bottom, to this new panel.The Overview will show when you have no nodes selected or hovered and displays the labels and types present in the current graph visualization. From here you can choose to restyle them, by clicking on any of them:Restyle the visualisation from the OverviewOnce you hover or select a node or relationship, its properties are displayed in the new panel. You can change styling from this view too, just as you can in the Overview. The panel can be collapsed or resized if you need to ajdust the space available for your graph visualization.The panel’s last expand/collapse state will be remembered and subsequent result frames will observe that latest state, ensuring the panel is always where you need it to be.View node and relationship propertiesProperty names and values can now be quickly copied, either individually or in bulk for use elsewhere:Copy propertiesAnd finally, to help you understand the data type of a property, hovering a property will now reveal its type.Inspect property data typesHelp Make Neo4j Browser BetterWe’ve optionally been collecting product analytics data from our Browser users running in Neo4j Desktop for some time and we’re now rolling this out more widely to other areas you may run Browser.When you run Neo4j Browser outside of Desktop for Aura, you’ll now be informed that you’ve opted into product analytics. This helps us make Browser better, but you will of course be given the option to opt-out. When running in Desktop and Aura, product analytics settings will continue to be managed by those applications.Browser Fixes and ImprovementsWe’ve also taken the opportunity to make fixes and improvements that you’ve been suggesting over at our Feedback site. Don’t forget to head over there and leave your own feedback or upvote existing requests.Neo4j Browser FeedbackGive feedback to the Neo4j Browser team so we can make more informed product decisions. Powered by Canny.neo4j-browser.canny.ioIf you’ve ever noticed that navigating the cursor between wrapped lines in long Cypher queries was odd, you’ll now see much more conventional cursor movements between lines.When complex structures (e.g. a node with all its properties) are returned and viewed in the table view, we now wrap less and make better use of horizontal space. There’s also a handy copy shortcut.When you arrange favorites into folders, you can now export the entire folder to a zip or file of your queries.And finally, an issue that caused the bottom bar of the result visualisation to disappear in fullscreen mode has been fixed. You can now enjoy fullscreen mode in all its glory again.Neo4j DesktopMeanwhile over in Neo4j Desktop, we’ve been making a few UI tweaks and addressing a slew of fixes and improvements…View Cached DatabasesIf you ever wonder what databases lie within a 4.x DBMS you haven’t used for a while, you can now find out without needing to start it. We now cache the databases in your 4.x DBMSs so you can easily see which databases they contain without starting them.Cached databases in a stopped DBMSWe hope this saves you a bit of time stopping and starting DBMSs to find things.New UI StylingA few small changes to UI colors and fonts have now been made to better align with the latest Neo4j branding:New Desktop UI StylingSample Projects Can Now Install Plugin DependenciesThe plugins (e.g. APOC, graph-data-science, neosemantics etc.) required by some sample projects can now be installed automatically, removing the need to do this as a manual step.We’ve also extended the available sample projects for you to explore, available under:Projects > + New > Import sample projectInstall sample projects (now with plugins!)Neo4j Desktop Bug Fixes and ImprovementsWe’ve done a bit of cleanup since the summer, making various fixes and improvements to Neo4j Desktop thanks to the feedback we’ve received at our Feedback page.Neo4j Desktop FeedbackGive feedback to the Neo4j Desktop team so we can make more informed product decisions. Powered by Canny.neo4j-desktop.canny.ioYou can now view graph apps release notes from a link in the sidebar after you’ve installed them, as well as before updating them.Desktop now allows you to create database names with hyphens.Deleted graph apps are now removed from the disk to free up space.Desktop now gives better OS compatibility check information for fresh installs.In the event Desktop application metadata becomes corrupted, Desktop will now start and provide more information.Improvements have been made to the reliability of offline bundled app installations.Windows users will now be warned if PowerShell (required for Neo4j Desktop) is not installed.Windows users will now only see the File Explorer presented once when using Open Data Path.The correct Database is now dumped from started DBMSs and given the correct Database name in the dump file.We fixed an issue that prevented graph apps from opening when they had an update pending.Thanks for reading this bumper installment and we look forward to bringing you more news from DevTools in the near future.Stay safe, keep warm, and stay productive!;Oct 27, 2021;[]
https://medium.com/neo4j/how-to-automate-neo4j-deploys-on-aws-2f36b7386e4c;David AllenFollowFeb 20, 2019·5 min readHow to Automate Neo4j Deploys on AWSNeo4j already provides some documentation on its site for how to do deployments of Neo4j to common clouds, including AWS. But in this article, I’ll provide sample shell scripts that can do this automatically for you.These are useful when you want to integrate Neo4j into your CI/CD pipeline and be able to create/destroy instances temporarily, and also just to spin up a sample instance. But really, if you can automate Neo4j deployment, then any other piece of software can make a Neo4j instance whenever it needs, which is extremely handy.If you have any feedback or questions on these approaches, drop by the Neo4j Community site on this thread and share!Neo4j and AWS CloudFormationRequirementsBefore we begin, you’ll need the aws command line interface program, which you can download and install with directions here. The AWS CLI is the main way you can automate all things AWS.It will also be necessary to generate an access token, and make sure that the environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY are defined for you.CloudFormationNeo4j provides CloudFormation templates for Neo4j Enterprise standalone (that is, a single database instance), Neo4j Causal Cluster (highly available clusters) and Neo4j Community. So first thing’s first, pick which one you would like to deploy. We’ll cover all three in this article.CloudFormation is really just a recipe for Amazon Web Services that tells AWS how to deploy a whole set of interrelated resources. By deploying all of this as a stack we can keep all of our resources together, and delete just one thing when we’re done.In general, Neo4j CloudFormation templates have a few properties worth knowing about before we begin:They deploy one more more EC2 VMs in a region you specifyIf you’re deploying more than one VM, those VMs are spread across multiple availability zones within the region, so that if one AZ goes down your entire database doesn’t go down.They deploy a new VPC and put Neo4j inside of it. In this way you can control network access by tuning your VPC and security rules.ApproachHow to deploy is going to be very simple: we just create a new CloudFormation stack, pointing to the right stack template URL to tell AWS what to deploy. We then will give each of our stacks various parameters to control how much hardware we’re using and so on.We’ll need to specify several common parameters, which you’ll see in the scripts below. Here’s an explanation of what they are.SSH Key: When your VMs are created, this is the name of your SSH key on AWS that you’ll be able to use to SSH into the instances as the user ubuntu”Network Whitelist: This is set to 0.0.0.0/0 by default, which means that any IP on the internet can contact your instance. If you want to lock it down to just your company’s IP block, this is where you’d specify that.Instance: This is the AWS instance type you want to launch, which controls how much hardware you’re giving the database.Region: Where in the world you want to deploy Neo4j. The template generally supports at least these regions, but may not work in any AWS region due to availability: us-east-1, us-east-2, us-west-1, us-west-2, eu-west-1, eu-central-1, ap-southeast-1, ap-northeast-1, ap-south-1, sa-east-1.Let’s get started. Simply run any of these scripts, and it will result in a CloudFormation stack being deployed.Deploying Neo4j Enterprise StandaloneThis will create a single instance of Neo4j without high-availability failover capabilities, but it’s a very fast way to get started.#!/bin/bashVERSION=3.5.3export SINGLE_TEMPLATE=http://neo4j-cloudformation.s3.amazonaws.com/neo4j-enterprise-standalone-stack-$VERSION.jsonexport STACKNAME=neo4j-enterprise-$(echo $VERSION | sed s/[^A-Za-z0-9]/-/g)export INSTANCE=r4.largeexport REGION=us-east-1export SSHKEY=my-ssh-keynameaws cloudformation create-stack \   --stack-name $STACKNAME \   --region $REGION \   --template-url $SINGLE_TEMPLATE \   --parameters ParameterKey=InstanceType,ParameterValue=$INSTANCE \     ParameterKey=NetworkWhitelist,ParameterValue=0.0.0.0/0 \     ParameterKey=Password,ParameterValue=s00pers3cret \     ParameterKey=SSHKeyName,ParameterValue=$SSHKEY \     ParameterKey=VolumeSizeGB,ParameterValue=37 \     ParameterKey=VolumeType,ParameterValue=gp2 \     --capabilities CAPABILITY_NAMED_IAMDeploying Neo4j Enterprise Causal ClusterWhen you need a clustered deploy, you want to use the Causal Cluster template. Note that the major difference here from the last one is the ClusterNodes parameter, where you can pick any number from 3 on up to indicate how many core nodes you want in your cluster.#!/bin/bashVERSION=3.5.3export CLUSTER_TEMPLATE=http://neo4j-cloudformation.s3.amazonaws.com/neo4j-enterprise-stack-$VERSION.jsonexport STACKNAME=neo4j-enterprise-$(echo $VERSION | sed s/[^A-Za-z0-9]/-/g)export INSTANCE=r4.largeexport REGION=us-east-1export SSHKEY=my-ssh-keynameexport CORES=3export READ_REPLICAS=1aws cloudformation create-stack \   --stack-name $STACKNAME \   --region $REGION \   --template-url $CLUSTER_TEMPLATE \   --parameters \ParameterKey=ReadReplicas,ParameterValue=$READ_REPLICAS \ParameterKey=ClusterNodes,ParameterValue=$CORES \        ParameterKey=InstanceType,ParameterValue=$INSTANCE \     ParameterKey=NetworkWhitelist,ParameterValue=0.0.0.0/0 \     ParameterKey=Password,ParameterValue=s00pers3cret \     ParameterKey=SSHKeyName,ParameterValue=$SSHKEY \     ParameterKey=VolumeSizeGB,ParameterValue=37 \     ParameterKey=VolumeType,ParameterValue=gp2 \     --capabilities CAPABILITY_NAMED_IAMDeploying Neo4j CommunityUsing Neo4j community is barely any different than using Neo4j Enterprise (in terms of the deploy process) other than the different CloudFormation stack URL.#!/bin/bashVERSION=3.5.3export COMMUNITY_TEMPLATE=http://neo4j-cloudformation.s3.amazonaws.com/neo4j-community-standalone-stack-$VERSION.jsonexport STACKNAME=neo4j-comm-$(echo $VERSION | sed s/[^A-Za-z0-9]/-/g)export INSTANCE=r4.largeexport REGION=us-east-1export SSHKEY=my-ssh-keynameaws cloudformation create-stack \   --stack-name $STACKNAME \   --region $REGION \   --template-url $COMMUNITY_TEMPLATE \   --parameters ParameterKey=InstanceType,ParameterValue=$INSTANCE \     ParameterKey=NetworkWhitelist,ParameterValue=0.0.0.0/0 \     ParameterKey=Password,ParameterValue=s00pers3cret \     ParameterKey=SSHKeyName,ParameterValue=$SSHKEY \     ParameterKey=VolumeSizeGB,ParameterValue=37 \     ParameterKey=VolumeType,ParameterValue=gp2 \     --capabilities CAPABILITY_NAMED_IAMChecking to see if your instance is upIn each case, the commands above simply submit a CloudFormation stack to be deployed, they don’t wait until the stack is available. If you want to do this (so that you can trigger some next step in a software pipeline) you’ll want to wait for the CloudFormation stack to finish deploying, like so:aws cloudformation wait stack-create-complete --region $REGION --stack-name  $STACKNAME This call will block until you’re finished deploying. Finally, you’ll want to get the stack outputs, like this:aws cloudformation describe-stacks --region $REGION --stack-name  $STACKNAME In general this will output a lot of ugly JSON. To cut straight to the outputs of the stack, use the jq tool. By piping the output through:jq -r .Stacks[0].Outputs[]You’ll get a nice set of outputs and be able to spot the IP address and password of your new instance. By the time the CloudFormation template is finished deploying, the service will be live and ready to go!Cleaning up and Removing Your StackWhen you’re done with your CloudFormation stack, you can delete it similarly. I use a script like this, and simply pass it the stack name that I created above as an argument to the script.Keep in mind you must specify the region correctly.#!/bin/bashecho  Deleting stack $1 aws cloudformation delete-stack --stack-name  $1  --region us-east-1That’s it!;Feb 20, 2019;[]
https://medium.com/neo4j/learn-japanese-characters-using-neo4j-483585abc5b8;Jimmy CrequerFollowSep 20, 2019·6 min readLearn Japanese characters using Neo4jBuilding a kanjis quiz app with GraphQL, React, and a Graph Database in 2 daysTLDRIn just two days, we were able to create a fully working kanjis quiz app, using the GRANDstack, by going through the following steps :Import some CSV datasets to create our graph database.Run the Jaccard algorithm to create more relationships between our nodes.Run the PageRank algorithm to compute an additional property on some nodes.Create APIs using GraphQL to generate random quiz questions.You can find the source code from the Github repository.Last week, I joined a 2-day hackathon event held in my company. The theme was New, Fun, Speed” and our team aimed to build a small quiz app to learn the Japanese characters in a new and faster way, while having fun. My company is located in Japan and we thought other fellow foreigner colleagues could benefit from this idea.We decided to use the GRANDstack to build our app :GraphQL for the API endpointsReact for the frontendApollo for facilitating communication between API and frontendNeo4j as our database technology to store the Japanese characters and build connections between themLet’s get into it!Build the graphImport the datasetsWe found a Japanese-Language Proficiency Test (JLPT) open dataset in a handy CSV format, composed of the kanji, its readings separated by ・” and its meanings, separated by  ” as follows :国,コク・くに,country高,コウ・たか.い・たか・~だか・たか.まる・たか.める,tall high expensive今,コン・キン・いま,now東,トウ・ひがし,eastWe were able to import this data using the following Cypher query from the Neo4j Browser. In addition, the JLPT has five levels: N1, N2, N3, N4 and N5, with N1 being the most difficult and N5 the easiest, so we decided to also add Level nodes to represent this difficulty.UNWIND [ 5 ,  4 ,  3 ,  2 ,  1 ] AS levelLOAD CSV FROM  https://raw.githubusercontent.com/jimmycrequer/roth-2019/master/neo4j/data/vocabulary_6501  + level +  .csv  AS rowMERGE (k:Kanji {value: row[0]})WITH row, k, levelMERGE (l:Level {value:  N  + level})WITH row, k, lMERGE (k)-[:HAS_LEVEL]-(l)WITH row, kUNWIND split(row[1],  ・ ) AS readingMERGE (r:Reading {value: reading})WITH row, k, rMERGE (k)-[:HAS_READING]->(r)WITH row, kUNWIND split(row[2],    ) AS meaningMERGE (m:Meaning {value: meaning})WITH row, k, mMERGE (k)-[:HAS_MEANING]->(m)Some Japanese characters share readings and meanings, and we already had pretty cool relationships at this stage, but we decided to enrich our dataset by adding radicals information and create more relationships between characters whose shape is composed of the same radicals. We were lucky to find a dataset that perfectly fitted our needs, having the following structure :radical,meaning,kanjiList｜,stick,亜唖逢悪以伊井稲印引....丶,dot,以浦永泳詠往欧殴鴎蒲釜....Each character in the kanjiList” field must be treated independently. Since Neo4j allows flexible schema, it is very easy to add nodes/relationships on the fly! The key for us when importing this dataset was to use the MATCH” keyword instead of the MERGE” keyword because we wanted to add radicals for only kanjis that already existed in our dataset, and ignore the others (this second dataset had way more characters than our first one).LOAD CSV WITH HEADERS FROM  https://raw.githubusercontent.com/jimmycrequer/roth-2019/master/neo4j/data/radicals.csv  AS rowUNWIND split(row.kanjiList,   ) AS kanjiMATCH (k:Kanji {value: kanji})WITH row, kMERGE (r:Radical {value: row.radical})WITH k, rMERGE (k)-[:HAS_RADICAL]->(r)Here is how our graph looked like at this point.Number of nodes and relationships per labelExample of the kanji country”Compute similarity using Jaccard algorithmThe next step for us was to actually use the relationships of our graph to find similar kanjis. To compute this, we decided to go for the Jaccard algorithm. Neo4j provides implementation for lots of algorithms and they are pretty straight-forward to use.MATCH (k:Kanji)-[]->(n)WITH {item: id(k), categories: collect(id(n))} AS userDataWITH collect(userData) AS dataCALL algo.similarity.jaccard(data, {topK: 50, similarityCutoff: 0.1, write:true, writeProperty:  jaccardSimilarity })YIELD nodes, similarityPairs, write, writeRelationshipType, writePropertyRETURN nodes, similarityPairs, write, writeRelationshipType, writePropertyThis algorithm created a new relationship called SIMILAR” between our nodes labelled Kanji. Let’s look into some results.MATCH (k1:Kanji)-[r:SIMILAR]->(k2:Kanji)WITH k1, r, k2ORDER BY r.jaccardSimilarity DESCRETURN k1.value AS kanji, collect(k2.value)[0..5] AS similarKanjisLIMIT 6Kanjis look very similar!Compute score using PageRank algorithmOur ultimate goal was to create a quiz app, and we wanted to implement a points system to give a reward to our users when they get a correct answer. To implement that, we decided to attribute a score to every kanji using PageRank algorithm.Our reasoning was :N1 kanjis are harder than N5 kanjis, and there are more of themKanjis sharing the same meaning, reading are harder to get rightKanjis sharing the same radicals are harder to differentiateIn other words, similar” kanjis can be considered harder. Our first try was to use the SIMILAR” relationships computed in the previous step.CALL algo.pageRank(Kanji, SIMILAR, {iterations:20, dampingFactor:0.85, weightProperty:  jaccardSimilarity })YIELD nodes, iterations, loadMillis, computeMillis, writeMillis, dampingFactor, write, writePropertyRETURN nodes, iterations, loadMillis, computeMillis, writeMillis, dampingFactor, write, writePropertyThen we set a new score property using the new pageRank” property :MATCH (k:Kanji)SET k.score = round(k.pagerank * 100)And we were very pleased with the results. Difficult kanjis were attributed a higher score than simple ones.Top most difficult and easiest kanjis after computing PageRank algorithmThat’s it for the graph building! I am sure we could have improved it, by tuning some algorithm parameters and/or adding more data sources, but we were starting to run out of time and moved on to the next step : build the API!Build the APIWe wanted to create an API to fetch new questions for the quiz. At first, we wanted to retrieve, for a given kanji, one of its meaning (correct answer) and three different meanings (wrong answers) with a single endpoint.Finding kanji’s meanings is pretty straight-forward as we just need to look at the HAS_MEANING” relationship.Finding wrong meanings is really where we were able to make use of our graph. Existing kanji quiz apps seem to just take random meanings and use them as wrong propositions, making it relatively easier to get the correct answer. We wanted our app to be more difficult. Our idea was to retrieve the meanings from kanjis that are similar to the one we are trying to guess, using the SIMILAR” relationship computed by the Jaccard algorithm. Similar kanjis might have similar meanings and confusion can certainly happen.Lastly, we added some randomness to get different output for the same kanji and here is how our schema looked like.type Kanji {  id: ID!  value: String  score: Int  randomConnectedMeanings: [Meaning]    @cypher(      statement:            MATCH (this)-[:HAS_MEANING]->(m:Meaning)        WITH m, rand() AS rand        WITH m        ORDER BY rand        RETURN DISTINCT m             )  randomNotConnectedMeanings: [Meaning]    @cypher(      statement:            MATCH (this)-[:SIMILAR]-(:Kanji)-[:HAS_MEANING]->(m:Meaning)        WHERE NOT (this)-[:HAS_MEANING]->(m)        WITH m, rand() AS rand        WITH m        ORDER BY rand        RETURN DISTINCT m             )}type Meaning {  id: ID!  value: String}Usage example from the GraphQL playground.Example of quiz questionWith the same logic, we ended up adding more endpoints to diversify the type of quiz questions :For a kanji, guess the correct readingFor a meaning, guess the correct kanjiAnd some more…SummaryIn just two days, we were able to create a fully working kanjis quiz app, using the GRANDstack, by going through the following steps :Import some CSV datasets to create our graph database.Run the Jaccard algorithm to create more relationships between our nodes.Run the PageRank algorithm to compute an additional property on some nodes.Create APIs using GraphQL to generate random quiz questions.You can find the source code from the Github repository, please don’t hesitate to reach out if you have any comments or ideas.;Sep 20, 2019;[]
https://medium.com/neo4j/visualize-graph-embedding-algorithm-result-in-neuler-767bb0dd8275;Tomaz BratanicFollowMay 6, 2021·4 min readVisualize Graph Embedding Algorithm Result in NEulerQuickly inspect graph embedding algorithm results in Neo4j graph data science playground application NEuler.NEuler is a graph data science playground application designed to help you execute and understand graph algorithms in Neo4j. With only a couple of clicks, you can import example data, execute various graph algorithms, and visualize their results. It is available as an extension to Neo4j Desktop, and you can also use it in combination with Neo4j Sandbox.In this blog post, I will use the Movies sandbox project to demonstrate how to quickly visualize graph embedding results with a t-SNE scatter plot.Setting Up the Neo4j Sandbox EnvironmentYou can follow this link to automatically create a Movies sandbox project. If you choose to, you can select a different sandbox project. There are more than 10 Sandbox projects available, ranging from Twitter and Open Street Map to contact tracing projects.Once you have selected and created your desired sandbox environment, you can open the NEuler application by choosing it in the dropdown menu.How to open NEuler application in Neo4j sandboxFollow the login screen and select the default database in the onboarding process.Select default database in the onboarding processNow you should arrive at the main screen of the NEuler application. You can either import new sample datasets, run algorithm recipes, or execute single algorithms.To follow this blog post, choose to run a single algorithm.Select the Run single algorithm optionYou should now see all the available graph algorithms in the NEuler application.Available graph algorithms in NEulerThere are more than 30 graph algorithms available. As mentioned, here, you will learn how to execute graph embedding algorithms and visualize their results with TSNE scatter plot. Choose either Node2vec or FastRP algorithm. They are both graph embedding algorithms. A graph embedding algorithm calculates a fixed-length vector representation for each node in the graph whilst maximally preserving properties like graph structure and information. These embeddings are a lower-dimensional representation of the graph and preserve the graph’s topology.To learn more about the graph embedding algorithms, take a look at the documentation.I have chosen the FastRP algorithm in this example. You can use the default configuration or you can test various configuration parameters to see how it affects results. I have configured the embedding dimension to 128 and set the Rows to show” parameter to 150.FastRP algorithm configurationThe only thing left to do is to run the algorithm.Results of a FastRP algorithmOnce the algorithm is finished, you can observe the results in the table form. Each node has an embedding or a fixed-size vector assigned to it. In my case, I have used the embedding size of 128, so the vector of each node has the size of 128. Usually, the node embeddings are used in a downstream machine learning workflow. To quickly inspect and visualize the results of the embedding algorithm, you can click on the Scatter Plot option in the left menu.TSNE scatter plot of the graph embedding algorithm resultNEuler application uses the t-SNE algorithm under the hood to reduce the dimensionality of the embeddings to a size of 2. By reducing the vector dimensionality to 2, we can visualize them with a scatter plot. You can play around with various algorithm configurations to determine how it affects the embedding results. If you want to export the code needed to execute the algorithms outside the NEuler application, you can simply copy the generated code from the Code tab.Generated code used to execute graph embedding algorithm FastRPHopefully, this example will help you get started with node embedding algorithms in Neo4j Graph Data Science plugin. Download Neo4j Desktop, or try out one of the Neo4j Sandbox projects to get started with network analysis and graph algorithms.P.S. NEuler also supports network visualizations. Learn more about how to do them in the application documentation.Network visualizations in NEuler;May 6, 2021;[]
https://medium.com/neo4j/the-perfect-dorm-room-assignment-exploring-the-bipartite-matching-problem-with-neo4j-c59ade420acb;Nathan SmithFollowOct 7, 2019·5 min readThe perfect dorm room assignment: Exploring the bipartite matching problem with Neo4jI listened to a webinar recently where Jim Webber, Chief Scientist at Neo4j, recommended the book Networks, Crowds and Markets by David Easly and Jon Kleinberg. I found a copy of the book, and I heartily endorse Mr. Webber’s recommendation. Easly and Kleinberg bring together multiple disciplines related to network theory, including mathematics, economics, and social science. The book could be used as a textbook, but the examples are so engaging and clearly explained that it could also be read as a popular math/economics book. Think Freakonomics for graphistas.Each chapter closes with a set of exercises. You can do them with pencil and paper, but since I have been learning Neo4j, I have been thinking about how I could use Cypher and the APOC and Graph Algorithms libraries to explore the ideas in the text.Chapter 10 deals with matching markets. Easly and Kleinberg illustrated the bipartite matching problem by describing a scenario where there an equal number of college students and dorm rooms. Each student would prefer to live in only a subset of the possible rooms. We want to match each student to exactly one room that they find acceptable.Let’s set up this scenario in Neo4j. A blank Neo4j sandbox works well for this purpose.First, we create seven students.UNWIND [Alan, Bert, Carol, Donna, Elaine, Frank, Gena] AS studentNameCREATE (s:Student {name:studentName})RETURN sNext we create seven rooms.UNWIND RANGE(1,7) AS roomNumberCREATE (r:Room {roomNumber:roomNumber})RETURN rNow we randomly set up [:INTERESTED_IN] connections from students to rooms. This statement matches all possible Student to Room connections, and the WHERE clause keeps a random sample of 30% connections. You can experiment with a different percentage of starting connections to make the matching problem harder or easier.MATCH (s:Student), (r:Room)WITH s, r, rand() AS randomWHERE random < .3MERGE (s)-[:INTERESTED_IN]->(r)If you drag your rooms and students into two columns in Neo4j browser, your graph should look something like this.So far we have a very simple Neo4j schema. As I work through the process of matching students and rooms, I will change the relationship type from [:INTERESTED_IN] to [:MATCHED_WITH]. I will also add a :Matched label to the students and rooms to help me keep track of which rooms and students have already been assigned.The algorithm for finding a perfect matching of students to rooms goes as follows:1. Find a student (we’ll call her student A) who is not matched with a room.2. Choose a room that the student A is interested in.3. If that room has already been assigned to a different student (we’ll call him student B), see if student B is interested in another room.4. If student B’s alternative room has already been matched, see if the owner is interested in a different room.5. Continue searching in this manner until you have a path where the relationship types alternate between [:INTERESTED_IN] and [:MATCHED_WITH]. The path should start at an unmatched student and end at an unmatched room.6. Switch all the relationship types in the path from [:INTERESTED_IN] to [:MATCHED_WITH], and [:MATCHED_WITH] to [:INTERESTED_IN].7. Add :Matched labels at the start and the end of the path to show that they have been matched.Every time we step through that algorithm, a new student gets matched with a room. All students who were formerly matched with rooms remain matched with a room at the end of the run. Easly and Kleinberg provide a proof that if it is possible to create a perfect matching between the two groups in a bipartite graph, repeatedly running through this algorithm will produce that matching.Now let’s see how to code the algorithm with Cypher and APOC. Here’s the whole code block. We’ll walk through a section at a time.CALL apoc.periodic.commit( MATCH (s:Student)WHERE NOT s:MatchedWITH s LIMIT 1CALL apoc.path.expandConfig(s,    {relationshipFilter:INTERESTED_IN,MATCHED_WITH,INTERESTED_IN,     nodeFilter:Room|Student,      minLevel:1,      maxLevel:7,      filterStartNode:false}) YIELD path AS alternatingPathWITH s, alternatingPath,    nodes(alternatingPath)[-1] AS lastNode,relationships(alternatingPath) as relsWHERE lastNode:Room     AND NOT lastNode:MatchedWITH s, lastNode, rels LIMIT 1SET lastNode:MatchedSET s:MatchedWITH s, relsUNWIND rels AS relCALL apoc.refactor.setType(rel,      CASE TYPE(rel) WHEN MATCHED_WITH                     THEN INTERESTED_IN                     ELSE MATCHED_WITH END) YIELD outputRETURN count(*) )First, find a student who isn’t matched.MATCH (s:Student)WHERE NOT s:Matched...Now we’ll use APOC’s path.expandConfig method to find an alternating path from this student to an unmatched room.The relationshipFilter parameter INTERESTED_IN,MATCHED_WITH, INTERESTED_IN shows the alternating pattern we are looking for. Because filterStartNode is set to false, the first INTERESTED_IN relationship connects our start node to the rest of the path. From there, the MATCHED_WITH, INTERESTED_IN pattern continues to the end of the path.There are only seven rooms, so I don’t need to repeat the pattern more than seven times. That’s why the maxLevel parameter is set to 7. I need at least one step, so the minLevel is set to 1....WITH s LIMIT 1CALL apoc.path.expandConfig(s,     {relationshipFilter:INTERESTED_IN,MATCHED_WITH,INTERESTED_IN,     nodeFilter:Room|Student,      minLevel:1,      maxLevel:7,      filterStartNode:false}) YIELD path AS alternatingPath...Next, we need to check that the final room node that we found has not already been claimed, so we add a where clause. We also only need one alternating path, so we limit the results....WITH s, alternatingPath,     nodes(alternatingPath)[-1] AS lastNode,     relationships(alternatingPath) AS relsWHERE lastNode:Room     AND NOT lastNode:MatchedWITH s, lastNode, rels LIMIT 1...Finally, we use SET to add :Matched labels to the start and end of our path. Use apoc.refactor.setType() to switch the relationship type for each relationship in our path....SET lastNode:MatchedSET s:MatchedWITH s, relsUNWIND rels AS relCALL apoc.refactor.setType(rel,      CASE TYPE(rel) WHEN MATCHED_WITH                     THEN INTERESTED_IN                     ELSE MATCHED_WITH END) YIELD output...We use apoc.periodic.commit() to run through the algorithm repeatedly until all the students are matched, or we find a student that cannot be matched. If a student can’t be matched, there must be a constricted set in the data. That means there is a subgroup of students and rooms they are interested in where the number of students is greater than the number of rooms. If this happens, there is no matching that can make all students happy.My example had a constricted set. Because Frank and Alan are both only interested in room 3, there is no way we can make them both happy.;Oct 7, 2019;[]
https://medium.com/neo4j/augment-intelligence-with-graph-power-878bce611a8f;Ralf BecherFollowApr 12, 2019·5 min readAugment Intelligence with Graph PowerIn my two decades of working in the Data Warehousing, Business Intelligence and Analytics field, nothing has bothered me more than being tied to a relational data model.While this abstraction has its strengths in filtering and aggregating or rolling up large amounts of row/record shaped data, the major issue I have with it is that it hides the context and connectivity of the data points across records or tables.Of course, you always have the option todrill through your data back and forth,filter here and there,make a comparison analysis,calculate sophisticated KPIs and so on.But you’d still end up with a limited view (tabular) of your information.As an example, let’s compare a typical BI dashboard:A Fraud Analysis BI Dashboard in Qlik Sensewith a Graph representation of the same data:Same Fraud Data as in Dashboard modeled as a Graph in Neo4j, showing one component of the whole GraphWith this Graph, we can see and investigate the relations between the entities in our fraud use case: persons, shared credit cards, phone numbers etc.We see not just records but entities in their context.In fact, we can do more — we can also use the power of the Neo4j graph engine and the Neo4j graph algorithms to query the graph and to calculate KPIs that help us to understand the fraud and find suspicious elements or actors.Graph algorithms capture the topological information in one or more scalar metrics per node or relationship. They can be seen as an unsupervised learning method which labels our data to belong into certain groups.Let’s use the following Graph algorithms (free O’Reilly eBook) to investigate the fraudulent behavior:Page Rank:measures the transitive influence or connectivity of nodesBetweenness Centrality:amount of influence a node has over the flow of information in a graph, how it connects different clustersTriangle Counting / Clustering Coefficient:number of triangles each node in the graph participates inCommunity Detection:sets of connected nodesIn the following example we, compute pageRank on a derived graph (person to person) based on shared identities.Example Cypher Call in Neo4j to calculate and store the Page Rank between Persons in the Fraud GraphGraph result where I marked Persons with a high Page Rank, perhaps these acting as agents in a Fraud RingThe next step is to integrate the results into our BI-dashboard, to have all results under at our fingertips in one consolidated view.Qlik has released the server-side extension (SSE) protocol for Qlik Sense and QlikView in 2017. It allows us to connect and integrate the Qlik associative engine (QIX) with any other third-party system, such as a calculation engine and/or database. There are already some SSE implementations to integrate Qlik with R or Python to use it for advanced analytics tasks like forecasting or predictive analytics and machine learning.Recently, my company TIQ Solutions has implemented and released a Neo4j SSE. It is used to call and live-query the Neo4j Graph Database from Qlik in these scenarios:Qlik data load process (LOAD Script)any Qlik UI expression (measures in charts, tables expression for labels, etc.)TIQ Solutions Neo4j Datablending Extensions for Qlik SenseNeo4j Datablending and Live-Querying with Qlik SenseThe communication and data flow between Qlik’s BI web frontend and the SSE backend is handled by Qlik’s engine API on one side and the gRPC protocol on the other side. The SSE will process all function calls and integrates with Neo4j via Bolt protocol.The SSE protocol provides a couple of functions you can implement besides your own defined functions in your SSE code. In case of our Neo4j SSE we implemented these functions:TIQ Neo4j SSE — implemented functionsLet’s consider an SSE call we can add in a Qlik UI expression, e.g., in a table column or chart as a measure. The query result should give us the Page Rank by Person for the most suspicious fraudsters (see the Where Clause in Cypher statement):TIQ Neo4j SSE — example call in a Qlik expressionThe function call gets a Cypher statement as parameter (green) which will be processed in Neo4j. The QIX engine injects the dimensional data from the field in Qlik (blue).The data is then sent to the SSE in groups of rows (bundeled rows). In the case of our data set with the amount of around 100k persons, the QIX engine will split it into 3 or more calls.The SSE will inject the list of persons from each group into the Cypher WHERE clause place holder IN [#] (yellow) and executes the Cypher statement against Neo4j.The resulting measure max(p.pagerank) will then be matched to the incoming rows by person’s name. What’s important to mention here that the Cypher result set needs to include the dimension p.full_name, which is needed to map the results to the existing data.This also works the same way for a multi dimensional table or chart. In that case, the sequence of dimensions needs to be the same for the function call parameters and Cypher result set and in the WHERE clause placeholders the dimensional position needs to be added: WHERE … x IN [#1] AND y IN [#2]Finally, we can use a Cypher query to select the fraudsters from the whole dataset:Use a Cypher Query via SSE to select Fraudsters by Graph KPIs in Qlik SenseAnd then we can integrate the KPIs from the Graph algorithms into the Dashboard (see table and scatter plot) that gives us more insights:Screencast: Integrated Graph KPIs in Qlik Sense Dashboard Objects, TIQ Network Extension on the right sideThis integration shows how we can add the Graph power to an existing Qlik Sense Dashboard. I think there are many more use cases where we could break up the limitation of relational BI by adding a layer of context via graph data and injecting more complex calculations to leverage hidden insights.If you want to learn more, see the links below. If you are interested to try this out, please ping me via Twitter or ralf.becher@tiq-solutions.deReferencesQlik SenseNeo4j Graph DatabaseCypher Query LanguageGraph algorithmsSSE ProtocolTIQ Solutions Neo4j Graph Extensions for Qlik SSEFree O’Reilly eBook on Graph Algorithms;Apr 12, 2019;[]
https://medium.com/neo4j/neo4j-ops-manager-and-spring-data-neo4j-52c097aa08b1;Michael SimonsFollowJul 15, 2022·5 min readNeo4j Ops Manager and Spring Data Neo4jNom, nom, nom…In the last week of June 2022, Neo4j Ops Manager (NOM) has been released. Our colleague Chris published a great post with many insights into functionality but also into technical details about how NOM is build: Introducing Neo4j Ops Manager: The Tool You Need to Boost Your Ops Team’s Productivity.Introducing Neo4j Ops Manager: The Tool You Need to Boost Your Ops Teams ProductivityWith the growth of the graph category, the transition to cloud and the rise of graph data science, we are seeing more…neo4j.comOne part caught my eyes of course:The Neo4j Ops Manager server is a Java application, which manages the main logic, and also hosts the UI and agent APIs. It requires Java17 to run, and leverages Neo4j Spring integration and SDN (Spring Data Neo4j) to persist the metadata into a Neo4j database (persistence). Ideally this is a dedicated single instance database (for which a limited use, resource limited license is included).My colleagues Ali and Sascha haven’t cursed me too much while working on the backend which I take as an indicator that things went relatively smooth, which makes me really happy. It validates the approach that the whole SDN team took when developing version 6, which spun off SDN/RX and replaced SDN5+OGM in April 2021.Nom (Foto by Julie Elliott on Flickr: https://www.flickr.com/photos/angelinacupcake/4083276493)One of our main reasons to rework SDN was the goal to be fully compatible with an immutable domain-object-approach and be part of the ongoing reactive story.Now, I need to take a short detour: My own blog post about resilience, preparing for failure and retrying Try. And then retry. There can be failure. has been shared widely and it is an important topic till this day: The way Neo4j Causal Cluster works requires diligence when setting up a connection to it. There are some error scenarios that can safely be retried. All the official drivers provide a built-in feature to do this.However, those so called transaction functions” bring also transaction management along: They receive a (hopefully) small unit of work and execute it until it succeeds.Try. And then retry. There can be failure.With persistent connections, the exceptional case should be expected and not considered to be a surprise. Here is how…medium.comThat works well in many scenarios, but not everywhere. While it is a good idea for small applications, to execute a couple of statements and not needing to configure the retry mechanism, it doesn’t play well when your target ecosystem provides its own transaction management, such as Spring Framework and Spring Data in particular do.Why is that? A transaction in a Spring application usually reaches at least across a service and your preferred data access mechanism of choice, often a repository. While repository based queries alone could, of course, be wrapped into a transactional function, an elaborate service call might even involve external systems.How could the Neo4j driver then know whether the whole unit of work is safe to retry and even what constitutes the unit of work?And apart from that: Wrapping repository calls into Neo4j transactional or retryable functions would mean moving the control of the transaction flow from the driving factor (the application framework and the build-in transaction manager guided by the application) to the underlying driver. This can’t be the goal.Does that mean Spring Data Neo4j doesn’t work with Neo4j cluster setups? No, of course not!But you need to be diligent of potential error scenarios, for example in cases where you have a quickly changing cluster setup in the backend. While we generally recommend a retry mechanism (such as Resilience4j), the Neo4j driver is pretty good at catching error scenarios before a session and a transaction is opened and acquired. It will retry those anyway. If problems happen during the execution of a transaction, then not so much.Back to NOM: It uses the reactive variants of Spring Data Neo4j, which makes it even a bit harder to get retries properly implemented. Luckily, there’s a build-in Retry-Operator in Project Reactor which can be combined with Springs TransactionalOperator. The latter allows making reactive flows fully transactional in the Spring world.The thing to keep in mind is, that the Retry-Operator works by re-subscribing to the upstream publisher. That means build the whole flow, including transactional aspects and as the last step, apply the retry mechanics.How does this look like?First, a utility method is given to create a transactional operator with some defaults that work. It needs a properly configured ReactiveTransactionManager:Step 1: Create a reusable transactional operatorIn a second step, a factory method named retrySpec is defined:Step 2: Define one (or more) specifications how retries should be performed (here with an exponential backoff strategy)Here, Project Reactors Retry operator is configured to use an exponential backoff strategy and to apply Spring Data Neo4j’s predicate to check if something can be retried according to the drivers definition (here checking for an exception type).Both methods get combined into transactionalWithRetries:Turn any flow into a transactional flow which is retried as a whole if necessary and possibleWith the ReactiveNeo4jTemplate interaction does roughly look like this:Example of using the utility method of step 3 with Neo4j reactive templateIf there’s already an ongoing, declarative transaction (Yes, @Transactional will work too in a reactive world), the interaction would look roughly like this:Using only the retry specification with transactional, reactive repositoriesYes, assessing the risk of failure and putting proper mitigations in place is a bit of effort, but usually worth it. Like our NOM-team you don’t want to retry a whole, big interaction of which you don’t know if can safely be retried, but only specific cases where the retry operation makes sense.In the reactive world, additional operators are your tool of choice.Big shoutout from the drivers and Spring Data Neo4j team at Neo4j towards the NOM team. 🎉Get Neo4j Operations Manager at the Download Center;Jul 15, 2022;[]
https://medium.com/neo4j/using-neo4j-aura-from-java-and-spring-boot-f1c1684894f8;Michael SimonsFollowDec 19, 2019·3 min readUsing Neo4j Aura from Java and Spring BootLast month, we released Neo4j Aura with a big announcement.I want to share some quick information on how to use Aura from your Java application. Aura provides Neo4j instances, similar to your local setup. You need to use the Neo4j Java Driver which implements the Bolt Protocol.On Secure ConnectionsAura is a secure cloud offering and transport between your hosts and Aura is of course encrypted. Aura uses SSL certificates to identify itself. Those certificates are validated certificates being part of a trust chain. Aura requires the connection to be encrypted.The Neo4j Java Driver up to version 1.7.5 defaulted to encryption is on and trust all certificates”, meaning it would even trust certificates not trusted by the system which does give a false sense of security. Having encrypted traffic that anyone can intercept by impersonating the other side is not secure traffic.Therefore, in the most recent release of the driver, 4.0.0, we decided to turn off encryption by default and trust only certificates known to the system. That means if you want to have encrypted traffic between your client and the database, make sure the database provides a certificate thats verifiable by the client. Either through the system certificates or a custom certificate chain.What does that mean for Java applications and Aura?Applications using Java Driver 1.7.xPlain Java applications using org.neo4j.driver:neo4j-java-driver:1.7.5Spring Boot 2.1. applications using neo4j-java-driver-spring-boot-starter 1.7.xSpring Boot 2.1. applications using Spring Data Neo4j 5.1 with Neo4j-OGM 3.1Micronaut Neo4jcan just enter their Aura credentials and are good to go. Those applications are using a version of the Bolt driver that by default tries encrypted traffic. It’s still recommended to explicitly enable encryption, so you’re on the safe side when migrating: In case there are any certificates in your trust change that aren’t trustworthy, you will notice.Applications using Java Driver 4.0.0Plain Java applicationsusing org.neo4j.driver:neo4j-java-driver:4.0.0Use a Config instance:import org.neo4j.driver.AuthTokenimport org.neo4j.driver.AuthTokensimport org.neo4j.driver.Configimport org.neo4j.driver.Driverimport org.neo4j.driver.GraphDatabaseimport org.neo4j.driver.Sessionpublic class AccessAura {   public static void main(String... a) {      Config config = Config.builder().withEncryption().build()      String uri =  neo4j://XXXX.databases.neo4j.io       AuthToken auth = AuthTokens.basic( neo4j ,  secret )      try (Driver driver = GraphDatabase.driver(uri, auth, config)){         try (Session session = driver.session()) {            // Do something with you session         }      }   }}Spring Boot 2.2. applicationsusing neo4j-java-driver-spring-boot-starter:4.0.0Use Spring’s application.propertiesorg.neo4j.driver.uri=neo4j://XXXX.databases.neo4j.ioorg.neo4j.driver.authentication.username=neo4jorg.neo4j.driver.authentication.password=secretorg.neo4j.driver.config.encrypted=trueSpring Boot 2.2. applicationsusing Spring Data Neo4j 5.2 with Neo4j-OGM 3.2.4You have two options here. One is is adding the above starter and switching properties from spring.data.neo4j to org.neo4j.driver as shown above, Spring Boot will recognize this and configure Neo4j-OGM accordingly.If you don’t want an additional dependency, add this configuration:import org.springframework.boot.autoconfigure.data.neo4j.Neo4jPropertiesimport org.springframework.context.annotation.Beanimport org.springframework.context.annotation.Configuration@Configurationpublic class AdditionalNeo4jConfig {   @Bean   public org.neo4j.ogm.config.Configuration             neo4jOgmConfiguration(Neo4jProperties properties) {      String uri = properties.getUri()            .replaceAll( bolt\\+routing ,  neo4j )      String username = properties.getUsername()      String password = properties.getPassword()      return new org.neo4j.ogm.config.Configuration.Builder()         .uri(uri)         .credentials(username, password)         .encryptionLevel( REQUIRED ).build()   }}This takes the original Spring Data Neo4j properties and uses them to create an adapted OGM configuration. The important part here is setting the required encryption level.Spring Boot 2.2. using SDN/RXUse your Spring’s application.propertiesorg.neo4j.driver.uri=neo4j://XXXX.databases.neo4j.ioorg.neo4j.driver.authentication.username=neo4jorg.neo4j.driver.authentication.password=secretorg.neo4j.driver.config.encrypted=trueQuarkus with the Neo4j extensionThis is work in progress, needs some work in Quarkus. At the moment you would need to provide your own Driver instance. If the PR linked above is merged, configuration will be similar as with our Spring Boot starter:quarkus.neo4j.uri=neo4j://XXXX.databases.neo4j.ioquarkus.neo4j.authentication.username=neo4jquarkus.neo4j.authentication.password=secretquarkus.neo4j.config.encrypted=trueHappy coding in Java Land with Neo4j Aura.;Dec 19, 2019;[]
https://medium.com/neo4j/exploring-the-updated-neo4j-java-driver-a79a42ec451f;Michael SimonsFollowApr 3·5 min readExploring the Updated Neo4j-Java-Driver — Finding Trees in the ForestHow to turn a list of flat elements into a hierarchy with Java, Cypher, and the updated Neo4j driver for JavaPhoto by Adarsh Kummur on UnsplashRichard Macaskill wrote about The New Steer” for the Neo4j-Drivers a while back, and I would like to pick up on that topic, focussing on the Java Driver.A New Steer for the (Neo4j) Drivers?The Neo4j drivers team has been working on simplifying the experience of new users getting started with Neo4j.medium.comI would like to take the post from my friend Lukas Eder from Data Geekery as an inspiration.How to create hierarchies of Java objects from flat lists with CollectorThis article illustrates various ways of turning a flat representation of hierarchical data into an object or JSON…blog.jooq.orgCreating the Test DataWe are dealing with a parent/child relationship as it occurs in a hierarchical file system:create (:Path {name: child}) -[:HAS_PARENT] ->(:Path {name: parent})A simple HAS_PARENT relationshipTraversing a path with Java and turning the result into a Neo4j graph looks like this when using the new executableQuery API present in the 5.7 version of the Neo4j-Java-Driver:var paths = Files.walk(root)        .map(p -> Map.of(             parent_id , p.getParent().toString(),             id , p.toString(),             name , p.getFileName().toString()))        .toList()    driver        // creates an executable query        .executableQuery( MATCH (n) DETACH DELETE n )        // and executes it. There is no need to consume or close the result        .execute()    // Using an eager result    var result = driver        // Again, creating the executable query        .executableQuery(               UNWIND $paths AS path WITH path            MERGE (c:Path {id: path.id, name: path.name})            MERGE (p:Path {id: path.parent_id})            MERGE (c)-[r:HAS_PARENT]->(p)            RETURN c, r, p               )        // but enriching it with parameters        .withParameters(Map.of( paths , paths))        .execute()    // Gives you access to the result summary, including counters and more,    // no need to consume something upfront    var counters = result.summary().counters()    System.out.println(        counters.nodesCreated() +   nodes and   +        counters.relationshipsCreated() +   relationships have been created )    // The returned records are already materialized, iterating them multiple    // times is safe and does not involve multiple round trips    // the summaryStatistics here is a Java Streams API, not Neo4j Driver    var c1 = result.records().stream()        .mapToInt(r -> r.get( c ).get( name ).asString().length())        .summaryStatistics().getMax()    var c2 = result.records().stream()        .mapToInt(r -> r.get( p ).get( name ).asString().length())        .summaryStatistics().getMax()    var format =  | %1$-  + c1 +  s | %2$-  + c2 +  s |%n     System.out.printf((format),  Name ,  Parent )    System.out.println( |  +  - .repeat(c1 + 2) +  |  +  - .repeat(c2 + 2) +  | )    result.records().forEach(r -> {        var c = r.get( c ).asNode()        var p = r.get( p ).asNode()        System.out.printf(format, c.get( name ).asString(), p.get( name ).asString())    })}The above content demonstrates a couple of different topics already:There is no need to think about transactional functions or retriesThe executableQuery method creates a query that might or might not be enriched with parameters prior to executionWhen executed without a Collector, there is no need to consume the result furtherThe eager result however can be used as many times as necessaryThe result summaries are always available, no further action is necessaryThis API is an excellent choice for scripts, simple Java programs (or any Java program that does not need to hook into external transaction boundaries, such as Spring or Quarkus transactions).The program above gives me for the project in which I created the demo for this post the following output:22 nodes and 21 relationships have been created| Name                     | Parent             ||--------------------------|--------------------|| testride52               | null               || pom.xml                  | testride52         || .idea                    | testride52         || encodings.xml            | .idea              || uiDesigner.xml           | .idea              || jarRepositories.xml      | .idea              || inspectionProfiles       | .idea              || Project_Default.xml      | inspectionProfiles || .gitignore               | .idea              || workspace.xml            | .idea              || misc.xml                 | .idea              || compiler.xml             | .idea              || src                      | testride52         || test                     | src                || java                     | test               || main                     | src                || resources                | main               || java                     | main               || ac                       | java               || simons                   | ac                 || ExecuteQueryApiDemo.java | simons             |The most beautiful part of that API however is that it has native support for Java’s fantastic Collectors API. In the first example, we used it with a built-in Java collector.For example, retrieve all the names as a list:List<String> names = driver    .executableQuery( MATCH (n:Path) RETURN n.name AS name )    .execute(        Collectors.mapping(r -> r.get( name ).asString(),         Collectors.toList())    )But honestly, for this use case, you can use the eager result and a simple mapping function on the list of records.The collectors API is relevant for all client-side grouping and counting tasks. Jump back to the beginning and have a look at Lukas’ post. Notice the several suggestion to turn a hierarchy of paths into a hierarchy of Java objects.Can we follow them? First, with more or less pure Cypher, such as Lukas did with SQL? Of course, we can rather easily query the graph and return a tree as JSON:static void printWithApocAndComplexStatement(Path root, Driver driver) {  var result = driver.executableQuery(       MATCH (r:Path {name: $nameOfRoot})    MATCH (l:Path) WHERE NOT (EXISTS {MATCH (l)<-[:HAS_PARENT]-(:Path)})    MATCH path=(r) <-[:HAS_PARENT*]-(l)    WITH collect(path) AS paths    CALL apoc.convert.toTree(paths, false, {nodes: {Path: [-id]}}) YIELD value    RETURN apoc.convert.toJson(value) AS result       )   .withParameters(Map.of( nameOfRoot , root.getFileName().toString()))   .execute()  System.out.println(result.records().get(0).get( result ).asString()) }However, that requires us to have APOC installed in our database. We can do much better with the new executableQuery API. Given this record: record File(  @JsonIgnore String id,  String name,  @JsonInclude(JsonInclude.Include.NON_EMPTY) List<File> children) { }and verbatim taking the intoHierarchy Collector from the jOOQ post, we can just do this:static void intoHierachyAndPrint(Driver driver) throws IOException {    var result = driver        .executableQuery(               MATCH (p:Path) <-[:HAS_PARENT]-(c:Path)            RETURN                elementId(c) AS id,                elementId(p) AS parentId,                c.name AS name               )        // This will take care of iterating a non-eager-result-set        // for us plus all the added benefits of using retries internally        // It wont allow us to take the non-eager-result set out of        // transaction scope which is an excellent thing        .execute(intoHierarchy(            r -> r.get( id ).asString(),            r -> r.get( parentId ).asString(),            r -> new File(r.get( id ).asString(), r.get( name ).asString(), new ArrayList<>()),            (p, c) -> p.children().add(c)        ))    new ObjectMapper()        .writerWithDefaultPrettyPrinter()        .writeValue(System.out, result)}The result of that method — formatted as JSON — looks like this:[ {   name  :  testride52 ,   children  : [ {     name  :  .idea ,     children  : [ {       name  :  .gitignore     }, {       name  :  inspectionProfiles ,       children  : [ {         name  :  Project_Default.xml       } ]    }, {       name  :  jarRepositories.xml     }, {       name  :  uiDesigner.xml     }, {       name  :  encodings.xml     }, {       name  :  compiler.xml     }, {       name  :  misc.xml     }, {       name  :  workspace.xml     } ]  }, {     name  :  src ,     children  : [ {       name  :  main ,       children  : [ {         name  :  java ,         children  : [ {           name  :  ac ,           children  : [ {             name  :  simons ,             children  : [ {               name  :  ExecuteQueryApiDemo.java             } ]          } ]        } ]      }, {         name  :  resources       } ]    }, {       name  :  test ,       children  : [ {         name  :  java       } ]    } ]  }, {     name  :  pom.xml   } ]} ]The full source of the example is available as a GitHub Gist, directly runnable via JBang.Happy coding!;Apr 3, 2023;[]
https://medium.com/neo4j/discover-auradb-free-importing-gedcom-files-and-exploring-genealogy-ancestry-data-as-a-graph-98eee452673;Michael HungerFollowFeb 4, 2022·12 min readDiscover AuraDB Free: Week 18 — Importing GEDCOM Files and Exploring Genealogy/Ancestry Data as a GraphTwo weeks ago, a colleague asked if I could help him import his personal ancestry data into Neo4j. He sent me a GEDCOM file, and I gave it a try.If you missed our livestream, here is the recording:As you can guess, family trees are much better handled as a graph than as a bunch of text fragments.Here are some examples from history and pop culture, from the royal family, to Sirius Black’s family tree in Harry Potter” and the complex Game of Thrones” relationships, to the amazing Netflix series Dark” that I just started re-watching with my daughters. (Spoilers hidden behind links.)I looked at the GEDCOM file format (see below) and was reminded of a COBOL data file format (or formats I used in the long past for storing data myself).I remembered that my friend and colleague Rik Van Bruggen had imported his ancient family history into Neo4j eight years ago (also using GEDCOM but with some desktop application).But I wanted to have something simple that I could run on the command line.First thing I found was a tool written in go that was quite intriguing because it allowed to query GEDCOM files with a syntax similar to the JSON tool jq. You could extract individual attributes of a person as well as relationships to parents, children, etc../gedcom query -format csv -gedcom ~/Downloads/pres2020.ged .Individuals | { name: .Name | .String, born: .Birth | .String, died: .Death | .String, id: .Identifier, sex:.Sex, parents:.Parents | .Identifier, parentNames:.Parents } | head -3born,died,id,name,parentNames,parents,sex,,@I2184@,Paul Stobbe,[],[],Male19 Aug 1946,,@I1@,William Jefferson Clinton, [William Jefferson Blythe II (b. 27 Feb 1918, d. 17 May 1946) ⚭ Virginia Dell Cassidy (b. 6 Jun 1923, d. Jan 1994)] ,[@F2@],MaleBut, unfortunately, I couldn’t figure out how to return the IDs of the parents — only their names, which are not unique.So I looked further and found this really useful Python library that allows you to parse and query GEDCOM files.DatasetsAs I cannot share the personal information from my colleague, I found some public GEDCOM datasets that we can use.There are datasets for the British Royals, US-Presidents, Shakespeare, and Brontë. We’ll use the first two in our exploration.Here is an example section from the Presidents file — as you can see parsing that format would be quite tedious.1 NAME Barack Hussein /Obama/ II2 SOUR @S48@3 PAGE Gale Research Company Detroit, Michigan Accession Number: 9223923 DATA4 TEXT Record for Dr. Barack Hussein Obama3 _LINK https://search.ancestry.com/cgi-bin/sse.dll?db=4394&h=10717780&indiv=try1 SEX M1 BIRT2 DATE 4 AUG 19612 PLAC Honolulu, Honolulu, Hawaii, USA3 MAP4 LATI N21.30694 LONG W157.85832 SOUR @S48@3 PAGE Gale Research Company Detroit, Michigan Accession Number: 9223923 DATA4 TEXT Record for Dr. Barack Hussein Obama3 _LINK https://search.ancestry.com/cgi-bin/sse.dll?db=4394&h=10717780&indiv=try1 OCCU US President No. 44, Democratic2 DATE 20 JAN 20092 PLAC Washington, District of Columbia, USA3 MAP4 LATI N38.8954 LONG W77.03671 _PHOTO @M26@1 OBJE @M26@1 FAMS @F1061@1 FAMC @F1105@Pre-Processing with PythonThe python-gedcom library has a parser that reads a file and then allows to inspect its element and provide methods to provide attributes for each element.Elements can be IndividualElement, FamilyElement, FileElement or ObjectElement. Here, we’re interested in the IndividualElement and its attributes.For our Python script, we iterate over the elements of the file, and for the people (individuals), we get the:first namelast nameyear of birthyear of deathsexid (pointer)There is much more data available, but for our model these attributes are good enough.For the parental information mother and father we get the parent’s entries for this individual from the parser. We filter them by gender and get their ids to constitute the relationships later.In the datasets we’re looking at here, there are only male and female as genders and binary parents — for real data we can extend this.We’ll put all that data into a list and output it as comma-separated lines. In a real tool, we’d use dataframes for that.code/gedcom/gedcom2csv.pyYou pass the GEDCOM file as the argument to the script and redirect the output into a CSV file.python3 gedcom2csv.py pres2020.ged > presidents.csvExample output from the Presidents file.Now we can take these CSV files and import them into Neo4j.Data ModelThe data model is really straightforward, just a Person node with attributes like first and last name, birth and death-years, sex, and id.Then we have relationships MOTHER and FATHER to other persons based on the ids.Create a Neo4j AuraDB Free InstanceGo to https://dev.neo4j.com/neo4j-aura to register or log into the service (you might need to verify your email address).After clicking Create Database you can create a new Neo4j AuraDB Free instance. Select a Region close to you and give it a name, e.g. {db-name}.Choose the blank database” option as we want to import our data ourselves.On the Credentials popup, make sure to save the password somewhere safe. The default username is always neo4j.Then wait 3–5 minutes for your instance to be created.Afterwards you can connect via the Open Button with Neo4j Browser (you’ll need the password).Then also the connection URL: neo4j+s://xxx.databases.neo4j.io is available and you can copy it to your credentials, as you might need it later.If you want to see examples for programmatically connecting to the database, go to the Connect” tab of your instance and pick the language of your choice.ImportAs before, we use the handy, upcoming data importer visual tool for Neo4j to import the data (see the video).Load the fileCreate the person nodeMap properties from the file (except for mother and father)Select the id as id propertyDrag out the relationship and connect it back to the node again to create a self relationship”, and name one FATHER and one MOTHERMap the file again to the relationships from id to the ids of mother and father respectivelyData Importer UIThen use the connection URL from your AuraDB Free instance, and the password you hopefully saved into the Run Import form and click Run.After a few seconds you should see these results. For each graph element you can show the constraint creation and the actual import statement that you could use in your own code.First Exploration with Neo4j BrowserWe want to extend the data in the database a bit with additional, inferred information.You can open Neo4j Browser” from your AuraDB Free instance to execute the statements. You need the password again to log in.There we can start exploring by opening the left sidebar and clicking on the relationship-count.Then we see the graph data and can select one of the existing properties to be displayed as caption.As both last and first names are not conclusive let’s combine them into a name property, which makes it nicer to display our data.MATCH (p:Person)SET p.name = p.first +   + p.lastCREATE INDEX ON :Person(name)So we can pick that for the display and see data.We can also query our data directly, e.g. if we want to see the Kennedy” families down to arbitrary relationship depth.MATCH path=(p:Person)<-[*]-()WHERE p.last = KennedyRETURN pathVisualization with Neo4j BloomNeo4j Browser is more a developer tool for writing, executing Cypher queries, and visualizing their results.Neo4j Bloom is a no-code visualization and exploration tool. You can just start typing Graph patterns into the search bar or if you have indexes (like ours on Person(name)) just type the name, hit return, and see a beautiful graph visualization that you then can explore further.You can lay out the graph data both as force graph and hierarchical layouts. The latter is especially interesting with hierarchical data like our ancestry.So let’s try it. Open Neo4j Bloom from your AuraDB Open Button and log in with username (neo4j) and your password.Enter: Abraham Lincoln Person Person” into the search bar, select the graph pattern, and hit return.You should see something like this:If you want to see all the data, justenter: Person” into the search barselect all nodes with ctrl/cmd+aright click on a node and choose Expand→AllWhat was surprising to me was the long chains of parental relationships going all the way through the presidential data and not a lot of isolated islands of subgraphs.Perhaps the US is not so far from an inheritance of power after all :)I styled the relationship by color, but you could also style the nodes with icons, color them by unique values (e.g. male/female), or size them by the year people were born (more recent→bigger) or the number of children/ancestors (currently would need a property on the node for the styling).Here is an example for a styled graph.Adding New RelationshipsOf course, we can add new, inferred relationships to our data, either from itself or by integrating it with external data.In our example, we want to:Add a global” family by nameAdd a core-family to its membersAdd sibling relationshipsAdding Family NodesThe global family” is not 100 percent correct, as we create a global ancestral Family node just by last-name and connect every person with the same last name to it (think Smith”).MATCH (p:Person)MERGE (fam:Family {name:p.last})MERGE (p)-[:FAMILY]->(fam)But we can use it to spot a common family that we’re sure of.Another option is to create a CoreFamily for each parents-children family, so we would create the CoreFamily node in context of (one of) the parents.As either parent could be not existing, we would use a fallback mechanism to create that core-family for a person.MATCH (p:Person)// parents might not be thereOPTIONAL MATCH (p)-[:FATHER]->(f)OPTIONAL MATCH (p)-[:MOTHER]->(m)WITH p, coalesce(f,m,p) as root, // non-null members [member in [f,m,p] WHERE NOT member IS NULL] as members// create family in context of root personMERGE (root)-[:CORE_FAMILY]->(fam:CoreFamily)SET fam.name = root.lastWITH members, famUNWIND members as memberMERGE (member)-[:CORE_FAMILY]->(fam)What we could then do is to connect the core-families from one generation to the next:MATCH (curr:CoreFamily)<-[:CORE_FAMILY]-      (member)-[:CORE_FAMILY]->(ancestors:CoreFamily)WHERE exists {      (parent)-[:CORE_FAMILY]->(ancestors:CoreFamily),       (member)-[:FATHER|:MOTHER]->(parent)}MERGE (curr)-[:ANCESTORS]->(ancestors)Family Ancestry for the Kennedy Family in Neo4j BrowserFamily Ancestry in Hierarchical Layout in BloomWe can derive siblings from two people having a joint parent — in our case one joint parent is enough. We could also change the query where both parents need to be the same.Siblings with Single Shared Parent// from person to parentsMATCH (p:Person)-[:MOTHER|FATHER]->(parent)// other person that shares a parentMATCH (sib:Person)-[:MOTHER|FATHER]->(parent)// not the same personWHERE p <> sib// create a sibling relationship (undirected)MERGE (p)-[:SIBLING]-(sib)Siblings with Two Shared Parents// from person to parentsMATCH (dad)<-[:FATHER]-(p:Person)-[:MOTHER]->(mom)// other person that shares a parentMATCH (dad)<-[:FATHER]-(sib:Person)-[:MOTHER]->(mom)// not the same personWHERE p <> sib// create a sibling relationship (undirected)MERGE (p)-[:SIBLING]-(sib)If we look at some of them we see binary to 7-sided sibling clusters.Direct Import with the Python DriverInstead of exporting a CSV file we can also directly write the data to Neo4j.In a copy of our script, we install the neo4j driver dependency and add the import for GraphDatabase.The information for the NEO4J_URI and NEO4J_PASSWORD comes from environment variables that we need to set to our connection details from Neo4j Aura. We use that information to create our driver.Instead of creating and outputting a row list for each individual person, we populate a dict per and add it to a data list.The Cypher statement that we use to create our node data in Neo4j can be directly copied from the data importer.The relationships to the parents are a tiny bit trickier as either of them might not be there in the data.So either we could:Run a double-pass over the data, filtering out individuals that have no parents of the current typeAdd a conditional subquery to add the relationshipDefault to an unknown” parent and clean up aftercode/file-gedcom2neo4j-py#!/usr/bin/python3# usage: NEO4J_URI= bolt://localhost  NEO4J_PASSWORD=secret ./gedcom2neo4j.py file.ged# https://pypi.org/project/python-gedcom/# https://pypi.org/project/neo4j/import sysimport osfrom gedcom.element.individual import IndividualElementfrom gedcom.parser import Parserfrom neo4j import GraphDatabasedriver = GraphDatabase.driver(os.getenv(NEO4J_URI), auth=( neo4j , os.getenv(NEO4J_PASSWORD)))file_path = sys.argv[1]gedcom_parser = Parser()gedcom_parser.parse_file(file_path)root_child_elements = gedcom_parser.get_root_child_elements()statement_w_cleanup =            UNWIND $data as row         MERGE (p:Person {id:row.id})         SET p += row {.first, .last,.sex,            death: toInteger(row.death),birth:toInteger(row.birth)}        SET p.name = p.first +   + p.last        MERGE (m:Person {id:coalesce(row.mother,unknown)})        MERGE (p)-[:MOTHER]->(m)        MERGE (f:Person {id:coalesce(row.father,unknown)})        MERGE (p)-[:FATHER]->(f)        WITH count(*) as total        MATCH (d:Person {id:unknown})        DETACH DELETE d        RETURN distinct total           statement_conditional =            UNWIND $data as row         MERGE (p:Person {id:row.id})         SET p += row {.first, .last,.sex,            death: toInteger(row.death),birth:toInteger(row.birth)}        SET p.name = p.first +   + p.last        CALL { WITH row, p           WITH * WHERE NOT coalesce(row.mother,) =            MERGE (m:Person {id:row.mother})           MERGE (p)-[:MOTHER]->(m)           RETURN count(*) as mothers        }        CALL { WITH row, p           WITH * WHERE NOT coalesce(row.father,) =            MERGE (f:Person {id:row.father})           MERGE (p)-[:FATHER]->(f)           RETURN count(*) as fathers        }        RETURN count(*) AS total           data = []root_child_elements = gedcom_parser.get_root_child_elements()for e in root_child_elements:    if isinstance(e, IndividualElement):        (first,last) = e.get_name()        row = { first :first,  last :last}        row[ birth ]=e.get_birth_year()        row[ death ]=e.get_death_year()        row[ gender ]=e.get_gender()        row[ id ]=e.get_pointer()        parents = gedcom_parser.get_parents(e)        row[ mother ]=next(iter([p.get_pointer() for p in parents if p.get_gender() == F]),None)        row[ father ]=next(iter([p.get_pointer() for p in parents if p.get_gender() == M]),None)        data= data + [row]with driver.session() as session:    total = session.write_transaction(        lambda tx: tx.run(statement_w_cleanup, data = data).single()[total])    print( Entries added {total} .format(total=total))One tricky aspect that we ran into in our livestream was that the IDs of people are not unique globally, but only per GEDCOM file.So if you import the British Royals into the same database as the American First Families, you get a whole mess. It then makes sense to either prefix the id’s with the filename that they came from or try to create a more global identifier of — full-name, birthday, and birth location and even that might not be unique but could at least be used to merge multiple datasets together.Another aspect of that uniqueness check could be to check the topological context of an individual, i.e. with the same name, birthday, birthplace location, and see if they also have the same parents and/or the same children.ConclusionWe would love to hear if this was helpful for you to import your GEDCOM files into Neo4j and what insights you found or added in terms of information.After sharing our experiments internally we learned that actually several of our colleagues had written custom GEDCOM parsers to get their family history into Neo4j.Happy graphing!;Feb 4, 2022;[]
https://medium.com/neo4j/create-neo4j-database-model-with-chatgtp-1ed91f16b724;Konrad KalicińskiFollowJan 17·4 min readCreate Neo4j Database Model with ChatGPTProper modeling of a graph database may be challenging. Because it requires a little bit of a different approach than relational database, we need to take into consideration what types of questions we want to answer.Luckily, nowadays we can use tools like ChatGPT that can help us to model our data. Of course, it won’t replace a skilled data engineer however, it can be a good starting point to evaluate our problem. Let’s check how it will perform with the semi-real case (it’s something that I’m working on after hours).The problemI have a son with Down Syndrome and it’s challenging to track all the supplements he takes and their influence on all the processes in the body.So I want to build a database of supplements and their ingredients. Here are few informations I’d like to include in the database:supplements can have multiple ingredientssupplements are brandedeach supplement ingredient can have influence on processes in the bodysome supplements can be taken together to increase their effectsome supplements cannot be taken together and there must be time between intakes to reduce harmto check if a supplement works, some blood tests must be performedeffects of the supplement should have a reference to the docs (medical publications, official papers, etc.) to support its influence, as well as supplements that should/could be intaken together.Let’s try ChatGPT!OK, so now it’s time for a try. Let’s just throw the questions right into ChatGPT and see what happens:The question / prompt:ChatGPT prompt for a model with these requirementsThe response:ChatGPT response with the model descriptionPretty cool, although we would like to have a manufacturer as a separate node. Let’s add that requirement:ChatGPT response with enhanced modelNow, let’s ask for CYPHER queries that can answer some of our questions:Generated Cypher queries for the model (part 1)Generated Cypher queries for the model (part 2)Looks pretty good, but it’s not ideal.If we want to enhance a particular item, we could also use chat for that. Take a look at the Supplement and the dosage property. It does not include a case when a child is growing and may need different volumes of the supplement depending on age or weight. Let’s update the model.Model update for different age dosagesOr, if we want to add some information about any node, for example, the cost of the supplement in different online stores.Cost of supplements in different online storesWe also can ask about Cypher code that generates the sample database so we could play with it:give me the Cypher code for the latest version of the entire modelAnd it will give it to us. We can even ask for more data to be returned, and it will generate a few items, too:generate dummy data for all nodes and relationshipsAs well, it will return some more results.Although, I don’t want to ask too much, just in case AI takes over the world and wants to exterminate all humans — perhaps it will spare me )ConclusionAs you can see, ChatGPT can be pretty useful to generate basic data model. And the more specific you are, the better results you’ll get. The nice thing about it is that it has the context of the entire conversation. So it’s pretty comfortable. As a parent of a kid with DS, or even as someone who wants to track all the supplements I take, I see this chatbot as an opportunity to make some parts of building applications easier. And fun.For more explorations of ChatGPT with Neo4j also see:Week 38 — Exploring ChatGPT for Learning, Code, Data, NLP and FunLast week ChatGPT was launched by open.ai. In todays stream we want to see if we can apply it to Learning Graph…medium.com;Jan 17, 2023;[]
https://medium.com/neo4j/discover-aura-free-week-39-nobel-prize-d96f2c09f297;Michael HungerFollowDec 14, 2022·7 min readDiscover Aura Free: Week 39 — Nobel PrizeThe Nobel Prizes in Physics, Chemistry, Medicine, and Literature — and the Nobel Peace Prize — were awarded over the last few weeks, and we thought it’d make a nice dataset to import and query as a graph. There are a few dimensions to it, and you can use it to create a knowledge graph if you connect it to papers, authors, publications, and research data.If you missed our livestream, here is the recording:First, I found a Kaggle Dataset with some CSV and JSON data, but that ended in 2019 — fortunately, I spotted a link to the original data source with API links to https://nobelprize.org.Alfred Nobel Prize MedalSo I went there and looked at this year’s Prizes and some Lesser Known Facts, which we can also use for our queries:People that were awarded more than one prizeAge of laureatesMost common affiliations with institutions and their impactBreakdown by countryYears without awardsSadly, the low percentage (10%) of non-white-male recipients of Nobel PrizesData SourceIn the footer of the page, there’s actually a link to a developer page, which is great.They have a new v2.1 REST API with an OpenAPI specification, so you can grab prizes (664) and laureates (981) with a lot of detail and pagination. The developer page also links to a Linked Data (RDF) API with a SPARQL endpoint.There is also an older v1 API that provides both JSON and CSV outputs based on the same data (I checked the id’s). That’s also what the code-example on the developer page talks about.To make it easy to start, I just used the option of getting the v1 API responses as CSV for prizes and laureates.We can later add to the data in the graph by querying select parts of the data as JSON from the new v2 API and merging them into our graph.Data ModelWe developed the data model incrementally based on the data in the CSV, our understanding, and the questions we wanted to ask.Nobel Prize Data ModelSo we got as nodes:Prize (Nobel Prize)Person who received the PrizeYear for the PrizeCategory for the PrizeInstitution the person is affiliated withCountry for birth, death of the person, and the institutionCreate a Neo4j AuraDB Free InstanceGo to https://dev.neo4j.com/neo4j-aura to register or log into the service (you might need to verify your email address).After clicking Create Database you can create a new Neo4j AuraDB Free instance.Create InstanceChoose the  Empty Instance  option as we want to import our data ourselves.On the Credentials popup, make sure to save the password somewhere safe. It’s best to download the credentials file, which you can also use for your app development.AuraDB Credentials DownloadThe default username is always neo4j.Then wait two to three minutes for your instance to be created.Afterwards, you can connect to the instance via the  Open  Button with Workspace (you’ll need the password), which offers the  Import  (Data Importer),  Explore  (Neo4j Bloom), and  Query  (Neo4j Browser) tabs to work with your data.Connect Dialog WorkspaceOn the database tile, you can also find the connection URL: neo4j+s://xxx.databases.neo4j.io (it is also contained in your credentials env file).If you want to see examples of programmatically connecting to the database go to the  Connect  tab of your instance and pick the language of your choice.Instance DetailsData ImportUnfortunately, the prize itself has no real unique id in places where they refer to it, they use a combination of category and year.So I used xsv select 1-8,category,year prize.csv > prize2.csv to duplicate the two columns at the end. And then ran a regular expression replacement in VS Code to replace the comma between the last two elements with a dash, so this is now our id column, called categoryYear with entries like chemistry-2022.The other bit that we had to fix was to replace dates like 0000-00-00 with nothing in the laureates CSV (i.e. a null value) and also replace the -00-00 suffix from some dates with nothing as well so that just the year remained (but the column can still be imported as datetime) as Cypher’s date functions don’t like the zero value months and days.Then we mapped out the different fields to nodes and relationships — thankfully data importer often pre-filled the mapping for us for the relationships and ids.Neo4j Data Importer with CSV and MappingOne particular aspect where we changed the mapping in the model was to extract threeCountry meta-nodes to represent the countries coming from the three different sources (born, died, institution) and create the right entries and relationships. Each of those three country mappings has the same property name that the different source column names are mapped to.Data Importer PreviewAfter finishing the mapping we could run the preview, see that we mapped our data correctly, and then click  Import. ExploreAfter import, we’re sent directly to the Explore tab which gives us an initial view of our data  Show me a graph.  We can now style our data by picking the right captions and icons in the right-side legend.Explore: Show me a GraphWe can also explore our data starting from a node, here Harvard Medical School and then expanding the pattern to people, their prizes, and years. After getting the results we can select all (Cmd+A/Ctrl-A) and choose  Expand All  from the context menu, so we get a more complete picture of the context of that institution.Explore: Context of InstitutionQueryTo answer some of the initial questions from the facts section, we moved to the  Query  tab.First looking at Laureates with more than one prize, we can express that as a pattern, of people having received two prizes.Most that show up here are organizations — it gets interesting when looking at people who won prizes in different categories with a WHERE p1.category <> p2.category, which are actually just two  Marie Curie  and  Linus Pauling. match (p1:Prize)<-[:RECEIVED]-(p)-[:RECEIVED]->(p2:Prize)where id(p1)<id(p2)return p1.category, p1.year, p.firstname + p.surname as name, p2.category, p2.yearorder by p1.category ascQuery: Multiple PrizesAlternatively, you can also query for the base pattern and then aggregate per person how many and which prizes they got and filter for recipients that had more than one.match (p:Person)-[:RECEIVED]->(pr:Prize)with p, collect(pr) as prizeswhere size(prizes) > 1return p.surname, p.firstname, size(prizes) as count,       [pr in prizes | pr {.category, .year}] as prizesAffiliations with institutions have a big impact on the Nobel prize, as you can see in the following query, with institutions from the US being over-indexed.match (i:Institution)<-[:AFFILIATED_WITH]-()-[:RECEIVED]->(pr:Prize)return i.name, i.country, i.city, count(*) as count order by count desc limit 20Query: InstitutionsTo compute the age, we turn the year of the prize into a date (date({year:prize.year})) and the born and died datetimes from the data importer into dates (date(p.born)), then we can compute the age difference by using duration.beetween(date1, date2).years and sort accordingly.match (p:Person)-[:RECEIVED]->(pr:Prize)return p.firstname, p.surname, p.born, pr.year, duration.between(p.born, pr.year).years as yearsorder by years asclimit 10Query: Age — YoungestNominationsThere is also data on nominations available — actually quite a lot with 20,424 nominations.Unfortunately, you cannot access it through the API, just through a crude PHP search interface with HTML output. So to get that data you’d have to scrape it from the web.There is also a visualization page available — perhaps that’s an easier way to get to the data. (It seems it is via this URL that returns JSON).We also learned that nomination data is kept secret for 50 years, so the latest data available is from 1971. Probably to keep feuds, bribery, and similar research vengeance until after the laureates and nominators are dead.ConclusionAs mentioned in the introduction, this dataset can be nicely combined with citation datasets and perhaps research grants and projects in general. So you could see how the influence of Nobel laureates spreads across the research networks and which institutions are perhaps more privileged than others.Definitely a good starting point for a research knowledge graph. Let us know in the comments if you have more ideas or found this useful.;Dec 14, 2022;[]
https://medium.com/neo4j/neo4j-etl-now-ready-for-the-cloud-f76abd3eda5;Michael HungerFollowSep 21, 2019·2 min readNeo4j ETL now Ready for the CloudI’m happy to announce that the latest release (1.4.1) of the Neo4j-ETL tool graph-app now also works well with remote databases.Now you can import your relational data into any self-hosted or provisioned remote database in a few simple steps.1. Add your remote database to Neo4j Desktop2. Add the ETL-Tool to Neo4j Desktop and then your project. Best via the graph app gallery (install.graphapp.io)3. Make sure the database is activated i.e. Desktop connected to a running database4. Start the ETL tool5. Add a JDBC connection to your RDBMS6. Start the mapping process7. Edit the relational to graph mapping as needed8. Import the relational data via Mode Batch” (you can leave the settings unchanged)9. DoneExplainer video for importing RDBMS data into a remote Neo4j instanceWe had to adapt two things to make this possible. Thanks a lot to our partners at Larus BA, Italy for working on these features.In the past, the ETL-Tool used LOAD CSV of a local file url, which doesn’t work anymore if your database is remote and can’t access your local CSV files.So we added a mode that reads the CSV locally and sends batches of rows as parameters to the remote Neo4j Database, where they are inserted using UNWIND and MERGE.Secondly as Desktop doesn’t provide a local directory for remote databases to store mapping information, config and the CSV dump in, we use the temporary directory on your machine.We also enabled support for the bolt+routing scheme to connect to a causal cluster of Neo4j instances.Graph model of the imported dataPlease try out the ETL tool and let us know how it works for you for different relational databases and Neo4j deployments. If you encounter issues please raise them on GitHub.Happy importing,Michael;Sep 21, 2019;[]
https://medium.com/neo4j/combining-3-biochemical-datasets-in-a-graph-database-8e9aafbb5788;Tom NijhofFollowOct 3, 2022·6 min readCombining 3 Biochemical Datasets in a Graph DatabaseThe open measurement graph will be used to find connections between different measurements in different experiments and conditions. In a previous blog, we added chemical compounds — and their synonyms — from PubChem into the graph. In this blog, we are going to populate the database with NCI60 measurements.Before we do that, we will first design the graph structure. Biomedical research is done in experiments, each holding multiple conditions. An experiment is a set-up, and a condition is a single instance of this set-up with one set of variables.In the case of NCI60, the experiment is a cell growth medium that is measured for confluence at timepoint zero and 48 hours. The cell line and compound will be variable. A condition would be cell line A498 with compound NSC 19893.In the case of NCI60, it has 5065 experiments. All of them are done as identically to the others as possible. However, slight variations can occur.This includes: - who did the experiment- what were the values of everything NOT measured- for biomedical research: how did the cell lines evolve between experiments?This is why we note down the experiment.Graph designThe PubChem graph has two node types and one relationship type.(Synonym)-[:IS_ATTRIBUTE_OF]->(Compound)We keep this in mind while designing, so the rest will follow a similar pattern.An experiment has a few things it is always using (USES) — these are the constants of the experiment. An example of this is the protocol or the growth medium of the cells (compound). Connecting these to the experiment makes it clear they do not vary between conditions and prevent repetition. Some User OWNS this experiment.The protocol has a synonym in between so different synonyms of the same protocol can be used, given that people rarely agree on a single name.Next up are the Conditions, which are attributes of an experiment (IS_ATTRIBUTE_OF). This way, they have the same relationship as synonyms to compounds. A condition USES its variables synonyms (compound and cellLine from NCI60) and MEASURES a Measurement (like GI50). A condition is not limited to how many measurements it does, or to how many variables it uses.An example of two conditions of the same experiment, both measuring GI50 but on different cell lines and different compoundsNCI60 datasetNCI (National Cancer Institute, USA) has the NCI60 dataset. This dataset contains 50k+ chemicals tested on ~60 cell lines. Almost all of these cell lines are human cancer cell lines.If we plot the NSC compounds against the number of unique Cell lines tested with that NSC compound, we find that most compounds are tested with at least 50 cell lines. This is for the GI50 part of the data (concentration of the compound needed for 50% Growth Inhibitor).The missing cell lines can come from failed experiments since compounds are not fully tested if the effect is too small in the first 1 dose step’” of the protocol.Number of unique combinations of NSC compounds and cell linesIn order to add the experiment to the database, we need to do two things: figure out which synonym fits the NSC number, and introduce cell lines to connect to.Introducing cell linesCell biologists like to make use of cell lines in their experiments. These are made, selected, or designed to last for many generations. The cell lines give consistency to the different experiments. These cell lines are also shared around the world, bringing different experiments closer to each other.If we zoom in on cancer, we will find that no two cancers are the same. Every patient has unique cells, followed by a unique mutation that turned the cell into cancer. If researchers use these in their experiments, no one can repeat their findings unless they get the exact same cells. This is something that is not always possible because many cancer cells will die after a number of divisions. Other times, the patient did not give permission to share with other researchers.Cancer cell lines are often immortal. A549 is one of these immortal lung cancer cell lines. If multiple experiments were done on A549, those could be more easily compared to each other than when every experiment used its own lung cancer cells.The NCI60 experiments are also done on these kinds of cell lines. For the ontology, the Chembl cells RDF is used and loaded in with n10s. This gives us 2000 cell lines, 1466 are human cell lines.While 2000 is very limited, the Chembl cells are easy to load in and can be connected to the Cellosaurus if I want to extend the cell lines.Chembl taxonomy goes from the cell line directly to the species and it is not very useful for NCI60, given that those are almost all done on human cell lines — except for four conditions that use the famous CHO cells (Chinese hamster ovary cells).With the help of a script, I selected the Chembl cell lines with the best match for the NCI60 cell lines. After that, a manual step follows to select the best match. The results are stored in a JSON.Picking the NSC SynonymIn a previous blog, I connected and found all NSC numbers related to a compound. In there, we found an NSC synonym for 55k NSC numbers (e.q. NSC 123” or NSC-123” as synonyms for NSC number 123). However, connecting an NSC number to multiple synonyms could cause problems, because it seems like multiple chemicals are used.The goal is to connect every NSC number to one — and only one — synonym.Single compound90.4% of the NSC numbers have only one compound shared between all its synonyms. For these, we pick the first synonym. This deals with 50k of 55k NSC numbers.Update synonymsIn a previous blog, we found that not all synonyms are up-to-date. We are going to reuse that function again on the 9.6% that does not have a single compound. If we do NOT do this, the next part will only match 14 of 5287 NSC numbers.Better connected oneIn some cases, one synonym is connected to all compounds of all other synonyms. If this is the case, we pick this one. This also works if all synonyms are connected to multiple compounds. This increases our coverage to 97.9%.Another well-connected synonymFor the next 1144 nsc numbers, we still do not have a synonym picked. In this case, we are going to look for a synonym that connects all compounds, but this synonym does not have to be an NSC synonym.Below, we see an example of nsc number 100723. The synonyms nsc100723” and nsc-100723” are connected to compound 1, but nsc 100723” is connected to compound 2. None of the nsc synonyms connect to both compounds, but the synonym 1-(2-chlorophenyl) hydrazine hydrochloride” does. This matches another 503 synonyms, bringing the coverage to 98.8%.The query is:// Get all NSC synonymsCALL {CALL db.index.fulltext.queryNodes(‘synonymsFullText’, $nsc_query)YIELD node, scorereturn node limit 10}// Get all compounds connect to the synonymsMATCH (node)-[:IS_ATTRIBUTE_OF]->(c:Compound)WITH collect(DISTINCT c) as compounds// Find all synonyms connected to ALL compoundsUNWIND compounds as cMATCH (s:Synonym)-[:IS_ATTRIBUTE_OF]->(c:Compound)WHERE ALL(compound IN compounds WHERE (s)-[:IS_ATTRIBUTE_OF]->(compound))// Return those synonymsWITH DISTINCT s as sRETURN s.name as name, s.pubChemSynId as synonymIdThe last 641 NSC numbersLet’s have a look at nsc 5038, one of the not matched NSC numbers. Its synonyms are connected to compounds 720071 and 221197, but these compounds are really not the same. Picking one or the other requires knowledge I do not have. This is the case for all random numbers I investigated. So this last 1.2% will be ignored for now.ResultUsing the filtering and selection above, the final script combines all three datasets. This results in 4m conditions, which are part of 4.6k experiments. In my next installment, I will explore the dataset.;Oct 3, 2022;[]
https://medium.com/neo4j/introducing-graphql-architect-19b0f2035e21;William LyonFollowJul 20, 2020·8 min readIntroducing GraphQL ArchitectBuild Low-Code GraphQL APIs Powered By Neo4jGraphQL Architect is a graph app for Neo4j Desktop that enables developers to build, query, and deploy GraphQL APIs backed by the Neo4j graph database, all from within Neo4j Desktop.Install GraphQL Architect from the Neo4j Desktop Graph Apps Gallery.This video shows how to install and use GraphQL Architect to build and query a GraphQL API within Neo4j Desktop.Low Code GraphQL With GraphQL ArchitectThe goal of GraphQL Architect is to simplify the process of developing, querying, and deploying GraphQL APIs.Features:Query local or remote GraphQL APIs using GraphiQL.Generate and run a local GraphQL API from an existing Neo4j Database, without writing any code. The schema will be inferred from the database’s data model.Alternatively, drive your Neo4j database’s property graph model by defining its GraphQL schema using the GraphQL Schema Definition Language (SDL).Add custom logic by adding Cypher statements to your GraphQL schema using the @cypher GraphQL schema directive.Edit the GraphQL schema using a schema-aware GraphQL type definition editor with linting and semantic auto-complete suggestions.Export and deploy your GraphQL API application.With GraphQL Architect you can build, query, and deploy GraphQL APIs backed by Neo4j using just GraphQL SDL — no code is necessary. Custom logic can be added to your API using the Cypher query language.GraphQL Architect is a low code tool for building, querying and deploying GraphQL APIs powered by Neo4j. You can install it from the Neo4j Desktop Graph Apps Gallery.InstallationInstall GraphQL Architect from within Neo4j Desktop using the Graph App Gallery or from install.graphapp.ioGraphQL Architect is a Graph App for Neo4j Desktop. That means you’ll first need to have Neo4j Desktop installed, which you can download here. Neo4j Desktop allows you to manage multiple Neo4j projects, install Neo4j DBMSs, work with remote databases, install and launch Graph Apps, and much more. Think of Neo4j Desktop as Mission Control” for working with Neo4j.Once you have Neo4j Desktop installed, open the Graph Apps Gallery and look for Graph Architect in the list of apps available. Just click on the Install” button and you’ll be ready to start using GraphQL Architect!The Neo4j Desktop Graph Apps Gallery offers a host of Graph Apps for Neo4j Desktop that can be installed with just a click.Getting Started With GraphQL ArchitectOnce you’ve installed GraphQL Architect and have a Neo4j database running, select the Open” drop-down for the database you’d like to use and select GraphQL Architect”. You can use GraphQL Architect with Neo4j 3.x or 4.x instances.Opening the GraphQL Architect graph app for the sample Movie database that ships with Neo4j DesktopThis will open the GraphQL Architect application in a new window. You may be prompted to install the APOC standard library for Neo4j if you don’t already have it installed. APOC is a dependency of the neo4j-graphql.js library used for generating database queries from GraphQL.The main view of the GraphQL Architect Graph App. From here you can generate GraphQL type definitions for a Neo4j database, start and query a local GraphQL API connected to your database, add custom logic by editing the GraphQL type definitions and adding Cypher, or export and deploy your GraphQL API once you’re done developing and testing.Using With An Existing DatabaseOne of the most powerful features of GraphQL Architect is the ability to infer a GraphQL schema from an existing Neo4j Database. This means you can run a GraphQL API with full Create, Read, Update, Delete (CRUD) operations with just the click of a button!When starting GraphQL Architect with an existing Neo4j database a GraphQL schema will be generated from the database. This schema includes entry points into the GraphQL API (the Query and Mutation types), mapping each node label to a type in the GraphQL API. The generated API also includes arguments for ordering, pagination, and filtering.To start a local GraphQL server using this generated schema click the Start GraphQL Server” button. You can then use the embedded GraphiQL tool to query the GraphQL API.For example, here we use the generated GraphQL API to query the sample movies Neo4j database for movies released before 2000 starring Emil Eifrem using GraphiQL in GraphQL Architect.Querying the default movies Neo4j database using GraphQL in GraphQL Architect.You can learn more about the GraphQL schema generation in the neo4j-graphql.js documentation.Using With A New DatabaseAbove we showed the generated type definitions in the case of connecting to an existing database. What if we are starting a new project and are starting with an empty database? We can use GraphQL type definitions to define the property graph model we want to create in Neo4j.Here, we define two types in GraphQL, a Person and Company. Each has an id and name field, and the Person has a relationship field connecting it to a Company.type Person {  personId: ID!  name: String!  works_at: Company @relation(direction: OUT, name:  WORKS_AT )}type Company {  companyId: ID!  name: String!}When we start the GraphQL server by clicking Start GraphQL Server”, our generated GraphQL API includes CRUD operations for each type and relationship field defined in our schema, including mutations for creating data.The generated GraphQL mutations for our Company, Person data model defined using GraphQL SDL.We can then use these generated mutations to add data to our Neo4j database. For example, here we create a Person node, a Company node, and a relationship connecting them using the GraphiQL tool embedded in GraphQL Architect:In Neo4j, we’ve now created a graph that looks like this:You can learn more about designing a property graph model using GraphQL in the GraphQL Schema Design guide.Schema EditorIn GraphQL Architect, the GraphQL schema editor is where you can update your GraphQL schema and add custom logic using Cypher. Powered by Monaco GraphQL (part of the GraphiQL project) the schema editor gives you schema-aware linting and auto-complete suggestions, as well as all the functionality from the Monaco editor (part of the VSCode Project) such as code formatting and different color schemes such as dark mode.Schema Aware Autocomplete And LintingWhile editing your GraphQL type definitions in GraphQL Architect you’ll have access to smart schema-aware autocomplete suggestions, taking into account the types and shape of your schema. Just use ctrl-space to trigger the autocomplete suggestions, powered by the GraphQL Language Services library and Monaco GraphQL.As you use the editor, your type definitions are continuously parsed, offering linting and syntax checking, helping to find errors in your schema as you edit.Adding Custom Logic With CypherSo far we’ve seen how to use the generated CRUD operations provided by GraphQL Architect, but what if we want to add custom logic to our API? Custom logic can be defined using the @cypher GraphQL schema directive. By annotating fields in our GraphQL type definitions using the @cypher schema directive we can map custom Cypher queries to fields in the GraphQL schema.Here we add a float scalar field average which computes the average rating across all movies that an actor has acted in:We can also use @cypher schema directives to define object fields. Here we compute similar movies by looking at common actors, a simple graph recommendation query:You can learn more about adding custom logic with the @cyher GraphQL schema directive in the neo4j-graphql.js documentation.Run Local GraphQL ServerRunning a local GraphQL server is powered by Apollo Server and the neo4j-graphql.js library. However, the specifics of running a Node.js GraphQL server are abstracted away with GraphQL Architect, allowing you to focus on the design of your API and your application.Once you’ve designed your GraphQL type definitions, click the Start GraphQL Server” button in the sidebar menu in GraphQL Architect. This will start a local API that you can query using the built-in GraphiQL window in GraphQL Architect. Or build an application against the endpoint using Apollo Client or other GraphQL client tooling.Using With Cloud DatabasesOf course, sometimes we’ll want to be able to develop and test our GraphQL API with a Neo4j instance hosted in the cloud, either on Neo4j Sandbox, Neo4j Aura, or running somewhere else.To do this, create a remote database in Neo4j Desktop, by clicking on the Add Database” button and selecting Connect To Remote DBMS”. Then you’ll be able to input your connection credentials and connect it to Neo4j Desktop. Once activated in Neo4j Desktop, GraphQL Architect will connect and you’ll be able to use all of the features of GraphQL Architect that you could with a local database.Deploy And Export Your GraphQL APIGraphQL Architect is great for developing and querying your GraphQL API locally. When you’re ready to export and deploy your API, just click the Deploy button.Currently, you can export your API application as a new project (including the Node.js code required to run the application) or update a project created with the create-grandstack-app CLI. We’ve just started to scratch the surface of deployment options for GraphQL Architect. If there’s a service you’d like to see added as a deployment target please let us know!Send Us Your Feedback!We’d love to know if you have any feedback while using GraphQL Architect. Is there a feature you’d like to see added? Is there a bug you’d like to report? Just click the Feedback” button and let us know. This kind of user feedback is invaluable for us as it allows us to continually improve.Click the Feedback” button to submit feedback and feature requests.ResourcesInstall GraphQL Architect in the Neo4j Desktop Graph Apps Gallery.Learn more about using GraphQL and Neo4j in the neo4j-graphql.js documentation and on GRANDstack.ioCheck out the GRANDstack Blog to keep up to date on what going on in the Neo4j GraphQL ecosystem.The book Fullstack GraphQL Applications with GRANDstack covers building GraphQL APIs using Neo4j from a full-stack perspective.;Jul 20, 2020;[]
https://medium.com/neo4j/two-hip-friends-jhipster-and-neo4j-774639e9693;Michael SimonsFollowMay 15, 2020·9 min readTwo Hip Friends — JHipster and Neo4jSpring Data Neo4j⚡️RX available as a new core module in JHipsterTLDRFor the impatient, here’s a complete, running example. You need to havenpm, curl, Docker and Maven installed:mkdir bootiful-music && cd bootiful-musicnpm install -g generator-jhipstercurl -L https://r.neo4j.com/hipster-music -o bootiful-music.jhjhipster import-jdl bootiful-music.jhdocker-compose -f src/main/docker/neo4j.yml up -d./mvnwAccess the application at http://localhost:8080.Welcome screen for the bootiful music applicationAnd now for the story behind it:Let’s create some good relations.Back in September 2019, the Spring Data and Object mapping team at Neo4j was asked about their opinion about an integration of our Spring Data module with JHipster.There was an ongoing issue at that point opened by one of Neo4js long-time partners, Larus in Italy:”Provide First-Class Support to Neo4j”.The original idea was to use the existing Spring Data Neo4j module (which we call SDN+OGM). This module was created with a lot of JPA / Hibernate inspiration and it is very flexible in some ways. It can connect to Neo4j via Neo4j’s native Bolt protocol as well as HTTP and it can also fire up an embedded Neo4j instance and use its internal Java Graph-API.However, that flexibility comes with a price: Many moving parts, many modules and many things to combine. The OGM in SDN+OGM stands for Object Graph Mapping and it is the pendant to JPA/Hibernate when you compare SDN+OGM to Spring Data JPA.Users need to make very educated choices to combine all those things. Not the optimal baseline to add to an application generator that itself already has tons of options to choose from.At this point in time, Gerrit Meier and Michael Simons had been working on a successor to SDN+OGM for a couple of months already. The successor is called SDN/RX and is replacing SDN+OGM going forward.What makes SDN/RX a good candidate for the integration with JHipster? Let’s have a look at some of the decisions we have made while creating SDN/RX as a Spring Data implementation for Neo4j from scratch.SDN/RXVertical building blocksSDN/RX should have gotten building blocks that stack nicely together, pretty much like Lego bricks do. We decided, that SDN/RX would only use the recommended way of talking to Neo4j, which is the Bolt protocol.At the lowest abstraction level this is achieved with the Neo4j Java Driver. Its configuration is exposed into the Spring Boot infrastructure via a custom starter and the namespace org.neo4j.driver.That configuration fits of course nicely into the things that need to be generated for JHipster.Integration with Spring Data and Spring’s platform or reactive transaction manager starts at the level of the Neo4j Client. The client is part of SDN/RX which is configured through a separate starter, spring-data-neo4j-rx-spring-boot-starter. The configuration namespace of that starter is org.neo4j.data.The client is mapping agnostic. It doesn’t know about your domain classes and you are responsible for mapping a result to an object suiting your needs.The next higher level of abstraction is the Neo4j Template. It is aware of your domain and you can use it to query for arbitrary domain objects. The template comes in handy in scenarios with a large number of domain classes or custom queries for which you don’t want to create an additional repository abstraction.The highest level of abstraction is a Spring Data Repository.All abstractions of SDN/RX come in both imperative and reactive fashions.SDN/RX building blocksUtilize the Spring Data MappingContextSpring Data Commons provide the concept of a MappingContext and SDN/RX implements it via a Neo4jMappingContext. The purpose of a mapping context is providing access to persistent entities and their properties.By utilizing existing Spring Data components, SDN/RX is standing on the shoulders of giants: The heavy duties of class scanning, determining properties, and the associations to map from and to the database, is completely delegated to a module that has been proven to work for many stores such as MongoDB, Redis, and others.Full support of all Spring Data featuresAs we built the library using our Neo4j driver and client on top of Spring Data infrastructure, SDN/RX can easily support all Spring Data features that SDN+OGM couldn’t, such as:Full support for immutable entities (represented for example via Lombok @Data annotated entities, Kotlin’s data classes or even JDK 14 Records (a preview feature))Support of all Spring Data keywords in derived finder methods (See Repository query keywords”)Query by example”Domain events (can used for auditing)Many extension points for running custom queries with ad-hoc mapping through our Cypher-DSLAnd of course, support for both imperative and reactive database access.What could the Neo4j team provide?From a backend developers perspective, JHipster generates the following things:Basic configuration classes for initializing a Spring Data module in the Spring contextDomain entities as neededRepositories as neededServices on top of that if the users wishes forControllers using the services or the repositories directlyThe fact that SDN/RX doesn’t leave out any Spring Data feature, makes it a perfect fit for providing the repository implementation. This is what Neo4j brought to the table.Entering an a new world. The JHipster generator.JHipster itself is not a framework per se, but an application generator creating two sides of an application:The backend, using (among others)Spring BootSpring Security (including Social Logins)Spring MVC REST + JacksonSupport of various databases through Spring Data modulesThe front end, using either AngularJS or React, styled with BootstrapBoth the backend and the frontend are generated having both best practices, configurability and practicability in mind.While the Spring Data Neo4j team was quite familiar with most of the things happening in the backend (Spring, Spring Boot and Spring Data), we didn’t know to much how the generator itself works. Technically JHipster is a Yeoman generator running on NPM. The templates for the artifacts generated are written with Embedded JavaScript templating (EJS).We started to play around with this and learned about a couple of things:There are core modules and blueprints in JHipsterThere are different concepts for storing JHipsters authentication (it can work with Okta but also stores identities local)The local store is independent of the entity storeLuckily, there are a lot of connections in the Java world, personally and through many Java User Groups. The Neo4j team was able to bring in Frederik Hahne.We met at the Java Forum Nord in September 2019 to discuss SDN/RX and learn more about JHipster:Gerrit, Frederik and Michael at Java Forum Nord 2019We decided to evolve the existing ticket into a JHipster core module on the same level as the support for JPA, Mongo, Couchbase and ElasticSearch.From Neo4j’s side, we provided:Additions to the templates, which have mainly been additional shims for different annotations (for example @Node instead of @Table) and of course adapted imports and the like. It has been a pleasant experience to add to the EJS code.We needed a way to execute database migrations on Neo4j to create initial users. For that, we created the Neo4j-Migrations, which is now part of the JHipster Neo4j support.Based on that work, Frederik over to integrate the module further into JHipsterAdditional work on the JHipster sideMichael did a great job in preparing nearly all relevant templates and prompts to support Neo4j as a database. We needed to do a lot of grunt work, like adding translations, updating generated tests or adding Neo4j to more complex and less used templates (e.g. OAuth2).We needed some way to test the new option in our CI. We have already a lot of checks for each pull request. Therefore we execute additional tests for less frequently used options only nightly.Our tests generate sample entities with different options and relations, so this was good test if we covered all options. Besides some edge cases, there where no severe problems revealed.To support Neo4j, the JHipster domain language was extended with a new database options, such that Neo4j can be used with the preferred workflow.We developed the Neo4j support while webflux/reactive support was still in alpha stage from JHipsters side and so we couldn’t implement fully reactive support with the first shot. During the last weeks the JHipster team did a great effort to bring reactive support out of alpha. As a result we could extend Neo4j support to all reactive templates.As JHipster wants to make the developers life easier, Neo4j support has been added to the Herokusubgenerator. When you deploy an application to Heroku, it will automatically provisioned with a Neo4j database plugin and all required settings will be set correctly. For other deployment options (e.g. kubernetes) Neo4j support will be added in the next couple of weeks.JDLThe JHipster domain language is a dedicated language to define JHipster applications and entities as well as complete microservice architectures. We will use it to define entities and their relations in the following.An example applicationWe try to build an application similar to Michaels bootiful music example, but with JHipster and of course without writing a single line of code. :)You need a JDK (>=8) as well as Node.js (>=12.16.3) to follow this example.Create the applicationAfter installing JHipster via npm install -g generator-jhipster you start the generator with the command jhipster. You can answer all questions with the default choice, except the database.After starting Neo4j with the generated docker-compose script (docker-compose -f src/main/docker/neo4j.yml up -d) you can bring up the application with ./mvwn. This will build the frontend, the backend and start the application. The application can be accessed via localhost:8080.Generate EntitiesInstead of using the JHipster entity generator, which can be cumbersome to use, we import the following JDL to create all entities, web apis and a CRUD user interface:jhipster import-jdl bootiful-music.jdlWhich we fetched from Michaels repositorycurl -L https://r.neo4j.com/hipster-music -o bootiful-music.jhHere is the content of the JDL file:entity Artist {    name String required}entity Genre {    name String required}entity Track {    name String required}entity Album {    name String required}relationship OneToOne {    Album{artist(name)} to Artist    Album{genre(name)} to Genre}relationship OneToMany {    Album{track(name)} to Track{album(name)}}paginate Artist, Genre, Track, Album with paginationIf you would like to write a JDL with syntax support and graphical view of the defined relationships one can use the JDL Studio.JDL StudioYou can restart the application with ./mvnw and create some albums with the generated user interface (login with the default credentials admin/admin or user/user).Editing an Album entityYou can explore the generated nodes and relations with the Neo4j browser of course. Just open it at http://localhost:7474 You see the entities just created via the UI as well as the nodes created during database migration on startup.Some of the Nodes and Relationships created by our AppOur conclusionJHipster integrates many moving pieces, relevant to today’s enterprise applications:Stable backends for applicationsA sane way to bootstrap the frontendSecurity out of the box, either through an integrated OAuth server, JWTs or solutions such as OktaA wide choice of database backends, which just got this great additionThe new Spring Data Neo4j module SDN-RX fits perfect into this ecosystem. Backed by the Spring Data standard, a reactive Neo4jRepository works just like a reactive R2DBC repository. SDN/RX connects directly over the Bolt protocol to any Neo4j instance, regardless whether it’s running on premise, in a Docker container, in any cloud marketplace, or  as a service  in Neo4j Aura.Having Neo4j and SDN/RX gives you support for one more great database inside a well known application generator.So now go ahead, try it out and let us know how well the new integration worked for you. Looking forward to your comments.Find out more about JHipster and Neo4jHere are some entry points for further reading.https://www.jhipster.tech/https://neo4j.com/https://neo4j.github.io/sdn-rxhttps://github.com/michael-simons/neo4j-migrations;May 15, 2020;[]
https://medium.com/neo4j/importing-mapping-metaphor-into-neo4j-90ac9ead4d44;Michael HungerFollowSep 24, 2017·5 min readImporting Mapping Metaphor into Neo4jI came across this tweet, which sounded really interesting.The Metaphor Map of English shows the metaphorical links which have been identified between different areas of meaning. These links can be from the Anglo-Saxon period right up to the present day so the map covers 1300 years of the English language. This allows us the opportunity to track metaphorical ways of thinking and expressing ourselves over more than a millennium see the Metaphor in English section for more information.The Metaphor Map was built as part of the Mapping Metaphor with the Historical Thesaurus project. This was completed by a team in English Language at the University of Glasgow and funded by the Arts and Humanities Research Council from 2012 to early 2015. The Metaphor Map is based on the Historical Thesaurus of English, which was published in 2009 by Oxford University Press as the Historical Thesaurus of the Oxford English Dictionary.The site is really nice and fun to explore, with an interesting data visualization of the metaphoric connections between areas of language and thought:Metaphorical connections visualizedMetaphor in EnglishWhen most people think of metaphor, they cast their minds back to school and remember examples from poetry and drama, such as Shakespeare’s Juliet is the sun”. This is unsurprising metaphor is usually described as a literary phenomenon used to create arresting images in the mind of the reader. However, linguists would argue that metaphor is far more pervasive within our language and indeed within thought itself.Useful natural language correlation network are always fun to work with, so let’s have a look at it in a graph database.Install Neo4j & APOCDownload and install Neo4j-Desktop from http://neo4j.com/download/other-releasesCreate a project and database and add the APOC procedure library.I also installed Neo4j Graph Algorithms to use later.Start the database.Download DataAll the data is taken from:Mapping Metaphor with the Historical Thesaurus. 2015. Metaphor Map of English Glasgow: University of Glasgow.Download CSV from hereselect Advanced Search”,select all categories (that you’re interested in)select Connections between selected sections and all other sections”Metaphor Strength: Both”Click Search”Select View results as a table”Click the Download” icon in the left boxThe downloaded file metaphor.csv” should contain almost 12k lines of metaphors:Copy metaphor.csv into the import folder of your database ( Open Folder ) or in an http-accessible location to load via an http-url.Run ImportOur data model is really simple, we have::Category nodes with id and name.:Strong or :Weak relationships between them with the start property for the start era and examples for the example words.A more elaborate model could model the metaphorical connection as a node, with the example words extracted as related nodes. I was just not sure, what to name each metaphor, that information was missing in the data. But for this demonstration the simpler model is good enough.For good measure we can add a constraint.create constraint on (c:Category) assert c.id is uniqueRun this Cypher statement to import in a few seconds// load csv as individual lines keyed with header namesLOAD CSV WITH HEADERS FROM  file:///metaphor.csv  AS line// get-or-create first category (note typo in name header)merge (c1:Category {id:line.`Category 1 ID`}) ON CREATE SET c1.name=line.`Categroy 1 Name`// get-or-create second categorymerge (c2:Category {id:line.`Category 2 ID`}) ON CREATE SET c2.name=line.`Category 2 Name`// depending on direction flip order of c1,c2with line, case line.Direction when > then [c1,c2] else [c2,c1] end as cat,// split words on  and remove last empty entry     apoc.coll.toSet(split(line.`Examples of metaphor`,))[0..-1] as words// create relatiosnship with dynamic type, set era & words as relatiosnship propertiescall apoc.create.relationship(cat[0],line.Strength,{start:line.`Start Era`, examples:words},cat[1]) yield rel// return rows processedreturn count(*)I rendered the category nodes pretty large so that you can read the names, and have the Strong” links display their words” instead.Categories, connected by Strong relationships, with example words showing on the connections.For finding categories quicklycreate index on :Category(name)Running graph algorithms.Degree distributioncall apoc.stats.degrees(Strong>)unioncall apoc.stats.degrees(Weak>)degree distribution for Strong and Weak linksTop 10 Categories by in-degreeMATCH (c:Category)WITH c,size( (c)-->()) as out,size( (c)<--()) as inRETURN c.id, c.name,in, outORDER BY in DESC LIMIT 10╒══════╤═════════════════════════╤════╤═════╕│ c.id │ c.name                  │ in │ out │╞══════╪═════════════════════════╪════╪═════╡│ 2D06 │ Emotional suffering     │119 │7    │├──────┼─────────────────────────┼────┼─────┤│ 2C02 │ Bad                     │119 │7    │├──────┼─────────────────────────┼────┼─────┤│ 3M06 │ Literature              │116 │29   │├──────┼─────────────────────────┼────┼─────┤│ 1O22 │ Behaviour and conduct   │109 │10   │├──────┼─────────────────────────┼────┼─────┤│ 3L02 │ Money                   │106 │44   │├──────┼─────────────────────────┼────┼─────┤│ 2C01 │ Good                    │105 │2    │├──────┼─────────────────────────┼────┼─────┤│ 1P28 │ Greatness and intensity │104 │2    │├──────┼─────────────────────────┼────┼─────┤│ 2A22 │ Truth and falsity       │104 │5    │├──────┼─────────────────────────┼────┼─────┤│ 2D08 │ Love and friendship     │100 │17   │├──────┼─────────────────────────┼────┼─────┤│ 2A18 │ Intelligibility         │99  │5    │└──────┴─────────────────────────┴────┴─────┘Outgoing Page-Rank of CategoriesAfter installing the neo4j-graph-algorithms into the plugins” directory, and adding the config option to enable it, we can also run a few graph algorithms on the data.call algo.pageRank.stream(Category,Strong) yield node, scorewith node, toInt(score*10) as score order by score desc limit 10return node.name, score/10.0 as score╒══════════════════════════════════════╤═══════╕│ node.name                            │ score │╞══════════════════════════════════════╪═══════╡│ Greatness and intensity              │5.6    │├──────────────────────────────────────┼───────┤│ Colour                               │3.5    │├──────────────────────────────────────┼───────┤│ Unimportance                         │3.5    │├──────────────────────────────────────┼───────┤│ Importance                           │3.4    │├──────────────────────────────────────┼───────┤│ Hatred and hostility                 │3.4    │├──────────────────────────────────────┼───────┤│ Plants                               │2.9    │├──────────────────────────────────────┼───────┤│ Good                                 │2.9    │├──────────────────────────────────────┼───────┤│ Age                                  │2.8    │├──────────────────────────────────────┼───────┤│ Love and friendship                  │2.7    │├──────────────────────────────────────┼───────┤│ Memory, commemoration and revocation │2.6    │└──────────────────────────────────────┴───────┘Funny that both importance and unimportance have such a high rank.Betweenness CentralityWhich categories connect others:call algo.betweenness.stream(Category,Strong) yield nodeId, centrality as scorematch (node) where id(node) = nodeIdwith node, toInt(score) as score order by score desc limit 10return node.id, node.name, score╒═════════╤═══════════════════════════════════════════╤═══════╕│ node.id │ node.name                                 │ score │╞═════════╪═══════════════════════════════════════════╪═══════╡│ 2C01    │ Good                                      │165912 │├─────────┼───────────────────────────────────────────┼───────┤│ 1E02    │ Animal categories, habitats and behaviour │131109 │├─────────┼───────────────────────────────────────────┼───────┤│ 3D05    │ Authority, rebellion and freedom          │108292 │├─────────┼───────────────────────────────────────────┼───────┤│ 2D06    │ Emotional suffering                       │87551  │├─────────┼───────────────────────────────────────────┼───────┤│ 1J34    │ Colour                                    │83595  │├─────────┼───────────────────────────────────────────┼───────┤│ 1E05    │ Insects and other invertebrates           │77171  │├─────────┼───────────────────────────────────────────┼───────┤│ 3D01    │ Command and control                       │71873  │├─────────┼───────────────────────────────────────────┼───────┤│ 1O20    │ Vigorous action and degrees of violence   │65028  │├─────────┼───────────────────────────────────────────┼───────┤│ 1C03    │ Mental health                             │64567  │├─────────┼───────────────────────────────────────────┼───────┤│ 1F01    │ Plants                                    │59444  │└─────────┴───────────────────────────────────────────┴───────┘There are many other explorative queries and insights we can draw from this.Let me know in the comments what you’d be interested in.;Sep 24, 2017;[]
https://medium.com/neo4j/neo4j-clj-a-new-neo4j-library-for-clojure-2df1a2a45783;Dr. Christian BetzFollowMay 18, 2018·8 min readneo4j-clj: a new Neo4j library for ClojureOn designing a ‘simple’ interface to the Neo4j graph databaseWhile creating a platform where humans and AI collaborate to detect and mitigate cybersecurity threats at CYPP, we chose to use Clojure and Neo4j as part of our tech stack. To do so, we created a new driver library (around the Java Neo4j driver), following the clojuresque way of making simple things easy. And we chose to share it, to co-develop it under the Gorillalabs organization. Follow along to understand our motivation, get to know our design decisions, and see examples. If you choose a similar tech stack, this should give you a head start.neo4j-clj presentation videoWho we areGorillalabs is a developer-centric organization (not a Company) dedicated to Open Source Software development, mainly in Clojure.I (@Chris_Betz on Twitter, @chrisbetz on Github) created Gorillalabs to host Sparkling, a Clojure library for Apache Spark. Coworkers joined in, and now Gorillalabs brings together people and code from different employers to create a neutral collaboration platform. I work at CYPP, simplifying cybersecurity for mid-sized companies.Most of Gorillalabs projects stem from the urge to use the best tools available for a job and make them work in our environment. That’s the fundamental idea and the start of our organization. And for our project at CYPP, using Clojure and Neo4j was the best fit.Why Clojure?I started using Common LISP in the 90ies, moved to Java development for a living, and switched to using Clojure in production in 2011 as a good synthesis of the two worlds. And, while constantly switching roles from designing and developing software to managing software development back and forth, I specialized in delivering research-heavy projects.For many of those projects, Clojure has two nice properties: First, it comes with a set of immutable data structures (reducing errors a lot, making it easier to evolve the domain model). And second, with the combination of ClojureScript and Clojure, you can truly use one language in backend and frontend code. Although you need to understand different concepts on both ends, with your tooling staying the same, it is easier to develop vertical (or feature) slices instead of horizontal layers. Check out my EuroClojure 2017 talk on that, if you’re interested.Graphs are everywhere — so make use of themFor threat hunting, i.e. the process of detecting cybersecurity threats in an organisation, graphs are a natural data modelling tool. The most obvious graph is the one where computers are connected through TCP/IP connections. You can find malicious behaviour if one of your computers shows unwanted connections. (Examples are over-simplified here.)But that’s just the 30.000-feet view. In fact, connections are between processes running on computers. And you see malicious behaviour if a process binds to an unusual port.Processes are running with a certain set of privileges defined by the user” running the process. Again, it’s suspicious if a user who should be unprivileged started a process listening for an inbound connection.You get the point: Graphs are everywhere, and they help us cope with threats in a networked world.Throughout our quest for the best solution around, we experimented with other databases and query languages, but we came to Neo4j and Cypher. First, it’s a production quality database solution, and second, it has a query language you really can use. We used TinkerPop/Gremlin before, but found it not easy to use for simple things, and really hard for complex queries.Why we created a new driverThere’s already a Neo4j driver for Clojure. There’s even an example project on the Neo4j website. What on earth were we thinking creating our own Neo4j driver?Neo4j introduced Bolt on Neo4j 3.x as the new protocol to interact with Neo4j. It made immediate sense, however, neocons did not pick it up, at least not at the pace we needed. Instead, it seemed as if the project lost traction, having had only very few contributions for a long time. So we needed to decide whether we should fork neocons to move it to Neo4j 3.x or not.However, with bolt and the new Neo4j Java Driver, we would have implemented a second, parallel implementation of the driver. That was the point where we decided to go all the way building a new driver: neo4j-clj was born.Design choices and code examplesCreating a new driver gave us the opportunity to fit it exactly to our needs and desires. We made choices you might like or disagree with, but you should know why we made them.If you want to follow the examples below, you need to have a Neo4j instance up and running.Then, you just need to know one namespace alias for neo4j-clj.core and one connection to your test database (also named db):Using raw” CypherThe most obvious thing is our choice to keep raw” Cypher queries as strings, but to be able to use them as Clojure functions. The idea to this is actually not new and not our own, but borrowed from yesql. Doing so, you do not bend one language (Cypher) into another (Clojure), but keep each language for the problems its designed for. And, as a bonus, you can easily copy code over from one tool (code editor) to another (Neo4j browser), or use plugins to your IDE to query a database with the Cypher queries from your code.So, to create a function wrapping a Cypher query, you just wrap that Cypher string in a defquery macro like this:And, you can easily copy the string into your Neo4j browser or any other tool to check the query, profile it, whatever you feel necessary.With this, you can easily run the query like this:and, depending on the data in your test database, will end up with a sequence of maps representing your hosts. For me, it’s something like this:This style makes it more clear that you should not be constructing queries on the fly, but use a defined set of queries in your codebase. If you need a new query, define one specifically for that purpose. Think about which indices you need, how this query performs best, reads best, you name it.However, this decision has some drawbacks. There’s no compiler support, no IDE check, as Cypher queries are not recognized as such. They are just strings. However, there’s not much Cypher support in IDEs anyhow. That’s different than with yesql, where you usually have SQL linting with appropriate files.Each query function will return a list. Even if it’s empty. There’s no convenience function for creating queries to get a single object (for something like host-by-id). If you know theres only one, pick it using first.Relying on the Java driver, but working with Clojure data structuresWe just make use of the Java driver, so basically, neo4j-clj is only a thin wrapper. However, we wanted to be able to live in the Clojure world as much as possible. To us, that meant we need to interact with Neo4j using Clojure data structures. You saw that in the first example, where a query function returns a list of maps.However, you can also parameterize your queries using maps:This example is more complex than necessary just to make a point clear: You can destructure Clojure maps {:host {:id  ... }} by navigating them in Cypher $host.id.Nice thing is, you can easily test these queries in the Neo4j browser if you set the parameters correct:Joplin integration built-inWe’re fans of having seeding and migration code for the database in our version control. Thus, we use Joplin and we suggest, you do, too. That’s why we built Joplin support right into neo4j-clj.With Joplin, you can write migrations and seed functions to populate your database. This isn’t as important in Neo4j as it is in relational databases, but it’s necessary, e.g. for index or constraint generation.First, Joplin migrates your database, if it isn’t at latest stage (path-to-joplin-neo4j-migrators points to a folder of migration files, which are applied in alphabetical order):And each migration file has (at least) the two functions up and down to perform the actual migration. For example:Also, you can seed your database from a function like this:Now you can seed your database like this. Here, we use a config identical to the one from migration:With this seed function, you see a style we got used to: We prefix all the functions created by defquery with db> and we use the ! suffix to mark functions with side-effects. That way, you see when code leaves your platform and what you can expect to happen.Tested all the wayBeing big fans of testing, we wanted the tests for our driver to be as easy and as fast as possible. You should be able to combine that with a REPL-first approach, where you can experiment on the REPL. Luckily, you can run Neo4j in embedded mode, so we did not need to rely on an existing Neo4j installation or a running docker image of Neo4j. Instead, all our tests run isolated in embedded Neo4j instances. We just needed to make sure not to use the Neo4j embedded API, but the bolt protocol. Easy, my colleague Max Lorenz just bound the embedded Neo4j instance to an open port and connected the driver to that, just as you would do in production.Using a with-temp-db-fixture, we just create a new session against that embedded database and test the neo4j-clj functions in a round-trip without external requirements. Voilá.Use it, fork it, blog itneo4j-clj is ready to be used. We do. We’d love to hear from you (@gorillalabs_de or @chris_betz). Share your experiences with neo4j-clj.There are still some rough edges: Maybe you need more configuration options. Or support for some other property types, especially the new Date/Time and Geolocation types. We’ll add stuff over time. If you need something specific, please open an issue on Github, or add it yourself and create a Pull Request on the ‘develop’ branch.We welcome contributions, so feel free to hack right away!;May 18, 2018;[]
https://medium.com/neo4j/how-do-you-know-if-a-graph-database-solves-the-problem-a7da10393f5;Jennifer ReifFollowAug 8, 2018·9 min readCourtesy of Google ImagesHow Do You Know If a Graph Database Solves the Problem?One of the greatest questions to consistently badger a developer is what technology should I use?”. The analysis from days of thought and input determines which option (from an increasingly growing number) best suits the need, manages volume and demand, plans for long-term strategy, simplifies/reduces support, and gets approved by colleagues and management.This may sound even simpler than it sometimes can be in real life. The decision’s complexity is often compounded by how much buy-in is needed, and the current constraints of existing technology and developer knowledge. For instance, investing in an unknown or newer solution means understanding that there will be learning costs that will need to be allocated.If you are researching graph databases, you may have been awed by the complex analysis it can handle or the simplicity that it allows for you to interact with your data. Perhaps you were star-struck by pretty visualizations or the possibilities of lightning-fast queries. Then again, maybe you are desperate to learn something new and want to experiment with this graph database stuff.But how do you know for sure that graph is the right solution for your business or technical need? What kind of investigation do you need to be certain of its value? What makes a graph database special over another solution for your project?In this post, I want to highlight some of the scenarios that can guide you towards or away from using a graph database. These are not strict guidelines, but rather some opportunities to evaluate whether graphs fit your use case before exploring it in-depth as a solution.Self-Evaluation: Are You Desperate to Use a Graph Database on Anything?I think we as developers (or <insert position title here>) want so strongly to use or learn something new that we choose the solution and apply it to the next victim” project that comes up. Most of us probably know that we should not do this, but in reality, we may not step back to think about our actions until it is too late to change solutions.To alter this mindset, we need to put each problem through analysis first before evaluating various solutions. What is the motivation for using this technology? What will it provide that others cannot? Asking these and other questions will help you understand the reasons why you should or should not use the technology.Possible solutions should be drawn out and well-researched to see what the advantages and disadvantages of each are. From there, a few different individuals should review to catch any missing thoughts or remove any options that do not meet enough requirements.When Are Graph Databases NOT a Good Fit?As with every company, Neo4j is biased towards its products and their usefulness. We all wish our products could be used for everything, but there are too many individual and unique ideas, people, problems, and technologies for a one-size-fits-all solution to exist (and that’s a good thing!).Most of what you will learn about any company’s product is likely from the company itself, which usually focuses on the positive aspects and everything that you can do with it.….But what about knowing what you cannot or should not use it to do?If your use case passes all of the following scenarios, this should help solidify that graph is an excellent option. If your use case fits any of these scenarios, though, hopefully it will dissuade you and provide reasons against potentially ending up with the wrong tool for the wrong job.There may be additional cases that do not work well for graphs that I am not aware of, but this list is of those that I know thus far.1. Where data is disconnected and relationships do not matter.If you have transactional data and do not care how it relates or connects to other transactions, people, etc, then graph is probably not the solution. There are cases where a technology simply stores data, and analysis of the connections and meanings among it is not important.You might have queries that rely on sequentially-indexed data (next record stored next to previous one in storage), rather than relationship-indexed data (record is stored nearest those it is related to). Searching for individual pieces of data or even a list of items also points to other solutions, as it is not interested in the context of that data.Overall, graph solutions will focus and provide the most value from data that is highly-connected and analysis that is looking for possible connections (hidden or obvious). If this doesn’t fit your use case, another kind of technology may suit it better.2. Where optimizing for writing and storing data and do not need to read or query it.Though this was mentioned in the point above, I want to mention this one separately, as well. If the use case is only looking to write data to the store and not expecting to analyze or query results, then graph may not solve the problem.Requirements for write-only transactions and for simple queries that do not have SQL join statements are good indicators that your use case may not be suited to a graph database. Graph databases are designed to traverse stored data very quickly and retrieve results in milliseconds. If the use case is not expected to utilize this advantage, then you probably want to find another solution.3. Where core data objects or data model stay consistent and data structure is fixed and tabular.If you have constant, unchanging types of data that you are collecting, then graph may not be the most appropriate solution. Graphs are well-suited to storing any or all elements and can easily adapt to changing business and data capture needs.Take, for instance, a scenario in which you need to track the number of people who call your business. You only need to store an ID, name, and phone number in your Customer table for this. Since you don’t need to retain additional info, the columns on the table will not change and everyone calling your business can be assigned an ID, name, and phone number. This is a good example for a relational database.If the requirements are expected to grow where this system is used as the main customer system and other types of analysis will be needed, the table will change to possibly include email address, company name, orders, etc. At that point, some fields may or may not be filled (not all customers have made orders or work for a company). The business may also need to maintain other types of entities for orders and such, or the meaning of a customer may change where employees can also be customers.Long story short, if the requirements are narrow in scope for a specific need and not expected to expand or morph over time, then graph may not be the best fit.4. Where queries execute bulk data scans or do not start from a known data point.If your queries are doing table scans to find a match or searching for data that fits a general category, then a graph solution is not best-suited to the task. A graph database is built and optimized for traversing relationships from a starting data point or set. It is not optimized for searching the entire graph without a specific starting point or set in mind.Queries, such as the first one below, will end up traversing a potentially-massive graph that has a variety of different types of information for a single result (is Jennifer an order or item or customer or something else?). However, the next query starts from a particular user and looks at who that person knows.//Query 1:MATCH (n)WHERE n.name = JenniferRETURN n//Query 2:MATCH (n:Person {name: ‘Jennifer’})-[r:KNOWS]->(p:Person)RETURN pWhen the majority of your queries look like the first one and performance of those queries is extremely important, you need to consider non-graph solutions. While graph can still handle those queries, it is not optimized for maximum performance on bulk scans or unknown starting points.5. Where you will use it as a key-value store.If you are only interested in a lookup operation, then a graph database is not the solution for you. As we have discussed above, graph analysis benefits from relationships among data. A lookup result from a known key does not maximize the function of what graph databases were created to do.As an example, someone might use a database as a cache to store session data for an application. You might store the session ID in cache, but then write the session details to the database. When you need to retrieve session details or run analysis on those, you would send the session id (as the key) to return the value (probably properties stored on a single node).This method does not utilize any relationships because it is using a known key to return a single node or detail data on that node. When reviewing your use case, ensure that you understand the storage and retrieval mechanisms of each technology. Doing a lookup might fit a key-value store or even relational database more appropriately, providing better performance on functionality in which they were built to excel.6. Where large amounts of text or BLOBS need to be stored as properties.If you are storing and retrieving entity properties that contain extremely large values (such as BLOBs, CLOBs, text paragraphs, etc), then another technology solution might be a better choice. Graph databases are very good at traversing relationships between small data entities, but not ideally suited to store a lot of properties on a single node or large values in those properties. The reason for this is because the query can hop from entity to entity at top speed, but then also needs extra processing to pull out the details of each entity it finds along a path.Now, sometimes, this issue can be corrected by re-organizing the data model, which lends to a more graph-friendly use case. For instance, if you stored all information about an employee on a single graph node (address, job info, orders, benefit elections, salary info, etc), that would create a very cumbersome node with a lot of properties with potentially large values. You could re-model this where there would be separate entities for company, address, position details, etc. This would simplify the model and trim down performance on queries looking for an employee’s address, for instance, as queries would then follow the relationship from Employee to Address and utilize the advantage of graph databases.However, you may have some cases where you need those large values stored in a single property, and the queries are not graph-specific. For this type of use case, a graph database is not recommended.RecapOf course, no single item listed above will always appear alone. Many of these items come in tandem with others, so you may find one or more of these in your case, as well.There could be aspects of your project that demonstrate reasons against using a graph database, as well as reasons in support of one. While that may complicate the decision, it is ultimately left to the evaluation of the positives/negatives of each technology to determine the best fit.When are Graph Databases a Good Fit?I will not spend too much time here, as I briefly mentioned some of graph technology’s key strengths in the above paragraphs, and you can learn more from company resources, employee discussions, and customer feedback. However, I want to close with some positives and provide an overview. :)Business or technical needs where users want to understand relationships in their data (hidden and obvious) will thrive with a graph database. If you want to know what customers are interested in to gear messages in their topic areas or understand how a network map is laid out and the impacts of each component, a graph database is perfectly suited to these types of use cases and queries. Graphs can allow businesses to create well-rounded, diverse customer profiles and scrutinize bank transactions to find outliers that could be signs of fraud.They also exceed performance expectations when traversing relationships among data for data science and analytics purposes. Graph algorithms are expanding the value of running more complex analysis on connected data to highlight patterns for decision-making.Graph technology is used in all types of industries for business-critical use cases and backbone processes. Anything where data looks like the image below is an indicator that graph can maximize value.Courtesy of Google imagesConclusionI have only scratched the surface of each point for what a graph database can and cannot do. There are much finer and minute details that go into the decision to use one technology or another.With this post, I simply want to give you a few of the tools to help in that decision. Whether you choose a graph database or not, the goal is to find the best tool to meet (and hopefully exceed) the requirements.Best wishes on your next project and happy evaluating! :)ResourcesNeo4j Graph DatabaseReasons to Use Graph TechnologyWhy Choose Graph Databases?Graph Database Use Cases;Aug 8, 2018;[]
https://medium.com/neo4j/podcast-search-graphql-api-with-neo4j-and-the-podcast-index-97c5cf4bade6;William LyonFollowDec 7, 2020·9 min readPodcast Search GraphQL API With Neo4j And The Podcast IndexBuilding A Podcast Application With GRANDstack: Episode 1A few weeks ago I was complaining about the podcast app I was using. There was some problem with updating my playlists and I was grumpy because the app wasn’t pulling down any episodes. My wife suggested I just build my own podcast application and use that instead. She was just trying to get me to shut up and stop complaining, but I thought that actually sounded like a fun project. So this week on the Neo4j livestream we kicked off a new project: building a podcast application with GRANDstack!In this first week, we went over the graph data model we’ll need for this application and started building the GraphQL API for the project, focusing on implementing podcast search functionality using the Podcast Index API. This blog post covers what we built in that stream, but you can watch the recording of the stream hereGraph Data ModelingWhen beginning a new project I always like to start with a data modeling exercise. In my mind graph data modeling is an iterative process that begins as soon as you start defining the business requirements of the application. The steps of the graph data modeling iterative process:Identify the entities. These become nodes.How are these entities connected? These connections become relationships.What attributes describe the entities? These become node properties.What attributes are specific to how entities are connected? These become relationship properties.Once you have an initial model, work through the business requirements, and try to identify a traversal through the graph model that fulfills the requirement. If you can’t traverse the graph to answer the necessary question, you may need to update the graph model.After going through this process initially, we ended up with a graph model that looks like this (created using the Arrows graph diagramming tool.)Podcast SearchThe first feature I wanted to implement is podcast search since that’s the first thing our users will want to do. Our GRANDstack application will be powered by a GraphQL API so we want to expose this podcast search functionality in our GraphQL API. We’ll make use of GRANDstack’s @cypher schema directive functionality for adding custom logic to implement this podcast search functionality, but instead of searching a Neo4j database, well query a 3rd party API endpoint to search for podcasts.Podcast IndexWe don’t want to maintain an index of all possible podcasts in existence, instead, we’ll use a 3rd party API to enable searching for podcasts, in this case, a service called the Podcast Index. The Podcast Index is committed to maintaining an open API of podcasts for the preservation of free speech. In line with its goal, anyone can register for an API token and begin building an application using its API.Searching EndpointLooking at the API docs for the Podcast Index we can see there is an API endpoint for searching for podcasts. To use this endpoint we include a single query parameter which is our search term:/api/1.0/search/byterm?q=neo4j{   status :  true ,   feeds : [    {       id : 969306,       title :  Graphistania: Podcast for Neo4j Graph Database community ,       url :  http://feeds.soundcloud.com/users/soundcloud:users:141739624/sounds.rss ,       originalUrl :  http://feeds.soundcloud.com/users/soundcloud:users:141739624/sounds.rss ,       link :  http://blog.bruggen.com ,       description :  Podcast by The Neo4j Graph Database Community ,       author :  The Neo4j Graph Database Community ,       ownerName :  Graphistania ,       image :  http://i1.sndcdn.com/avatars-000135096101-qekfg1-original.png ,       artwork :  http://i1.sndcdn.com/avatars-000135096101-qekfg1-original.png ,       lastUpdateTime : 1606220307,       lastCrawlTime : 1607132366,       lastParseTime : 1606220308,       lastGoodHttpStatusTime : 1607132366,       lastHttpStatus : 200,       contentType :  application/rss+xml charset=utf-8 ,       itunesId : 975377379,       generator : null,       language :  en ,       type : 0,       dead : 0,       crawlErrors : 0,       parseErrors : 0,       categories : {         102 :  Technology       },       locked : 0,       imageUrlHash : 775621088    }  ],   count : 1,   query :  neo4j ,   description :  Found matching feeds. }Authorization HeadersAccording to the Podcast Index API documentation, each request need to be authenticated using Amazon style” request authorization token headers. This means that each request must include the following headers:User-Agent - to identify our application  GRANDcast.FM X-Auth-Date - the current unix epoch time, expressed in seconds.X-Auth-Key - our API key. We can get a free API key simply by registering.Authorization - a SHA1 hash of our API key, API secret, and the current unix epoch time concatenated togetherSince we want to use the @cypher schema directive to query the Podcast Index, well need to format and parse the request to this API using Cypher. How can we make a HTTP request, format the request headers, and parse a JSON response with Cypher?APOC to the RescueFortunately, the APOC standard library for Neo4j has some helpful procedures and functions that will enable us to query the Podcast Index API with Cypher. We’ll make use of the following APOC procedures and functions:apoc.load.jsonParams - load data from a JSON URL, including passing a request payload and our authorization headersapoc.static.get - reading static values such as API credentials from a config file so we dont have to check our secrets into version controlapoc.util.sha1 - compute the SHA1 hash of a string. Well use this to compute the hash for the Authorization token in our request.apoc.text.urlencode - encode text so it can be safely used in a URL. We use this URL encode the search term parameter.apoc.map.values - convert a map into a list of values. Well use this to work with the categories data for podcast search results.First, we’ll add our API credentials from Podcast Index to apoc.conf:apoc.static.podcastkey=<YOUR_API_KEY_HERE>apoc.static.podcastsecret=<YOUR_API_SECRET_HERE>We’ll now be able to reference these values using the apoc.static.get() function.Putting everything together the Cypher query to search for podcasts using the Podcast Index API endpoint looks like this:WITH toString(timestamp()/1000) AS timestampWITH {  `User-Agent`:  GRANDstackFM ,  `X-Auth-Date`: timestamp,  `X-Auth-Key`: apoc.static.get(podcastkey),  `Authorization`: apoc.util.sha1([apoc.static.get(podcastkey) + apoc.static.get(podcastsecret) + timestamp])} AS headersCALL apoc.load.jsonParams( https://api.podcastindex.org/api/1.0/search/byterm?q=  + apoc.text.urlencode($searchTerm), headers, , ) YIELD valueRETURN valueGraphQL APINow that we’re able to use Cypher to search for podcasts it’s time to start building our GraphQL API. We’ll use the create-grandstack-app command line tool to quickly create the skeleton for our GraphQL server application.create-grandstack-appThe create-grandstack-app CLI is a utility for creating GRANDstack applications based on the GRANDstack starter project. Here well use it to create the GraphQL server application, but not the frontend since were not ready to start thinking about the client application yet.npx create-grandstack-app grandstack.fmThis command will fetch the latest release of the GRANDstack Starter project and ask us a few questions about how to connect to our Neo4j database.We now have a functional GraphQL server application in the grandcast.fm/ directory that can serve a GraphQL endpoint backed by our local Neo4j database. Since we selected the API-Only option we dont yet have a frontend for the application.@cypherGraphQL Schema DirectiveNow it’s time to edit the GraphQL type definitions of our GraphQL server application. We want to:create a Query field podcastSearch that returns a list of PodcastSearchResult objectsadd the Cypher query we wrote above as a @cypher schema directive, defining the logic for this podcastSearch fieldthe podcastSearch field will take a String argument searchTerm that will be passed as a Cypher parameter to the indicated Cypher queryWe’ll replace the template type definitions found in grandstack.fm/api/src/schema.graphql with the following:type Query {  podcastSearch(searchTerm: String!): [PodcastSearchResult]  @cypher(    statement:        WITH toString(timestamp()/1000) AS timestamp    WITH {      `User-Agent`:  GRANDstackFM ,      `X-Auth-Date`: timestamp,      `X-Auth-Key`: apoc.static.get(podcastkey),      `Authorization`: apoc.util.sha1([apoc.static.get(podcastkey)+apoc.static.get(podcastsecret) +timestamp])    } AS headers    CALL apoc.load.jsonParams( https://api.podcastindex.org/api/1.0/search/byterm?q=  + apoc.text.urlencode($searchTerm), headers, , ) YIELD value    UNWIND value.feeds AS feed    RETURN {     itunesId: feed.itunesId,     title: feed.title,     description: feed.description,     feedURL: feed.url,     artwork: feed.artwork,     categories: apoc.map.values(feed.categories, keys(feed.categories))    }         )}type PodcastSearchResult {  itunesId: String  title: String  description: String  feedURL: String  artwork: String  categories: [String]}Next, in src/index.js well add the PodcastSearchResult to the types to be excluded from the schema augmentation process. Since this type represents results from the Podcast Index API call we dont want to generate query and mutation operations for this type. Well also comment out the init(driver) line to initialize the database, saving that for later.We’ll leave the rest of the file untouched:import { typeDefs } from ./graphql-schemaimport { ApolloServer } from apollo-server-expressimport express from expressimport neo4j from neo4j-driverimport { makeAugmentedSchema } from neo4j-graphql-jsimport dotenv from dotenvimport { initializeDatabase } from ./initialize// set environment variables from .envdotenv.config()const app = express()/* * Create an executable GraphQL schema object from GraphQL type definitions * including autogenerated queries and mutations. * Optionally a config object can be included to specify which types to include * in generated queries and/or mutations. Read more in the docs: * https://grandstack.io/docs/neo4j-graphql-js-api.html#makeaugmentedschemaoptions-graphqlschema */const schema = makeAugmentedSchema({  typeDefs,  config: {    query: {      exclude: [PodcastSearchResult]    },    mutation: {      exclude: [PodcastSearchResult]    }  }})/* * Create a Neo4j driver instance to connect to the database * using credentials specified as environment variables * with fallback to defaults */const driver = neo4j.driver(  process.env.NEO4J_URI || bolt://localhost:7687,  neo4j.auth.basic(    process.env.NEO4J_USER || neo4j,    process.env.NEO4J_PASSWORD || neo4j  ),  {    encrypted: process.env.NEO4J_ENCRYPTED ? ENCRYPTION_ON : ENCRYPTION_OFF  })/* * Perform any database initialization steps such as * creating constraints or ensuring indexes are online * */const init = async (driver) => {  await initializeDatabase(driver)}/* * We catch any errors that occur during initialization * to handle cases where we still want the API to start * regardless, such as running with a read only user. * In this case, ensure that any desired initialization steps * have occurred *///init(driver)/* * Create a new ApolloServer instance, serving the GraphQL schema * created using makeAugmentedSchema above and injecting the Neo4j driver * instance into the context object so it is available in the * generated resolvers to connect to the database. */const server = new ApolloServer({  context: { driver, neo4jDatabase: process.env.NEO4J_DATABASE },  schema: schema,  introspection: true,  playground: true})// Specify host, port and path for GraphQL endpointconst port = process.env.GRAPHQL_SERVER_PORT || 4001const path = process.env.GRAPHQL_SERVER_PATH || /graphqlconst host = process.env.GRAPHQL_SERVER_HOST || 0.0.0.0/* * Optionally, apply Express middleware for authentication, etc * This also also allows us to specify a path for the GraphQL endpoint */server.applyMiddleware({ app, path })app.listen({ host, port, path }, () => {  console.log(`GraphQL server ready at http://${host}:${port}${path}`)})Querying with GraphQLWe can now start our GraphQL server application and start searching for podcasts:➜ npm run start...Successfully compiled 6 files with Babel (411ms).GraphQL server ready at http://0.0.0.0:4001/graphqlNow our GraphQL server application is running locally at localhost:4001/graphql. If we open up a web browser and navigate to that URL well see the GraphQL Playground tool. GraphQL Playground allows us to view the results of introspection of our GraphQL endpoint. This allows us to see the entry points for the GraphQL schema (the Query, Mutation, and Subscription fields) as well as the types, fields, and how the types are connected, essentially the schema of the API.Let’s query the GraphQL endpoint to find podcasts using the search term Neo4j”:{  podcastSearch(searchTerm:  Neo4j ) {    title    description    artwork    feedURL    itunesId    categories  }}We now have a GraphQL API to enable our users to search for podcasts. Next, we’ll need to enable users to subscribe to podcasts and for that to work we’ll need to think about how to add users and enable them to log in and authenticate. So that’s what we’ll focus on next time.ResourcesCode on GithubNeo4j YouTube ChannelBuilding a GRANDstack Real Estate search applicationBuilding a travel guide with Gatsby.js, GraphQL, & Neo4jArrows graph diagramming toolPodcast Index APIFullstack GraphQL Application With GRANDstack book;Dec 7, 2020;[]
https://medium.com/neo4j/create-a-data-marvel-part-7-connecting-the-graph-bc7ed9e2b843;Jennifer ReifFollowJan 24, 2019·7 min readCreate a Data Marvel — Part 7: Connecting the Graph*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)In the posts leading up to this one, we have taken data from the Marvel API, organized it into a graph model, imported the data to Neo4j, created a Spring application from the Spring Initializr page, and set up 5 out of 6 of our entities (Character, Creator, Event, Series, Story) with domain, repository, and controller classes. Finally, we are ready to start knitting all these pieces together using the ComicIssue domain and code the connectors that define a graph — the relationships!In this post, we will work through some coding for our ComicIssue model, repository, and controller classes, as well as code one additional class for a service. We will talk about what purpose the extra class fills and what goes into it when we get to it. For now, let’s code!The ComicIssue classThis class will look similar to our other classes we coded in the last couple of blog posts. To create it, simply right-mouse click on our comicissue folder and choose New->Java Class. Name it ComicIssue, and Spring will generate the skeleton for us.We will have our annotations, Lombok shortcuts, and our fields. We can see this in the class code below.package com.example.demo.comicissueimport ...@Data@NoArgsConstructor@RequiredArgsConstructor@NodeEntitypublic class ComicIssue {  @Id @GeneratedValue  private Long neoId  @NonNull  private Long id  @NonNull  private Integer pageCount  @NonNull  private Double issueNumber  @NonNull  private String name, thumbnail, resourceURI  @Relationship(type = INCLUDES”)  private List<Character> characters = new ArrayList<>()  @Relationship(type = CREATED_BY”)  private List<Creator> creators = new ArrayList<>()  @Relationship(type = PART_OF”)  private List<Event> events = new ArrayList<>()  @Relationship(type = BELONGS_TO”)  private List<Series> series = new ArrayList<>()  @Relationship(type = MADE_OF”)  private List<Story> stories = new ArrayList<>()  public List<Character> getCharacters() { return characters }  public List<Creator> getCreators() { return creators }  public List<Event> getEvents() { return events }  public List<Series> getSeries() { return series }  public List<Story> getStories() { return stories }}What is different is the bit after our fields. We have added some annotations, array lists, and getters here. This is where we connect our ComicIssue entity to our other entities. Recall the data model that we drew back in our first post.Before today’s post, none of those arrows between our nodes existed in our application (they do in the database, though). Let us take just one of these relationships and examine the code a bit more closely.@Relationship(type = INCLUDES”)private List<Character> characters = new ArrayList<>()public List<Character> getCharacters() { return characters }In that block, we use the @Relationship annotation from the object-graph mapper (OGM) to specify that a relationship exists between the current entity (ComicIssue) and the entity retrieved in the ArrayList (in this case, Character). The type annotation argument tells us the kind of the relationship that exists between the two entities. Since we have several different relationships that can exist in our graph (CREATED_BY, MADE_OF, etc), we want to specify that the INCLUDES relationship exists between a ComicIssue and a Character.The line beneath the annotation then sets up an ArrayList variable that will contain a list of Character entities. Finally, we add a getter” method that will also return a list of type Character. This means that when we retrieve a ComicIssue, we can access the characters that are connected to that ComicIssue by an INCLUDES relationship.We use this same logic to set up the relationships between the ComicIssue and each of our other entities, as we saw in the initial code block. With those connections, we will be able to retrieve any of the other entities connected to a particular ComicIssue.The repository classNow that we have the model class completed, we can set up our data access layer with the repository class. Just as we did with our other classes, we will create a new Java class, name it (ComicIssueRepo), and set the Kind to Interface.This interface will be just a bit different from our previous ones because we will be accessing all of our data through the ComicIssue entities, so we need to define some methods to retrieve different kinds of information. We will still extend our interface from the Neo4jRepository, same as we did with our other repositories.package com.example.demo.comicissueimport ...public interface ComicIssueRepo extends Neo4jRepository<ComicIssue, Long> {  ComicIssue findByName(@Param(name”) String name)  Iterable<ComicIssue> findByNameLike(@Param(name”) String name)  @Query(MATCH (i:ComicIssue)-[r]-(n) RETURN i,r,n LIMIT {limit}”)  Collection<ComicIssue> graph(@Param(limit”) int limit)}In our code, we decided to define 3 methods — findByName(), findByNameLike(), and graph(). The first method allows us to search for a ComicIssue by name and retrieve it from the database. It uses the @Param annotation to pass the String name that we will type in to search. Since we are searching for a particular ComicIssue, we only expect a single result to come back.The next method is doing a fuzzy search for a possible group of matching comic issues. If we wanted to look for anything that had Captain America” in the name, then we would see many results retrieved. Again, we have our @Param annotation to allow us to pass a String value as the ComicIssue name, and the method will return an Iterable of type ComicIssue with the list of possible results.These two methods are different from the third method because we didn’t use the @Query annotation on them. This is because Spring Data Neo4j (through the OGM) will derive the first two queries for us based on the method name. Common methods such as findByName() and findByNameLike() can be auto-mapped because they are straightforward and broadly applicable. This way, we don’t need to write queries for these common methods and can focus on other data that we need to retrieve with custom queries. Let us review this third method and explain each component.@Query(MATCH (i:ComicIssue)-[r]-(n) RETURN i,r,n LIMIT {limit}”)Collection<ComicIssue> graph(@Param(limit”) int limit)It begins with an @Query annotation for us to write a Cypher query to be executed. In this case, we want to retrieve a section of the graph to show on the screen for a visualization. Our query, then, is matching ComicIssue entities that have any type of relationship to any other entity. It then returns the ComicIssue nodes (alias i), the relationships (alias r), and the connected nodes (alias n) and limits the results to an arbitrary number (we will pass a parameter into limit). The limit prevents the visualization and browser performance from getting overwhelmed with everything in the database since we have a large amount of data. The line after the annotation is our method definition that uses the @Param annotation to pass in the argument for limit and expects a collection (a type of Iterable) of ComicIssue entities in return.That’s all we need! This class will create the data access layer and act as the go-between for the database and types of requests.What I LearnedThough there were a few bits of new information in these classes, it was really only to add the connections between this central class (ComicIssue) and the other classes. Our class structures and their functions matched our expectations from previous code, so we just needed to map the relationships and set up the methods for accessing the data through the ComicIssue.As always, I will share the highlights of what I learned.Much of the code mirrored what we did with other classes, so I could focus on the new piece — the relationships between entities. Understanding what annotations did what and which arguments were needed or unnecessary took some research and some testing. Your use case and data will not be exactly what someone else has, so you will also need to experiment a bit to ensure you don’t have any useless code. You cannot rely simply on copy/paste (nor do you probably want to).This project was a bit trickier than other existing code examples out there because we had so many entities that all connected to each other (most examples have 2 or 3 entities, ours has 6). However, this just prepares us for the variety of complex data sets in real-world scenarios. Real data will not be simple or consistently-organized, so projects like this are more applicable to real-world data scenarios.Next StepsProgress in these later posts may seem slower, but we are stair-stepping our way through more difficult material and code. Knowing how each piece fits together and why something functions the way it does is vital to replicating the process with different use cases and data. Understanding why helps you apply the logic and create your own applications without relying on copy/paste or trial-and-error code.In the next post, we will walk through the controller class and the service class, too! This will wrap up our code for handling requests and data before we drop in the final piece for a pretty user interface with html. See you soon!ResourcesFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j GuideProject Lombok docsPrevious parts of this blog series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6;Jan 24, 2019;[]
https://medium.com/neo4j/using-auradb-neo4j-as-a-source-and-sink-for-aws-msk-e18a6e865633;Stu MooreFollowMar 31·13 min readUsing Neo4j AuraDB as a Source and Sink for AWS MSKNeo4j now supports AWS Managed Streaming for Kafka (MSK) through MSK Connect, configured as a source and sink with AuraDB Free.Photo by Good Free Photos on UnsplashBack in January, I took on the Product Manager role for Neo4j Connectors — the products which enable customers to stream data into and out of Neo4j using products like Apache Kafka / Apache Spark and their commercial alternatives.In this blog, I will show you how Aura/Neo4j supports AWS Managed Streaming for Kafka (MSK) through MSK Connect with our connector configured as a source and sink with AuraDB Free.We are using the same database as the source and sink so that you can see how it can be configured to work depending upon your needs — it also means you can do all the graphy stuff for free because you only get one free AuraDB database.On Thursday the 30th of March, there is a joint hosted online event called Neo4j AWS DevDay, which includes a series of great talks on:- AWS and Neo4j’s Partnership- Deploying Neo4j From the AWS Marketplace- Supervised Learning with AWS SageMaker- Importing Data into Neo4jIt is going to be a great event packed with demos. If you are interested, you can register here.If you were at the event and wanted to replicate what John demoed then this is the blog for you.AWS | Neo4j ML Dev Day Virtual WorkshopNeo4j is the leader in graph database and analytics, helping over 75 percent of the Fortune 100 to drive innovation and…go.neo4j.comBack to Creating a Graph With MSK ConnectWe’ll use the cities of the world dataset available here — a CSV file of cities, their populations, locations, and counties — as our dataset, importing it into AuraDB as a set of isolated nodes for each City with properties for the Country. The Sink Connector will read the data on the topic, and write it back to AuraDB creating a relationship LOCATED_IN between each city and the country.London is LOCATED in EnglandWe are going to use a Terraform module Ed Randle created for you, to build the AWS MSK cluster and networking required to connect to AuraDB.Why Terraform? Well if you aren’t familiar with Terraform, it is basically Docker Compose for building anything in the cloud, and there is a lot of networking, IAM roles, and permissions to get right for MSK and the module does that all for you. It is well worth the short amount of time you need to invest to set up Terraform.What Do You Need?An AuraDB accountAn AWS Account, an S3 bucket, and a Cloudwatch log group for monitoring (recommended) but you can send the logs to the S3 bucket.Terraform on your laptopAbout an hour of hands-on timeAbout a 3/4–1hr to work on other stuff while stuff happens in the backgroundYour favorite snacksWhat Will You End Up With?A graph of cities of the world located in their respective countries running in AuraDBA running MSK Cluster, MSK Connect configured with a source and sink connectorA VPC and all the networking you didn’t know you needed (and an easy way to tear it all down)Please note, while AuraDB Free is free for life, we’ll be creating some AWS resources that you will need to pay for (we have used micro instances where possible, but there are some larger ones that are required for the MSK Cluster). Internet Gateways aren’t cheap to run, and you will incur cost per day and charges for sending data out so bear this in mind. And tear down the estate after you’re done testing.PreparationSign up for AuraDB Free, you can create one here. Make a note and download the URI and the password (save it in your favorite password vault) while the database is being provisioned carry on with the rest of the setup.Create an S3 bucket if you don’t have one already, and go to GitHub https://github.com/neo4j-contrib/neo4j-streams/releases to get neo4j-kafka-connect-neo4j-5.0.2-kc-oss.zip and upload it to your S3 bucket.Please dont use the neo4j-kafka-connect-neo4j-5.0.2.zip, because that is intended for use with Confluent.Install Terraform on your laptop if you don’t have it already, follow this guide.Install Terraform | Terraform | HashiCorp DeveloperTo use Terraform you will need to install it. HashiCorp distributes Terraform as a binary package.developer.hashicorp.comCheck you can create an S3 micro instance and destroy it — this tutorial is a good test if you are new to Terraform.Go to Ed’s GitHub repo and take a local clone, and follow the instructions in the readme to create your main.tf and provider.tf files for Terraform.GitHub - neo4j-partners/msk-framework-terraform: A terraform module to create an MSK (kafka) test…This repository hosts a terraform module which performs the following tasks (when invoked as a child module): Creates…github.comWhen you are ready, runterraform apply -auto-approveNote, Terraform apply takes about 30 mins to build the environment vs several days :), so you can crack on with some other work.Once it’s completed, the Terraform module will output the commands you need to connect to the public Bastion machine.ssh -A -o StrictHostKeyChecking=no ec2-user@10.0.x.135Then, you need to hop to the client machine that has the Kafka utilities installed on it so that you can create the topic:ssh -A -o StrictHostKeyChecking=no ec2-user@10.0.x.135cd kafka./create-topic.shWhat Question Do We Want to Ask Our Graph?We have a list of cities of the world which includes information on the country like geolocation and the population.Sample of data from cities of the worldLet’s start off by asking which city exists in which country, and which countries share city names.To demonstrate the capabilities of the Sink and Source Connectors, we are going to do this in two phases.First, create source data to be streamed into the topic we’ll label it ‘TestSource and each node will have three properties ‘city’, ‘country’ and ‘timestamp’.Please note that the timestamp property currently needs to be a long value, you can use timestamp() or datetime().epochMillis as source.Before we try and import all the data from the CSV, let’s create a test node in AuraDB using the Query tab this will help you understand the configuration that is required by the Connector:CREATE (:TestSource {city: London, country: England, timestamp: datetime().epochMillis})The result is the following node for England:The resulting TestSource node for LondonThe node can be queried with:MATCH (ts:TestSource) WHERE ts.city = London return tsFor the Connector’s configuration, we need a more general statement that picks up the creation of new cities. Neo4j’s Connector for Confluent provides $lastCheck to track the time, and we return all the properties of interest:neo4j.source.query=MATCH (ts:TestSource) WHERE ts.timestamp > $lastCheck RETURN ts.city AS city, ts.country AS country, ts.timestamp AS timestampThe Sink Connector needs to stream this data back from the topic, my-topicand create a graph that maps nodes with the City label, to nodes with a Country label through the relationship LOCATED_IN.London is LOCATED in EnglandThis simple graph can be created with the following Cypher in AuraDB’s Query tab:CREATE (ci:City {name:  London }) MERGE (co:Country {name:  England }) MERGE (ci)-[:LOCATED_IN]->(co)And queried with:MATCH (ci:City) WHERE ci.name = London return ciWe need to change the CREATE statement for the Connector’s Sink configuration, and switch to using MERGE so we don’t create duplicate nodes:neo4j.topic.cypher.my-topic=MERGE (ci:City {name: event.city}) MERGE (co:Country {name: event.country}) MERGE (ci)-[:LOCATED_IN]->(co)Create a Custom PluginIn the AWS Console, go to MSK and select ‘Create custom plugin’.Please note, it is considered bad practice to mix Terraform created resources with manually created ones because Terraform won’t be able to delete resources if they have any dependencies. I am doing this intentionally to help demonstrate how the Connector configuration relates to the sink and source data.Browse to your S3 bucket, and select the Neo4j Connector for Apache Kafka zip.Give the plugin a name e.g. neo4j-connectorCreate custom pluginSelect the null version, and ‘Choose’Select the null versionCreate a Connector for MSK ConnectSelect the connector you creaed and click NextProvide name, and descriptionPaste the following into the Connector configuration in the next section, but be sure to replace the neo4j.server.uri and the password with yours (the example below contains the text redacted). Note, the uri for Aura should look like neo4j+s://x57g8ii4.databases.neo4j.io:7687 — you will need to add the port 7687 as this isn’t shown in the Aura console. Notice the configuration line for the neo4j.source.query is the one we created above to query the source data.topic=my-topicconnector.class=streams.kafka.connect.source.Neo4jSourceConnectorkey.converter=org.apache.kafka.connect.storage.StringConverterkey.converter.schemas.enable=falsevalue.converter=org.apache.kafka.connect.storage.StringConvertervalue.converter.schemas.enable=falsetasks.max=2errors.retry.timeout=-1errors.retry.delay.max.ms=1000errors.tolerance=allerrors.log.enable=trueerrors.log.include.messages=trueneo4j.server.uri=neo4j+s://redacted.databases.neo4j.io:7687neo4j.authentication.basic.password=redactedneo4j.authentication.basic.username=neo4jneo4j.database=neo4jneo4j.streaming.poll.interval.msecs=5000neo4j.streaming.property=timestampneo4j.streaming.from=LAST_COMMITTEDneo4j.source.query=MATCH (ts:TestSource) WHERE ts.timestamp > $lastCheck RETURN ts.city AS city, ts.country AS country, ts.timestamp AS timestampCode pasted into the configuration for the connectorSelect ‘Autoscaled’, and all the defaultsGo with Autoscaled and all the defaultsSelect the IAM role with the prefix you specified in the main.tf.For example, in mine, I specified t1-neo4j and selected t1-neo4j-env-role.Select all the defaults, and the IAM role with the prefix from main.tfNote the two warnings in Access permissions remain even after you select the IAM Role, you can ignore these because Terraform has created the role IAM policies and role.Click ‘Next’, and then again until you see the Logs set up. Go with CloudWatch, it will save you a load of pain downloading and extracting log files if you need to debug the connector and flow of data from Aura and back again.It is pretty straightforward to set up a Log group, but you will need that to exist before you continue.Deliver your logs to CloudWatch for faster troubleshootingCreate a Sink ConnectorRepeat the same process, but this time create a Neo4jSinkConnectorToAura using the following, replacing the neo4j.server.uri (and port) / password for your AuraDB Free instance — this is where we use the MERGE statement for the neo4j.topic.cypher.my-topickey.converter=org.apache.kafka.connect.json.JsonConverterkey.converter.schemas.enable=falsevalue.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter.schemas.enable=falsetopics=my-topicconnector.class=streams.kafka.connect.sink.Neo4jSinkConnectortasks.max=1errors.retry.timeout=-1errors.retry.delay.max.ms=1000errors.tolerance=allerrors.log.enable=trueerrors.log.include.messages=trueneo4j.server.uri=neo4j+s://redacted.databases.neo4j.io:7687neo4j.authentication.basic.username=neo4jneo4j.authentication.basic.password=redactedneo4j.topic.cypher.my-topic=MERGE (ci:City {name: event.city}) MERGE (co:Country {name: event.country}) MERGE (ci)-[:LOCATED_IN]->(co)Now go make a cup of tea/coffee and grab a few biscuits/cookies, because it is time to check slack etc. while you wait (times vary but about 10mins is usual).Check both connectors are Running when you return. If there are errors you will need to go to CloudWatch to review the logs (some troubleshooting steps have been included at the end of the blog).Now Back to the Fun, Graphy Bit*Go to Aura Console and test that we can create source information for MKS and have it streamed back via the Sink connector:CREATE (:TestSource {city: London, country: England, timestamp: datetime().epochMillis})The resulting TestSource node for LondonThe Neo4jSourceConnectorFromAura will stream the data into my-topic, you can then run the MATCH statement to see the resulting graph which has come back via the Neo4jSinkConnectorToAuraMATCH (ct:City) WHERE ct.name = London return ctAnd there you go, two nodes and one relationship in a graph.The start of a graphPS. Ed could have got you here a bit quicker with Terraform, but it is really helpful to create the Sink and Source connectors yourself. Plus, if you want to customize the set up with your own data then it is helpful to have done this all manually.Now as graphs go, that is pretty underwhelming, so let’s clean up:MATCH n DETACH DELETE nAnd then bring in all the cities in the world to be able to answer the questions:create constraint if not exists for (ts:TestSource) require (ts.city, ts.country) is uniqueLOAD CSV WITH HEADERS FROM   https://storage.googleapis.com/meetup-data/worldcities.csv AS lineCALL { with line     MERGE (ts:TestSource { city: line.city, country: line.country})     ON CREATE SET ts.timestamp=datetime().epochMillis} IN transactions OF 1000 ROWSYou can now explore the world, and see which city names exist in each country, and where they overlap:MATCH (ct:City) WHERE ct.name = London return ctNote, London exists in England and United Kingdom because of the test node we created. I didn’t delete it, so it exists alongside all the imported data.If you double-click on United Kingdom you can see all the cities that are LOCATED_IN‘MATCH (ct:City) WHERE ct.name = ‘London’ return ct’ in the context of the importSide note: If you want to bring in more data you can use the following statement which will include the longtitude, latitude. Population data is available, but it is very sparse so probably not worth it.create constraint if not exists for (ts:TestSource) require (ts.city, ts.country) is uniqueLOAD CSV WITH HEADERS FROM https://storage.googleapis.com/meetup-data/worldcities.csv AS line  CALL { with line     MERGE (ts:TestSource { city: line.city, country: line.country })     SET ts.timestamp = datetime().epochMillis,     ts.lat = toFloat(line.lat), ts.lng = toFloat(line.lng)} IN transactions OF 1000 ROWSCustomize This to Use Different DataIf you want to change the topic name, the data, and the queries used, then have a read through the following summary of steps required to tailor the configuration.To create new topics, then ssh to the MSK client machine (from the bastion):./create-topic.sh topic-name2. Delete the Sink and Source Connectors if you want to change AuraDB URI / password, or the queries used (takes about 10 mins). Sorry for the bad user experience here, that’s how this works on AWS.3. Create a new Source Connector (you’ll need to use a new name if you don’t want to wait for the old one to be deleted), and change the following entries in the configurationtopic=my-topicneo4j.source.query=MATCH (ts:TestSource) WHERE ts.timestamp > $lastCheck RETURN ts.city AS city, ts.country AS country, ts.timestamp AS timestamp4. If you change the source dataset or results, then you will need to create a new Sink Connector too:topics=my-topicneo4j.topic.cypher.my-topic=MERGE (ci:City {name: event.city}) MERGE (co:Country {name: event.country}) MERGE (ci)-[:LOCATED_IN]->(co)Caution: the source uses ‘topic’, and the sink uses ‘topics’ because you are allowed multiple sinks. Please make sure you get the singular/plural forms correct otherwise the Connector will fail on creation.5. Import your new data… check out this blog on Data Importer as a handy way of creating a model, mapping CSV files, and importing data into Neo4j.6. In the Query tab, run the Cypher statement in the Aura console that matches the topic definition in 3.CREATE (:TestSource {city: London, country: England, timestamp: datetime().epochMillis})7. The Neo4jSourceConnectorFromAura will stream the data into the topic you have specified, and then you can run your new match statement, or use the Query tab:MATCH (ct:City) WHERE ct.name = London return ctDon’t Forget to DestroyBefore we can tear everything down with Terraform, we must delete manually created resources within the MSK environment — using the MSK console Delete the Source and Sink Connectors. You can leave the S3 bucket if you want, and you may want to delete the CloudWatch logs too but these won’t affect the Terraform script. Now runterraform destroy -auto-approvePS: You can keep AuraDB Free, it is free after all. It will just be auto-paused after 3 days of non-usage to save some computing resources.Some Handy TroubleshootingIn the CloudWatch logs, search for ‘ERROR’ this will highlight problems like not being able to connect to Aura. For example:Worker-0a6a93757f8865f6d] [2023-03-26 16:48:15,274] ERROR [Neo4jSourceConnectorFromAura|task-0] WorkerSourceTask{id=Neo4jSourceConnectorFromAura-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:191)[Worker-0a6a93757f8865f6d] org.apache.kafka.connect.errors.ConnectException: java.lang.IllegalArgumentException: Illegal port: -1Illegal port: -1 means you forgot to add ‘:7687’ on the URI you provided in the sink or source connectors.ERROR [MSK-source-connect-to-auradb|task-0] Error: (streams.kafka.connect.source.Neo4jSourceService:164)[Worker-06882f5c982acf346] org.neo4j.driver.exceptions.ServiceUnavailableException: Could not perform discovery for database neo4j. No routing server available‘No routing server available’ you may have a connectivity issue between MSK and Aura — test your routing with the nc command. Perform an apt get nc on the MSK Client machinenc -vz uri-name.databases.neo4j.io 7474Ncat: Version 7.50 ( https://nmap.org/ncat )Ncat: Connected to x5.205.xxx.74:7474.Ncat: 0 bytes sent, 0 bytes received in 0.02 seconds.nc -vz uri-name.databases.neo4j.io 443Ncat: Version 7.50 ( https://nmap.org/ncat )Ncat: Connected to x5.205.xxx.74:443.Ncat: 0 bytes sent, 0 bytes received in 0.02 seconds.nc -vz uri-name.databases.neo4j.io 7687Ncat: Version 7.50 ( https://nmap.org/ncat )Has the source connector found data to put in the topic?Poll returnsIf you don’t paste the correct Cypher statement into the Source/Sink connector’s config, miss off-characters, or create nodes/relationships in the Source that don’t match the configuration of the Sink, you may get problems parsing messages in the topics. E.gErrorData(originalTopic=my-topicOR Unrecognized tokenWhen the Sink connector is working you will find the following messageCommitting offsets asynchronouslyConclusionHope you liked using Ed’s Terraform module to create the environment for MSK and the ease with which you can create sink and source connections between Aura and MSK’s topics with the Neo4j Connector for Apache Kafka.Hope you enjoy the AWS Dev Day if you are able to attend and give the Connector a go with your own data.AWS | Neo4j ML Dev Day Virtual WorkshopNeo4j is the leader in graph database and analytics, helping over 75 percent of the Fortune 100 to drive innovation and…go.neo4j.comIf you have feedback, please share it as a GitHub issue in the Connector Repository: https://github.com/neo4j-contrib/neo4j-streams;Mar 31, 2023;[]
https://medium.com/neo4j/drawing-graphs-with-arrows-app-ee5735caa04d;Alistair JonesFollowJan 12, 2021·5 min readDrawing graphs with Arrows.appArrows.app is a new tool from Neo4j Labs. It’s an easy way to draw pictures of graphs for web pages, documentation or presentations.Written by: Alistair Jones and İrfan Nuri KaracaArrows.appWeb-based tool for drawing pictures of graphs, for use in documents or presentations.arrows.appExample of the kinds of graph you can draw with Arrows.appIn this blog post, we’re going to take a short tour of the features, and show some of the useful things you can do with the tool.Starting Arrows.appTo use Arrows.app, navigate to arrows.app in a modern web browser. Chrome, Safari or Firefox are recommended.Your first graph starts with one empty node in the middle of the screen:Creating nodesTo create more nodes, you can click the Add Node” button to the right side of the screen:However, you won’t need to use this button very often, because usually it’s easiest to create new nodes automatically when creating new relationships.Creating relationshipsTo create relationships, you need to hover just outside the node (halo) and then drag to create them like this:Drag to create relationshipThis creates a new relationship, and also a new node. If you hover the new node over an existing node, the relationship will connect to the existing node instead of creating a new node. This makes it easy to make connections in your graph, for example to form a triangle:Connect to an existing nodeLining things upYou drag nodes to move them around. Dashed lines appear to help you line them up nicely.Guides help to arrange nodesNode captions and relationship typesDouble-click on a node to start adding text. Alternatively, if you have a node selected, you can just press Enter.Adding a node captionTo finish editing, press escape, or click away.Relationships work the same way you can double click or press enter to start typing text.Adding a relationship typeRelationship DirectionRelationship arrows appear in the direction that they were drawn. If you need them in the opposite direction, use the reverse” button.Reverse Relationship TypeLabels and propertiesIn Neo4j we use labels to distinguish different types/roles of nodes. Add labels using the + Label” button:Both nodes and relationships can have properties. Add properties using the + Property” button:StylingBy default, the graphs you draw will look like the examples above: simple and bold, suitable for a quick sketch. The Choose Theme” button reveals a number of other styles that may look better for the specific graph that you are drawing:Themes for styling the graphYou can override styles in the theme by adjusting the parameters in the inspector on the right hand side of the window. Changes you make in the inspector apply to selected nodes and relationships. For example, use this to change the colour of a node.Setting node colourIf you have nothing selected, then the whole graph is affected. For example, if you don’t care around the direction of relationships in your graph, change directionality” to undirected” to hide all the arrow heads.Setting directionalityExporting ImagesThe Download / Export” button gives you files to download or share in various formats.The PNG images are available in several resolutions, and cropped to exactly the edge of the graph. PNG images are perfect for presentations — paste them into Powerpoint or Google Slides.There is also SVG export in case you need a vector graphics format. This is very useful if you want to tweak the image after export. For example, if you import into Sketch, each of the individual elements is still editable.Cypher and Neo4jGraphs naturally live in a Neo4j database. Arrows.app makes it easy to create a small graph in your Neo4j database by creating a Cypher statement for you. Click the Download / Export” button choose the Cypher tab. You get the full data, including all the labels and properties if you have set them. Copy and paste the Cypher or, if you have Neo4j Desktop running, use the super-convenient Run in Neo4j Browser” button.Wrapping upSo to recap, you can use Arrows to create PNG, SVG or Cypher. They are great for using in presentations, documents, or to create Neo4j graphs.Uses for Arrows.appTo see all this in action, watch the short demo video on YouTube:;Jan 12, 2021;[]
https://medium.com/neo4j/navigating-a-technical-conference-talk-from-submission-to-delivery-333fdca788b7;William LyonFollowMar 30, 2021·37 min readNavigating A Technical Conference Talk From Submission To DeliveryGraphStuff.FM Podcast Episode #2In the latest episode of GraphStuff.FM: The Neo4j Graph Database Developer Podcast, Lju and I shared some of our learnings and experiences from speaking at developer conferences. If you’re thinking about presenting at a developer conference, then this episode is for you!With the submission deadline for NODES 2021 (The Neo4j Online Developer Expo & Summit) coming up we thought this was the perfect time to dig into how we think about submitting, building, and delivering technical conference talks. Be sure to submit your talk proposal to NODES before the deadline if you’d like to share your graph story with the world!Lju Lazarevic (00:00:01):Hello, everybody. Thank you very much for joining us today. This is GraphStuff.FM, our very first live episode. So a little bit quickly for those of you who are wondering what’s GraphStuff? So this is a new podcast that we’ve put together, and we’re mainly going to be talking about graphs from a developer’s perspective. So we’re going to be your co-hosts: Lju Lazarevic and Will Lyon.Lju Lazarevic (00:00:29):And just very quickly for those of you who are wondering, this is not a replacement for the podcast of Rick Van Bergen does. So this is very much us looking at more of the developer point of view with regards to things around graphs and what they’re encountering around that. So the things we’ll cover in these areas will be graph native under the bonnet, for example. So how exactly does Neo4j work? Thinking processes around growth modeling, so what decisions did you go through? What considerations? What about the Neo4j graph platform or the various bits and pieces in there? What do they mean? How they work together? And so forth.Lju Lazarevic (00:01:11):And this is going to be something we are going to be doing on a regular basis. And if you would like to be notified as episodes come through, do check out GraphStuff.FM and you will be able to follow us in your favorite podcast provider. So the main driver behind today’s episode is all about how to get started when you put together a presentation. So what is the whole journey behind that? And a lot of this is being driven by our upcoming conference NODES. So I’m going to hand it over to Will, who’s going to tell us a bit about NODES.William Lyon (00:01:48):Yeah. So NODES is the Neo4j Online Developer Expo and Summit. This is coming up on, I think the third edition of NODES, this is basically Neo4j’s online developer conference. The date of NODES is June, 17th — when it will be live for everyone to join around the world. But the call for proposals has been live for a few weeks now, and the deadline is April 5th for the call for proposals.William Lyon (00:02:24):So if you’re interested in sharing your graph story with the world, definitely please consider submitting a conference proposal. And we’ll talk a bit about how do you structure that conference proposal? What motivates you to put a talk together? How do you come up with ideas? So we’ll go through all of this to give you ideas of how to structure your proposal.William Lyon (00:02:54):But then also, what we want to talk about today is: Okay, you’ve submitted your proposal, it’s been accepted, and you now have a month, two months, a couple of weeks, whatever, to think about putting your talk together. How do you approach that? What are some things that Lju and I have learned? And hopefully, some of this will be useful for you. So if you’re interested in learning more about NODES or submitting your proposal, go to neo4j.com/nodes. We’ll also link in the show notes the link to the call for proposals and all of that. Cool. So let’s get into it, maybe to kick off the discussion. Lju, I thought of a fun question to start with. Do you remember your first conference talk?Lju Lazarevic (00:03:44):Oh my goodness. Yes. So this was quite a while back in a different lifetime and it was an academic conference that I was presenting at, and it was all about detecting birds through wing oscillations. So the idea that you could try and identify different bird species effectively based on how they flap their wings. And it was quite interesting because there were lots of people there and it was bit of a daunting one as well.Lju Lazarevic (00:04:14):So the first time I did this, it was a bit daunting, because I’d get a bit nervous. But, I’ve had practice now. I was in front of a large group of people, I was talking about a subject where some bits I was quite familiar, but other bits, it was like, whoa, there is so much about this that I don’t know. And always had a bit of the back of my mind going, Oh my goodness, what happens if somebody asks me a question that I can’t answer?” So it was an interesting experience. It was a fun experience, but yes, I remember it well. What about yourself?William Lyon (00:04:53):Yeah. So for me, the first conference talk, the meet-ups blur together and I’m not sure exactly which was first. I gave some talks at some of the local meetups and there’s a local community conference as well, that I’ve spoken at for a few years. And I can’t remember exactly in what order, but for me, when I think of my exposure to public speaking more broadly, the thing that was much more impactful for me was I spent a year teaching introduction to computer science at my university when I was working on my master’s degree, I had a teaching scholarship or whatever you call that.William Lyon (00:05:40):And this I think was really quite a challenge for me to give a lecture three times a week about intro to computer science and it was a pretty large class. So it was very intimidating for me initially, and I remember just being very overwhelmed initially and very scared, it was in the forestry building, which was across campus from the computer science building. I think we didn’t have a room big enough or something, and just going over into the forestry building was intimidating for me at the time.William Lyon (00:06:13):So anyway, this was, I think a really good experience for me and this really informed how I think about things like connecting with the audience and making sure that I’m engaging my students and a lot of these things were really formative for how I think about public speaking. So anyway, thinking about, yeah, first public speaking exposure, that’s what I think about.Lju Lazarevic (00:06:41):It’s a great experience, it can definitely be scary to begin with, but it’s a fantastic experience. And what would be really good now is to talk about why would you sign up for this? We talked about being a bit nervous, or a bit frightened, or worried what people are going to ask us a huge audience. Why would you step forward and do a conference talk?William Lyon (00:07:04):I think there’s a lot of reasons, right? I think that, especially for developers, you see a lot of these very technical conferences out there and people sharing knowledge, sharing what they’ve learned, helping others build new skills. And I think it’s seen almost as a step in a career development phase, right? Being able to present technical material to your peers, sharing something that you’ve learned.William Lyon (00:07:35):So I think you can really benefit from a professional perspective by being able to hone and exercise that professional technical communication, right. Because I think this is an important skill to have just day-to-day right, as being able to communicate about technical topics with your peers. So I think that that can be a big motivator.William Lyon (00:08:02):I think for me, really the biggest motivation is the learning process. They say that you really haven’t learned something until you can teach it to others. And I found that this is really, really true. If you really want to learn something yourself, try to teach it to someone else and giving a conference talk, really gives you the opportunity to go through that exercise. So for me, that’s the number one motivation.Lju Lazarevic (00:08:33):I think another element as well is personal development. So you’ve touched on that with the idea of how do you reinforce that you’ve really learned the subject and understand it, but there’s another reason as well. I think when you present, which is, if you do have a phobia of meeting new people, what happens if a group of people that you’re unfamiliar with, or you have an idea or an opinion and you feel a bit nervous about sharing it? This is a really great way to be really structured about how you approached that and getting that exposure. It’s a really great way to just keep doing it over and over, and it gets easier over time.Lju Lazarevic (00:09:16):And that also fits in with career development and being more at ease at being able to present your ideas and thoughts to an audience. And it works on the personal level as well as the career level. It also reinforces that learning, and I think that’s really important. And another relevance as well, I think it’s really important to promote your brand. So especially now, as we’re ever more connected and it’s not just, here’s my CV, and this is what I’ve done. I think it can be very important as well — to demonstrate your thought leadership, where your skills are, and being able to be confident in a presentation format to be able to do that really helps define your brand and who you are.Lju Lazarevic (00:10:01):And if you don’t want to do it for yourself again, it’s something really useful to define the brand of your company, or if you’re working on a project within your company, being able to maximize your awareness of what you’re doing and how you’re doing it and why you are doing it. So I think there are many reasons as to the benefits of why you should go and do a conference talk.Lju Lazarevic (00:10:25):So, I guess another element that might come up where something would go, okay, you’ve sold it to me. I get the value of doing a presentation, but you know what? I am absolutely terrified. And I’m terrified because I get nervous of presenting to people, I don’t know how to come up with ideas for a conference. What if I don’t absolutely know my subject inside out or the terror, the fear of fears? What happens if somebody asks me a question that I can’t answer there and then? So what approaches have you come across? What do you use to manage these hesitations?William Lyon (00:11:12):Yeah, that’s really common, right? I think there’s a saying that more people are afraid of public speaking than there are of dying or something like that. I have no idea if that’s true or not, but it’s certainly something that is scary the first few times you do it. Right. And I think it’s fundamentally, it’s something that from my perspective, anyway, I had to practice doing, and once you’ve done it a few times, then a lot of that nervousness and terror goes away, at least I’ve found.William Lyon (00:11:48):And for me going back to teaching a class, the first few lectures I gave I was very nervous and yeah, like terrified to get started. And I found giving a lecture to a group of 60 or 70 students three times a week — once you’ve done that a few times, then it starts to smooth out and get easier and you enjoy a lot of the feedback that you’re getting and engaging with your audience is a really rewarding experience.William Lyon (00:12:20):And so once I had done that a few times, I would just think, well, okay, I’ve done this before, this is nothing that I should be nervous and terrified about. So I think it’s something to think of it as an achievement to overcome and just some practice to get in. And once I think you’ve started to experience the benefits of seeing folks that are engaged and seeing some of that feedback is really, really reinforcing. And I think the feedback can be a big help.Lju Lazarevic (00:12:57):Absolutely. I think the important thing there, as well as just keep doing it. The more times you do it, the easier it gets, the more comfortable you will be, the more confident you will be as with many things in life. The more times you do it, the easier it gets. And whilst you don’t have that experience yet, whilst you’re quite new practice, practice, practice. I think I read somewhere, you get some seasoned keynote speakers will spend something like three, four times the amount of time that they will be delivering their speech to how much time they spend practicing it.Lju Lazarevic (00:13:39):So if your keynote is 20 minutes, you could easily be spending up to two hours plus the number of multiple times, you’re practicing it over and over again, so you’re comfortable. So that is a key thing to do so that you know your content inside out. So it’s not a surprise when you’re delivering it. So that’s one thing that you’re certain about in your environment and the other thing as well, it’s your story.Lju Lazarevic (00:14:08):So even if you’re nervous and you’re starting, it’s your story, you came up with it, you know it well, and just tell it and you’ll find that you’ve started to settle into a groove. And then you may even shock horror even be enjoying the process because people are there to hear your story. So yeah, many things you can do to counteract that. I guess another big one around that is what do you do if you’re asked an awkward question?William Lyon (00:14:42):There are questions that you have no idea how to answer. Right. A lot of questions are actually comments and maybe feedback. So I think for those kinds of questions where someone is maybe making a comment or a statement that you know, rather than them trying to share something, rather than digging deeper into a subject. I like to reinterpret their question and comment as the question that I would like to answer. That’s somewhat related to what they’re discussing. Right?William Lyon (00:15:18):And there are lots of examples of this, but I think it’s really important to remember that during the conference presentation and even the Q and A, that you’re speaking to the larger audience, right? You’re not… It’s not beneficial for the larger audience necessarily to go on some tangent that you maybe don’t know anything about or something that’s maybe only relevant for the person asking the question.William Lyon (00:15:45):So I think as you’re thinking of how to answer these things, try to think of that context, does… If it’s something that you don’t know anything about, I think maybe the audience doesn’t either, right. And they lack that context. So I think taking a step back and maybe saying, okay, well, we’ll add a higher level here’s the context around the question that you’re asking, and I don’t really know anything about the specific thing that you’re asking about, but here’s how I understand that it fits into the larger context and lay things out that way.William Lyon (00:16:17):I think is actually really appreciated for others in the audience watching who maybe don’t really understand maybe the specific thing that the person is asking about. So rephrasing taking a step back, setting some context I think it can be a helpful way to just rephrase it and think about things. But also fundamentally, if someone asks you a question that you don’t know the answer to, it’s perfectly reasonable to say I don’t know the answer to that, let’s move on to something else, there’s absolutely nothing wrong with that. I’ve done that, I’ve seen people do that.William Lyon (00:16:51):From the audience perspective, it’s more helpful for them to have you identify something and say, Hey, that’s not something I’m an expert on, let’s move on to something else,” rather than waste their time trying to, I don’t know, trying to make something up. So perfectly fine to say, I don’t know, let’s move on to something else.Lju Lazarevic (00:17:12):And it’s also perfectly fine to say, let’s exchange details I can come back to you on that. So if that’s something you want to explore, you don’t know the answer, but you can look that up, how to look that up then again, that’s perfectly fine. And touching on something as well you said there about the audience. It does happen where somebody asks you a question, you don’t know it, but sometimes it’s okay to ask the audience because there may be somebody there who can answer that question.Lju Lazarevic (00:17:43):So you’ve got so many options that you can engage the audience. And again, if you have that person who asks that awkward question, you have the option as well to turn it back to them and say, well, what are your thoughts on this? If there’s somebody who’s trying to give their view, you have a number of ways to answer it.Lju Lazarevic (00:18:04):But if you’re thinking, this is too hard again, it’s perfectly okay to just say, I’m not too sure I can get back to you on that, as the easy of the options, you’ve got many different ways to tackle that fun, fun topic. Okay. So hopefully we’ve put you all at ease when it comes to thinking about why you should do a presentation and how would you tackle some of those awkward situations if you’re a bit new to presenting. So the next one is coming up with that killer idea that you’re going to do a presentation on. So how do you approach this normally?William Lyon (00:18:47):I think there are a few fundamental ways to come up with ideas for conference talks. I think there’s sort of looking at some of the interesting problems you’ve solved, some of the challenges that you’ve overcome recently like wanting to share some of that I think is a great source of inspiration for conference talks, because if it’s something that you’ve learned, something that you’ve struggled with, some challenge that you’ve overcome, some application that you’ve built, some design pattern that you’ve learned, that’s evidence there that it’s something that will be of interest to a larger group, right?William Lyon (00:19:24):So I think that that is a really great area. There’s also the area of I know something about this, I’d like to learn a little bit more, why don’t I just challenge myself to learn enough about this to be able to give a conference talk about it, right? It goes back to a point we made earlier about being able to teach something to someone else as the ultimate form of learning it yourself.William Lyon (00:19:58):So I liked to joke about this idea of conference-driven development that once you have committed to giving a conference talk about this, you have this forcing function now of making sure that you actually can build this thing and give a presentation about the topic. I think that there’s a fine line here though between choosing something that you’re not really is realistic to dig into and go into the depth of building a conference talk around it, versus something that you have some tangential familiarity with, and it’s something of interest for you, right?William Lyon (00:20:40):So there’s a fine line there, but I think that can be to just having that conference talk, if it’s something that aligns with your professional goals, right? Maybe you have an OKR with your team, a goal of something that you need to build, something that you need to dig into any way from a technical perspective. If this aligns with that, then that can be a great motivator for you to dig into that topic in a lot more depth.Lju Lazarevic (00:21:09):Absolutely. And the other one can be, you see a common problem coming up over and over again. So let’s say you’re active in a community and you keep seeing the same question come up over and over again. And you go, you know what? I think there’s a gap here. Maybe this is something I can put together for an educational talk. So you want to get your community friends, have the understanding about how to come over that. So that’s another great way you can come up with some ideas for a talk.Lju Lazarevic (00:21:43):Another option too is to have a look at past talks and this doesn’t have to be necessarily around Neo4j. This can be any technical talk or any non-technical talk. Is there something that inspires you? Is there a speaker that inspires you? What is it that inspires you? Is it because they’re solving a problem? Is it because there’s a specific call to action that they make? Is there something there that helps trigger some thinking around what you can do as a result? So is there something you can follow this style or you follow the way that they discover a problem and have to solve it? So that’s potentially another way of finding some ideas for inspiration.William Lyon (00:22:22):So let’s talk a bit about the process of submitting a proposal. Most conferences have what’s called a CFP or call for proposals or call for papers. And this is basically, the conference putting out this open call for submissions, for proposals, for conference talks and speakers. And every conference does it a little bit differently, but I think there’s a common structure and a common theme to the CFP.William Lyon (00:22:54):So let’s maybe spend a few minutes talking about, how do we write a good proposal? What are the basic components of the proposal? How do we make it compelling? Because I think this is the first step really of going down that path of giving a conference presentation of making sure that it’s compelling for the organizers of the conference to say, Hey, this looks really good. Let’s accept this and put it on the agenda.”William Lyon (00:23:23):Because I know conferences get a lot of proposals, and a lot of times it can be difficult to sort through the proposals and identify the talks that are really going to stand out and that the organizers want to have in their conference. So I think it is important to make sure that your proposal really does stand out and makes it clear who you are, what you’re going to talk about, and that this can be a compelling talk for the audience.William Lyon (00:23:56):So those are the things that I think the organizers are thinking about as they’re going through the CFP. Let’s talk at maybe just a high level, what are the individual components of a proposal? And then maybe talk about how to structure each one of those to make it compelling and stand out.William Lyon (00:24:17):So the basic components of every CFP are a title for your conference talk, and then a description, an abstract. Typically the title is a one-liner, Overview of Cypher Query Language, something like that. A description is something that the conference will put on the website or in the schedule, usually a couple of sentences like, In this talk, we take a look at the Cypher Query Language and how to use it for writing basic queries.” Then the abstract is typically a bit more detailed and sometimes the abstract is not shared publicly. A lot of times, this is just for you to share with the conference organizers, here’s how I’m going to structure the talk, here are the main bullet points going into a lot more detail than what the description does.William Lyon (00:25:26):So that’s how I think about it. And again, this can be a bit different. But I write the description for the prospective attendee who’s trying to see, Is this a talk I’m interested in going to?” I read the abstract almost as an outline of what the talk will be. And then the fourth component of a proposal is your bio. So who are you? What experience do you have? What’s your job? Why are you qualified to give this talk? So those are the basic components. I think then it’s important also to think about how to make each of these compelling and how do you structure these? So do you have any thoughts on that Lju and how to make your, your CFP compelling?Lju Lazarevic (00:26:14):I think the easiest way to make it compelling is to be really clear in your mind who your audience is? Who do you want to come to your talk? Be very clear about what you want your audience to go away with. Do you want them to be enlightened? Do you want them to be going away thinking about something, something that’s triggered a thought function that they’ve not considered before? Do you have a call to action? So what exactly do you want your audience to go away with?Lju Lazarevic (00:26:55):And I think that goes a long way towards driving what your title is going to look like, because if the title doesn’t give a good view of what your audience is going to go away with, then you keep iterating until it does. Same idea with the description in the abstract. Does it give you a clear view when you look through that? What kind of audience you want and what you want them to go away with, I think just having those two questions and cycling through and checking that will get you a long way there.William Lyon (00:27:34):So let’s say that your proposal has been accepted. The organizers have said, Yes, this sounds like a great talk. We want you to come to speak at our conference either online or in person.” Now we need to think about actually building the talk, right? And hopefully, we have some time ahead of us to prepare, we don’t need to rush this. I’d say this is probably an important thing to note is the more time that you can take to start building out the talk, I think the less stressed you will feel and ultimately, the higher quality the talk can be.William Lyon (00:28:15):Do you have a couple of months, do you have a month, do you have a few weeks? Whenever you have gotten that acceptance from the conference organizer, I would say, start building your talk then, or you can even start before that if it’s something that you’re truly passionate about.William Lyon (00:28:35):But anyway, let’s talk a bit about that process of how do we build the talk? And I think building the talk is a good way to think of it because there are several important components to the talk, right? It’s not just the presentation itself, it’s the assets around that, right? Are you building a demo? Are you creating slides? What is the delivery mechanism that you’re going to have? Are you having visuals and diagrams? Is there a report that you’re sharing along with us? There’s a lot of things to think about.William Lyon (00:29:16):Just to kick off the discussion here of building the talk. In my view, I think that the narrative or the story is actually the most important fundamental part of the talk and everything in my mind is secondary from that. Of course, it needs to be technically accurate, it’s having a working demo, having nice diagrams and visuals. Those are all important, but fundamentally you’re telling a story here, and figuring out what that narrative is for me is my fundamental starting point.William Lyon (00:29:56):So what I like to do is I think there’s a very important creative exercise and creative connection between visual thinking and the tactileness of drawing something out, almost like mind mapping. So for me to figure out what the narrative is, I do a couple of different things. One is, I start with a few bullet points of what are the key takeaways? So Lju was saying, when you’re writing that proposal, think who are the audiences? What are the key takeaways for them going to be? And write that out. And that forms the fundamental sort of heart of the narrative.William Lyon (00:30:42):And then, I like to go to a whiteboard or a paper, and just diagram out the different components, the different pieces of the talk that I want to share and what sets the context. So for me, it’s a very creative process to find that core narrative, and that really drives a lot of my process. How about you Lju, where do you start with the building of the talk process?Lju Lazarevic (00:31:16):The story, there always has to be a story. And in my mind, there’s a number of reasons for this. I think a story helps your audience because there is a journey here, there is a lesson to be learned, and you think across history, stories are the best delivery mechanism for that. How do you deliver learning through a story? Another really big reason for it is it really helps you to think about how are you going to put together a presentation? So a story innately gives you structure.William Lyon (00:31:54):Maybe it might be helpful to share an example of what we mean by a story and narrative to put some context around this. So I’ll share an example from a talk that I worked on recently for the City JS Conference, and the title of the talk was a Graph Data Love story, and the narrative is, there are these two technologies that come from fundamentally different worlds. This was a talk about, GraphQL and Neo4j and how to use them together, and then the benefits you get from using them together.William Lyon (00:32:33):These two technologies, one is loved by front-end developers, GraphQL, and the other is a database that is serving a very different need, but when we take these two technologies together, they each have something to offer the other in the terms of applying a type-safe schema to the database and from Neo4j being optimized for the type of nested queries that you often have in GraphQL.William Lyon (00:33:06):So I’m telling the story of how these two technologies, what they are independently, but then when we use them together, here are the benefits that we get. And that’s maybe not a super complex story, but that’s the fundamental narrative that flows through the talk. So when we’re talking about a story and narrative, that’s the sort of thing that we mean.Lju Lazarevic (00:33:32):Yeah, absolutely. And again, I think certainly from a tech talk perspective, the story structures tend to be quite similar. So I’ll pick the example I used for my most presentation last year. And again, a similar process was I tell the story and my story was, I was exploring a data set. So this was the wine dataset for those of you who are familiar. And then we talk about the next point of the story, which is a pain, I came across the pain, my pain was how do I deal with dirty data?Lju Lazarevic (00:34:08):So I’ve poured on my journey, my pain is this dirty data. We then continued the story where we talk about the options we explored and how can we solve this. And then the story has a happy ending, which is we’ve successfully cleaned it using these methodologies, and if anybody else finds themselves in this similar predicaments, they can use the fables that the lessons from this fable to apply to their problem.Lju Lazarevic (00:34:35):So it doesn’t have to be a big intricate story, but there is a story there to drive it. And the nice thing about having a story, is it so much easier to deliver. You don’t have to memorize what you put on some slides. You can have image slides, you can have diagrams with no words on there, because you know that story, you can talk to your story, but it’s difficult to talk to a slide of facts, for example.Lju Lazarevic (00:35:04):So it helps engage your audience, it keeps it interesting, it makes the process of making the presentation interesting, and it makes it so much easier to deliver. It’s win, win, win everywhere. So I guess the next bit from here is how do you tighten up that story? We’ve talked about generating the ideas, we’ve talked about having a story to help drive that. How do you work through the approach of getting it a bit more polished up and making the story tight?William Lyon (00:35:39):I think a large part of that, of developing the story is this visual thinking exercise, right? So I go to the whiteboard or, lately I’ve been using a drawing app on my iPad for this, and almost just drawing a mind map diagram of the key points? So I have my takeaways as the center, right? So I have my three to four bullet points of the takeaway for the audience.William Lyon (00:36:12):And we’ll talk about the audience in a second, that’s another really, really important component. But, I’ve decided on here’s what I want the audience to get out of that, and that’s the core piece of this visual thinking diagram exercise. And then I end up drawing it… It’s almost like a graph. It’s like notes that are the parts of the narrative that I want to convey.William Lyon (00:36:40):So in my Neo4j and GraphQL, Graph Data Love Story talk, the key components of the narrative are, I want you to understand the benefits that you get from using these two technologies together. And then I have three or four examples. Then I expand on that a bit more and in this diagram, and see how those pieces fit together. I then start to think of how best to convey this point to the audience?William Lyon (00:37:12):Should I do this in a code example? Should I do this as a diagram? Should I do this as a demo? And I think it really depends on what it is, but I think you can clearly start to see where one of those formats is better than the other. Diagrams and visuals can be, I think, a really important piece of building the talk because that’s the visual aid for your audience.William Lyon (00:37:41):As Lju said, you know how to sort of tell this story verbally and having some visual cue to show how the pieces fit together, while you’re talking about this can be very helpful visual aid for your audience, and everyone learns in a different style, right? Some of us learn best from visuals, some of us learn better from hearing something explained to us, some of us are more experiential.William Lyon (00:38:10):So I think it’s more important to address those different aspects throughout your talk. So you want to have a compelling story and narrative, you want to have some visuals and diagrams that explain how the pieces fit together. And then I think it’s also useful to have code examples and demos where appropriate that are going to be more compelling for the experiential to take away from that where they need to see this, this more concrete example.William Lyon (00:38:41):And ideally, if there’s a GitHub repo or somewhere that those folks can then go, pull-down and play around with, to actually really cement those. So I think that’s an important thing just to remember, and again, this maybe goes back to the concept of empathizing with the audience a bit to really recognize that folks learn in lots of different ways and try to incorporate those different elements into your presentation. That will address the different ways that different people learn from those different styles.Lju Lazarevic (00:39:14):Absolutely. So let’s talk a little bit about the audience. So how do you target your presentation to your audience? What are you looking for? What approach do you use?William Lyon (00:39:28):Yeah, that’s a good one. We had a question on Twitter from Nathan Smith here. How do I judge the technical level of the audience of my topic, which I think is really important and a fundamental thing to think about upfront as you’re structuring your talk. What do I expect my audience to know? Who is my audience? Who am I speaking to?William Lyon (00:39:56):I have a default persona almost that I think of when I structure my talks and that persona is me immediately before I didn’t know the thing that I’m talking about. And so looking at my background, I’ve worked as a software developer for a few startups, full-stack sort of thing. So not too deep in any one area, but more experience with the breadth and how the pieces fit together.William Lyon (00:40:28):So that’s how I think of my audience. And then I refine it from there. But I think also a lot of conferences have more formal ways for attendees to self-select into the talks. So there’s often tracks at conferences where it’s clear, okay, this is the track for the high level architect, this is the track for the data scientists, this is the track for the front-end developer.William Lyon (00:41:09):And if you know ahead of time what these tracks are going to be, and oftentimes this is part of the CFP process where you’re specifically indicating what track or multiple tracks that you think your talk would fit in. That can be really helpful for you as a presenter to understand. Here’s the structure that the conference organizers are setting out. So I know what audience I’m speaking to based on who’s going to be interested in that track.William Lyon (00:41:39):And then a lot of conferences will also have a level. So this is a beginner talk, this is the intermediate, this is advanced, and use that to then structure your talk based on the audience that you expect would be interested in that. To give you an example, I gave a talk at GraphQL Summit last year, two years ago.The title was something like, GraphQL Resolve Info Deep Dive, okay, so that you’re talking in the title, you’re talking about a very specific piece of GraphQL implementation. You’re saying this is a deep dive. Okay. So this is not going to be a beginner’s talk for someone who’s completely new to GraphQL.William Lyon (00:42:23):So I think I would try… Try to allow your audience to self-select for what group they are. Okay, if we’re talking about the GraphQL resolve info object, well, I have to know what GraphQL is, I have to know that this is probably something to do with implementing GraphQL backends. So maybe I’m targeting backend developers who are at least intermediately familiar with GraphQL. Do anything you can to allow your audience to self-select, and then, that can help you to inform the technical level of your audience.Lju Lazarevic (00:43:00):Absolutely. So then there are two takeaways there. So number one, have a look at what the conference is asking for. So if the conference has made it explicit that it’s going to be beginner-level content, or if they’ve made explicit, it is advanced-level content, then you’ve been told what the audience level is. If you’ve not been given that specific instruction, then you pick your audience.Lju Lazarevic (00:43:27):So you decide which audience you want to target, and then you do the material around that. And please help your audience out as you’re talking about that, make it clear in the title or the description who exactly that talk is for. And then, the audience can self-select whether that is the right tool for them or not. So many options there. And we talked a little bit about the different ways you can use content for your talk.Lju Lazarevic (00:44:00):So we talked about the use of visuals, diagrams, people may put in videos, sounds demos, also helpful. Looking at the different ways of engaging with your audience, the one thing I’ll say very quickly about slides is try to be sparing with them. I have noticed you get a lot of conferences where you have a lot of slides. And I came from this old school view, that’s each slide should be a minimum of two minutes in length.Lju Lazarevic (00:44:38):So I think just a quick comment on that, less is more, and this is where diagrams are amazing. So if you’re looking at a big block of text and this is where it all comes into the whole idea of the more time you can spend to prepare the better. I’m just highlighting this because you do see at some conferences, you get slides and it’s just full of text and ask yourself, can that be represented as a diagram? Can you bring a theme together?Lju Lazarevic (00:45:06):Is that information that you want your audience to sit there and read? Or do you give them a picture and provide them with a link? So that’s one thing I will mention quickly there because you do see a lot and it’s an easy way to lose the attention of your audience if they suddenly are not listening to you anymore because they’re quickly reading a slide of text. So that’s one thing I quickly wanted to flag. Is there anything that you would suggest as well as sort of things to be aware of when putting together a presentation for an audience?William Lyon (00:45:39):I think that’s a really good observation about the slides. I think of slides as they’re providing the basic structure for the talk. So I like to have these section subheading slides. Make it clear how to structure the talk and then in between those diagrams that show how the pieces fit together.William Lyon (00:46:09):Your point of linking to source material to references is really useful. If there’s some blog post or page from the documentation that has the information that I’m talking about. Rather than duplicating that onto the slide, I will just put a link to that on the slide as well. And I think that that’s really important as well to think about, how should the slides be treated after the talk? Right.William Lyon (00:46:44):So another question we got from the chat here is should there be handouts? How pretty should do my slides need to be? If I think they’re not pretty enough, how can I make them better? So this I think is drawing on a couple of points here that are really important. One is yes, think of your slides as handouts that someone can refer to later.William Lyon (00:47:09):So I like to use Google Slides for all of my slides because it’s very, very easy to share. So I’ll make them public, I’ll put a short Bitly link at the beginning so that anyone can just go to that link at the beginning, and they have access to the slides with links and treating it as reference material for them to take away and go home with.William Lyon (00:47:36):But then also, of course, you’re using your slides during the talk and you want them to look nice, you want them to be functional. I probably, tend to more to the functional aspect than making sure that they’re pretty and visually compelling mostly because I don’t have a lot of expertise or skills in that area, but I know some folks that will use something like Fiverr. So they’ll do an initial pass of maybe a diagram or the slides, and then go on to Fiverr or Upwork to have someone with more visual skills, do like a more in-depth, high-quality version of the visuals, or even just touch up the slides in general. That can be one option that I’ve seen folks do.William Lyon (00:48:28):I think another important aspect when we think of how to structure our talk and then what components to include in it? Is the question of, to include a demo or not. So I think that there are lots of different viewpoints here that there are advantages and disadvantages. I think oftentimes in a technical presentation that having at least some code samples to put things in a more practical context is useful.William Lyon (00:49:02):Oftentimes what I find myself doing is building a demo application that is like a reference architecture, reference application for the talk. A while ago, I think this was at the Kafka summit talk, Dave Allen and I, who gave a talk there, built a fraud detection application using Neo4j and Kafka and graph data science and in a GRANDstack application to build the front-end dashboard.William Lyon (00:49:34):And so we built this thing and there are a lot of pieces to it and it worked and we used it mostly though as a reference, here’s the repo for all of the pieces, you can pull it down and see how it works. It’s not something that we walked through each component of it live during the talk, we just wanted to show here’s at a high level, how the pieces fit together. If you want more detail, go find the code on GitHub.William Lyon (00:50:06):So I think it’s a balance there, because you have limited time to present your demo. You want to make sure that you’re not wasting time trying to set up configuration if something breaks, fixing it live. So I think some combination of screenshots and minimal examples that you want to show, but then also making sure that the code is accessible for folks, later on, is how I like to approach demos.Lju Lazarevic (00:50:35):And I think another question we’ve got here resigned to both through some of the Twitter questions I’ve been coming in, is should I try to be funny?William Lyon (00:50:45):I have a strict rule that I follow for this one. I think it really depends on your personality. I think that if you are, having that personality that is really engaging and you’re comfortable being funny and telling jokes and connecting with your audience, that can be a great way for your audience to engage with you.William Lyon (00:51:13):I don’t think I have that personality. I think I typically have a dry sense of humor that can come across as unintended sarcasm and whatnot. So I typically don’t try to be funny throughout the talk. I think there’s nothing wrong with that though if that fits your personality, but I would say don’t try to force it. So the strict rule that I have is I limit myself to one joke per conference talk.William Lyon (00:51:44):And you may not realize this because maybe the joke is not obvious if you’ve seen some of my talks, but there’s at least one joke somewhere in all of them. And this is sometimes like a meme slide or something like that. Because I think that fundamentally your audience is interested in having a good time and connecting with you, and of course humor is a great way to connect with them. I don’t think that it makes you less authoritative of a speaker or anything like that. So yeah, if it fits your personality, sure go for it.Lju Lazarevic (00:52:22):Be yourself. If you’re want to be a joker and crack a joke, then be yourself. If that is not your personality, I think you walk in dangerous territory if you try and be an anti-pattern of yourself. So be yourself. it’s your story. Tou’re telling it with your true self.Lju Lazarevic (00:52:56):Let’s talk a little bit about what happens after the presentation. So you’ve put in a lot of work. A fair bit of work goes into a call for paper. Significantly more work goes into creating the content. And then a load of work afterward goes into practicing for the session and getting ready. So you’ve put this big investment of time into this presentation process. What do you do next?William Lyon (00:53:19):I think that fundamentally, think about how you can repurpose the content, right? If the talk was recorded and now a lot of conferences have moved online and the talks are on YouTube, be sure to share the recording with your network. Certainly, find the YouTube channel for the conference and share that with your network.William Lyon (00:53:46):But also think about how you can convert that content to other formats, could you turn it into a blog post? Could you turn it into a tweet thread? The sort of thing, because that you have made an investment in sort of building this, and I think certainly you want to be able to share it across different formats other than just the audience that will have seen your talk, right? So this I think is really important, not to treat it as just a one-off thing that you’ve built and thrown away, but really use it as a base to build on top of as well.Lju Lazarevic (00:54:26):I’m coming to the point you made earlier about some of the reasons why you might go through this process, why are you doing a presentation? If you’re using this as an opportunity to learn a new subject, then go share it with the world. You’ve made that investment, you’ve done the talk at the conference. And even in the unfortunate event if you don’t get accepted, you still invest a lot of time into this project.Lju Lazarevic (00:54:51):So are there meetup groups you can go to? Lots of meetup groups, depending on your technical area, would love to hear that story. So can you contact meetup groups, online, and in-person when we’re allowed to do those as well? You have got many options to share that. And the thing is as well, this is a journey. So maybe this is a project you’re working on, you’re going to have new installments.Lju Lazarevic (00:55:15):So that becomes now the next stage that you can then go off and do a presentation on. So you’ve got a lot of opportunities there to keep reusing and repurposing that content. So it’s not the case if you do it once and that’s it, there is a lot of things you can do with it afterwards as well.William Lyon (00:55:34):We should make sure that we answer any questions from the chat that we didn’t get to. I see one here from usernames second, asking how do we prepare for the talk for the technical part as well, and your recommendations? I mean, how long do we usually take to prepare?William Lyon (00:55:56):I think the second part of that, I think is really important, is how long do you spend preparing your talk? I think is a really interesting question. Lju do you have any guidelines there on the time you spend preparing for a talk?Lju Lazarevic (00:56:18):That’s a great question. The rule of thumb for me will be the less familiar I am with a subject, the more time I’m going to be spending preparing for it. So if it’s something that I’m really confident in, I know inside out, if it’s a 30-minute talk, I could easily be spending maybe a day or so, two days if it’s brand new slides for that. And then I could easily be spending maybe a few hours just going over the presentation over and over again, doing any iterations and changes.Lju Lazarevic (00:56:55):And that’s something that I know really, really well. If it’s something I’m less familiar with. So I mentioned earlier about if it’s a subject that I’m not familiar with, I will practice the content over and over again. If any curveball is thrown at me, that’s one less thing to worry about. But even if I’m familiar, a lot of time goes in so easily we’re talking, brand new slides, two and a half days at least, what about yourself?William Lyon (00:57:30):I like to spend a lot of time thinking. So getting back to the concept of the narrative and the story and developing that, I like to spend a lot of time just thinking about it and visualizing this, and working through different aspects. Almost I’m imagining what the talk sounds like and what the key points are. So I like to spend a lot of time in the background, I guess, just thinking about this and going through that process, even before I start to prepare anything, either on paper, or the slides, or the demo.William Lyon (00:58:10):So that process is sort of going on in the background, ideally for maybe a couple of weeks as I’m thinking about this in the background, but then when it comes time to actually prepare and build the materials, it really varies. It’s so hard to say, if it’s something where I’ve already built an application or something, and I want to pull out some of the interesting aspects from the project I worked on, that’s a lot less time because I have a lot of the source technical materials to build from there.William Lyon (00:58:43):In terms of how to build up confidence and not be overwhelmed with not having enough to talk about or not feeling completely confident there, I like to build lots of different demos and technical things that I often don’t end up having time to present during the presentation, but just having those there in the slides as a reference or a pointer and say, Okay, so we didn’t get to talk about this, but for more info here’s example application on GitHub, here’s a demo of this aspect.”William Lyon (00:59:22):So in terms of helping to prepare for the technical parts and building up the confidence that you have enough material to talk about, I over-index on having that technical content there and oftentimes end up not even using it. But I think it’s useful to have as a follow-up for your audience to dig into more after the fact. And then also that then it gives me a lot to repurpose for content, for blogs and tutorials and whatnot. So, yeah, that’s how I think about it.Lju Lazarevic (00:59:58):Absolutely. And I think before we leave it there, I’m going to raise one of the comments that have been said by Zephis, which is a friendly reminder to look at the cameras. So we have been a little bit naughty during this session where we’ve probably been looking at our notes, but absolutely when you are delivering a presentation, either online, make sure you’re looking at the camera as much as possible and in person try and connect with your audience. So keep scanning and looking eyes. So that is absolutely a very good point. Well made and taken on board.William Lyon (01:00:33):Yeah. And I think that’s important to remember as well. No one is really an expert on giving these kinds of presentations. So we’re just trying to share what we think has been helpful. Hopefully talking through this was helpful, but take everything that we say with a grain of salt, right? Because these are things from our experience that work, but not necessarily will work for you and not pretending to be experts in this domain.Lju Lazarevic (01:01:04):Absolutely. With that, we’re going to call it a wrap. Thank you everybody for joining us. You can find the following episodes of our podcast on GraphStuff.FM.Be sure to subscribe to GraphStuff.FM in your favorite podcast app to be notified of new episodes. And please submit a review to Apple Podcasts as it helps others to find our episodes.Be sure to register for NODES 2021 and if you’re interested in submitting a talk be sure to submit before the deadline!;Mar 30, 2021;[]
https://medium.com/neo4j/getting-certificates-for-neo4j-with-letsencrypt-a8d05c415bbd;David AllenFollowSep 17, 2018·8 min readGetting Certificates for Neo4j with LetsEncryptUsing Neo4j’s cloud VMs, a common question is how to set up valid SSL to protect data in transit. This article will cover how to do it with LetsEncrypt, a popular free certificate authority.The instructions below will work with most any public cloud-hosted instance of Neo4j, so let’s get started. Updated June 2020: includes instructions for Neo4j 4+. Updated February 2022: includes notes and warning on Neo4j debug.logtraces you might see.Come along and ride on a fantastic voyageWhy are we doing this?You need valid SSL certificates in order for the browser and various client applications to trust that your site is what it says that it is. If you’ve created a Neo4j instance in a public cloud and you’ve seen browser warnings about this site is untrusted” or add a special exception” — valid certificates solve this.The surefire way to know that you don’t have a valid certificateAdditionally, some common connection errors for Neo4j and cypher-shell derive from the lack of a valid certificate. So here we go.PrerequisitesYour machine must have a valid DNS address in order to have a valid SSL certificate. Certificates typically aren’t granted for bare IP addresses because it’s a lot harder to prove that you own/control a bare IP address.Important: If you’re doing this for a causal cluster, you must do it for each machine in the cluster, not just one.This tutorial doesn’t cover how to get a valid DNS address, as the process is different for each cloud, and each network setup. Before using these instructions, ensure that you have a DNS name for your machine, and verify that it resolves correctly to the IP of your machine before proceeding.When I do this, I register domains with Google Domains, (let’s say mydomain.com) and I use Google Cloud DNS to map my neo4j machines (say, machine1.cluster.mydomain.com) to the IP address of the VM on Google Cloud. But there are many different possible ways to accomplish this part that you can find online.Install LetsEncryptOn each machine, we’re going to install certbot, a tool which generates certificates. To do that, we’ll pull some commands from the certbot install guidelines.sudo apt-get updatesudo apt-get install software-properties-commonsudo add-apt-repository ppa:certbot/certbotsudo apt-get updatesudo apt-get install -y certbotWhat is Let’s Encrypt?Let’s Encrypt is a certificate authority that provides free certificates to help encourage people to use encryption across the Web. Encryption makes the web safer and more secure for everyone. If you need more background on certificate authorities, have a look here.Generate a certificateCertbot will generate the certificate for us. In order for it to work, and to verify that you have the domain you claim, we’ll need to ensure via the firewall that port 80 is open (just temporarily) so the tool can do its job. Port 80 is not normally open for Neo4j cloud instances, so make sure to set that up as a separate step.Here is how we generate the certificate - sudo certbot certonly. As shown in the following, we’re specifying option 1 (we want to create a temporary webserver on port 80 to verify ourselves) and providing the domain name we’re generating a cert for (node2.cluster.graph.center)$ sudo certbot certonlySaving debug log to /var/log/letsencrypt/letsencrypt.logHow would you like to authenticate with the ACME CA?- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -1: Spin up a temporary webserver (standalone)2: Place files in webroot directory (webroot)- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -Select the appropriate number [1-2] then [enter] (press c to cancel): 1Plugins selected: Authenticator standalone, Installer NoneStarting new HTTPS connection (1): acme-v02.api.letsencrypt.orgPlease enter in your domain name(s) (comma and/or space separated)  (Enter cto cancel): node2.cluster.graph.centerObtaining a new certificatePerforming the following challenges:http-01 challenge for node2.cluster.graph.centerWaiting for verification...Cleaning up challengesIMPORTANT NOTES: - Congratulations! Your certificate and chain have been saved at:   /etc/letsencrypt/live/node2.cluster.graph.center/fullchain.pem   Your key file has been saved at:   /etc/letsencrypt/live/node2.cluster.graph.center/privkey.pem   Your cert will expire on 2018-12-16. To obtain a new or tweaked   version of this certificate in the future, simply run certbot   again. To non-interactively renew *all* of your certificates, run    certbot renew  - If you like Certbot, please consider supporting our work by:   Donating to ISRG / Lets Encrypt:   https://letsencrypt.org/donate   Donating to EFF:                    https://eff.org/donate-le$The success message at the bottom says we now have certificates located in the /etc/letsencrypt/live/* subdirectories.Configuring Neo4jLetsEncrypt maintains these certificates in a directory called live”. These are actually symlinks to files in another directory. This layer of indirection allows the certbot program to update the certificates periodically as needed, so that you don’t have to change cert locations.So when we place these into the neo4j structure, we’re going to be careful to create symlinks and not copy the files. By doing so, we can take advantage of the refresh abilities of Let’s Encrypt we might want later on.The first thing we have to do is adjust the very tight permissions on the certificates directory, by changing group ownership to neo4jand to make them readable by Neo4j.# Change group of all letsencrypt files to neo4jsudo chgrp -R neo4j /etc/letsencrypt/* # Make sure all directories and files are group readable.sudo chmod -R g+rx /etc/letsencrypt/* Next, we set up symlinks and the directory structure neo4j expects.WARNING — Follow the right instructions for your Neo4j version! Everything up until this point will work with both Neo4j 3.5 and Neo4j 4+ At this point, make sure to follow *only one set* of the following two blocks for your Neo4j version, because in version 4+, directory structure and config changed!Neo4j 4+ InstructionsThe 4.0 series has some key differences in how to configure SSL from 3.5. There is a directory of certificates per connector” (bolt, HTTPS, cluster), and the config options have changed a bit.cd /var/lib/neo4j/certificates# Move old default stuff into a backup directory.sudo mkdir bakfor certsource in bolt cluster https  do   sudo mv $certsource bak/donesudo mkdir boltsudo mkdir clustersudo mkdir httpsexport MY_DOMAIN=graph.somehost.comfor certsource in bolt cluster https  do   sudo ln -s /etc/letsencrypt/live/$MY_DOMAIN/fullchain.pem $certsource/neo4j.cert   sudo ln -s /etc/letsencrypt/live/$MY_DOMAIN/privkey.pem $certsource/neo4j.key   sudo mkdir $certsource/trusted   sudo ln -s /etc/letsencrypt/live/$MY_DOMAIN/fullchain.pem $certsource/trusted/neo4j.cert done# Finally make sure everything is readable to the databasesudo chgrp -R neo4j *sudo chmod -R g+rx *Finally, we make adjustments to our neo4j configuration file. (Keeping in mind that if you’re using cloud VM images, this is /etc/neo4j/neo4j.template, and in other environments it’s /etc/neo4j/neo4j.conf)dbms.default_listen_address=0.0.0.0dbms.default_advertised_address=your.hostname.com# BOLT Connectordbms.connector.bolt.tls_level=REQUIREDdbms.ssl.policy.bolt.enabled=truedbms.ssl.policy.bolt.private_key=/var/lib/neo4j/certificates/bolt/neo4j.keydbms.ssl.policy.bolt.public_certificate=/var/lib/neo4j/certificates/bolt/neo4j.certdbms.ssl.policy.bolt.client_auth=NONE# HTTPS connectordbms.connector.https.enabled=truedbms.ssl.policy.https.enabled=truedbms.ssl.policy.https.client_auth=NONEdbms.ssl.policy.https.private_key=/var/lib/neo4j/certificates/https/neo4j.keydbms.ssl.policy.https.public_certificate=/var/lib/neo4j/certificates/https/neo4j.cert# Directoriesdbms.ssl.policy.bolt.base_directory=/var/lib/neo4j/certificates/boltdbms.ssl.policy.https.base_directory=/var/lib/neo4j/certificates/httpsNeo4j 3.5 Instructionscd /var/lib/neo4j/certificatessudo mkdir revoked trusted bak# Move old generated certificates into a backup directorysudo mv neo4j.* bakexport MY_DOMAIN=graph.somehost.com# Configure cert neo4j will usesudo ln -s /etc/letsencrypt/live/$MY_DOMAIN/fullchain.pem neo4j.cert# Configure private key neo4j will usesudo ln -s /etc/letsencrypt/live/$MY_DOMAIN/privkey.pem neo4j.key# Indicate that this cert is trusted for neo4jsudo ln -s /etc/letsencrypt/live/$MY_DOMAIN/fullchain.pem trusted/neo4j.certFinally, we make adjustments to our neo4j configuration file. (Keeping in mind that if you’re using cloud VM images, this is /etc/neo4j/neo4j.template, and in other environments it’s /etc/neo4j/neo4j.conf)dbms.connectors.default_listen_address=0.0.0.0dbms.connectors.default_advertised_address=your.hostname.combolt.ssl_policy=defaultdbms.ssl.policy.default.base_directory=/var/lib/neo4j/certificatesdbms.ssl.policy.default.allow_key_generation=falsedbms.ssl.policy.default.private_key=/var/lib/neo4j/certificates/neo4j.keydbms.ssl.policy.default.public_certificate=/var/lib/neo4j/certificates/neo4j.certdbms.ssl.policy.default.revoked_dir=/var/lib/neo4j/certificates/revokeddbms.ssl.policy.default.client_auth=NONEWhat This Config DoesLet’s work through explanations of what these do and why you need them. You can find a complete reference in the Neo4j SSL Framework documentation. (Make sure to select the right documentation for your version!)The default advertised address for the node is set to the public DNS you configured. This is important so that clients can connect to that DNS name, and also so that when the Neo4j Browser application does redirects, it does so to a trusted DNS entry with a signed cert. The Neo4j Browser application tends to issue redirects to the default_advertised_address. As a result browsers will get confused if this is misconfigured, because it will look like an SSL site trying to redirect to some other, untrusted site (for example, a bare IP address).We configure the bolt transport and/or bolt connector” (terminology here differs between Neo4j 3.5 and 4.0) to use the default” policy for SSL and certain settings.The directory where everything is storedThe private keyThe public certificateFinally, client_auth=NONE means that the client does not have to similarly auth with a valid cert (i.e. it is OK if the client doesn’t have a cert)Note: client_auth=NONE is important if you want to connect using clients like Chrome and Neo4j browser, because Chrome on most people’s machines does not have a valid cert, and hence cannot pass the client_auth check that Neo4j is capable of enforcing.Restart neo4jAfter all of this configuration, we need to restart neo4jsudo systemctl restart neo4jWhat if I have a Cluster?If you’re using a Neo4j Enterprise causal cluster the instructions are exactly the same, except you have to do this once per machine in the cluster, and you need to verify that each machine has a *different address or hostname* that it advertises, so that routing client queries will work correctly.The extra nit for clusters, is that you’ll notice in the Neo4j 4.0 configuration section, you will also need to set up the connector for cluster” (rather than just bolt and https) to ensure that intra-cluster communication is secured using the same certificate. That’s all.I’m Still Getting an ErrorA very important thing about SSL certificates is that they prove a server’s identity by a domain name. In the above guide, suppose you request a certificate for mygraph.mycompany.com. That certificate proves that machine is secure when addressed by that name.But sometimes, users might SSH into that box and then do:cypher-shell -a neo4j+s://localhost:7687Take note of the hostname! We’re asking for a secure connection to localhost. This is going to fail validation, because when you connect, you’ll get proof that the server is mygraph.mycompany.com, and cypher-shell will fail the connect because it isn’t localhost.Further, you might see something like this in Neo4j’s debug.log022-02-15 13:22:27.498+0000 ERROR [o.n.b.t.TransportSelectionHandler] Fatal error occurred when initialising pipeline: [id: 0x00ceb907, L:/127.0.0.1:7687 ! R:/127.0.0.1:37498]io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Received fatal alert: certificate_unknown(...)Caused by: javax.net.ssl.SSLHandshakeException: Received fatal alert: certificate_unknownThis is the error on the server side showing that certificate validation failed. It’s easy to fix address the server by its verified name, not by a bare IP address or different hostname, this permits the validation to succeed.Renewing LetsEncrypt certificates on a scheduleBy running certbot with a renew flag, you can update certificates to push out the expiry date. Doing this for example, monthly via a cron job keeps the certificate fresh.You can check that this will work by running:sudo certbot renew --dry-runAnd check that the output has no errors.The nice part about our symlink structure above is that our links are always pointing to the certs in the live” directory, which themselves are symlinks to whatever the latest certbot has created. For that reason, we’ve automated keeping our certificates fresh with certbot!Remember though that for the challenge/response, we do need to keep port 80 open so that the periodic renewal can function.Final Thoughts and Optional ConfigurationIf you’re using certificates and SSL, you should strongly consider disabling HTTP access on port 7474 to your Neo4j instance. Why offer unencrypted traffic when you’ve configured nice secure encrypted traffic?Many users may wish to configure Neo4j’s ports to use port 443 for HTTPS rather than the usual port 7473. This just makes it more convenient for users to hit your instance by going to https://mycool.host.name/Happy Graph Hacking!;Sep 17, 2018;[]
https://medium.com/neo4j/discover-auradb-free-week-32-marvel-api-data-a0d5b371dc8c;Michael HungerFollowAug 22, 2022·11 min readDiscover AuraDB Free: Week 32 — Marvel API DataOver the weekend I was watching the first few (very short) episodes of the new Groot series with my daughter and thought we could revisit the Marvel Data API in our livestream.I am Groot SeriesIf you rather watch the stream recording than reading the article, here you go:If you want to see our past episodes, check out the recordings.If you are interested to provide feedback on the new Workspace experience in Neo4j Aura, that we also presented at GraphConnect, sign up here: https://neo4j.com/workspaceWe already had an awesome presentation of the Marvel Data Model and all it’s complex challenges that required to treat it as a graph by Peter Olson, VP Web & App Dev at Marvel, presented at GraphConnect in 2013.GraphConnect Videos 2013 - Graphing the Marvel Universe- Peter Olson @ GraphConnect NY 2013While many organizations use the graph to solve important real-world problems, Marvel Entertainment faces a different…vimeopro.comMy colleague Jennifer Reif had already explored the Marvel API in a 10 part series on our developer blog building a full stack app with Spring Data Neo4j.Neo4j Developer BlogDeveloper Content around Graph Databases, Neo4j, Cypher, Data Science, Graph Analytics, GraphQL and more.medium.comAs part of that series she also published a repository with import code for the Marvel API, which we are going to reuse today.graph-demo-datasets/marvel-api.cypher at main · JMHReif/graph-demo-datasetsThis file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below…github.comWe first have to create an account and get API keys from: https://developer.marvel.comMarvel Developer AccountThere is a public API key and also private API key that you have to hash with a current timestamp to make API requests. There is also a limit of 3000 API calls per day which is not a lot, given that there are more than 40,000 comics available in the Marvel API.The Marvel API is a REST API with endpoints forCharactersComicsCreatorsSeriesEventsStoriesIt’s also not really fast and you need to throttle your requests to not be blocked.But fortunately apoc.load.json can help us with accessing the API and retrieving results, and apoc.periodic.iterate can handle batching of our operations so that we don’t have to do it all manually.Both functions are also available in Aura, according to the AuraDB APOC docs.So let’s get started.Create a Neo4j AuraDB Free InstanceGo to https://dev.neo4j.com/neo4j-aura to register or log into the service (you might need to verify your email address).After clicking Create Database you can create a new Neo4j AuraDB Free instance. Select a Region close to you and give it a name, e.g. Marvel.Choose the blank database” option as we want to import our data ourselves.On the Credentials popup, make sure to save the password somewhere safe, best is to download the credentials file, which you can also use for your app development.The default username is always neo4j.Then wait 2–3minutes for your instance to be created.Afterwards you can connect via the Query Button with Neo4j Browser (you’ll need the password), or Import to Data Importer and Explore with Neo4j Bloom.On the database tile you can also find the connection URL: neo4j+s://xxx.databases.neo4j.io (it is also contained in your credentials env file).If you want to see examples for programmatically connecting to the database go to the Connect” tab of your instance and pick the language of your choice.The Data ModelMarvel Data ModelThe data model centers around the ComicIssue node, it is connected to Creator, Character, Story, Event and Series.Loading the DataInitially we can create the required constraints and indexes to allow fast lookup of nodes, to see if they already exist or to connect them.CREATE CONSTRAINT ON (char:Character) ASSERT char.id IS UNIQUECREATE CONSTRAINT ON (cre:Creator) ASSERT cre.id IS UNIQUECREATE CONSTRAINT ON (issue:ComicIssue) ASSERT issue.id IS UNIQUECREATE CONSTRAINT ON (series:Series) ASSERT series.id IS UNIQUECREATE CONSTRAINT ON (story:Story) ASSERT story.id IS UNIQUECREATE CONSTRAINT ON (event:Event) ASSERT event.id IS UNIQUECREATE INDEX ON :Character(name)CREATE INDEX ON :Creator(name)CREATE INDEX ON :ComicIssue(name)CREATE INDEX ON :Series(name)CREATE INDEX ON :Story(name)CREATE INDEX ON :Event(name)As Jennifer described in part 2 of her marvel data series, for importing the data from the API we’d use apoc.load.json.Create a Data Marvel: Develop a Full-Stack Application with Spring and Neo4j — Part 2Last week showed the first steps for a project with Neo4j, Spring, & Marvel data. This week, learn how to import the…medium.comWe set both API keys as parameters in Query (Neo4j Browser) with the following code:params  marvel_public :  <your public API key here> ,  marvel_private :  <your private API key here> One particularity of the API is that you also have to pass a timestamp and a hash which is computed from the public api key, private apic key and the timestamp with each API request, so that’s a suffix of URL parameters that we have to pass with each request.Fortunately apoc.date.format and apoc.util.md5 help us here too.WITH apoc.date.format(timestamp(),  ms , yyyyMMddHHmmss) AS tsWITH  &ts=  + ts +  &apikey=  + $marvel_public +  &hash=  + apoc.util.md5([ts,$marvel_private,$marvel_public]) as suffixWe will mostly use two API endpoints, the one for characters themselves and one for the comics of a character.Jennifer loaded the characters by first letter, i.e. A-Z fetching all characters for a letter at a time.But as we wanted to look at the MCU we can just use the character names we’re interested in.I found the page for teams and groups on the Marvel site which gave us The Avengers and Guardians of the Galaxy besides many others.Marvel Teams and GroupsSo we could just use their names (or prefixes) to fetch the character information from the API.// compute api key url parameter suffixWITH apoc.date.format(timestamp(),  ms , yyyyMMddHHmmss) AS tsWITH  &ts=  + ts +  &apikey=  + $marvel_public +  &hash=  + apoc.util.md5([ts,$marvel_private,$marvel_public]) as suffix// iterate over input, batch in 1 element at a time per txCALL apoc.periodic.iterate(// iterate over list of character namesUNWIND [ Thor , Groot , Rocket , Iron , Captain , Hulk , Hawkeye , Drax , Gamora , Star Lord , Mantis , Ant , Vision , Black , Wanda , Black , Nick , War ] AS prefixRETURN prefix,// load each character API-URL it its separate transactionCALL apoc.load.json( http://gateway.marvel.com/v1/public/characters?nameStartsWith= +prefix+ &orderBy=name&limit=100 +$suffix) YIELD value// iterate over result rowsUNWIND value.data.results as results// only for characters with comicsWITH results, results.comics.available AS comicsWHERE comics > 0// get-or-create character node, set attributes incl. resource-urlMERGE (char:Character {id: results.id})  ON CREATE SET char.name = results.name, char.description = results.description, char.thumbnail = results.thumbnail.path+ . +results.thumbnail.extension,      char.resourceURI = results.resourceURI,{batchSize: 1, iterateList:false, params:{suffix:suffix}})// yield results from batched updatesYIELD batches, total, timeTaken, committedOperations, failedOperations, failedBatches , retries, errorMessages , batch , operations, wasTerminatedRETURN batches, total, timeTaken, committedOperations, failedOperations, failedBatches , retries, errorMessages , batch , operations, wasTerminatedThis gives us our Characters in the database:Now we can fetch additional data for each Character through their Comics.We use the resourceUI from the Character and append the /comics?format=comic&formatType=comic&limit=100 suffix to fetch the comics details.For each comic there is also information on their creators, related series, events, and stories.We use the returned information to create nodes for each of them and connect them to Character and ComicIssue, see the Cypher statement below.It looks complicated but is fetching the data for each character and then iterate over all comics and creating (get-or-create) the nodes and their relationships in the same pattern.// suffixWITH apoc.date.format(timestamp(),  ms , yyyyMMddHHmmss) AS tsWITH  &ts=  + ts +  &apikey=  + $marvel_public +  &hash=  + apoc.util.md5([ts,$marvel_private,$marvel_public]) as suffixCALL apoc.periodic.iterate(// for each character which isnt connected to a comic yetMATCH (c:Character) WHERE c.resourceURI IS NOT NULL AND NOT exists((c)<-[:INCLUDES]-()) // load all their comics CALL apoc.load.json(c.resourceURI+ /comics?format=comic&formatType=comic&limit=100 +$suffix) // iterate over the comics in the update statement YIELD value WITH c, value.data.results as results WHERE results IS NOT NULL UNWIND results as result RETURN result, id(c) as id,// short sleep for API safetyCALL apoc.util.sleep(200) // look up character again MATCH (c) WHERE id(c) = id // get-or-create comic MERGE (comic:ComicIssue {id: result.id})  ON CREATE SET comic.name = result.title, comic.issueNumber = result.issueNumber, comic.pageCount = result.pageCount, comic.resourceURI = result.resourceURI, comic.thumbnail = result.thumbnail.path+ . +result.thumbnail.extensionWITH c, comic, result// connect comic to characterMERGE (comic)-[r:INCLUDES]->(c)WITH c, comic, result WHERE result.series IS NOT NULL// for all series of the comicUNWIND result.series as comicSeries// get-or-create seriesMERGE (series:Series {id: toInteger(split(comicSeries.resourceURI, / )[-1])})  ON CREATE SET series.name = comicSeries.name, series.resourceURI = comicSeries.resourceURIWITH c, comic, series, result// connect series to comicMERGE (comic)-[r2:BELONGS_TO]->(series)WITH c, comic, result, result.creators.items as items WHERE items IS NOT NULL// for all creators of the comicUNWIND items as item// get-or-create creatorMERGE (creator:Creator {id: toInteger(split(item.resourceURI, / )[-1])})  ON CREATE SET creator.name = item.name, creator.resourceURI = item.resourceURIWITH c, comic, result, creator// connect creator to comicMERGE (comic)-[r3:CREATED_BY]->(creator)// for all stories of the comicWITH c, comic, result, result.stories.items as items WHERE items IS NOT NULLUNWIND items as item// get-or-create storyMERGE (story:Story {id: toInteger(split(item.resourceURI, / )[-1])})  ON CREATE SET story.name = item.name, story.resourceURI = item.resourceURI, story.type = item.typeWITH c, comic, result, story// connect story to comicMERGE (comic)-[r4:MADE_OF]->(story)// for all the events of the comicWITH c, comic, result, result.events.items AS items WHERE items IS NOT NULLUNWIND items as item// get-or-create-eventMERGE (event:Event {id: toInteger(split(item.resourceURI, / )[-1])})  ON CREATE SET event.name = item.name, event.resourceURI = item.resourceURI// connect event to comicMERGE (comic)-[r5:PART_OF]->(event),// small batch{batchSize: 1, iterateList:false, retries:2, params:{suffix:suffix}})YIELD batches, total, timeTaken, committedOperations, failedOperations, failedBatches , retries, errorMessages , batch , operations, wasTerminatedRETURN batches, total, timeTaken, committedOperations, failedOperations, failedBatches , retries, errorMessages , batch , operations, wasTerminatedLoading additional Comic InformationWe can now visualize a few of their characters, comics and related information:MATCH (c:Character) WITH cORDER BY size((c)--()) DESC LIMIT 5CALL { WITH c    MATCH path=(c)<--(:ComicIssue)--()    RETURN path LIMIT 5}RETURN *Visualizing Comics and their related InformationExploring in Query (Neo4j Browser)In Neo4j Browser we can now query for instance through which series two characters are connected.MATCH (c1:Character)<--(:ComicIssue)-->(s:Series)<--(:ComicIssue)-->(c2:Character)WHERE id(c1) < id(c2)RETURN c1.name, c2.name, collect(distinct s.name) as seriesORDER BY size(series) DESC LIMIT 2Series Overlap between CharactersOr visualize a virtual network of character to character connections through these,MATCH (c1:Character)<--(:ComicIssue)-->(s:Series)<--(:ComicIssue)-->(c2:Character)WHERE id(c1) < id(c2)WITH c1, c2, count(distinct s) as freqWHERE freq > 2RETURN c1, c2, apoc.create.vRelationship(c1, SERIES, {freq:freq}, c2) as relwhich allows us to already visually see clusters.Characters by SeriesExploring with Explore (Bloom)We can open Explore (Bloom) from the AuraDB UI.And then run an search phrase like: Character name Iron Man <tab> Comic <tab> Story”We can also find multiple character nodes in Bloom and look at their connection through other elements of the graph.GraphQL APITo build a GraphQL API on top of our data we can just launch Neo4j GraphQL Toolbox against our AuraDB database.By clicking Generate type definitions it will inspect your database and generate already pretty good type definitions from your data. I just went over them and shortened the relationship-field names.type Character {	comics: [ComicIssue!]! @relationship(type:  INCLUDES , direction: IN)	description: String!	id: BigInt!	name: String!	resourceURI: String!	thumbnail: String!}type ComicIssue {	belongsTo: [Series!]! @relationship(type:  BELONGS_TO , direction: OUT)	createdBy: [Creator!]! @relationship(type:  CREATED_BY , direction: OUT)	id: BigInt!	includes: [Character!]! @relationship(type:  INCLUDES , direction: OUT)	madeOfStories: [Story!]! @relationship(type:  MADE_OF , direction: OUT)	name: String!	pageCount: BigInt!	partOfEvents: [Event!]! @relationship(type:  PART_OF , direction: OUT)	resourceURI: String!	thumbnail: String!}type Creator {	created: [ComicIssue!]! @relationship(type:  CREATED_BY , direction: IN)	id: BigInt!	name: String!	resourceURI: String!}type Event {	comics: [ComicIssue!]! @relationship(type:  PART_OF , direction: IN)	id: BigInt!	name: String!	resourceURI: String!}type Series {	comics: [ComicIssue!]! @relationship(type:  BELONGS_TO , direction: IN)	id: BigInt!	name: String!	resourceURI: String!}type Story {	comic: [ComicIssue!]! @relationship(type:  MADE_OF , direction: IN)	id: BigInt!	name: String!	resourceURI: String!	type: String!}Then with Build Schema the library generates an augmented GraphQL schema that adds filters, limiting, sorting and top-level queries and mutations and runs this as an API in your browser.Neo4j GraphQL ToolboxSo you can start running GraphQL queries against your database, like this one which fetches 3 comics and 2 creators each for each characters whose name starts with Captain.{  characters(where: { name_STARTS_WITH:  Captain  }) {    name    description    id    comics(options:{limit:3}) {      name      pageCount      createdBy(options:{limit:2}, where:{name_STARTS_WITH: C }) {        name      }    }  }}Which gives us this response:{   data : {     characters : [      {         name :  Captain America ,         description :  Vowing to serve his country any way he could, young Steve Rogers took the super soldier serum to become Americas one-man army. Fighting for the red, white and blue for over 60 years, Captain America is the living, breathing symbol of freedom and liberty. ,         id :  1009220 ,         comics : [          {             name :  Marvels Avengers: Captain America (2020) #1 ,             pageCount :  32 ,             createdBy : [{  name :  Chris Sotomayor  }]          },          {             name :  Despicable Deadpool (2017) #296 ,             pageCount :  32 ,             createdBy : []          },          {             name :  Avengers: Shards of Infinity (2018) #1 ,             pageCount :  32 ,             createdBy : []          }        ]      },...]}}To learn more on how to get started quickly with launching a GraphQL API with these type definitions, check out the Getting Started Neo4j GraphQL Docs.This was it for today, hope you enjoyed this exploration and you start exploring your own.Discovering AuraDB Free with Fun DatasetsWe run these  Discover AuraDB Free  live-streams alternating every Monday alternating at 9am UTC (11am CEST, 2:30 IST…neo4j.com;Aug 22, 2022;[]
https://medium.com/neo4j/create-a-typescript-apollo-server-and-live-database-with-unit-tests-4ab14ac46654;Yisroel YakovsonFollowAug 20, 2021·14 min readCreate a TypeScript Apollo Server and Live Database with Unit TestsA Quick Tutorial Using as an Example Neo4j and neo-forgeryMany of us have fallen in love with Apollo Server. It is a simplifying yet flexible abstraction layer. So many frameworks and middleware options usually combine without conflicts.A good example is using the neo4j-driver tool. You can generate a server that magically works with your database. And you can extend the magic with neo4j/graphql. That package automatically generates queries and mutations from your typeDefs, and provides powerful directives to generate resolvers that query the database.But when it comes to unit testing, the abstraction layer becomes a lot more imposing. How can you stub out the database calls when they are so effectively hidden in the server code?This tutorial will enable you to create an Apollo Server using unit tests and even TDD!If you do all of the steps, it will probably be about 30 minutes. Check out the full solution if you’d rather just look at the code or copy it file by file.This is a follow-up to a tutorial on mocking calls to a Neo4j database using neo-forgery. The technique shown there to mock a call works beautifully when you explicitly run a query using a declared session.How to Mock Neo4j Calls in NodeMy underlying premise is that anyone should be able to mock server calls of all types. So I built neo-forgery to mock…medium.comBut if you are using directives with an Apollo Server, you will need to mock the driver itself.This article guides you through creating a TypeScript Apollo Server with a live database. We use Neo4j as a database for a complete live stack. Then we show you how to run integration tests with an Apollo Server. Finally, we create unit tests from the integrations tests.This tutorial expects you to know the minimal basics of using Node.js, Apollo Server, and Neo4j. But I try to explain everything.StepsCreate an empty TypeScript project with AVA [3 minutes]Create an Apollo Server instance in TS with a live Neo4j database [2 minutes if you have a database. Maybe 10 minutes if you don’t]Create integration tests using executeOperation [5 minutes]Use neo-forgery to mock the database. [20 minutes]1. Create a New ProjectThe following four steps create a TypeScript project that is ready for testing using AVA.(1) Run these commands in a terminal to create a project with the AVA test runner using TypeScript:mkdir movies                  # make the code base directorycd movies                     # move to the directorynpm init -y                   # create a package.json filenpm init ava                  # add the AVA testrunnernpm i -D typescript ts-node   # add TypeScriptmkdir test                    # a folder for testsmkdir src                     # a folder for source code(2) Add this AVA specification to your package.json to use AVA with typescript, and to specify a test directory with files to test: ava : {   files : [     test/**/*.test.ts   ],   extensions : [     ts   ],   require : [     ts-node/register   ]},Or you can just copy over the solution package.json.(3) Add a tsconfig.json file with the following contents to enable certain things when we begin coding:{   compilerOptions : {     declaration : true,     importHelpers : true,     module :  commonjs ,     outDir :  lib ,     rootDirs : [ ./src ,  ./test ],     strict : true,     target :  es2017   },   include : [     src/**/*   ]}Notably, the rootDirs gives two root directories so that you can use linting also with your test files.2. Create an Apollo Server with a Live DatabaseIf you check out the Apollo Server Getting Started Tutorial, you’ll see simple steps for creating a books server. We’ll do almost the same thing, but we’ll use TypeScript. And we’ll recreate the data and resolvers (an enhanced version actually) with neo4j.(1) Add some packages to work with Apollo and neo4j.npm install apollo-server graphql  # server and graphqlnpm i neo4j-driver @neo4j/graphql  # neo4j capabilitynpm i dotenv                       # lets us store credentials in                                   # an env filenpm i -D neo-forgery               # the mocking tool(2) Create the following src/index.ts file:require(dotenv).config()const neo4j = require(neo4j-driver)import { Neo4jGraphQL } from @neo4j/graphqlimport {ApolloServer} from  apollo-server const { gql } = require(apollo-server)const typeDefs = gql`  type Book {    title: String    author: String  }`const schema = new Neo4jGraphQL({    typeDefs,}).schemaconst driver = neo4j.driver(    process.env.DB_URI,    neo4j.auth.basic(        process.env.DB_USER,        process.env.DB_PASSWORD,    ),)function context({ event, context }: { event: any, context: any }): any {    return ({        event,        context,        driver,    })}const server:ApolloServer = new ApolloServer(    {        schema,        context,    })// @ts-ignoreserver.listen().then(({ url }) => {    console.log(`🚀 Server ready at ${url}`)})Let me explain the contents. It is actually almost the same as the one shown in the popular Getting Started With Apollo Server Tutorial. But there are three critical differences:Because we will use TypeScript, we will import the type ApolloServer and use it in our declaration of server.The Getting Started tutorial adds the following books array and resolver rather than a database.const books = [    {        title: The Awakening,        author: Kate Chopin,    },    {         title :  City of Glass ,         author :  Paul Auster     }]const resolvers = {    Query: {        books: () => books,    }}We’re going to replace those with a live database. To do that, we have added: (a) a dotenv call to allow us to access database secrets (b) required packages and boilerplate to create schema (c) different parameters for the ApolloServer constructor, since we’ll be using schema rather than resolvers and typedefs.We remove the Query declaration from our typedefs. That will be generated automatically by the neo4j/graphql package, along with some resolvers for CRUD functions with the Book type.(3) Add a .env file at the root level which contains the credentials for some Neo4j database. If you don’t have one, you can create one for free in a few minutes with neo4j sandbox for the sake of this tutorial. If you create a sandbox, they will give you the credentials.// neo4jDB_URI=neo4j+s://<your db uri>DB_USER=neo4jDB_PASSWORD=<your password>(4) Launch your server locally in a terminal to create some data.$ ts-node src/index.ts🚀 Server ready at http://localhost:4000/Then click that link for http://localhost:4000. You should see something like this:Click Query your server and you should enter the ApolloGraphQL Sandbox. It should look something like this:We can now use this interface to run a first mutation that will populate our database.Click Root in the Documentation section and then you should see mutation as an option.Click mutation , and you should see the Fields open up to show a few mutations generated by @neo4j/graphql:We need to create the two books in the ApolloServer tutorial in our live database. You can read about it in the documentation by clicking on createBooks if you like. In the Operations panel, enter inmutation($createBooksInput: [BookCreateInput!]!) {  createBooks(input: $createBooksInput) {    books {      author      title    }  }}Then put the proper json into the Variables panel at the bottom:{     createBooksInput : [        {             title :  The Great Gatsby ,             author :  F. Scott Fitzgerald         },        {             title :  Beloved ,             author :  Toni Morrison         }    ]}Click the Run button and you should see the results of the query:You can now query for books as in the Getting Started tutorial, only now it’s with a live database.3. Create Integration TestsBefore we create unit tests for our server, let’s make integration tests. The essential difference is that the integration tests will make async calls to the live database, whereas the unit tests will mock those out.But even if you work with unit tests, you should have some integration tests anyway to make sure all is well with your database. The integration tests you can run periodically, and the unit tests you should run with every change to your code.Fortunately, an Apollo Server comes with a built-in function executeOperation that makes integration tests a breeze.(1) prepare the necessary files for integration testing.First, create some folders for testingmkdir test/intmkdir test/dataThen we need to add an AVA config file for integration tests int-tests.config.cjs:module.exports = {  files: [test/int/**/*.int.ts],   extensions : [     ts   ],   require : [     ts-node/register   ],   timeout :  2m }Finally, update scripts in the package.json file to include int-test: scripts : {   int-test :  ava --config int-tests.config.cjs ,   test :  ava },When we want to run an integration test, we’ll be able to do so by calling npm run int-test -- <fileName>. Or we can run all of them with simply npm run int-test. Note that integration tests should be in the directory test/int, and should contain the suffix int.ts.(2) I’m going to recommend refactoring src/index.ts to make it easier to create tests. Instead of creating server in the src/index.ts file, we’ll create a function createServer() that we import from a separate file. That way, we’ll be able to create test servers as needed, and ultimately mock servers.If you are in a hurry, you could just copy the solution test/src directory over which combines this refactoring with another one below. Or take a few minutes and work the changes through yourself so that you understand them.Create src/newServer.ts, which is essentially the same code lifted from src/index.ts and a new function:require(dotenv).config()const neo4j = require(neo4j-driver)import { Neo4jGraphQL } from @neo4j/graphqlimport {ApolloServer} from  apollo-server const { gql } = require(apollo-server)const typeDefs = gql`  type Book {    title: String    author: String  }`const schema = new Neo4jGraphQL({    typeDefs,}).schemaconst driver = neo4j.driver(    process.env.DB_URI,    neo4j.auth.basic(        process.env.DB_USER,        process.env.DB_PASSWORD,    ),)function context({ event, context }: { event: any, context: any }): any {    return ({        event,        context,        driver,    })}export function newServer():ApolloServer {    const server: ApolloServer = new ApolloServer(        {            schema,            context,        })    return server}Now we can remove virtually all of src/index.ts and simply call newServer there:import {newServer} from  ./newServer newServer().listen().then(({ url }) => {    console.log(`🚀 Server ready at ${url}`)})(3) Next let’s create our first int test 😍.The data from our first executed query in the Apollo Sandbox is all we need for a first integration test. A bit of copying and pasting will let you create the file test/data/createBooks.ts:export const CREATE_BOOKS_MUTATION = `mutation($createBooksInput: [BookCreateInput!]!) {  createBooks(input: $createBooksInput) {    books {      title,      author    }  }}`export const CREATE_BOOKS_PARAMS = {     createBooksInput : [        {             title :  The Great Gatsby ,             author :  F. Scott Fitzgerald         },        {             title :  Beloved ,             author :  Toni Morrison         }    ]}export const CREATE_BOOKS_OUTPUT = {     books : [        {             title :  The Great Gatsby ,             author :  F. Scott Fitzgerald         },        {             title :  Beloved ,             author :  Toni Morrison         }    ]}Then create the following integration file as test/int/index.int.ts :import {newServer} from  ../../src/newServer const test = require(ava)import {ApolloServer} from  apollo-server const server:ApolloServer = newServer()import {CREATE_BOOKS_OUTPUT, CREATE_BOOKS_PARAMS, CREATE_BOOKS_MUTATION} from  ../data/createBooks test(createBooks, async (t: any) => {    console.log(starting...)    let result: any    try {        result = await server.executeOperation({            query: CREATE_BOOKS_MUTATION,            variables: CREATE_BOOKS_PARAMS,        })    } catch (error) {        console.log(error)    }    t.true(!result.errors)    t.deepEqual(        // @ts-ignore        result.data.createBooks,        CREATE_BOOKS_OUTPUT    )})This code simply uses the AVA test runner to confirm that your server calls the createBooks resolver correctly. The executeOperation function is used to call createBooks. There are two assertions here. The first is that the result has no errors. The second is that the output is correct. Check out AVA to learn more about assertions and test generation.You can now run npm run int-test and you should see this:$ npm run int-test> tasks@1.0.0 int-test> ava --config int-tests.config.cjsstarting...  1 test passedAlthough we won’t do the other resolvers here, you can trivially repeat the above approach for your other two mutations and for the books query. You can copy over the test/data directory and int test file to get them all. In short:run each query with sample datacreate a file in test/data with the resultsadd to test/int/index.int.ts a test that makes sure the data returned is correct and there are no errors. IMPORTANT NOTE: in AVA you must use test.serial() in place of test() for any tests which must be performed in a particular order.Check out the completed integration test file.4 Use neo-forgery to mock the databaseYou could run your int test whenever you’d like, but it’s not appropriate for continuous integration (CI) or test-driven design (TDD). You’ll need real unit tests for those.Fortunately, it’s not hard with neo-forgery.(1) in your terminal, create a unit test directory and copy over the int test:mkdir test/unitcp test/int/index.int.ts test/unit/index.test.tsNOTE: it would honestly be more consistent to use the extension unit.ts rather thantest.ts. But test.ts is by far the most common convention. I do not want to use the test.ts extension with integration tests, because you could end up accidentally modifying a unit or int test when you intended the other.Try running npm test just to confirm that you get the same success as you got with the int test. But remember that it’s still calling the database.(2) refactor context out of newServerFor our unit tests, we’ll need a mock version of the server. To minimize duplicated code, we’ll refactorcontext to be a parameter for newServer, so that we can easily replace the live database with a mock one. That means that our src/index.ts file will now look like this:import {newServer} from  ./newServer import {context} from  ./context newServer(context).listen().then(({ url }) => {    console.log(`🚀 Server ready at ${url}`)})Here’s the src/context.ts file that it will use:const neo4j = require(neo4j-driver)const driver = neo4j.driver(    process.env.DB_URI,    neo4j.auth.basic(        process.env.DB_USER,        process.env.DB_PASSWORD,    ),)export function context({event, context}: { event: any, context: any }): any {    return ({        event,        context,        driver,    })}Those contents were simply pulled out of src/newServer, which becomes simpler:import {Neo4jGraphQL} from @neo4j/graphqlimport {ApolloServer} from  apollo-server require(dotenv).config()const { gql } = require(apollo-server)const typeDefs = gql`  type Book {    title: String    author: String  }`const schema = new Neo4jGraphQL({    typeDefs,}).schemaexport function newServer(context: any):ApolloServer {    const server: ApolloServer = new ApolloServer(        {            schema,            context,        })    return server}Just as a sanity check, you might run npm run int-test to confirm that you get:test/int/index.int.ts(6,29): error TS2554: Expected 1 arguments, but got 0.That is easily fixed by updating test/int/index.int.ts to use the context:import {context} from  ../../src/context const server:ApolloServer = newServer(context)Run it again, and you’ll see a reassuring green success message!(3) Now to create our mock context! 😃We will need a Query Set”, which is an array of the QuerySpec type. Check out the neo-forgery documentation for more. Let’s create a placeholder in a separate file test/unit/querySet.ts:import {QuerySpec} from neo-forgeryexport const querySet:QuerySpec[] = []Then we can create a file test/unit/mockContext.ts:const { querySet } = require(./querySet)const { mockDriver, mockSessionFromQuerySet } = require(neo-forgery)const session = mockSessionFromQuerySet(querySet)const driver = mockDriver(session)export function context({event, context}: { event: any, context: any }): any {    return ({        event,        context,        driver,    })}This file exports a context that contains a mock driver instead of the real thing. This mock driver will check only for the contents of querySet when a query is made, and if no matching query is found an error is returned. Since there are currently none, we can expect an error the first time we use it. But that’s okay because when you start using TDD you learn to thrive on failure! 😉(4) Run the test with the mock serverNow, this is cool… we can make a unit test by copying our int test with a tiny change:cp test/int/index.int.ts test/unit/index.test.tsNow just change the import of context to be our mock context:import {context} from  ./mockContext [If you are using the complete set of int tests, you should ideally replace any test.serial with a simple test, because for the unit tests, there will be no need to be executed in a particular sequence given that they have no side effects.]Then we can run the unit test:$ npm test> tasks@1.0.0 test> avathe query set provided does not contain the given query:query:-----------------CALL {CREATE (this0:Book)SET this0.title = $this0_titleSET this0.author = $this0_authorRETURN this0}CALL {CREATE (this1:Book)SET this1.title = $this1_titleSET this1.author = $this1_authorRETURN this1}RETURN this0 { .title, .author } AS this0, this1 { .title, .author } AS this1-----------------   params: { this0_title : The Great Gatsby , this0_author : F. Scott Fitzgerald , this1_title : Beloved , this1_author : Toni Morrison }createBookstest/unit/index.test.ts:2120:                               21:     t.true(!result.errors)   22:Value is not `true`:false› test/unit/index.test.ts:21:11─1 test failedYeah, we failed… but it’s a good kind of failure because it tells us what to fix! 👍 You see that @neo4j/graphql generated a query that we have to add to our set.As the neo-forgery documentation explains, a QuerySpec requires a query, any params, and a result. We can copy into our file test/unit/querySet.ts the query string and params.The result will require using the neo4j data browser. First, add the params by using the params: command with the outermost curly braces removed::params  this0_title :  The Great Gatsby , this0_author :  F. Scott Fitzgerald , this1_title :  Beloved , this1_author :  Toni Morrison The browser should confirm that they were set:Then run the query, and click on the CODE button on the left. Open Responses, and you can copy the resulting array:You can use the utility wrapCopiedResults from neo-forgery toconst createBooksOutput = {    records:        [            {                 keys : [                     this0 ,                     this1                 ],                 length : 2,                 _fields : [                    {                         title :  The Great Gatsby ,                         author :  F. Scott Fitzgerald                     },                    {                         title :  Beloved ,                         author :  Toni Morrison                     }                ],                 _fieldLookup : {                     this0 : 0,                     this1 : 1                }            }        ]}The new test/unit/querySet.ts looks like this:import {dataToStored, QuerySpec, storedToData} from neo-forgeryconst createBooksQuery = `CALL {CREATE (this0:Book)SET this0.title = $this0_titleSET this0.author = $this0_authorRETURN this0}CALL {CREATE (this1:Book)SET this1.title = $this1_titleSET this1.author = $this1_authorRETURN this1}RETURN this0 { .title, .author } AS this0, this1 { .title, .author } AS this1`const createBooksParams = {     this0_title :  The Great Gatsby ,     this0_author :  F. Scott Fitzgerald ,     this1_title :  Beloved ,     this1_author :  Toni Morrison }const createBooksOutput = {    records:        [            {                 keys : [                     this0 ,                     this1                 ],                 length : 2,                 _fields : [                    {                         title :  The Great Gatsby ,                         author :  F. Scott Fitzgerald                     },                    {                         title :  Beloved ,                         author :  Toni Morrison                     }                ],                 _fieldLookup : {                     this0 : 0,                     this1 : 1                }            }        ]}export const querySet: QuerySpec[] = [    {        name: createBooks,        query: createBooksQuery,        params: createBooksParams,        output: createBooksOutput,    }]Running the test again gives us a reassuring green message:It is trivial to follow those steps for the other errors as well.Final WordsIf you’ve done these steps, you know how to create an Apollo Server with a live Neo4j database and to create both integration and unit tests. That means that you can build an Apollo Server using TDD!You probably would benefit from the tutorial on mocking calls to a neo4j database if you want to see more detail about using neo-forgery.A good follow-up tutorial would be incorporating in auth if there’s enough interest in this one. So please clap if you would like to see more!;Aug 20, 2021;[]
https://medium.com/neo4j/scale-up-your-d3-graph-visualisation-part-2-2726a57301ec;Jan ZakFollowMar 19, 2020·7 min readScale up your D3 graph visualisation, part 2WebWorker layout and PIXI.js rendering performance improvementsIn the first part, I started with a usual D3-based graph visualisation and replaced SVG rendering code with PIXI.js, a 2D drawing library, which uses WebGL with automatic fallback to Canvas. In this part, I’m going to present to you a few further possible performance improvements. I have also expanded available configuration with multiple layouts and sample graph generators, so that layout performance and visual quality can be compared.The presented techniques are available in the live demo, feel free to tinker with the code.Live demoWebWorker layoutAlthough JS supports async programming thanks to the event loop pattern, this is not a true parallelism. JS is a single-threaded language.Any long-running function delays running other functions, potentially freezing UI responsiveness, because it affects user input event handlers as well. Graph layouts belong to this category, they are CPU-intensive algorithms with usual time complexity O(n log n) or worse.This situation improved with the introduction of WebWorkers, a native browser feature that enables us to run code in a parallel background thread. WebWorker code doesn’t have access to the main thread global scope. Communication between threads happens strictly via message passing with postMessage method, which either clones the message payload, or transfers message ownership to WebWorker thread, so that it can’t be used by the main thread anymore. Note that this avoids the need for thread-safe data structures known from other languages. However, in the case of larger messages, it is recommended to strip the message only to fundamental properties or use the ownership transfer method.WebWorker flow (source)If you use a build tool in your frontend development flow, such as Webpack or Rollup, take note that providing code to WebWorker thread is very different. This code can’t be part of the usual JS bundle, but it must be referenced as a standalone JS file. Therefore you’ll need to specify a separate main entry point in the build tool to get a separate bundle with dependencies.For simple use cases, there is a shortcut. It is possible to store the code in a string in an in-memory blob, where you can import dependencies from external files by importScripts function and run this blob as a virtual file.Combining all these tips, here is a minimal function running d3-force layout inside WebWorker. It can be called from the main thread as any other usual function returning a promise:Culling and levels of detailThere was a significant rendering performance issue in the previous demo. With many nodes, the visual experience during mouse interaction with the graph was sluggish, FPS was dropping to unacceptable low numbers. Measuring with Spector.js helped me a lot to find the bottlenecks. The good rule of thumb is to limit the number of GPU calls and the number of active textures in GPU memory. Reusing textures to draw similar objects is highly preferable. In the rest of this article, I’m going to focus on reducing these numbers.Culling is the process of removing objects that lie entirely outside of the viewport. It improves performance for zoomed-in views, where only a few graph elements are visible. I have implemented it with pixi-cull library. However, it doesn’t help with zoomed-out, full graph views, which are also desired in graph viz.For improving the performance of the full graph view, I have employed a technique of displaying different levels of detail by zoom level in 4 steps. When you start with a zoomed-out graph, only nodes as color circles are visible. When zooming in, first edges appear, then node icons, then node labels. This reduces the number of draw calls significantly for large graphs.Levels of detailSpritesWhen you start digging into how to optimize PIXI.js rendering, the most frequent suggestion is to use sprites wherever possible. Sprites are simple 2D objects with texture. The advantage is that PIXI.js batches drawing multiple of them into with a single WebGL shader.Therefore I have replaced all PIXI.Graphics instances by pre-rendering into PIXI.Texture, and later rendering it with PIXI.Sprite.Text renderingRemember that WebGL is primarily a low-level 2D API for GPU-accelerated drawing of triangles. It lacks high-level methods like fillText from Canvas, which allows simple drawing of text by any font available on the computer (including imported web fonts). Generic support for rendering text is a massive feature on its own. There is an excellent write-up Techniques for Rendering Text with WebGL in Three.js I highly recommend to read about all possible solutions and font preprocessing. PIXI.js provides some of them ready-to-use so that you don’t have to implement the solutions manually.PIXI.TextPreviously I used PIXI.Text. It is the most straightforward method, which doesn’t need any font preparation. You can render any font available, same as with rendering with Canvas. Internally it renders the full desired text string into Canvas and uses it as a texture for a sprite.However, there is a severe performance drawback. Each PIXI.Text instance obviously produces a new texture, leading to many draw calls if there are lots of texts on the scene.PIXI.BitmapTextI have switched to PIXI.BitmapText. It expects you to provide a font preprocessed into an image (also called texture atlas) with all supported characters + font descriptor file defining coordinates of each character in the image (example). The image is used as a shared texture, and any text is rendered by drawing separate characters next to each other from the texture with alpha test 0.5.There is one caveat of bitmap format to keep in mind. The bitmap image must be pre-rendered with the maximum resolution used for the text font size, potentially even larger if you allow your users to zoom in. Scaling the text beyond pre-rendered resolution produces pixelated text.Bitmap texture atlasDistance FieldsThe last and most flexible technique uses a font preprocessed into a distance field and stored in Signed Distance Field (SDF) or more recent Multi-channel Signed Distance Field (MSDF).In bitmap format, each pixel encodes only if the pixel is inside or outside the shape. In distance fields, each pixel encodes the distance from the shape edge. The significant benefit of distance fields is that you can scale from low resolution to higher resolutions for rendering with no pixelation.SDF format encodes the distance in shades of gray. If you open SDF image, don’t get confused that the image looks blurry, it’s only the effect of that 50% gray effectively means the shape edge, >50% means inside, <50% means outside.When you scale SDF text a lot, you start noticing chipped or rounded corners instead of sharp corners. To fix this, either you can increase the resolution of the pre-rendered SDF image, or switch to MSDF.SDF texture atlas and scaling (source)MSDF format encodes the distance in all three color channels, and for each pixel, all color channels participate in the decision that the pixel is inside or outside of the shape. If you open MSDF image, it looks really funky, but don’t be afraid. There are clever tricks behind it.MSDF texture atlas and scaling (source)There is pixi-sdf-text plugin for PIXI.js implementing text rendering from both SDF and MSDF formats. Unfortunately, it supports only PIXI.js v4. It hasn’t been upgraded to PIXI v5 yet. This is an opportunity to be contributed by the first engineer who strongly needs it :)Final wordsThere are many ways of how the performance of a custom graph visualisation can be improved. This is an excerpt of techniques I have added to my live demo of graph visualisation in PIXI.js, and I’m exploring further options.Of course, if your use case or engineering capacity doesn’t allow you to spend time with low-level graphical rendering, a great choice is ready-to-use commercial libraries (Keylines, Ogma, yFiles, in alphabetical order). They already solved all the complexities, expose high-level methods that can be used immediately, and provide quick support.Also, I have recently switched to continue my journey as an independent consultant. If you have an interesting problem to solve, don’t hesitate and drop me a line!;Mar 19, 2020;[]
https://medium.com/neo4j/diversify-your-stock-portfolio-with-graph-analytics-4520a5e46b3d;Tomaz BratanicFollowOct 13, 2021·6 min readDiversify Your Stock Portfolio with Graph AnalyticsLearn how you can use correlation between stock prices to infer a similarity network between stocks — and then use that network information to help you diversify your portfolioA couple of weeks ago, I stumbled upon the stock market volume analysis in Neo4j by Bryant Avey.Pattern-Driven Insights: Visualize Stock Volume Similarity with Neo4j and Power BIThis pattern-driven visualization technique was presented at the Power BI Bootcamp in July 2021. The Bootcamp session…medium.comIt got me interested in how we could use graph analytics to analyze stock markets. After a bit of research, I found this Spread of Risk Across Financial Markets research paper. The authors infer a network between stocks by examining the correlation between stocks and then search for peripheral stocks in the network to help diversifying stock portfolios. As a conclusion of the research paper, the authors argue that this technique could reduce risk by diversifying your investment, and — interestingly — increasing your profits.Disclaimer: This is not financial advice, and you should do your own research before investing.Photo by Daniel Lloyd Blunk-Fernández on UnsplashWe will be using a subset of Kaggle’s NASDAQ-100 Stock Price dataset. The dataset contains price and volume information of 102 securities fro the last decade.NASDAQ-100 Stock Price Data2010 to till date daily trends of NASDAQ-100 Stockswww.kaggle.comFor this post, I have prepared a subset CSV file that contains the stock price and volume information between May and September 2021.We will use the following graph model to store the stock information:Graph model schema. Image by the author.Each stock ticker will be represented as a separate node. We will store the price and volume information for each stock ticker as a linked list of stock trading days nodes. Using the linked list schema is a general graph model I use when modeling timeseries data in Neo4j.If you want to follow along with examples in this blog post, I suggest you open a blank project in Neo4j Sandbox.Neo4j SandboxStart learning Neo4j quickly with a personal, accessible online graph database. Get started with built-in guides and…neo4j.comNeo4j Sandbox provides free cloud instances of Neo4j database that come pre-installed with both the APOC and Graph Data Science plugins. You can copy the following Cypher statement in Neo4j Browser to import the stock information.:auto USING PERIODIC COMMITLOAD CSV WITH HEADERS FROM  https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/stocks/stock_prices.csv  as rowMERGE (s:Stock{name:row.Name})CREATE (s)-[:TRADING_DAY]->(:StockTradingDay{date: date(row.Date), close:toFloat(row.Close), volume: toFloat(row.Volume)})Next, we need to create a linked list between stock trading days nodes. We can easily create a linked list with the apoc.nodes.link procedure. We will also collect the closing prices by days of stocks and store them as a list property of the stock node.MATCH (s:Stock)-[:TRADING_DAY]->(day)WITH s, dayORDER BY day.date ASCWITH s, collect(day) as nodes, collect(day.close) as closesSET s.close_array = closesWITH nodesCALL apoc.nodes.link(nodes, NEXT_DAY)RETURN distinct done AS resultHere is a sample linked list visualization in Neo4j Browser:Linked list between trading days for a single stock. Image by the author.Inferring relationships based on the correlation coefficientWe will use the Pearson similarity as the correlation metric. The authors of the above-mentioned research paper use more sophisticated correlation metrics, but that is beyond the scope of this blog post.The input to the Pearson similarity algorithm will be the ordered list of closing prices we produced in the previous step. The algorithm will calculate the correlation coefficient and store the results as relationships between most correlating stocks. I have used the topKparameter value of 3, so each stock will be connected to the three most correlating stock tickers.MATCH (s:Stock)WITH {item:id(s), weights: s.close_array} AS stockDataWITH collect(stockData) AS inputCALL gds.alpha.similarity.pearson.write({  data: input,  topK: 3,  similarityCutoff: 0.2})YIELD nodes, similarityPairsRETURN nodes, similarityPairsAs mentioned, the algorithm produced new SIMILAR relationships between stock ticker nodes.A subgraph of the inferred similarity network between stock tickers. Image by the author.We can now run a community detection algorithm to identify various clusters of correlating stocks. I have decided to use the Louvain Modularity in this example. The community ids will be stored as node properties.CALL gds.louvain.write({  nodeProjection:Stock,  relationshipProjection:SIMILAR,  writeProperty:louvain})With such small graphs, I find the best way to examine community detection results is to simply produce a network visualization.Network visualization of stock similarity community structure. Image by the author.I won’t go into much detail explaining the community structure of the visualization, as we only looked at three months period for 100 stock tickers.Following the research paper idea, you would want to invest in stocks from different communities to diversify your risk and increase profits. You could pick the stocks from each community using a linear regression slope to indicate their performance.I found there is a simple linear regression model available as an apoc.math.regr procedure. Read more about it in the documentation. Unfortunately, the developers had different data model in mind for performing linear regression, so we first have to adjust the graph model to fit the procedure input. In the first step, we add a secondary label to the stock trading days nodes that indicate the stock ticker it represents.MATCH (s:Stock)-[:TRADING_DAY]->(day)CALL apoc.create.addLabels( day, [s.name]) YIELD nodeRETURN distinct doneNext, we need to calculate the x-axis index values. We will simply assign an index value of zero to each stock’s first trading day and increment the index value for each subsequent trading day.MATCH (s:Stock)-[:TRADING_DAY]->(day)WHERE NOT ()-[:NEXT_DAY]->(day)MATCH p=(day)-[:NEXT_DAY*0..]->(next_day)SET next_day.index = length(p)Now that our graph model fits the linear regression procedure in APOC, we can go ahead and calculate the slope value of the fitted line. In a more serious setting, we would probably want to scale the closing prices, but we will skip it for this demonstration. The slope value will be stored as a node property.MATCH (s:Stock)CALL apoc.math.regr(s.name, close, index) YIELD slopeSET s.slope = slopeAs a last step, we can recommend the top three performing stocks from each community.MATCH (s:Stock)WITH s.louvain AS community, s.slope AS slope, s.name AS tickerORDER BY slope DESCRETURN community, collect(ticker)[..3] as potential_investmentsResultsConclusionThis is not financial advice — do your own research before investing. Even so, in this blog post, I only looked at a 90-day window for NASDAQ-100 stocks, where the markets were doing well, so the results might not be that great in diversifying your risk.If you want to get more serious, you would probably want to collect a more extensive dataset and fine-tune the correlation coefficient calculation. Not only that, but a simple linear regression might not be the best indicator of stock performance.You can start with your graph analysis today and skip the environment configuration hassle by using the free Neo4j Sandbox instances. Let me know how your approach turned out!As always, the code is available on GitHub.;Oct 13, 2021;[]
https://medium.com/neo4j/share-your-graph-story-with-the-world-at-graphconnect-2020-16a06b0dfe39;William LyonFollowNov 7, 2019·5 min readShare Your Graph Story With The World At GraphConnect 2020!Call for talk proposals is now open — seeking developer stories about all things graph!Neo4j CEO Emil Eifrem gracing the screens of Times Square in New York City.In this post, we want to give you an overview of what to expect at GraphConnect 2020 and convince you to share your graph story with the world by submitting a talk proposal to speak at GraphConnect (the Call For Proposals closes November 15th, so be sure to submit by then!).What is GraphConnect?GraphConnect is the world’s largest conference for the global graph community and is coming back to Times Square in New York City in April 2020! You can expect a packed agenda: 1 day of hands-on training, 2 days of talks, the world-famous Graph Hack hackathon, a chance to connect with graph experts in the DevZone, and much more!Ryan Boyd demoing the all-new Neo4j Graph Platform at GraphConnect 2017 and introducing GRANDstack to the world.See The Latest Innovations In The Graph EcosystemGraphConnect is known as the place where cutting edge graph technology is first announced, demoed, and released to the world. I’m sure GraphConnect 2020 will be no exception.From live demos during the keynote to technical deep dives over two days of talks to user stories sharing how they are leveraging graphs to solve real-world problems, you won’t want to miss hearing about the latest evolution of graph technology at GraphConnect.Access the ExpertsAmy Holder, co-author of the Graph Algorithms” O’Reilly book signing copies of her book and answering questions.GraphConnect brings together graph experts from across the globe with two goals:To share their graph expertise with you, andTo solicit your feedback. Are the things they’re working on helping you to solve problems? If not, what kind of things can they build to help you solve those problems?Hands-On TrainingDave Shiposh and Nigel Small offering some guiding help during the hands-on exercises at GraphConnect training day.Choose 2 half-day hands-on trainings from over a dozen options at the GraphConnect training day. Whether you are just starting your graph journey and looking for an introduction to Cypher or a seasoned graph pro interested in sharpening your graph data science skills there is sure to be a training just for you.GraphConnect trainings are taught by experts with years of experience building and maintaining applications with Neo4j, not to mention many of the experts who built the tools they’ll be covering in the trainings.The Graph HackReshama Shaikh proposing a Graph Hack project for applying graph-based NLP to the Lessons Learned NASA datasetThe annual Graph Hack is the time to meet new graph friends and hack on fun projects you’d never have time to work on outside of a hackathon.In 2018 the theme was Neo4j Buzzword Bingo and hackers were tasked with using as many of the tools and integrations in the Neo4j ecosystem as possible together in their projects.What will the theme be in 2020? You’ll have to come to GraphConnect to find out!The team from GraphAware wowing Neo4j Engineer Nigel Small with their graph wares.ExhibitorsThe graph ecosystem is composed of companies and developers building amazing tools to analyze data with graphs, enable graph data visualization, and consultants with expertise building graph applications for some of the largest companies in the world.Many of these companies will be at GraphConnect as sponsors and exhibitors where you can see what they’re working on firsthand.Talks From The CommunitySenior Software Engineer Ashley Sun talking about how the LendingClub DevOps team uses Neo4j to manage microservices.Sure, in the keynote Emil and other Neo4j folks will talk about the latest and greatest new advancements in the world of graphs and Neo4j, complete with demos of awesome new features, like the introduction of the graph platform at GraphConnect 2017 but it’s the talks from the community that really make GraphConnect so special. And this is where you come in…And Here’s Where You Come InI’m pretty sure Jennifer is just showing cat gifs to everyone here 😹Now that you’ve seen what goes on at GraphConnect, it’s pretty clear that the real value of GraphConnect is the people. And that’s why we need you! Not just to come to GraphConnect as an attendee and participant, but we need you to share your graph story with the world on stage at GraphConnect 2020 in New York!How To Share Your Graph StoryStep 1. Submit a talk proposal via the online submission form. To submit a talk proposal we just need a title and description of your talk. You don’t need to have the full talk written, just tell us what you want to talk about!Step 2. Come to GraphConnect and share your story on stage! There are two basic formats: talks (45 minutes) and lightning talks (15 minutes).What Kind of Things Should You Submit?While there is no specific set of formats or topics (the only hard requirement is that the topic has something to do with graphs), here are some formats that have worked well in the past:Developer best practices: Lessons learned from the front lines of Neo4j production deployments. Share your knowledge, experience and code with other developers so we can all learn together.Business case studies: How did graph technology transform your business model or bottom line? How did you get sign-off from decision-makers? How has the change impacted your operations?Technical case studies: You had a problem, and you solved it. Show us how you did it! Walk us through your design thinking. Show us your stack. How does graph tech fit in, and why was it the best solution?We specifically want to support first-time speakers and members of groups that are underrepresented in the technology industry. Not interested in speaking yourself but know someone who has a great graph story? Share this post with them!So what are you waiting for? Submit your proposal to speak at GraphConnect 2020! Just be sure to do it before the deadline of November 15th :-);Nov 7, 2019;[]
https://medium.com/neo4j/whats-new-in-bloom-1-5-f425df37e32f;Ljubica LazarevicFollowJan 20, 2021·4 min readWhat’s New in Bloom 1.5?The latest version of Neo4j Bloom is out. Let’s a look at the new features and improvementsNicolas Reymond on UnsplashThe latest version of Neo4j Bloom, 1.5, came out this month. In this release, we see some exciting new features, as well as improvements to functionality and performance. Let’s check them out!New FeaturesThe latest new features are:A mini map for the scene.New style rule to uniquely color nodes in a category based on distinct property values.Choice of sampling data when generating or refreshing the perspective.Let’s have a look at the mini map and the new style rule a bit more detail.For this, I will be using the Northwind graph, available from Neo4j Browser. You can launch it by running the following — :play northwind-graph.Follow through all the commands in the browser guide to load the data if you are following along.Then open Bloom (in Desktop, Sandbox or Aura), connect to your database and create a new perspective. I have set productName and categoryName as the captions for my Product and Category nodes in the perspective.A Mini Map for the SceneSo there you are, you put in that all important search phrase, and lo and behold, thousands of result nodes come back. But you need to investigate all of those nodes! The mini map feature now allows you to see where exactly you are in the scene, allowing you to easily move around and explore all of the data in view. As well as seeing where you are in the scene, you can use the mini map to move your viewing position as well.With my Northwind dataset, I’m going to use the following search phrase:Category ProductWhich brings back the following:Bloom 1.5 with mini map bottom rightAs you can see, we now have a mini map (red circle) that allows us to navigate all of the data on the scene. I’m going to take the liberty of using the hierarchy mode to restructure the view (blue circle), so the result is rendered like this:Categories with products in hierarchy rendering modeAnd now we can easily see and inspect all of the data on the scene, thanks to the mini map.New Style Rule to Uniquely Color NodesNeed a way to differentiate your nodes based on a unique property? For example, you want to color your nodes based on different country values. Maybe you want to highlight different product categories. Or perhaps, you are using the Graph Data Science Library, and want to clearly identify your detected communities? This new style rule allows you to do just that.Continuing with our example, I’m going to use the new style on our Category node. Upon inspection, our Category type has a categoryId property, so this is what I’m going to use to uniquely color the categories:Color nodes by unique property valueAnd there we go — now we can easily visually differentiate between the categories.Improvements and FixesThere are also a number of improvements to functionality and performance in this release. Let’s explore them now.Those Little TouchesMore of the caption text on nodes is visible when zooming in.You can now escape special characters in search phrases.When creating a range-based style rule, the minimum and maximum property values will automatically filled, based on values from the scene. (So you need to have data visible already for this to work, it doesn’t query the database for min-max values)A fix for situations when there are similarly named properties , and they don’t appear in the Category Propertykey Value search pattern type.Speeding It UpNode and relationship property lists are now only fetched when generating or refreshing the perspective. This will significantly speed up Bloom start time as well as on load time for very large databases. Be aware that you will need to manually refresh the perspective when adding, removing, or updating properties on your graph.Clearing up AccessPerspectives are no longer automatically shared with the PUBLIC role when working with the Neo4j database version 4.1 or later.Have fun with the latest release and let us know how you get on!;Jan 20, 2021;[]
https://medium.com/neo4j/codebase-knowledge-graph-204f32b58813;Vlad BatushkovFollowJul 6, 2021·9 min readCodebase Knowledge Graph.NET code analysis using Neo4j DatabaseThis article is an introduction into a field of graph-based code analysis. My name is Vlad, I am a Tech Lead at Agoda, and have spent the last four years working on architectural changes of high-load systems, including a clean architecture redesign of Gateway API and modularization of the Agoda website.Today, we will discuss a base concept of graph-based code analysis and learn how to build a Codebase Knowledge Graph (or Code Knowledge Graph or simply CKG) for a .NET Core project using Strazh.Codebase Knowledge Graph: a graph concept agnostic to a programming language.Strazh: tool to build a Codebase Knowledge Graph from a C# codebase.This article uses examples of C# codebases. But if you are not familiar with C#, this article will still be useful for you. The concept of the Codebase Knowledge Graph is universal you can learn the concept and create your own implementation of a Graph Builder for your favorite language.How it worksIntroductionWhen talking about C# code analysis, we always talking about the Roslyn compiler. Roslyn rules are a well-known practice to analyze the code and improve code quality.Libraries like SonarAnalyzer.CSharp, Microsoft.VisualStudio.Threading.Analyzers, AsyncFixer, and Agoda’s in-house library Agoda.Analyzer are the perfect tools to catch code smells” in a codebase: don’t use this — use that avoid this — prefer that, and so on.From my personal experience, I can say that the majority of developers are interested in code analysis and want to have this knowledge in their skills toolbox.I decide to step forward in the code analysis and create an ability to analyze codebase in a different direction.This is the idea: Let’s collect information about the project and build a graph from it.Then, having a codebase as a graph, we can explore it, query the data, and analyze nodes and relationships in a graph as they are elements of a code. Code analysis not used by writing Roslyn rules, but by using a Cypher queries.Strazh literally means a guardian.” This is a reference to Zion guardians, also known as Sentinel or squiddy” from the original Matrix movie. (Other names in the Neo4j family — like Neo4j itself, Cypher, and APOC — are also taken from the Matrix universe.)ExampleThis is a simple class of LoggerProviderAdapter transformed into a graph structure:You can see the graph of next code elements:class LoggerProviderAdapter (dark green node)interface ILoggerAdapter (light green node)the file, where this class is declared (yellow node)methods (blue nodes)As you can see, Debug method called LogDebug method is not a method of LoggerProviderAdapter otherwise it would also have a relationship to this class.Why is it useful?Graph-based analysis does not aim to replace Roslyn rules. Rather, it is an alternative technique that enriches our toolbox with exciting possibilities, such as:Fast access to various codebase insights with rich visualization (Neo4j Browser, Bloom, and other Neo4j tools)Ability to catch design violations and mistakesAbility to find different coverages and various statistical informationHelp you to estimate changes, features, and refactoringGraph generationTheoryFirst, I want to explain how graph generation works, and then we will discuss the graph itself. This section is a quick tour of how the transformation from codebase to graph technically works.To generate a graph from C# codebase I’ve created a .NET Core console application Strazh, written also in C#. The application is also able to run as a Docker container. We will try Strazh in action at the end of this article.From the technical point of view Strazh is an ETL tool (Extract, Transform, Load). Building a Codebase Knowledge Graph is the next process. First, we need to Extract codebase models, then we Transform models into RDF triple, and finally, we Load RDF triple into a graph database as the graph’s nodes and relationships.Syntax and Semantic TreeCode of program is constructed from a combination of programming language syntax and semantics. Syntax is a grammar to define that combination of symbols is a valid code. Semantics gives a meaning to syntactic constructions — it is a layer of abstraction over acting code models.static string GetFullName(string firstName, string lastName)  => firstName + lastNameExample of Syntax Tree (LINQPad)Roslyn analyzerRoslyn is a powerful .NET compiler that provides APIs for code analysis. You can traverse in your codebase exploring Syntax and Semantic Trees to extract from it anything you want.RDF tripleRDF (Resource Description Framework), or a semantic triple, is a specific structure that used to represent facts. RDF triple is based on expression: subject — predicate — object. Codebase semantic models are extracted from code and transformed into RDF triples. Now that we have a collection of RDF triples we can build a graph from it.Back to LoggerProviderAdapter, here is a RDF triple example:Subject class LoggerProviderAdapter” (Node :Class)Predicate have” (Relationship :HAVE)Object method Debug” (Node :Method)Examples of Triples: Class declared in a File. Class have a Method. Method invoke some another Method.This is what a graph builder is doing: compile source code of project, extract known semantic models as RDF triples, and insert them into database to build a graph.So far so good, and now we are going to take a deep-dive into details of CKG and explore graph schema.Codebase Knowledge GraphGraph basicsKnowledge Graph is a graph-structured model that integrates free-form data into a network of entities, their semantic types, properties, and relationships. Free-form is a very important statement here. The more data we use to build a graph, the more useful insights we can mine from it.Codebase Knowledge Graph is a graph-structured dataset with free-form relationships between entities of codebase, semantic models of programming language, project structure, and other interlinked knowledges.Codebase Knowledge Graph layersAll layers intercept with each other to give you a powerful set of analytical information of your codebase. Lets observe graph schema and explore layers one by one, from top to the bottom.Codebase Knowledge Graph schemaProject DependenciesOn the top level we have Projects and Packages. This layer is good for an architectural overview and to help with design decisions. Using this layer, you can explore dependencies between Projects. Primitive example below.UnitTests project depends on Project (red nodes) with expanded Packages (orange nodes) used by UnitTestsDuring a migration phase of our modular redesign of the Agoda website, at some point we had around 70 Projects in a single solution.Breaking the MonolithModular redesign of Agoda.commedium.comCodebase Knowledge Graph helps us catch horrible dependency violations and fix them at an early stage.Project DependenciesYes, there are some existing tools to get the same diagram, but Neo4j gives us a long list of options to explore solution and codebase layers: Cypher, APOC, DGS, Neo4j Browser, Bloom, and many more native tools are fast and easy to use.Folder StructureGoing one level deeper we have a File System layer to structure Folders and Files. This layer is useful data to validate correctness of a project’s structure, when the project follows any standard pattern.Pattern-Oriented Software Architecture - WikipediaPattern-Oriented Software Architecture is a series of software engineering books describing software design patterns.en.wikipedia.orgExploration of files and folders is simple, as it returns the whole file system tree.Yellow nodes are Folders and grey nodes are Files. Red node is Project.TypesNext layer is a collection of main code units: classes and interfaces. Classes are an essential part of your codebase, and using this layer you can analyze OOP practices and design patterns.Design Patterns - WikipediaDesign Patterns: Elements of Reusable Object-Oriented Software (1994) is a software engineering book describing…en.wikipedia.orgFor example, let’s take a look at each method that is declared in an interface and at the same time implemented in a class.We can build a new Relationship [:IMPLEMENTED_AS] and use it for more efficient traversing across the codebase. This also enriches our knowledge graph with a higher level concept to have a better language to query and explore the software design space.Visually it is beautiful like this:MethodsMethods invoke other methods and instantiate types. This layer is a core of application business logic and builds a major part of the graph. Having this layer you can traverse in a program business logic and find relationships between application functionality.For instance, we can find out which other methods calls our method BuildKnowledgeGraph” transitively.Invocation chain of method in Strazh project started from method BuildKnowledgeGraph”Now let’s try to build a Codebase Knowledge Graph from your codebase! Your codebase — is your Knowledge Graph.Strazh in ActionPractical partStrazh is a console app and uses a standard command line interface for execution.Usage:  strazh [options]Options:  -c, --credentials (REQUIRED)required information in format `dbname:user:password` to connect to Neo4j Database  -t, --tieroptional flag as `project` or `code` or all (default `all`) selected tier to scan in a codebase  -d, --deleteoptional flag as `true` or `false` or no flag (default `true`) to delete data in graph before execution  -s, --solution (REQUIRED)optional absolute path to only one `.sln` file (cant be used together with -p / --projects)  -p, --projects (REQUIRED)optional list of absolute path to one or many `.csproj` files (cant be used together with -s / --solution)  --version                                     show version information  -?, -h, --help                                show help and usage informationYou can ask to analyze codebase based on single solution file or provide a list of any amount of projects — but not a mix of them. You also need to provide credentials for Neo4j Database and specify a tier to analyze. Strazh analyze and build a graph only for specified tiers.Tier Project” includes two graph layers:Project DependenciesFolder StructureTier Code” includes two graph layers:TypesMethodsThe Tier separation was created specifically to differentiate our need of Architectural and Code level results. When you only need the Project diagram, run Project” tier and save some time.DockerStrazh is available as docker image in a Docker Hub. Simply pull and run a container. I recommend you use a docker-compose file to speed up the process.Example 1: SolutionIn this docker-compose file example we will analyze a Solution file with only a Project” tier. This will result in a graph with only projects and their nuget packages and the dependencies between them.Several tips about the docker:You need a running instance of Neo4j Database and grant to Strazh a role with write permissions. Run services one after another.Easiest setup is to use a default database neo4j” with a default admin role neo4j.” As you can see, provided neo4j:neo4j:strazh” credentials match prepared NEO4J_AUTH: neo4j/strazh settings in Neo4j.Also it’s always good to have Neo4j with APOC and GDS plugins inside to power up your queries.Example 2: ProjectsAnother example shows how to build a Codebase Knowledge Graph with default All” tier for two explicitly provided projects. This example perfectly matches the classic pair: Project + Unit-Tests for it.The Neo4j service configuration is the same. If you set up everything correctly you should see in the output something like this:Strazh outputNow you can open a Neo4j Browser (https://localhost:7474) and start your first graph-based code analysis!I hope that Strazh will help you in your project someday. If so, please don’t forget to share your case in the comments.Thanks for reading.Looking for ContributorsI wrote a first line of code for a Strazh project in March 2020, and finally it’s an open-source project with the early beta release in June 2021.I am a novice in the open-source world, and a lot of things in the Strazh project need to be improved.This is why I need your help! If you’re interested in an exotic project like Strazh, please fork it and contribute.vladbatushkov/strazhYour codebase - is your Knowledge Graph. Strazh is a .NET Core console app to build a Codebase Knowledge Graph from a…github.comContributors are welcome!Personal Thank youPeter SzaboSushmi ShresthaSergey MeshcheryakovRoyee GoldbergKeattisak ChinburaratReferenceshttps://threat.tevora.com/visualizing-dotnet-class-relationships-neo4j/https://github.com/daveaglick/Buildalyzer;Jul 6, 2021;[]
https://medium.com/neo4j/you-need-to-see-these-graphhack-projects-b10aca92454d;Karin WolokFollowSep 30, 2019·4 min readYou NEED to see these GraphHack projects!!THIS is what motivates me the most to do what I do. These projects are incredible!Part of what’s awesome about this GraphHack, is its’ theme, Extending the Graph Ecosystem. Hackers were tasked to build something that benefits others in the community.What am I so moved by it? Because this is a representation of what a community truly is!Everybody wins, because everyone helps.A collaborative and supportive ecosystem.There’s more…A major part my team’s (Neo4j Developer Relations) focus is to develop apps, tools, and integrations, that can be of value to the community.We even have an internal program around it, Neo4j Labs, where we work with community members to build and create projects that others may find useful. These projects then have the opportunity to graduate and become formally-maintained and supported Neo4j projects.This hackathon… gave our community the torch!Not only do they drive the direction of what’s being built, but they actually built it! This, my friends, is true community initiative. ❤(PS, if you didn’t participate, that’s ok! Just make sure you let the project-creators know if you liked their project! VOTE for your favorite project in the GraphHack Gallery!!!!!!)GraphHack winners will go to GraphConnect 2020 (training included) to present their project in DevZone, provided compensation of hotel and airfare, get an exclusive invite to the Neo4j Ecosystem Summit, and will rub elbows with Neo4j executive staff.How these hackers give meaning to my job — I can’t even ‘graphsplain’ it. )Here’s a quick glance of the GraphHack submissions.NeoWithNicA tutorial for beginner programmer where you will learn, how to build a website backed with Neo4j using the GRANDstack.By: Nicole Marie TrappNeo4j Scraper ProceduresThese procedures provide the option to do web scraping from Cypher. Sometimes you need to scrape tables, lists, texts from the web, or simply get the text content of an URL. Now you can do it in Cypher.By: Janos Szendi-Vargaassociated-rulesA Neo4j database extension to build associated rules from transactions.By: Min Li, Haibo Fu, Liang Chen, Weihao(Luke) XiaFormat the explanatory dictionary with Neo4j databaseCollect relevant words/definitions in format of an explanatory dictionary and populate Neo4j database.By: sergiy tBaryonBaryon is a react component to visualize Neo4j graph as a media.By: satoshi mayumiMeetup MixerA community-expanding, knowledge-building app.By: Sarah Staab, Erin Schuberth, Nathan Smith, Steve PetrieNode Local Relationship IndexesSolution to the supernode problem with indexes on supernodes.By: František Hartman, Sergio De Lorenzis, Will EvansGraph Algorithm Visualization ToolOur idea is to visualize the process of how a community detection algorithm captures the graph structure and compare the result of different types of community detection algorithms with same input graph.By: Jingwei Cao, 欢 王, NMAGZVisualize Breast Cancer w/ GraphXRVisualize Breast Cancer using GraphXR and Node4j Desktop.By: Andrew KamalneomapA Neo4j Desktop application to visualize nodes with geographic attributes on a map.By: Estelle ScifoNeo4j Jmeter load testingWhen working with Neo4j, sometimes you need to create custom procedures to handle advanced use cases. This project aims at making it easier to do load testing on them with JMeter.By: Nicolas Mervaillie, Luanne MisquittaVisual Studio Code Cypher Query Language Tools for Neo4jVSCode extension for Cypher and Neo4j. Initial version is syntax highlighting and basic code completion. Much more functionality to follow.By: Anthony GatlinPyEmbeoGraph embeddings for Neo4j in python.By: Vedashree Patil, Nikhil Akki, Arun KuttyGraphsplainingGraphsplain is your all-in-one solution to optimizing your graph. By evaluating your current graph and queries, Graphsplain will ‘splain ways to help make your graph faster, stronger, and grap ier.By: Jacob McCrumb, Michael McKenzie, Mike FrenchFlatToCypherCreate graphs directly from flat files.By: Abhishek Pathak, Rashmi Ravi Chandur, Sandeep ChandurChart,neoCreate custom exportable charts right inside Neo4j Desktop.By: Piyush Agrawal, Shashwat Gulyani, Subham Banga, Shweta GulyaniIsomorphic Addition ProjectA script library for generating polynomials equivalent to integers using linear combinations of sum of series quadratics.By: Thomas PriesGo to the GraphHack Submission Gallery:Global GraphHackNeo4js Global GraphHackglobalgraphhack.devpost.com;Sep 30, 2019;[]
https://medium.com/neo4j/editing-data-in-neo4j-graphs-doesnt-have-to-be-hard-8e9791c731bc;Chris PFollowJun 10, 2020·5 min readEditing Data in Neo4j Graphs doesn’t have to be hardLet’s be honest: graph databases are remarkable. They can handle billions of nodes and relationships and really, if you’re still using a RDBMS — with those 70s style tables and foreign-keys — you need to reconsider!I stopped using any other type of database about 5 years ago and only use graphs for all of the software projects I design, develop and manage.One of these graph solutions, my personal favorite (and I believe the market leader with customers like ebay and NASA), is neo4j. My experience with it: it’s fast it’s reliable it has its strong points it has weaknesses. Overall though, I would recommend it to anyone, anytime.BUT…Neo4j comes with a nifty tool called Neo4j Browser that lets you enter queries in the (proprietary) query language Cypher” (which I really enjoy, BTW) and even shows results as nice visual graphs. However, it is a real pain the in proverbial butt to perform simple tasks like manipulating the actual data of nodes and relationships — a task we need to perform frequently in the projects we manage!Sure, graphs are great for analyzing, well, anything! Finding paths, defining communities, ranking relationships, and so on. And these kinds of use-cases are really all you ever hear about. Yes, graph DBs like neo4j can help with things like decoding the Panama Papers, figuring out how COVID-19 spreads across the globe or finding and visualizing trolls in social networks.But let’s just think about that last example for a second: if you build your social network on top of a graph database, and of course you should, then you will surely need to actually store some data — properties like username” or email” on nodes, or maybe a timestamp” on the LIKES” relationship between a person and another user’s picture of a cat. After all, the social network needs data to be of any interest, right?Let’s take a (very simple) look at a (very simple) task then. Let’s assume that we need to edit a single value on a single relationship like the star-rating of a review. We fire up the neo4j Browser” and look for the desired node:MATCH (p:Person)-[r:REVIEWED]->(m:Movie)WHERE p.username=’jessica’ AND id(m)=111RETURN p,r,mThe result is a simple (sub-)graph with the 2 selected nodes. We can click on the REVIEWED relationship to see its properties. Yes, it works. But as soon as there are more than just a few properties on a node/rel, or a large value like the full text of the review, this becomes very clumsy.Neo4j Browser is made to return query results and visualize graphs. It does that really well and even has some nice features to specify sizes, colors and preferred labels for the different types of nodes. BUT, it’s not made to show or even edit the actual data — the properties of nodes and relationships. So in order to change a value, this is what we have to type:MATCH (p:Person)-[r:REVIEWED]->(m:Movie)WHERE p.username=’jessica’ AND id(m)=111SET r.stars=3RETURN p,r,mAll of that just to set a single property?!We have to type a lot (and know exactly how) and we can easily make mistakes here: What if p wrote 3 different reviews of m? Then that query overwrites the star-rating of all of them!This is just a simple example, but believe me, I have messed up my fair share of data in (production-)graphs typing out huge Cypher queries just by making a tiny little mistake somewhere along the line! :facepalm:This is exactly the reason why I have stopped performing these kinds of manual queries and instead developed a nice GUI for neo4j nodes and relationships. The first version (back in 2016) was a WAR file you had to deploy in an app server like Tomcat. It was OK and helped me and my colleagues a lot. There was room for improvements, though, which is why I started from scratch, rewrote the entire thing (based on React this time), and put in every feature from every wish-list while I was at it!The result: Neo4j CommanderIt has become absolutely vital in our daily lives, filled with 2nd-level-support tasks, and I believe it can be an amazing tool for many other neo4j users/admins. It even comes with an UNDO history because, at the end of the day, we are all just human -)So let’s check out how we would perform that same operation illustrated above using Neo4j Commander:Find User with username ‘jessica’Open the relationship connecting the desired MovieAdd the stars” property of type Integer and set the value to 3SAVEMade a mistake? Hit the UNDO button.Need to edit the long review text? Use the full-screen editor.Need to calculate a value based on other values? Use the built-in calculator.Neo4j Commander is all about the actual data in the nodes and relationships in your Neo4j graph.Find out everything there is to know and drop us a note if you have any questions, feature-requests or bug reports on the neo4j Commander website: https://netbizltd.com/neo4jInstallationYou can install Neo4j Commander in Neo4j Desktop from the built-in Graph App GalleryOr paste the following URL into the Install form on the Graph Apps pane:https://registry.npmjs.org/neo4jcommanderIf you love what you see and want to unlock the full potential of Neo4j Commander, I — and of course the company that pays for my time developing this beast — appreciate your upgrade to the PRO version!;Jun 10, 2020;[]
https://medium.com/neo4j/now-available-womens-world-cup-2019-graph-cf3bd9e44e22;Mark NeedhamFollowJun 10, 2019·4 min readNow Available: Women’s World Cup 2019 GraphExplore the data behind the Women’s World Cup with our World Cup Graph.On Friday Women’s World Cup 2019 started, and over the weekend we’ve dusted off our World Cup scraping scripts and created a Women’s World Cup Graph.Image courtesy of https://www.behance.net/gallery/79721663/FIFA-Womens-world-cup-posterTLDRWe have a Neo4j Sandbox containing all data for the Women’s World Cup. You can launch that sandbox from neo4j.com/sandboxWomen’s World Cup SandboxGraph ModelWe’ve made some tweaks to the graph model that we used for Men’s World Cup 2018, so let’s have a look at our new and improved model.World Cup Graph ModelLet’s start with the Tournament node. We have one Tournament node for each World Cup tournament, so there are 8 of these nodes, one for each of the tournaments from 1991 to 2019. Teams participate in these tournaments, so we create that relationship between Team and Tournament nodes.Squads are NAMED by Teams FOR each of these tournaments, and a Person can either be in the squad or the coach for that squad.After exploring the data, I realised that people can only ever play for one team, so we have a REPRESENTS relationships between the Person and Team nodes.The relationship between Person and Matches has been simplified from the previous model. We’ve removed the concept of Appearance, and now have direct relationships from Player to Match. The PLAYED_IN relationship is used both for players who start a match, and those who come on as a substitute.And finally, Teams play in Matches. We capture the result of the match on the PLAYED_IN relationships.Show me the data!Once you launch the Sandbox we have a Neo4j Browser guide that will help us explore the graph. Let’s have a look at some of the queries that we can run against this dataset.Which teams have played in every World Cup?MATCH (tournament:Tournament), (team:Team)WITH team, collect(tournament) AS tournamentsWHERE all(t in tournaments WHERE (team)-[:PARTICIPATED_IN]->(t))RETURN [(team)-[:PARTICIPATED_IN]->()]Teams that played in every World CupWho won the previous World Cups?MATCH (t1:Team)-[p1:PLAYED_IN]-(m:Match)<-[p2:PLAYED_IN]-(t2:Team),      (m)-[:IN_TOURNAMENT]->(tourn)WHERE id(t1) < id(t2) AND m.stage =  Final RETURN tourn.name AS name, tourn.year AS year,       t1.name AS team1, t2.name AS team2,       CASE WHEN p1.score = p2.score            THEN p1.score +  -  + p2.score +   (  +                 p1.penaltyScore +  -  + p2.penaltyScore +  )             ELSE p1.score +  -  + p2.score       END AS result,       (CASE WHEN p1.score > p2.score THEN t1             WHEN p2.score > p1.score THEN t2             ELSE              CASE WHEN p1.penaltyScore > p2.penaltyScore THEN t1                   ELSE t2 END END).name AS winnerORDER BY tourn.yearWorld Cup WinnersWho are the top scorers across all the World Cups?MATCH (p:Person)-[:SCORED_GOAL]->(match)-[:IN_TOURNAMENT]->(tourn),             (p)-[:REPRESENTS]->(team)RETURN p.name, team.name AS team, count(*) AS goals,        apoc.coll.sort(collect(DISTINCT tourn.year)) AS yearsORDER BY goals DESCLIMIT 10Top Scorers across all World CupsWho’s the top scorer playing in the 2019 World Cup?MATCH (p:Person)-[:SCORED_GOAL]->(match)-[:IN_TOURNAMENT]->(tourn),             (p)-[:REPRESENTS]->(team)WITH p, team, count(*) AS goals,      apoc.coll.sort(collect(DISTINCT tourn.year)) AS yearsWHERE (p)-[:IN_SQUAD]->()-[:FOR]->(:Tournament {year: 2019})RETURN p.name, team.name AS team, goalsORDER BY goals DESCLIMIT 10Top scorers playing in World Cup 2019Top scorers playing in World Cup 2019Next StepsWe hope you enjoy the dataset and if you have any questions or suggestions on what we should do next let us know in the comments or send us an email to devrel@neo4j.com.We encourage you to take the data and either build your own APIs or applications or analysis notebooks on top of it. We’d love to hear all about your ideas;Jun 10, 2019;[]
https://medium.com/neo4j/climate-change-twitter-analysis-2016-cbc6c1fd8f1a;John SwainFollowFeb 14, 2017·14 min readClimate Change Twitter Analysis — 2016First published (in modified form) on Carbon Brief on 31 January 2017.Intro by Carbon Brief…Last year, Carbon Brief asked John Swain from Right Relevance to produce a map which visualised the climate change conversation on Twitter. In April, we published his map which was produced using data gathered over a few weeks. However, we wanted to produce something more substantial and insightful than this initial snapshot which captured just a brief moment in time. So we asked him to continue gathering data throughout the rest of 2016. Here, he explains his findings…BackgroundIn early 2016, we were commissioned by Carbon Brief to produce analysis of the conversation on Twitter about climate change. Data was collected during March 2016. Following the interest and reaction to the first analysis we produced for Carbon Brief last March, we agreed to continue collecting data and produce a much richer analysis of the whole year. But, first, it’s worth restating what the original brief was.In summary, the object of the project was:To show, both visually and by ranking, who the key influencers are on Twitter for the term climate change”. To show both how they cluster” and interconnect. To show the volume of interaction and where the hotspots” of activity are within the climate change Twitter universe. To use a transparent and bias-free methodology to both harvest and represent the data.”The search used during this period was for tweets in English including the following keywords: global warming” OR (global AND warming) OR climate change” OR (climate AND change) OR globalwarming OR climatechange OR #climate’Using this collected data, the principle of the analysis methodology is to examine the connections that users make by communicating with each other on Twitter via retweets, mentions and replies. We measure influence using the page rank and betweeness centrality algorithms. More details of the methodology can be found in this article.We have now updated the analysis to discover the most influential users on Twitter over the course of the whole year. The analysis covers the period of February to the middle of December during which we collected over 13m tweets from more than 3m individual users.For this much bigger graph, which includes tweets collected over a much longer period, it was necessary to filter some of the noise in order to produce an analysis which is consistent with the initial analysis conducted in March 2016 .VisualisationThe conversations can be visualised as a Maps as shown below.Full size zoomable versionWithin the map it is easy to identify groups of users which form communities of interest at a global level.The width of the line between users indicates the quantity of links. Groups of users with large numbers of links (retweets and mentions) between them stand out as thick and dark lines. The force-directed” algorithm is set up to organise users into groupings of strong mutual interactions, which tends to reflect mutual interests.Additionally, you can see who is engaging who by the direction of the catherine wheel” of lines coming out of a dot. If the lines are suggesting a clockwise motion, that person is being mentioned and retweeted a lot. Conversely those lines suggesting a anti-clockwise motion are just re-tweeting or mentioning others a lot. These users can be seen round the fringes and may indicate users who produce a lot of spam tweets but who are not removed by the filter we apply to remove very obvious bots.This is a very high level view of the main groups of users and there are many smaller groups which can be seen by visual inspection of the map at a detailed level.In addition to the visual layout for identification of communities we used machine learning community detection to identify how communities form within the conversation. The colours in the map represent the communities detected in this way which generally align with the layout but illustrate a slightly richer set of communities than identified by the postional layout.Tables of Main InfluencersWe use other graph algorithms to identify the most ‘influential’ users in the overall conversation. We use several measures to indicate different types of influence.We have produced tables ranking the most influential users over the year in four categories as described in this article:Twitter Conversation Performance MeasuresUsing graph theory we have devised a set of measures for identifying influential Users in Twitter conversations and…medium.comThe table below shows the tables for each of the five categories:Top OverallTop overall influence is measured by combining the quantity of connections (Retweets, mentions, replies), the quality of the connections (measured by Page Rank) and the reach of the Users tweets. These are adjusted to discount the skew towards Users with very large number of followers.Top ConnectorsThe value of a Users connectedness is measured using an algorithm called Betweenness Centrality. This measures how well a User is connected on the paths between all other Users — compared with everyone else. It was introduced as a measure for quantifying the control of a human on the communication between other humans in social networks.Most ‘Interesting’ UsersThe Interesting metric finds smaller Users who made a relatively high impact. It compares how well a User does in the overall ranking compared to how well they would be expected to do given the number of followers they have.It is useful for finding niche or local stories in a large network where they are difficult to find.Top Talked AboutMeasures how much a users is talked about rather than responded to. It measures the ratio of the amount of times a user is mentioned to the number of times a user is retweeted. The ratio is adjusted for the number of tweets the user makes and the number of followers the user has.This can indicate that a user is not active on Twitter but is being talked about in the wider world which is reflected by other users mentioning the user on Twitter.Top BrokersBrokers” are connectors between communities of users. Connectors (see above) measures how well connected users are between all other users. Brokers are those which connect (either by retweeting/replying themselves or being retweeted/mentioned by others) between different communities.DashboardWe show the information in a set of tables which can be seen in a Tableau Dashboard.Click here to see the Tables in the live dashboard.Click Top Tables tap to see the tables of influential users.These are the five most important tables.Screenshot taken from Interactive Tableau DashboardThe Right Relevance Topical Influence Service provides information on how influential Twitter users are in over 50,000 Topics based on their followers/following and links to published articles. The topics scores are generated by an algorithm based on the relationships between influential users rather than any human assessment of influence.For example here is the list of Topics in which Al Gore is influential. The number after each topic reflects their influence on a 0–100 scale.With the integration of these Influence scores we can filter these tables by particular areas of Influence. In the screenshot below there is a list of the Topics in the middle section. If a Topic is selected it only shows users who have influence in that Topic.For example if the ‘Renewable Energy” topic is selected in the main dashboard page as shown here.The resulting Tables now show the most influential users with topical influence in the Renewable Energy Topic.Influential users in the Renewable Energy Topic.Leonardo Di CaprioOver the last few days of February when the actor Leonardo DiCapro won an Oscar, citing climate change in his acceptance speech the Twitter conversation about climate change was dominated by Leonardo Di Caprio.The following image from the original analysis shows this surge in influence during that period.Twitter Network Map, week ending 06 March 2016, the highlighting the volume of users retweeting and mentioning Leonardo Di Caprio.This effect of users with a very large number of followers having an extreme influence is something that is hard to avoid with conventional analysis based on simple metrics like retweets. The techniques we use can correct for this effect to gain a better understanding of who is a genuine influencer within a subject. The techniques for overcoming short term distortions were discussed in the original article.Since then Leonardo Di Caprio has continued to be involved in the climate change conversation. In this analysis over a much longer period Leonardo Di Caprio is identified as a user with a major influence on the subject of climate change.Over the course of the year DiCaprio has been a consistent tweeter on the subject of climate change and has been involved in other projects to promote the cause he is advocating including the film Beyond the Flood. What our analysis shows is that Leonardo Di Caprio is someone who has significant influence due to the number of followers but that his engagement in the conversation is also significant and that he is a genuinely influential person on the subject of climate change.Leonardo DiCaprio (@LeoDiCaprio) | TwitterThe latest Tweets from Leonardo DiCaprio (@LeoDiCaprio). Actor and Environmentalist. Los Angeles, CAtwitter.comFlocks and TribesThere are two types of group or communities which we identify which are referenced above Flocks and Tribes. Both types of communities are identified by machine learning algorithms and not by any human decision making process. They are identified in different ways and have a different quality with regard to time.Flocks are the main groups you see identified by position and colour in the visual map. The connections between these users are by the tweeting activity (retweets, mentions, replies) so are a reflection of how people group within the conversation. This is a temporal phenomenon e.g. the US Election and related issues. The flocks detected by the algorithm include the UNFCCC”, Fox News” and even Carbon Brief”. Click on the flock’s name in the dashboard to reveal more analysis and data about that detected community.Tribes are detected by the relationships between users followers and following. These are the groups identified by Topical influence such as the renewable energy” topic above — when you select a Topic from the list in the dashboard the users are filtered to show the Tribe of users for that Topic. These relationships tend to reflect a more permanent set of shared interests.Using a football metaphor, tribes are the supporters of teams. Flocks form to discuss a particular game, player transfer, etc.Using a football metaphor Tribes are the supporters of teams. Flocks form to discuss a particular game, player transfer etc.Climate Change ScientistsThe reverse of the Leonardo Di Caprio effect can be seen when looking for the influence of climate scientist within the Twitter conversation.We have called this the ‘Expert Problem’. This occurs because experts tweet using technical language that is not picked up by the terms we use to collect tweets. One solution would be to track lots of technical terms, but If technical terms are added it is impossible to not introduce significant bias in favour of particular users.For example, including tweets that contain carbon dioxide” or greenhouse” could capture lots of tweets that are not connected to climate change. Including terms like climate science” may introduce a significant skew in favour of the academic community making them seem more influential overall than they actually are.This is in conflict to our objective of identifying those who make the most impact and have influence in the general conversation about climate change.What we can see, however, is how scientists gain influence through their ideas in two specific ways:By influencing others who do have a big voice within the general conversation.By having an idea which gains widespread notice — a scientific meme.Michael MannMichael Mann is an example of the former. Mann, the director of Earth System Science Center at Penn State University, is one of the world’s most prominent and quoted climate scientistsMichael E. Mann (@MichaelEMann) | TwitterThe latest Tweets from Michael E. Mann (@MichaelEMann). Climate Scientist, Professor & Director of the Penn State ESSC…twitter.comIn the main tables Michael Mann is highly placed as a Connector, in the Interesting category and Overall.Micheal Mann in the overall tablesAs can be seen from Michael Mann’s Twitter banner he does not have a very large number of followers compared to many of the users in the top overall table. The position that Michael occupies in the conversation map is a clue to the way in which he achieves influence on Twitter.Here is the section he occupies and notice how close this is to many other mainstream influential users.This map is the ego network for Michael Mann within this map. An ego network is all the other users that are directly connected to Michael and all the connections between those users.Michael Mann ego network within the twitter conversation map.This map illustrates two things:Michael Mann is connected to a large part of the most important users in the overall networkThe users in the ego network also communicate with each other.Here is the ego network for CNN for comparsion.It is clear that Michael Mann is more connected within the conversation amongst influential users who are themselves more engaged with other influential users.To put this in context, CNN has over 30m followers.When CNN tweet about climate change the overall reach (the total number of potential displays in users timelines), therefore, is very high. This table shows the difference of some of the key metrics between these users. Notice the big differences between the total reach and the BTW (Betweeness).Comparison of key metrics for CNN and Michael MannOn the dashboard click ‘User Detailed List’ to view this table.This illustrates two very different ways in which influence is created. CNN has a large following of users from the general public whilst Michael Mann is much less well known generally but has a high connectivity with important uses in the network.Ed HawkinsAnother example of expert problem is this tweet from Ed Hawkins who exerts influence in the second way identified above — the spread of a powerful idea.Ed Hawkins is a climate scientist from the UK with a fairly small user number of followers.Ed Hawkins (@ed_hawkins) | TwitterThe latest Tweets from Ed Hawkins (@ed_hawkins). Climate scientist at University of Reading | IPCC AR5 Author | Editor…twitter.comIn May 2016 Ed Hawkins tweeted about a graphical representation of the change in global temperatures since 1850. This went viral but was not picked up by our search term which looks for explicit phrases about climate change.However, a human can clearly see that it is about climate change. This is a clear example of the expert problem.A similar example from later in the year did not have anywhere near the same impact.The effect of Ed Hawkins viral tweet is picked up in our analysis because of the fact that many users tweet about it using terms which are picked up by our search terms.One way this phenomenon is measured by our analysis is the Interestingness” measure.As mentioned above Interestingness”:Compares how well a user does in the overall ranking compared to how well they would be expected to do given the number of followers they have.”So, whilst Ed Hawkins has a small number of followers and a lot of his tweets are aimed at a scientific rather than a general audience, we can still detect his influence overall.Presidential ElectionA clearly visible section of the map is the area shown below which is easily identifiable as the main participants in the US Presidential election.Section of the conversation map with users involved in the Presidential ElectionThere are two main points to note about the election and the result.Firstly, it was a significant feature of the election that climate change was not widely discussed within the debate. We carried out a significant analysis of the election in which we found very little mention of climate change in the main topics of discusion.Secondly, it is worth examining how Donald Trump’s influence is detected within this analysis. Whilst it is certainly the case that Donald Trump is highly influential within any subject of general interest to society as a result of becoming the President, the way in which this is represented in the conversation illustrates a fundamental shift in the nature of the global discussion of climate change as seen on Twitter.In the tables Trump is most talked about and most influential overall, a fair reflection of the years events, he is also one of the Top Connectors.Notice the blue bars in the Talked About” table. These illustrate the actual score of each user and it is very apparent just how much more highly Donald Trump scores in the Talked About” measure than other users.As mentioned above the Talked About” measure:…can indicate that a user is not active on Twitter, but is being talked about in the wider world which is reflected by other users mentioning the user.”This table shows that Donald Trump has no retweets at all in the entire 13m set of tweets collected despite the same table showing that Trump tweeted 469 times. In other words, all of Trump’s influence comes from other users mentioning him.The explanation for this seemingly unlikely fact is that the 469 tweets from Donald Trump were made before the collection started but replied to by users during the period of collection.This shows that Donald Trump has not been active in the climate change conversation himself during our period of analysis.If we plot the Talked About Ratio against the number of retweets the chart shows how far apart Donald Trump is from other important people.Shift of InfluenceIn this conversation the single most important person is not engaged at all in the actual conversation. His power comes from an external source. There are other powerful and well known people who have external power which is reflected in the Twitter conversation but no where near to the same extent as Donald Trump.If we look again at the overall map and the different groups we identified earlier we can observe that amongst the different views and topical interests that are reflected most of the most influential people are involved in the conversation.What we can observe that this has changed, the single most important individual person with immense power over the policy decisions regarding climate change is no longer involved in the conversation on Twitter.It is beyond the scope of our analysis to understand what this means in the wider world but it does indicate a profound change.;Feb 14, 2017;[]
https://medium.com/neo4j/announcing-the-neo4j-crime-investigation-sandbox-c0c3bd9e71b1;Joe DepeauFollowDec 12, 2018·3 min readAnnouncing the Neo4j Crime Investigation SandboxPersonally, I am always most excited by technology when it can have a positive impact on society and the lives of ordinary people. Curing deadly diseases. Advancing our understanding of our world and our universe. Helping people communicate regardless of distance or language barriers. Preventing and solving crimes.See examples of the analysis that can be done using the POLE Neo4j Sandbox dataset in this video.That’s why I’m so excited to announce the availability of a new Neo4j sandbox which we’ve created to show how the power of Neo4j’s graph platform can be used in intelligence-led crime investigations and social services case management.You can find the sandbox after logging in at: https://neo4j.com/sandboxThe sandbox comes pre-loaded with sample data and a step-by-step guide with queries and explanations — everything you need to get going!The data for the crime investigation sandbox is organised based on the POLE data model, commonly used in policing and other security-related use cases. POLE stands for Persons, Objects, Locations, and Events. The diagram below shows an overview of the POLE model:Using the sandbox demo data, our representation of the POLE data model as a graph looks like this:You can see the node labels that represent the four elements of the POLE model are:Persons: Person nodes and Officer nodesObjects: Object nodes, Vehicle nodes, Email nodes, and Phone nodesLocations: Location nodes (also grouped together into PostCode and Area nodes)Events: Crime nodes and PhoneCall nodesWhile this demo implementation of the POLE data model is simpler than a real-world graph might be, it still provides enough complexity and variety to allow us to simulate a number of different scenarios.crime investigations,finding vulnerable people in the graph,using geospatial search to find crimes within a certain distance from a location,and even looking for important persons and potential criminal gangs using two different graph algorithms.The sandbox database includes over 61,000 nodes and more than 105,000 relationships for you to work with, and the guide includes 21 Cypher queries to run against the sample data. These queries range from simple examples like finding the top 15 locations in the graph for crimes:MATCH (l:Location)<-[:OCCURRED_AT]-(:Crime)RETURN l.address AS address, l.postcode AS postcode,        count(l) AS totalORDER BY total DESC LIMIT 15To more complex examples which use graph algorithms to identify potential communities of criminals:CALL algo.triangleCount.stream(MATCH (p:Person)-[:PARTY_TO]->(c:Crime) RETURN id(p) AS id, MATCH (p1:Person)-[:KNOWS]-(p2:Person)  RETURN id(p1) AS source, id(p2) AS target,  {concurrency:4, graph:cypher}) YIELD nodeId, trianglesWHERE triangles > 0MATCH (p:Person)WHERE ID(p) = nodeIdRETURN p.name AS name, p.surname AS surname, p.nhs_no AS id,       trianglesORDER BY triangles DESCLIMIT 10I hope you find this graph use case as interesting and exciting as I do, and that the sandbox gives you plenty of food for thought on how graph database technology can be used to prevent and solve crimes as well as protect vulnerable people.Please let us know how you find working with this new Neo4j sandbox, and let us know of any exciting new queries and outcomes you may find!Neo4j Graph Database Sandbox - Get Started with GraphsThe Neo4j Sandbox enables you to get started with Neo4j, with built-in guides and sample datasets for popular use…neo4j.com;Dec 12, 2018;[]
https://medium.com/neo4j/15-tools-for-visualizing-your-neo4j-graph-database-ff7315873032;Niels de JongFollowApr 12, 2021·10 min read15 Tools for Visualizing Your Neo4j Graph DatabaseVisualizing graphs is hard. When I started working with graph data, I realized it takes significant work to create intuitive graph visualizations. Thankfully, a ton of tools have been developed that make graph visualization a cakewalk.In this article, I’m zooming in on some of my favorite tools. I group these tools into categories based on their functionality and purpose. After reading, you should have an overview of the graph visualization landscape, and (hopefully) find a tool that fits your visualization needs.Categories of Graph Visualization ToolsBefore we dig into the tools, it’s important to be aware of the categories of tools out there. All visualization toolkits were built with a specific purpose in mind, so you’ll have to make sure the tool’s purpose matches your need.I group all graph visualization tools into four main categories:Development tools, to help developers work with graphs.Exploration tools, to help analysts explore data relationships.Analysis tools, for revealing trends & discrepancies.Reporting tools, to create and organize data reports.In the image below, I organized some of the most popular graph visualization tools by their main category. On the vertical axis, I’ve plotted the product type (a Neo4j product, community project, or enterprise software).A structured view of the visualization tool landscape.Note that some tools sit in between two categories, in this case, the tool can be used for both purposes. Now that we have a high-level overview, lets dive into some of the tools.1. Neo4j Browser (Development Tool)(Image credit — Michael Hunger, Neo4j)The Neo4j Browser is likely the first thing you’ll run into when working with Neo4j. A tool for database developers to run Cypher queries, the Browser lets you render query results in a graph or table format. Although limited in visualization capabilities, I use it daily to rapidly design and optimize Cypher queries.Key features:Easy to get started, but limited visualizations and styling choices.Direct views of your graph data, but requires (technical) Cypher knowledge.Great for rapid query development, but no saving/embedding/sharing of visualizations.2. Neo4j Bloom (Exploration & Analysis Tool)(Image credit — Anurag Tandon, Neo4j)Neo4j Bloom is a tool for interactively exploring Neo4j graphs. Whereas the Neo4j browser is mainly used by developers, Bloom is suited better for data analysts — those who want to dynamically visualize big graphs. Bloom supports text-based search, allowing people with little Cypher knowledge to investigate a Neo4j graph.Key features:‘Point-and-click’ graph exploration.Performant views of large graphs with custom styling.Edit your Neo4j graph with a visual interface.Storing and sharing graph perspectives.Rendering different graph layouts.3. Neovis.js (Development Tool)(Image credit — Will Lyon, Neo4j)neovis.js is a JavaScript library to help developers build graph visualizations from Neo4j data. Wrapping for the popular library Vis.js, it provides a bridge between Cypher and a customizable graph visualization in the browser.JavaScript library (based on Vis.js) to draw Neo4j graphs.Connect directly to the Neo4j instance to get live data.User-specified style properties based on label, property or community.Configure hover/click functionality for nodes.4. Popoto.js (Development Tool)(Image credit — Popoto.js examples)popoto.js is another community-driven JS library for creating embeddable visualizations. popoto.js is based on the widely used D3.js library, supporting a large number of visualizations. It also contains an interactive and customizable visual query builder for Neo4j. There are a great number of examples available online on how to use popoto, as well as how to extend it with custom styling.Key features:An interactive visual Cypher query builder with a direct Neo4j connection.Embeddable into webpages, with a variety of examples available.Powered by the widely used D3.js visualization library.Custom styling by extension.5. KeyLines (Development Tool)(Image credit — Cambridge Intelligence)KeyLines is an Enterprise library for building graph visualizations in JavaScript. Compared to neovis.js & popoto.js, KeyLines has a far greater list of features (layouts, styling, grouping, filtering) and is built to be performant on large graphs. KeyLines is an enterprise-only product that powers some of the other standalone graph exploration tools (such as GraphAware Hume).Key features:JavaScript library for advanced graph visualizations.Support for Time-based analysis, geospatial graph analysis, social network analysis.Support for geographical (map) visualizations.Custom graph layouts (grouping) & styling.High-performance WebGL rendering.6. CytoScape (Development Tool)(Image credit — CytoScape)CytoScape is a tool originally built for visualizing biological networks, but it has recently seen uses in a variety of network analysis use-cases. The tool comes in two flavors:CytoScape Desktop (A standalone Java-based visualization tool)CytoScape.js (An open-source Javascript library for graph visualizations)CytoScape.js is by far the most extensive open-source library available for visualizing graphs — supporting graph layouts, advanced styling, event handling and much more.Even though Neo4j support is not a core feature of CytoScape Desktop, several plugins exist to build visualizations directly from your Neo4j database.Key features:Highly optimized graph rendering.Uses layouts for automatically or manually positioning nodes.Custom styling.Easily embeddable into web applications.Open source, easy to extend with custom functionality.7. yWorks Neo4j Explorer (Exploration Tool)(Image credit — yWorks)The yWorks Neo4j Explorer is a free, web-based tool for exploring a Neo4j database. The tool wasbuilt by yWorks, the company behind the widely used yFiles JS library, to demonstrate the capabilities of the yWorks framework for graph data. It consists of two main components:A ‘Schema view’, to see the structure of the node labels and relationship types in your Neo4j database.An ‘Explorer view’ for searching and navigating through your graph.The yWorks explorer is a great option for those looking to get started with graph exploration with a simple and intuitive interface, but has limited options in comparison to (paid) enterprise products.Key features:Visualize and explore the Neo4j database schema.Modify the database schema — hide and show parts of your model.View a Neo4j schema in different layouts (organic, hierarchical, radial).Basic Graph-based search for node/relationship properties.Graph exploration based on relationship types & node labels.Styling of nodes & relationships with colors, shapes and images.More details can be found in the blog post here.8. Linkurious Enterprise (Exploration & Analysis Tool)(Image credit — Linkurious)Linkurious Enterprise is an on-premises graph exploration and analysis tool. It’s a feature-packed tool intended to be used by data analysts, commonly used for investigation use-cases such as detecting money laundering, cyber threats, and other criminal activities. Linkurious supports an extensive list of features for graph-search and analysis, as well as graph manipulation.Key features:Interactive graph exploration.Predefined workflows for common use-cases.Graph-based search for node/relationship properties.Storing and sharing graph perspectives in a team.Dynamic graph editing & the ability to render different graph layouts.9. GraphAware Hume (Exploration & Analysis Tool)(Image credit — GraphAware)Hume is a graph exploration and analysis tool developed by GraphAware. It supports a complete ‘graph workflow’ — from building knowledge graphs (ETL) to text-based search, as well as data science applications.At its core, Hume is a powerful graph visualization tool. Graph-based search is a main feature of Hume, creating a workflow where searching the graph and exploration go hand-in-hand. Hume allows for custom user actions to be defined, letting you create a tailored experience for data analysts. Hume is used in a variety of business domains such as National Security, Marketing, Recommendation Engines, and Knowledge Engines.Key features:A platform for building knowledge graphs, with a strong focus on NLP.Interactive graph exploration & search.Integration with Neo4j graph data science.A variety of graph layouts, custom styling and node-grouping.Embeddable visualizations as iframes.Custom exploration actions.SSO support, strong focus on RBAC and security features.Native virtual relationships, perspectives, and time-based filtering.10. Kineviz GraphXR (Analysis Tool)(Image credit — Kineviz)KineViz is an enterprise graph analysis tool with the ability to render huge graphs (>100,000 elements) in two or three dimensions. KineViz supports loading data from CSV, JSON, or from Neo4j using Cypher. Its graph analysis capabilities include a variety of algorithms, including path finding and community detection. A detailed description of all KineViz’s features is available in the KineViz User Guide.Conduct times series, geospatial, and social network analysis.Perform statistical analysis on large and complex data sets.Visualize 100,000+ nodes in a variety of 3D and 2D layouts.Collaborate, export, and report on data in a variety of formats.11. Graphistry (Analysis Tool)Graphistry is a graph analysis tool, capable of visualizing huge graphs in the browser. It is one of the best tools available for rendering big graphs, supporting GPU rendering of 100,000 to 1,000,000 nodes and relationships. Data can be loaded into Graphistry from Neo4j directly, or through an open-source Python library.Key features:GPU-accelerated rendering of huge graph visualizations.Graph-based clustering, filtering and search.Define reusable workflows for graph analysis.Embedding visualizations in websites, dashboards and notebooks.Sharing of visualizations within an organization.12. Tom Sawyer’s Perspectives (Analysis & Reporting Tool)(Image credit — Tom Saywer Software)Perspectives is a standalone (Java- & .NET based) enterprise-grade graph visualization tool. Perspectives is a graph visualization SDK, and comes with a GUI to build applications. It supports a variety of graph layouts, as well as report types (maps, charts, timelines, tables, …). Graph clustering and flow computation can also be performed directly from the Perspectives interface.Key features:Standalone toolkit for building graph visualizations.A variety of graph layouts and styling options.Integrated graph algorithms for analysis.Flexible graph-based search and filtering.One of the tools built with Perspectives is a custom Graph Database Browser with exploration capabilities. Check it out here.13. Graphileon (Reporting Tool)(Image credit — Tom Zeppenfeldt, Graphileon)Graphileon is a dashboard development environment, built specifically for visualizing graph data. It allows you to easily design, build and share dashboards using data from Neo4j and a variety of other graph databases. Offering an extensive set of features, Graphileon can be used as a prototyping tool, as an application framework, or a nice way to present the contents of your graph database.Key features:Report on a Neo4j database with networks, tables, forms, charts, maps, timelines, calendars and more.Organize interactions in dashboards by means of functions and triggers.Style your visualizations using different layouts, icons, colors and shapes.Save visualizations as diagrams or as images.Embed dashboards and charts in other applications.Access control and sharing of dashboards within teams.Graphileon has both a personal edition (free to use) and an enterprise edition, the latter containing additional features.14. Charts (Reporting Tool)(Image credit — Adam Cowley, Neo4j)Charts is a Neo4j labs project, built to directly generate charts from a Neo4j database. Charts can be installed from Neo4j Desktop or accessed from the browser at https://charts.graphapp.io. Using Cypher, you’re able to create tons of visualizations on the fly using Nivo, a data visualization library for React. It also comes with an awesome visual Neo4j query builder to help those new to Cypher write queries in a heartbeat.Key features:Plot charts and organize them into dashboards.Save & load dashboards within Neo4j Desktop.Support for over 20 types of visualizations.Interactive Cypher query builder to write queries to populate your visualizations.Check out this post for more about Charts.15. NeoDash (Reporting Tool)NeoDash is a graph-app/web app to build dashboards from Neo4j data in minutes. Directly connecting to Neo4j via Bolt, it populates reports from Cypher query results. Query results can be rendered as tables, graphs, bar charts, etc., and users can interactively select parameters for reports. Basic styling options are available by overriding query parameters.Key features:A graph app & web application to build dashboards.Visualize query results as tables, graphs, bar charts with custom styling.Interactively select query parameters.Mix visualizations with Markdown text to create a ‘living document’ using your graph data.Load and save dashboards as JSON.Further ReadingI highly recommend checking out the Neo4j blog for more data visualization stories. If you’re working with D3.js, you should check out some of the articles by Jan Zak. Last, I’d like to mention other interesting tools that didn’t make the list:ReGraph — A performant graph visualization library for React.G6 — another JS library for graph visualizations.Graphlytic — a web app for collaborative graph exploration and analysis.SemSpect — a different kind of visualization & exploration tool.Wrapping UpI hope this overview helped you get an idea of the current state of the graph visualization world. There’s a lot of amazing tools out there, and I suggest you give them a try — after all, that’s the best way to see if a tool works for you!Only when you visualize your graph data, the real power of graphs is revealed: you’ll get insights that you’d never spot before.;Apr 12, 2021;[]
https://medium.com/neo4j/how-queries-work-in-neo4j-97229988941a;David AllenFollowOct 16, 2020·5 min readHow Queries Work in Neo4jNeo4j users find themselves needing to monitor the internal health of the system. Halin was built as a tool to help them do that, and presents a Tasks” screen for each member in a Neo4j cluster, that shows what’s running at any given time.Screenshot of tasks in HalinOften users want to know what’s going on with my query” — but the query isn’t the only thing that’s going on under the hood. To understand that, we have to look at two other system objects: Transactions and Connections. In this post, we’ll look at Transactions, Connections, and Queries, and how they all hang together in a Neo4j system.The query is the easiest first part: it’s a bit of Cypher you send to the database to answer a question.TransactionsA transaction is some sequence of events that is applied to a database, that either atomically succeeds or fails. Transactions create little bubbles” of computation and let the user know that the transaction either all worked, or none of it worked. Suppose you need to do three things:Create an employee named Peter.Create a job called Software Engineer Intern”.Link Peter to that job.You want all of that work to either succeed or fail. If creation of Peter” fails, you don’t want to end up in an intermediate state where you have a Software Engineer Intern” job that no one is doing. So you can do this by putting three different queries or operations in a single transaction.At a low level, you can CALL dbms.listTransactions() to show what’s happening at any given time, and you can kill them individually with CALL dbms.killTransaction(some-id).Very important to note — this output contains a currentQuery field. Because transactions may perform multiple operations, this could change, as the transaction handles multiple operations. Further, because it’s possible to manipulate the graph inside of the Java API — you don’t have to have any query at all! Not all changes to Neo4j are made via Cypher.Transactions have three key events in their lifecycle:They get opened.They run zero or more operations.They commit, or they rollback. This is the part where all of the operations atomically succeed or fail.ConnectionsThe system manages multiple connections, via both HTTP and bolt ports. As a result, transactions get tagged with the source they came from. You can inspect these at a low level by running CALL dbms.listConnections( , and you can kill them individually with CALL dbms.killConnection(some-id).CALL dbms.listConnections()In this screenshot, you can see the various connections Halin made to the database, and what address it was coming from. Each connection has an ID. Correspondingly, in the output of CALL dbms.listTransactions() you can find that some (but possibly not all) transactions are associated with a specific connection ID. This would let us know which client at which network address is trying to run which transaction.QueriesNow that we have those background ideas covered, back to queries. We can think of a query as a unit of work that is done within a transaction. In common cases, a single query will be the only unit of work. Remember: transactions don’t have to have queries, and a single query could be a part of a transaction, not the whole thing.Finally — queries might be associated with a connection, but don’t have to be. If a query is run by the database itself, it has no client connection.Simple ranging to complexIn the simplest possible case, a program connects to Neo4j, issues one query, gets the results, and disconnects. In that simplest case, there is one connection, one query, and one transaction, and they’re all neatly linked — easy peasy.But in a production system, there could be hundreds or thousands of transactions in flight from dozens or hundreds of connections, with each transaction surviving less than a second. In that kind of environment, transactions may not even live long enough for you to inspect them with Neo4j’s built-in procedures.In those cases, looking at CALL dbms.listTransactions() is a poor choice. By the time you have the table result back, the answer is already different! In these cases, you should consider enabling system monitoring with Prometheus or other similar approach. The built-in system procedures are best for manual administration of heavy or long-running queries and transactions.It’s all specific to a Cluster memberUp until this point, we’ve been thinking about this in the case of one Neo4j machine but if you’re running a Causal Cluster, you may have three or more machines. In this case, it’s critical to understand which machine your browser is pointed at. If you ask for a list of connections, you’ll get only the connections to the machine you’re talking to.This is why Halin puts the query / task view underneath of the cluster member tab. Watch out! If you connect to a cluster using the neo4j://protocol, using a routing driver, Neo4j Browser is routing your queries automatically for you, and you might not know which machine’s connections you are looking at. So for manual administration tasks, I’d recommend you use bolt:// to connect to a single cluster member so you know whose connections, queries, and transactions you’re seeing.ManagementWith these concepts, let’s now take a look at the built-in procedures and functions in Neo4j, and pull out only the bits that are interesting to management of connections, transactions, and queries. With all of the concepts described in this post, what these procedures do should now be clear and pretty easy to follow. A list of the relevant procedures is below.dbms.listTransactions()dbms.killTransaction()dbms.listConnections()dbms.killConnection()dbms.listQueries()dbms.killQueries()dbms.killQuery()ConclusionUsing all of these tools and concepts, you can have fine-grained control on exactly what your cluster is doing at all times, and be able to understand the results of what you’re seeing. Happy graph hacking!;Oct 16, 2020;[]
https://medium.com/neo4j/spring-data-lovelace-neo4j-ogm-3-1-3-went-ga-36614b60c889;Michael SimonsFollowSep 27, 2018·7 min readSpring Data Lovelace & Neo4j-OGM 3.1.3 went GAIt has been nearly a year since our last post about the things happening with Neo4j in the Spring World”. Time did not stand still and the Spring Data Neo4j / Neo4j-OGM Team has been working on improvements and building new features based on the foundations laid out with SDN 5.0 and Neo4j-OGM 3.0 last year.My name is Michael Simons, I am a Java Champion and the author of the German book about Spring Boot 2 and Spring 5. I joined the Spring Data Neo4j team in July 2018. Together with Gerrit Meier I have been working on the latest release of Neo4j-OGM which is 3.1.3. Neo4j-OGM is the underlying Object Graph Mapper technology used by Spring Data Neo4j.While Neo4j-OGM 3.1.3 has been released on September 19th prior to our own GraphConnect in New York, Spring Data Neo4j 5.1 has been released some days later as part of Spring Data’s Lovelace release train just in time for the Spring One Platform (S1P) conference.Time to have a look what’s new in the most recent versions.What’s new in Neo4j-OGM 3.1.3What are the new features and improvements in the object graph mapper itself? Neo4j-OGM can be used by itself, for example with Micronaut, a framework especially dedicated to the creation of microservices. It also works well when Spring Data’s features like repository abstraction or the eventing system are not needed or wanted. This could be the case in applications dealing with a large number of different nodes, that are not clearly part of aggregate roots. Spring Data has never be designed to be used in such a way that one needs to declare a repository per entity type.Improved @PostLoad mechanismEach @NodeEntity can have one method annotated with @PostLoad. Neo4j-OGM will call this method once the entity is loaded from the database. This is useful for composed information or other initialization methods that should only be called after the object has been fully populated.Listing 1. SomeEntity.java@NodeEntitypublic class SomeEntity {    private transient String computedInformation    @PostLoad    void computeTransientInformation() {        this.computedInformation =  Whatever     }}With Neo4j-OGM 3.1.3 @PostLoad requirements are the same as JSR 250 @PostConstruct has, especially:The method on which @PostConstruct is applied may be public, protected, package private or privateIn general, the method must not be final. However, other specifications are permitted to relax this requirement on a per-component basisThis change allows a better encapsulation of business logic in your aggregate roots. In the previous versions, @PostLoad methods needed to be public and could not be final. This lead to situations where those methods could be called from the outside or overwritten in ways that contradict your domain.There is one situations where you might be able to actually want to overwrite @PostLoad methods: Class hierarchies.Improved handling of class hierarchiesMany people like the approach of introducing some abstract base classes for their entities. We have seen this approach with JPA and also a lot with Neo4j-OGM. Common arguments for doing this are keeping common attributes like the id-Attribute or auditing information in a shared base class as shown in Listing 2.Listing 2. AbstractAuditableBaseEntity.javapublic abstract class AbstractAuditableBaseEntity {    @Id    @GeneratedValue    private Long id    @CreatedDate     private LocalDate createdAt    @LastModifiedDate    private LocalDate updatedAt    public LocalDate getCreatedAt() {        return createdAt    }    public LocalDate getUpdatedAt() {        return updatedAt    }}This AbstractBaseEntity provides all boilerplate needed: A generated id and some auditing. In previous versions of Neo4j-OGM you had to use @NodeEntity on this class as well to make our auto index manager work.This introduced meaningless labels (either the name of the class or some other artificial label made up by you). Now the logic works as stated in the documentation: Abstract classes in the hierarchy of an entity don’t contribute a label to nodes. The auto index manager creates indexes in such a way that they ensure uniqueness across subclasses, if there are intermediate classes (abstract or not) with labels in the hierarchy.@CreatedDate and @LastModifiedDate are annotations from Spring Data Neo4j, more about them later.Improvements on Neo4j-OGM FiltersNeo4j-OGM Filters can now traverse relationships of entities to query their attributes. We call this nested property filter support. The equal filter can now do a case-insensitive equals comparison without resorting to regular expressions.MiscellaneousNo software is without bugs. We fixed issues around array conversions, prevent possible accidental deletion of all nodes when a query doesn’t contain any label and more. Also, we started polishing code and deprecating unused things. In short: We are trying to create a clean state to be prepared for new features.Ada Lovelace portrayed by Margaret Sarah CarpenterAll these things are used in the latest and greatest installment of Spring Data Neo4j, called Lovelace after famous Ada Lovelace. Ada Lovelace was the first to recognize the potential of a general computing machine by publishing algorithms for a -back then- only proposed mechanical computer, the Analytical engine.What’s new in Spring Data Neo4j 5.1 LovelaceAs you might know, Spring Data itself is heavily inspired by Eric Evans’ famous book Domain Driven Design”. The ideas of repository and aggregate roots are just some of the ideas of this book. Spring Data is much more than just helpful derived finder methods. There is support for an eventing system, auditing and more.Modeling better domain models is part of SDN 5.1, together with all the things described in Improved @PostLoadmechanism”.Persistence constructorsExtending AbstractAuditableBaseEntity from Listing 2 to model a musical artist as in Listing 3, you’ll notice the missing public default constructor:Listing 3. ArtistEntity.java@NodeEntity( Artist )public class ArtistEntity extends AbstractAuditableBaseEntity {    @Index(unique = true)    private String name    public ArtistEntity(String name) {        this.name = name    }    public String getName() {        return name    }}Using Spring Data’s common infrastructure, you no longer need to provide useless default constructors in your domain. If there are several meaningful constructors from the business side of things, you have to annotate one of them with @PersistenceConstructor as shown in Listing 4. That way, you make clear which constructor is to be used when retrieving nodes from the database.Listing 4. BandEntity.java@NodeEntity( Band )public class BandEntity extends ArtistEntity {    @Relationship( FOUNDED_IN )    private CountryEntity foundedIn    @Relationship( HAS )    private List<MemberEntity> member = new ArrayList<>()    public BandEntity(String name) {        this(name, null)    }    @PersistenceConstructor    public BandEntity(String name, CountryEntity foundedIn) {        super(name)        this.foundedIn = foundedIn    }}Auditing supportAs mentioned in Listing 2, the example uses Spring Data Neo4j features.org.springframework.data.annotation.CreatedDate and org.springframework.data.annotation.LastModifiedDate are Spring Data annotations to mark attributes as targets for audit-informations. Those are enabled by providing a Spring @Configuration class like in Listing 5.Listing 5. Neo4jConfiguration.java@Configuration@EnableNeo4jAuditing // Turn on auditingpublic class Neo4jConfiguration {}Now those fields are automatically populated on creation and updates of the node in question. There are many more features like this that make a compelling reason to use Spring Data Neo4j for accessing Neo4j in your Spring projects.MiscellaneousSDN 5.1 directly uses Neo4j-OGM’s new features, like case-insensitive queries by supporting IgnoreCase in derived finder methods and traversing nested properties. For example, you can use a derived method as shown in Listing 6 to retrieve all Bands from Germany with a call like bandRepository.findAllByFoundedInCodeIgnoreCase( de ).forEach(System.out::println).Listing 6. BandRepository.javapublic interface BandRepository extends     Neo4jRepository<BandEntity, Long> {    List<BandEntity> findAllByFoundedInCodeIgnoreCase(        String countryCode)}The newly developed support for Springs Expression language in annotated @Query methods in SDN repositories has the same goal: Improving your repository design. Listing 7 may seem a bit constructed, but here SpEL is used to pass parameters to a query that are currently not directly supported.Listing 7. YearRepository.javapublic interface YearRepository extends     Neo4jRepository<YearEntity, Long> {    @Query( MATCH (y:Year {value: #{#year.value}}) RETURN y )    Optional<YearEntity> findOneByValue(Year year)}Enterprise customers can now use the @UseBookmark annotation in composed annotations. Composed annotations are meta annotations that can be used to express things in a language that’s better suited for a specific application than a generic @UseBookmark or similar.Spring BootToday one can claim that new Spring projects should be started with Spring Boot. The easiest way to do this is going to start.spring.io, select all the dependencies you need, especially Neo4j. The result is a project skeleton defined to best practices from more than two years. As said before: Spring Boot is not only about microservices, but about all developments with Spring.From Spring Boot 2.0.6 the reference documentation has been updated and refined to great extends regarding Neo4j. Read it here: Spring Boot and Neo4j (Note: This is a link to the Snapshot documentation until the final release of 2.1).Also, starting with Boot 2.0.6, one can omit the Neo4j-OGM version on the embedded driver, in case it should be included. It will be enough to declare org.neo4j:neo4j-ogm-embedded-driver as dependency.With Spring Boot 2.1, our enterprise customers don’t need to declare their own BookmarkManager themselves anymore if Caffeine cache is on the class path. Our starter will take care of this as well.OutlookThe Spring Data Neo4j team has started to use performance optimizations that are now possible in Spring Data in regard of non-eager initialization of beans and will continue to invest this. Recent updates to the class path scanner that is used to determine classes to be recognized by Neo4j-OGM are also investigated. We are also working on an improved support of Neo4j’s newer datatypes, especially the date time and spatial types that have been introduced together with the spatial features of 3.4.The Spring Data Neo4j / Neo4j-OGM Team is ready to incorporate all future developments regarding reactive Java drivers for Neo4j that are planed to be available in future releases. The common groundwork in Spring Data has been around for some time now and is ready to be used in SDN.The examples in this article are from my project bootiful-music” which you can find at GitHub, look especially at the knowledge module. The whole project will serve as an example for upcoming Spring Data Neo4j related talks.Happy Coding!;Sep 27, 2018;[]
https://medium.com/neo4j/how-to-set-up-gitops-for-a-full-stack-app-with-flux-helm-on-kubernetes-7ec88664bad2;Max AnderssonFollowApr 9, 2021·7 min readHow to Set Up GitOps for a Full-Stack App with Flux & Helm on KubernetesManaging infrastructure can be a pain, so much so that there are many attempts to make configuring, provisioning, and managing infrastructure a breeze. But where they all fall short is simplicity. To make things easier to work with, we want our infrastructure to be immutable, idempotent, scalable, and represented declaratively for reproducibility.We want our infrastructure to be immutable so that when we deploy our infrastructure definition, we understand if a specific configuration produces a healthy state or not. If you were to log in to a running production machine and make changes, you would have to make those changes every time you want to redeploy your service. That can cause many headaches if, for example, when people leave the company or go on vacation.Having your infrastructure immutable helps to keep things isolated and separated. If a configuration is causing issues, you can roll back to a previous configuration easily.Imagine having many developers dependent on making changes to the environment they are working in, and everyone is making changes to the environment. You can’t be sure that the current state of your development environment will align with the configuration your production environment is running.Therefore, defining your environment declaratively, these changes can be managed and version-controlled like the rest of your code. Infrastructure changes youve done can be deployed alongside new features, bug fixes, etc. It also allows for a simple workflow among teams to have insight into the current state of the infrastructure.Photo by Richy Great on UnsplashGitOps have been making a lot of noise in the DevOps world the last couple of years and is giving us a lot of automation straight out of the box. One such tool is Flux.What we want to accomplish:Set up a git repository to serve as our single source of truth.Setup a helm script to manage our templating for Kubernetes resources.Install a full-stack application.Prerequisites:HelmKubectlDocker CLIFluxEksctl (optional)Github CLI (gh) (optional)Kompose (optional)Set Up Your RepositoryMost developers have at least once set up a repository from scratch once in their life, so for a change, we will use Github’s new CLI tool to accomplish this, but you can do it any way you please. Github CLI installation instructions can be found here, and how to log in here. So let’s set up a private repository:$ mkdir fluxcapacitor$ cd fluxcapacitor$ gh repo create --private# Follow the instructions, all defaults should do fine. And take a note of the remote address provided from the output we will use this later: git@github.com:<username>/fluxcapacitor.gitNow some people like to debate Monorepo vs. Multirepo, but I prefer a stack-based repo. So let’s have a look at how we will set up the folder structure.The charts” folder will hold our helm charts in this case, we will make our own.The releases” folder will house our Flux releases, and this is where we tell Flux what to apply to Kubernetes. And finally, the src” folder will host all of our source code for the full-stack application.Set Up Your Cluster (Optional)We will be using an EKS cluster for this example but, any Kubernetes cluster should do. We will use the eksctl tools to set up the cluster and Flux integration in one go, but you can find additional instructions on setting up Flux here.Setting up the cluster:$ eksctl create cluster -r <aws_region>$ eksctl enable repo --git-url git@github.com <username>/<repo_name>.git --git-email <git_email> --cluster <cluster_name> --region <aws_region>Tell Kubernetes Your SecretsTo enable us to access your private git repositories, we need to tell Kubernetes what credentials to use to access your repository. First, you must get a personal access token for Kubernetes. You can follow the instructions here to get one. Then we will embed our access token in a Kubernetes secret to allow Flux to access our repository over HTTPS.$ kubectl create secret generic git-https-credentials \    --from-literal=username=<username> \    --from-literal=password=<personal_access_token>Let’s Set Up the ApplicationWe will deconstruct our GRANDstack to make it cloud-ready.First, make sure you have node and npm installed and ready to go. Then we will use npx to install GRANDstack in our src directory. We will choose to react-ts for our frontend, but you are welcome to use any frontend of your choosing.$ mkdir src$ npx create-grandstack-app fluxcapacitor-app? Please choose which project template to use React-TS? Install dependencies? No? Initialize a git repository? No? Now lets configure your GraphQL API to connect to Neo4j. If you dont have aNeo4j instance you can create one for free in the cloud at https://neo4j.com/sandboxHit <Return> When you are ready.? Enter the connection string for Neo4j(use neo4j+s:// or bolt+s:// scheme for encryption) bolt://localhost:7687? Enter the Neo4j user neo4j? Enter the password for this user letmeinGRANDstack is a composite project ready for development, but we want to deconstruct it to make it more accessible for our folder structure. So when you have gone through the setup process for the GRANDstack, move the following folders.$ mv fluxcapacitor-app/neo4j fluxcapacitor-db$ mv fluxcapacitor-app/api fluxcapacitor-api$ mv fluxcapacitor-app/web-react-ts fluxcapacitor-uiLet’s Prepare the ChartNow you could build your chart from scratch or embed an existing chart, but GRANDstack comes with a docker-compose file that we can convert with a bit of help from a tool called Kompose.Installing Kompose is quite simple.# Linuxcurl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-linux-amd64 -o kompose# macOScurl -L https://github.com/kubernetes/kompose/releases/download/v1.22.0/kompose-darwin-amd64 -o komposechmod +x komposesudo mv ./kompose /usr/local/bin/komposeNow lets convert the docker-compose file to a helm chart. First, navigate to the project root, then run:kompose convert -c -o charts/fluxcapacitor -f src/fluxcapacitor-app/docker-compose.ymlMake Room in Your RegistryNow, Kompose handles a lot of our transformation in the previous step, but since we will not be running our docker images locally, we need a docker registry to host our images. In this example, we will create registries on AWS using its client.$ aws ecr create-repository --repository-name fluxcapacitor-ui$ aws ecr create-repository --repository-name fluxcapacitor-db$ aws ecr create-repository --repository-name fluxcapacitor-apiWhen running one of these commands, you should get an output that looks something like below, and please take note of the repositoryUri” for your respective repository.{     repository : {         repositoryArn :  arn:aws:ecr:us-east-1:xxxxxxxxxx:repository/fluxcapacitor-ui ,         registryId :  xxxxxxxxxx ,         repositoryName :  fluxcapacitor-ui ,         repositoryUri :  xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/fluxcapacitor-ui ,         createdAt :  2021-03-10T13:11:20+01:00 ,         imageTagMutability :  MUTABLE ,         imageScanningConfiguration : {             scanOnPush : false        },         encryptionConfiguration : {             encryptionType :  AES256         }    }}If your using ECR on AWS as we do in this example, dont forget to log in to your docker client before the next step.aws ecr get-login-password | docker login --username AWS --password-stdin <xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com>Note that if you have your private docker registry somewhere else, then on ECR, you will have to set up some pull secret or other way for Kubernetes to pull your images.Now let’s build our services.$ cd src/fluxcapacitor-api$ docker build -t xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/fluxcapacitor-api .$ docker push -t xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/fluxcapacitor-apiRepeat the same process for fluxcapacitor-uiand fluxcapacitor-db.Refactoring the ChartFirst, lets add values.yaml to the root of the chart charts/fluxcapacitor/values.yamlwith the following structure and exchange the image string with your registry URI for the corresponding repository.api:  image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/fluxcapacitor-apidb:  image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/fluxcapacitor-dbui:  image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/fluxcapacitor-uiIn the templates folder, you will find the three deployment templates where you will have to add the change the image string (template.spec.containers.image) to match your values in the templates. An example for the API may look something like below. Make sure also to update the deployment.yamlfiles for DB and the UI.apiVersion: apps/v1kind: Deployment...spec:  ...  image: {{.Values.api.image}}...So now, let’s have a look at how Flux plays into what we have just set up.Making It Work with FluxAdd and commit the repository changes and push upstream to the origin master branch.$ git add .$ git commit -m  initial commit $ git push --set-upstream origin masterRun the Flux precheck command to make sure all dependencies are available.$ flux check --pre► checking prerequisites✔ kubectl 1.18.15 >=1.18.0-0✔ Kubernetes 1.18.9-eks-d1db3c >=1.16.0-0✔ prerequisites checks passedCreate a HelmRelease Object$ flux create hr fluxcapacitor \--source=GitRepository/flux-system \--chart ./charts/fluxcapacitor \--export --values=charts/fluxcapacitor/values.yaml \--target-namespace default > releases/fluxcapcacitor-release.yamlNow all you have to do is commit your GitHub changes, and Flux will provide your resources.Here Are Some Pro Tips If You Get Stuck.Tip 1: Flux checks your repo for changes with a 1–5 min interval. You can force Flux to reconcile directly by running$ flux reconcile source git flux-system && flux reconcile hr fluxcapacitorTip 2: Flux will only build a new distribution of your chart if you bump the version of the chart, so inside charts/fluxcapacitor/Chart.yaml , you will find the version to bump. Then rerun the command from tip #1 unless you want to wait.Tip 3: To let the UI access the API, you can add an ingress or load balancer to your service to reach it from the internet. (also update the GRAPHQL_URI environmental variable to point to the URI)Seeding the Database (Optional)To seed the database we have deployed, we will use a proxy to our API.$ kubectl port-forward  service/api 4001:4001 &$ cd src/fluxcapacitor-api$ source .env$ npm install$ npm run seedDbAccessing http://localhost:4001/graphqlyou can now start querying the endpoint.SummarySo we finally managed to set up the infrastructure we need to provision templated resources on Kubernetes, and all we have to do is commit them to our repository, and we are good to go. This setup allows us to make sure infrastructure evolves in pace with your application. We can now easily make changes to our infrastructure and better understand how things are defined. Adding some build steps for our building and publishing our docker images, we would be on our way towards a fully automated pipeline. Check out Skaffold and how you can use it for continuous development.;Apr 9, 2021;[]
https://medium.com/neo4j/world-cup-2018-graph-19fbac0a75db;Mark NeedhamFollowJun 20, 2018·4 min readNow Available: World Cup 2018 GraphWorld Cup 2018 is into its 2nd week and we felt it was about time that we refreshed the previous World Cup Graph that we created for the World Cup in Brazil 4 years ago.Image courtesy of https://www.behance.net/gallery/20875693/Russia-World-Cup-2018-Branding-IdentityGraph ModelThere have been some tweaks to the graph model so let’s have a look at our new and improved model for 2018.World Cup Graph ModelThe WorldCup node sits in the middle of our graph and all other parts of the model revolve around that. We have one WorldCup node for each tournament.Next up we have the host Country which is connected to the WorldCup node by a HOSTED_BY relationship. Matches also belong to a WorldCup and each Country names a Squad of Players that will represent them in a WorldCup tournament.A player is connected to an Appearance node for each match that they participate in either as a starter or substitute. If they score a Goal the Appearance node will connect to that Goal node.Show me the data!We have a Neo4j browser guide that you can use to import the data into your own, local Neo4j instance if you want to play along.:play worldcup-2018Let’s have a look at some of the queries that we can run against this dataset:Who’s hosted the World Cup?MATCH path = (wc:WorldCup)-[:HOSTED_BY]->(country)RETURN pathMost of the World Cup nodes have a single host Country. The outlier is 2002 which was hosted by Korea and Japan.Who’s hosted the World Cup more than once?MATCH (host:Country)<-[:HOSTED_BY]-(wc)WITH wc, host ORDER BY wc.yearWITH host, count(*) AS times, collect(wc.year) AS yearsWHERE times > 1return host.name, times, years╒═══════════╤═══════╤═══════════╕│ host.name │ times │ years     │╞═══════════╪═══════╪═══════════╡│ Mexico    │2      │[1970,1986]│├───────────┼───────┼───────────┤│ France    │2      │[1938,1998]│├───────────┼───────┼───────────┤│ Brazil    │2      │[1950,2014]│├───────────┼───────┼───────────┤│ Italy     │2      │[1934,1990]│└───────────┴───────┴───────────┘This one is interesting because you might expect Germany to show up as they hosted the World Cup in 1974 and 2006. The reason they don’t is because the first World Cup was pre-reunification when they were playing as West Germany which is represented by a different node in the dataset.Have the hosts ever won the World Cup?MATCH (match:Match {round:  Final })<-[hostPlayed:PLAYED_IN]-(host:Country),      (host)<-[:HOSTED_BY]-(worldCup),      (worldCup)-[:CONTAINS_MATCH]->(match),      (match)<-[oppositionPlayed:PLAYED_IN]-(opposition)WHERE (hostPlayed.score > oppositionPlayed.score) OR (hostPlayed.penalties > oppositionPlayed.score)RETURN host.name, worldCup.year, hostPlayed.score +  -  + oppositionPlayed.score AS score, opposition.nameORDER BY worldCup.year╒════════════╤═══════════════╤═══════╤═════════════════╕│ host.name  │ worldCup.year │ score │ opposition.name │╞════════════╪═══════════════╪═══════╪═════════════════╡│ Uruguay    │1930           │ 4-2   │ Argentina       │├────────────┼───────────────┼───────┼─────────────────┤│ Italy      │1934           │ 2-1   │ Czechoslovakia  │├────────────┼───────────────┼───────┼─────────────────┤│ England    │1966           │ 4-2   │ Germany FR      │├────────────┼───────────────┼───────┼─────────────────┤│ Germany FR │1974           │ 2-1   │ Netherlands     │├────────────┼───────────────┼───────┼─────────────────┤│ Argentina  │1978           │ 3-1   │ Netherlands     │├────────────┼───────────────┼───────┼─────────────────┤│ France     │1998           │ 3-0   │ Brazil          │└────────────┴───────────────┴───────┴─────────────────┘Who are the top scorers across all the World Cups?MATCH (player)-->(stats)-[:SCORED_GOAL]->(goal),      (stats)-[:IN_MATCH]->()<-[:CONTAINS_MATCH]-(wc:WorldCup)WHERE goal.type IN [ goal ,  penalty ]WITH  player.name AS player, count(*) AS goals,       collect(DISTINCT wc.year) AS competitionsUNWIND competitions AS competitionWITH player, goals, competition ORDER BY player, goals, competitionRETURN player, goals, collect(competition) AS competitionsORDER BY goals DESCLIMIT 5╒════════════════════════════════════╤═══════╤═════════════════════╕│ player                             │ goals │ competitions        │╞════════════════════════════════════╪═══════╪═════════════════════╡│ Miroslav Klose                     │16     │[2002,2006,2010,2014]│├────────────────────────────────────┼───────┼─────────────────────┤│ Ronaldo                            │15     │[1998,2002,2006]     │├────────────────────────────────────┼───────┼─────────────────────┤│ Gerd Mueller                       │14     │[1970,1974]          │├────────────────────────────────────┼───────┼─────────────────────┤│ Just Fontaine                      │13     │[1958]               │├────────────────────────────────────┼───────┼─────────────────────┤│ Pelé (Edson Arantes Do Nascimento) │12     │[1958,1962,1966,1970]│└────────────────────────────────────┴───────┴─────────────────────┘Who’s the top scorer playing in the 2018 World Cup?MATCH (player:Player)-->(stats)-[:SCORED_GOAL]->(goal),      (stats)-[:IN_MATCH]->()<-[:CONTAINS_MATCH]-(wc:WorldCup)WHERE goal.type IN [ goal ,  penalty ]WITH  player, count(*) AS goalsORDER BY goals DESCMATCH (player)-[:IN_SQUAD]->(squad:Squad {year: 2018}),      (squad:Squad)<-[:NAMED_SQUAD]-(country)RETURN player.name, country.name, goalsLIMIT 5╒═══════════════════╤══════════════╤═══════╕│ player.name       │ country.name │ goals │╞═══════════════════╪══════════════╪═══════╡│ Thomas Mueller    │ Germany      │10     │├───────────────────┼──────────────┼───────┤│ Cristiano Ronaldo │ Portugal     │9      │├───────────────────┼──────────────┼───────┤│ James Rodriguez   │ Colombia     │6      │├───────────────────┼──────────────┼───────┤│ Luis Suarez       │ Uruguay      │5      │├───────────────────┼──────────────┼───────┤│ Tim Cahill        │ Australia    │5      │└───────────────────┴──────────────┴───────┘Next StepsWe hope you enjoy the dataset and if you have any questions or suggestions on what we should do next let us know in the comments or send us an email to devrel@neo4j.com.We encourage you to take the data and either build your own APIs or applications or analysis notebooks on top of it. We’d love to hear all about your ideas. You can use libraries like py2neo, spring-data-neo4j, neo4j-client or neode. We, ourselves are really excited about GraphQL, so that’s what we build next — A World Cup API in GraphQL using the GRANDstack.io. Stay tuned.;Jun 20, 2018;[]
https://medium.com/neo4j/the-making-of-a-network-chart-7b4f61aaccd6;sweemengFollowJun 14, 2018·4 min readThe making of a network chartThis post explains how we took the investigative journalism data from the Sinar Project and imported it into Neo4j for further analysis.PrequelNote: This is actually older, but I never showed the method how we built the graph, how to organize data, and why our method works for this.Image export of query from neo4jGraph is one more interesting data structure available to us software developers. It began with the question whether a person can cross all the bridges in Königsberg only once, which was solved by Euler in 1736 (btw. you can’t).Konigsberg bridges.pngPermission is granted to copy, distribute and/or modify this document under the terms of the GNU Free Documentation…en.wikipedia.orgGraph is used for the path finding problem, which is why shortest distance algorithm is best expressed with a graph. But it turns out relationships between people can be well represented by graphs! For example, below are people involved directly in the company 1mdb”.Showing relationship between 1mdb and peopleThe relevationEnough of theory. Going to the present (or actually 3 years back — yes I procrastinated). I worked on an API for the Sinar Project that stores the connections between people and organizations in Malaysia. It is build on a standard called popolo.Here is an example API call for that data.An example query to the APIAs you can see from the query results from our API, the memberships field has a set of string ids, those are foreign keys to membership objects.Query in membershipsWithin the membership object there are a foreign key to an organization and to a person. As you can see this represents a relationship. In graph theory terms this will be an Edge, and person/organization will be a Node. You can represent this as a graph!If you follow each person or organization, it can go on and on because a person has multiple memberships to organizations, each organization has many members. That’s one reason why representing this data as graph is very useful.The hackMy favorite way to work with a graph is via a tool called Neo4j. Networkx can work too, but we have at least 4552 people, 3475 memberships, 351 organizations, using the tools can get complicated quickly.One reason I like Neo4j is, that it has an interactive query workbench. It uses Cypher, a very expressive query language for graphs. For networkx we needed to write different Python scripts/functions for different queries. Which is cool if we have fixed questions. A query language is useful if we are exploring data.But the thing is, our data was not stored in Neo4j by default. We needed to massage the data into a form usable by Neo4j for data import. You can get the python script at https://github.com/sinar/popit_relationship.We use py2neo because it provides an abstraction that I am comfortable with. Data is fetched using the requests library.Before we start here’s a few thingThere are 4 type of object that we store in popit that are relevant here. Organization, Person, Post and MembershipAll the data is accessible via the HTTP JSON API. https://api.popit.sinarproject.org/docsThe process is essentiallyPerson and Organization stored as NodePost is stored as a node, a post is a thing like CEO, CFO etcMembership is stored as a relationship with labels.Most of the objects in popit have start_date and end_date. This is stored as timestamp for both Nodes and RelationshipsEach have popit_id property which is the id of the object stored in popit. It is useful for debuggingHow data from popit is represented in neo4jIn the endThere is a good point to this exercise. From this research done by the Sinar Project, having a way to visualize relationships is very useful to investigate transparency issues. 1MDB as a example shows that some director is member of the board of multiple organizations involved. It is not a smoking gun for corruption but a strong indicator of one.What we want to show is:If you have foreign key in database, it can often be a relationshipA graph database can be expressive, more importantly it can visualize relationships between objects.Tools like Neo4j can have strong impact on real world issues.Though not every problem is similar to government transparency, so one should pick and choose parts of what we did here.The thing we can do better in the future is:Standardize on naming convention for relationshipsPost should be just label on a relationship.Querying time information — it would be nice to see the evolution of an issue.Source:Research on data and 1MDB at https://sinarproject.org/transparency/research-notes/uncovering-1mdb-with-popit-open-dataSource code of data loader is at https://github.com/sinar/popit_relationshipThe popit People and Org API” is publicly accessible at https://api.popit.sinarproject.org/docsThe popit source code is at https://github.com/sinar/popit_ng;Jun 14, 2018;[]
https://medium.com/neo4j/this-week-in-neo4j-7th-july-2018-e3a7cecfe4da;Mark NeedhamFollowJul 7, 2018·3 min readThis Week in Neo4j — 7th July 2018Neo4j Morpheus, Data Structures and Algorithms with Dr Jim Webber, Google Cloud Launcher Demo, English to Cypher with TensorflowWelcome to this week in Neo4j where we round up what’s been happening in the world of graph databases in the last 7 days.If you have suggestions of things you’d like to see in future editions or any questions let me know.Cheers, MarkFeatured Community Member: Estelle JoubertThis week’s featured community member is Estelle Joubert, Associate Professor at Dalhousie University. Estelle is a musicologist, which means that she studies music and culture, and has a particular focus on opera and political theory.Estelle is the principal investigator for a large-scale team-run project entitled Opera and the Musical Canon, which uses Neo4j. The database is used to visualize relationships between people (composers, singers, publishers) and operatic objects (scores, reviews, images), offering a nuanced ‘picture’ of how operatic fame was generated prior to 1800.Estelle was recently interviewed on the Graphistania podcast where she explains the project in more detail and describes some of the queries the projects aims to answer which made them switch over from a relational database.On behalf of the Neo4j community, thanks for all your work and good luck with your project Estelle!Follow Estelle on TwitterWeave Together Graph and Relational Data in Apache SparkThe video from David Allen and Amy Hodler‘s Spark Summit talk about Neo4j Morpheus is now available.In the talk they explain this upcoming tool which will make it possible to weave together graph and relational data in Apache Spark. They go on to explain use cases where this approach will work well, such as getting a 360 degree view of a customer, before doing some live demos of the technology.Neo4j Morpheus is still in early access mode, so send an email to devrel@neo4j.com if you’d like to participate and I’ll get you in touch with the right people.English -> Cypher using a neural networkIn David Mack‘s latest post he shows how to translate English to Cypher using the magic of sequence-to-sequence translation.The trained model is able to perform reasoning tasks such as recalling properties of objects, counting, finding shortest paths, and determining if two objects have a relationship.Read the blog postNeo4j Internals: Linked lists, trees, and hash maps.Dr Jim Webber has started a series of videos where he explains the data structures and algorithms used by Neo4j.He starts with a look at linked lists, trees, and hash maps.How deletes work in Neo4jThis week from the Neo4j Knowledge base we have an article explaining how deletes work in Neo4j.This article explains what happens when you delete nodes, relationships, and properties, and why, contrary to expectations, you will see the amount of space taken on the filesystem increase when doing bulk delete operations.Learn how deletes workGoogle Cloud — Neo4j Causal Cluster Launch DemoDavid Allen has created a video in which he demos how to launch a new Neo4j Causal Cluster on Google Cloud Platform.This uses the Launcher capability of Google Cloud along with the Deployment Manager to deploy a cluster of three Neo4j instances for highly-scalable graph queries.Analysing the World Cup with Neo4j BloomWith the release of Neo4j Bloom last week I thought I’d make a quick video showing how to use it to explore the World Cup Graph.I show how to describe some simple graph patterns, visually detect England’s hat trick scorers, and conclude by demonstrating how you can write your own custom Cypher queries and expose them as search phrases.;Jul 7, 2018;[]
https://medium.com/neo4j/neo4j-browser-embraces-the-monaco-editor-bc8415cac22e;GregFollowMar 9, 2021·3 min readNeo4j Browser Embraces the Monaco EditorImagine having a bit of VS Code at your fingertips when writing Cypher queries, and then telling us all about it.Neo4j Browser 4.2.4 launched last week, and with it brings exciting changes to improve your experience when writing and running Cypher queries. The latest Browser is available to use in Neo4j Desktop now and also at browser.graphapp.io.New Cypher EditorThe biggest change this release is the adoption of Monaco, the editor that powers VS Code. Monaco is now used by the Cypher editor you know and love and will allow us to keep growing and delivering new functionality. Highlights this release include syntax highlighting improvements and better autocomplete for browser commands.Watch the demo video to take a look at the changes:New detailed Cypher Editor DemoHere is a quick list:Command palette (F1)Quick comment/uncommentMoving query parts aroundFolding, indentationBetter highlighting… and more (see the video).Reusable Result FramesThe introduction of the new editor allows us to introduce a frequently requested feature we call reusable frames. Now, when you need to edit the query of an existing result frame to fix a mistake, you can do that directly in the result frame and update the results in situ.The reusable frame maintains its own local history of commands as well as updating the main Cypher editor, should you need to instantiate a new result frame from there.(Fullscreen) inline result frame editorSo it takes on a more notebook-like” approach to having cells with results that you reiterate on. And useful especially for teaching or focused work is to make that result frame full-screen with inline editing and querying, as you can see in the animation above.We continue to look at ways to improve the query editing and execution experience and would love to hear your feedback on this latest addition. Which you now can do from within Browser (see below).Improved Favorites ManagementThe last new feature this release improves the way favorites are managed. The overall process of creating and managing favorites has been streamlined to be easier to perform, including:Creating or updating your favoritesCreating a new empty favoriteIndividual favorite changesBulk favorite changesFavorite Statement ManagementFinally, this release also contains a performance improvement when visualizing results with large numbers of overlapping paths. For full details of the type of problematic queries it addresses, see Dan Flavin’s post on waiting for large query results in browser.Where’s My Neo4j Cypher Query Results? 😠 ⚡️ ⁉️Why a Cypher query run in the Neo4j Browser may not return in a reasonable amount of time, what is happening, and what…medium.comWe Love to Hear Your FeedbackFinally, we’ve introduced an easier way for you to:See new feature announcements in-productSubmit feedbackRequest featuresSee what others are talking about.Look out for the new notification bell in Browser’s Help & Learn sidebar, which also lights up if there are updates from us.Feedback Button and Updates in SidebarUX Research ParticipationIf you can spare an hour of your time and are interested in helping us make Neo4j Browser and our other tools even better for everyone, then we’d love your help.Please consider participating in one of our user research sessions by filling out this Google form.You will be compensated for your time.Photo by Vlad Hilitanu on UnsplashThanks for reading and we look forward to updating you on the next release of Neo4j Browser and our other tools!;Mar 9, 2021;[]
https://medium.com/neo4j/hands-on-graph-data-visualization-bd1f055a492d;Michael HungerFollowApr 13, 2018·7 min readHands on Graph Data VisualizationUsing Graph Visualization Frameworks with the Neo4j Graph DatabaseIt turned out that there is much more content on graph visualization that it fits into one article, so we will turn this into a series of indefinite length.We hope you enjoy it. Please let us know in the comments.Introduction: Querying Data from Neo4jA graph database stores its data as nodes and relationships each with type and attributes. That makes it easy to represent connected, real world information to find new insights in the data.To visualize the stored data, we often use Javascript data visualization frameworks.This time, we’ll be not drawing charts or maps but graphs, aka. networks. You can do the former easily too by using aggregation or geo-information drawn from the graph but that’s for another time.To query a Neo4j database we use Cypher a pictorial graph query language, that represents patterns as Ascii Art.Where we …​MATCH (a:Node)-[con:CONNECTED_TO]->(another:Node)RETURN a, con, anotherIf you haven’t installed Neo4j, you can do that quickly with Neo4j-Desktop. After installation just create a project and a graph, start up the engine and open Neo4j Browser (which itself is a React Application). In the command line on top, type :play movies graph and click on the 2nd slide the create script, run it once and you should have a basic dataset for playing around.You can also just spin up a Neo4j Sandbox which have many different datasets available, e.g. Twitter, Paradise Papers, Movie recommendations and more. For an empty database just use a Blank Sandbox” and then create the movie database as above. Note your password, the server ip-address and the bolt port (not HTTP!).So the simplest and fastest query is:MATCH (first)-->(second) RETURN id(first) AS source, id(second) AS targetThis query can pull a million relationships from a graph in a single second, much more than most visualization frameworks can visualize.It only retrieves the graph structure which is useful when zoomed out.For more details we might want to know labels of nodes and types of relationships, so we can color each accordingly.MATCH (first)-[r]->(second)RETURN { id: id(first), label:head(labels(first)) } AS source,       { type:type(r) } as rel,       { id: id(second), label:head(labels(second)) } AS targetIf we want to we can return more detail, e.g. caption, sizes, clusters, weights:MATCH (first)-[r]->(second)RETURN { id: id(first), label:head(labels(first)),          caption:first.name, size:first.pagerank,          cluster:first.community } AS source,       { type:type(r), size:r.weight } as rel,       { id: id(second), label:head(labels(second)),         caption:second.name, size:second.pagerank,          cluster:second.community } AS targetThose queries return incrementally more data, so perhaps it’s more sensible to pull that additional information only if the user zooms in.Then we can provide a list of already loaded, visible ids (with the first query) as filter:MATCH (first)-[r]->(second)WHERE id(first) IN $ids AND id(second) IN $idsRETURN     { id:id(first),label:head(labels(first)),caption: first.name,         size:first.pagerank, cluster:first.community } AS source,    { type:type(r), size:r.weight } as rel,    { id:id(second),label:head(labels(second)),caption:second.name,       size:second.pagerank, cluster:second.community } AS targetYou can pre-set params in Neo4j-Browser with :param ids:[1,2,3]For the full detail information of a node (with all possible properties), we can load that data when the user hovers or clicks on the node:MATCH (node)WHERE id(node) = $idRETURN { id: id(node), label:head(labels(node)),          caption:node.name, size:node.pagerank,          cluster:node.community,         outDegree: size((node)-->()), inDegree: size((node)<--()),          .* } AS detailsLoading Data with the Javascript DriverQuerying data with the Neo4j Javascript Driver is very performant and quite straightforward, I’ll mention the one only weirdness as we get there.You can add the driver via npm, or via CDN (rawgit) to your project.Let’s start by querying with the node shell:npm install neo4j-drivernodeimport module, create a driverconst neo4j = require(neo4j-driver)# for local installationconst driver = neo4j.driver(bolt://localhost,                             neo4j.auth.basic(user, password))# for sandboxconst driver = neo4j.driver(bolt://serverip:boltport,                             neo4j.auth.basic(user, password))If the connection fails, please check thatyour server is running,the address (esp. in the remote case) andthe authentication.create a sessionconst session = driver.session({database: dbName })get a single nodevar query =  MATCH (n) RETURN n LIMIT 1 var _ = session.run(query).then(console.log)result datastructure{ records:   [ Record {       keys: [Array],       length: 1,       _fields: [Array],       _fieldLookup: [Object] } ],  summary:   ResultSummary {     statement: { text: MATCH (n) RETURN n LIMIT 1,                   parameters: {} },     statementType: r,     counters: StatementStatistics { _stats: [Object] },     updateStatistics: StatementStatistics { _stats: [Object] },     plan: false,     profile: false,     notifications: [],     server: ServerInfo { address: localhost:7687,                           version: Neo4j/3.3.4 },     resultConsumedAfter: Integer { low: 0, high: 0 },     resultAvailableAfter: Integer { low: 4, high: 0 } } }We see the attribute records, each of which has a keys, a get(key) method, and private _fields and _fieldLookup attributes. Also there is a summary with resultConsumedAfter/resultAvailableAfter timing, statement, counters, plan, notifications, serverInfo.return keys and values for each recordvar _ = session.run(query).then(           result => result.records.forEach(              r => console.log(r.keys, r.keys.map(k => r.get(k)))))We see that unexpected Integer type, which is the  number  type of the Neo4j driver, due to Javascripts inability to represent whole numbers > 2^53 You can call value.toNumber() on it, more details in the docs/readme of the neo4j-javascript-driverturn single record into an objectvar query =  `MATCH (n:Movie)   RETURN id(n) as id, labels(n) as labels, n.title, n.released    LIMIT 1`var _ = session.run(query).then(result => console.log(result.records[0].toObject()) ){ id: Integer { low: 0, high: 0 },  labels: [ Movie ],  n.title: The Matrix,  n.released: Integer { low: 1999, high: 0 } }performance testA small performance test on my machine using a cross product which we limit to 1M results with 3 values each.var query =        MATCH (n),(m),(o) RETURN id(n), id(m),id(o) LIMIT 1000000var start = new Date()var _ = session.run(query).then(result => console.log(result.records.length,new Date()-start, ms ))> 1000000 1970 msIt shows that it takes a bit less than 2 seconds to retrieve that 1 million rows. If we were processing that data in a streaming manner and not build up a large array, it is much faster (less than 1 second).var query = `MATCH (n),(m),(o)              RETURN id(n), id(m),id(o) LIMIT 1000000`var count = 0var start = new Date()var _ = session.run(query).subscribe({onNext: r => count++, onCompleted : () => console.log(count,new Date()-start, ms )})> 1000000 912 msclose session & driversession.close()driver.close()Ok, now we should know how to query Neo4j.Code on WebpageWithin the browser our code will mostly look like this. All additional code will be setup or configuration of the visualization framework.If you connect to your live database from a webpage, make sure to set up a read-only account. Alternatively you can ask the user for login and password in a form. Upcoming versions of Neo4j will add possibilities of more restrictions on data.<script src= https://unpkg.com/neo4j-driver ></script><script>  // create driver & session  const driver = neo4j.driver( bolt://localhost ,                    neo4j.auth.basic( user ,  password ))  const session = driver.session({database: neo4j })  const start = new Date()  // run query  session    .run(MATCH (n)-->(m) RETURN id(n) as source, id(m) as target LIMIT $limit, {limit: neo4j.int(200)})    .then(function (result) {      // turn records into list of link-objects (can have different shapes depending on framework)      // note that we turn the node-ids into javascript ints      const links = result.records.map(r =>           { return {source:r.get(source).toNumber(),                     target:r.get(target).toNumber()}})      // close session as soon as we have the data      session.close()      // log results & timing      console.log(links.length+  links loaded in                    +(new Date()-start)+  ms. )      // gather node-ids from both sides      const ids = new Set()      links.forEach(l => {ids.add(l.source)ids.add(l.target)})      // create node-array      const nodes = Array.from(ids).map(id => {return {id:id}})      // create  graph  representation      const graphData = { nodes: nodes, links: links}      // pass graph data to visualization framework       // (here 3d-force-graph)      const elem = document.getElementById(3d-graph)      ForceGraph3D()(elem).graphData(graphData)    })    .catch(function (error) {      console.log(error)    })</script>Game of Thrones Graph with Weights and ColorsJavascript Graph Visualization FrameworksMost graph visualization frameworks have some kind of graph” API to take a number of nodes and a number of links or relationships and then render them either with SVG, Canvas or WebGL. Some of them take plain objects as input, others have an API to add nodes and relationships.Most of the frameworks offer several options for styling, interaction, graph-layout algorithms etc. When we discuss each in turn we will point out the details and where to find more information.Here are the open-source frameworks we’ll look in this series:There are a number of other, commercial frameworks and tools, that we also will try to introduce in this series, with the help of each vendor.yWorks yFilesLinkurio.us OGMAKeylinesTom Sawyer PerspectivesGraphistryGraphileonMost of them have Neo4j support built-in.In the next post next week, William Lyon will introduce neovis.js the Neo4j wrapper he created around vis.js.If you have questions around graph visualization, make sure to join the Neo4j-Users Slack and ask in the #help-viz channel.Stay Tuned;Apr 13, 2018;[]
https://medium.com/neo4j/streaming-graph-loading-with-neo4j-and-apoc-triggers-188ed4dd40d5;David AllenFollowSep 5, 2018·5 min readStreaming Graph Loading with Neo4j and APOC TriggersIn preparation for GraphConnect coming up, and an idea I have for the hackathon there, I recently needed to look into how to do triggers in Neo4j.In this article, we’ll cover what triggers are, how you can use them to do streaming data loading, and finally we’ll cover the details of how they work under the covers.What’s a Trigger?If you haven’t worked with triggers before, a trigger is a database method of running some action whenever an event happens. You can use them to make the database react to events, rather than passively accept data, which makes them a good fit for streaming data, which is a set of events coming in.Triggers need two pieces:A trigger condition (which event should the trigger fire on?)A trigger action (what to do when the trigger fires?)Triggers are available in Neo4j through Awesome Procedures on Cypher (APOC), and you can find the documentation on them here. Let’s jump in.My Use Case: Streaming Data LoadingI use APOC triggers from time to time for data loading. Imagine you have records flowing into neo4j over time, that represent friend relationships in a social network:CREATE (:FriendRecord { p1:  David , p2:  Mark  })CREATE (:FriendRecord { p1:  Mark , p2:  Susan  })CREATE (:FriendRecord { p1:  Bob , p2:  Susan  })These might be coming from an external application, or a messaging queue like Kafka.If this were bulk data, people might batch these into CSV and write some LOAD CSV code, which loads the data to a proper graph structure from the start. That won’t work for a streaming setup though, where the data arrival is continuous.It’s also important to separate the loading (in a trivial format) from the reformatting, because if the data is coming from some other app across Kafka, that app doesn’t know or care what your graph model is.So imagine your database is getting a constant stream of these FriendRecord” nodes, but what really want is a proper graph like this:(:Person { name:  David  })-[:FRIENDS]->(:Person { name:  Mark  })That’s our use case for triggers — streaming graph loading and transformation! Let’s get going.APOC SetupFirst, you have to install APOC into some Neo4j database you’re using. APOC then requires that in order to use triggers, you must set:apoc.trigger.enabled=true in your neo4j.conf, so don’t forget to do that.Simple Trigger ExampleHere’s the trigger we will be working with. This defines both the trigger condition, and the trigger action.CALL apoc.trigger.add(loadFriendRecords,        UNWIND apoc.trigger.nodesByLabel({assignedLabels}, FriendRecord) AS node     MERGE (p1:Person { name: node.p1 })     MERGE (p2:Person { name: node.p2 })     MERGE (p1)-[:FRIENDS]->(p2)     DETACH DELETE node ,           { phase: after })Let’s go through the arguments. The first argument loadFriendsRecord” is the name of the trigger. We can later use this name to ask for the status of the trigger or delete it if we need to. The second argument is a cypher query that is the trigger condition and action. That’s where all of the meat here is:UNWIND apoc.trigger.nodesByLabel portion fetches nodes with the label FriendRecord”. We’ll explain down below in depth how this works.The assignedLabels” part in the map indicates that the trigger condition fires when the label gets assigned to a node, or when it’s first created.UNWIND’ing that list, now our query has all of the recently created FriendRecord nodes. The rest of the query just specifies what to do with them. We’ll merge 2 new Person nodes, connect them, and delete the original FriendRecord node, which is what gets us from our incoming records to our target graph format.Finally, the third argument { phase: ‘after’ } allows you to control when your action fires. Your choices are before”, after”, or rollback”, which correspond to the lifecycle possibilities of a transaction in Neo4j. Because my trigger wants to modify data, the phase must be after”. (Trying to do that delete statement before the transaction even committed wouldn’t work). If you needed to take some action before a new PersonRecord came in, then you’d want before”. And if you wanted to do something on the failure of creating a PersonRecord, you’d use rollback”.How does it work?The core Neo4j database includes a facility for Transaction Event Handlers”. Neo4j plugins can create transaction event handlers and be called whenever the database commits a transaction. This basic facility lets any piece of software observe what’s happening in the data as it flows by.APOC Grabs TransactionsWhen you enable triggers in APOC, this causes APOC to register a transaction event handler with the database and start listening to all of those changes. Without any triggers, this listener does nothing at all.When you register a trigger, it comes with a selector”. That’s the {assignedLabels} part. Every trigger also has a phase” as we described above.Triggers Execute Against TransactionsSo APOC maintains a list of triggers, each with a selector, phase, and cypher query. And it has a transaction event handler. The rest is simple: every time a transaction comes into Neo4j, APOC runs the relevant triggers.Remember each transaction has three possible phases: before, after, and rollback. APOC gets each of these events, figures out which triggers want to execute in that phase, and then runs the cypher code you specify. Just before running this query, it sets on your query a special set of parameters into a map. Those parameters give details about the transaction being executed, such as which nodes were added and removed, and so on.So the cypher query that executes has pretty much full access to the data in the transaction!Filtering out relevant dataThe last piece to understand is this part in our original trigger:apoc.trigger.nodesByLabel({assignedLabels}, FriendRecord)This is at the heart of the trigger, because it defines when we want our code to execute. Whenever a new node is assigned the FriendRecord label!The first argument is a map that can contain assignedLabels, removedLabels, removedNodeProperties, or any combination of the three. This map can look confusing to beginners, because notice that the map keys have no values. They don’t need them APOC is only concerned with whether the key exists in the map.In the last step, we explained that when a trigger executes it gets passed a lot of the transaction state. With that state, this function then can quite simply pull out only the nodes that got assigned a FriendRecord” label. That becomes the array passed to UNWIND, and the rest is your code.ConclusionsWe’re only covering a small part of what triggers can do here, to give you a taste, specific to this use case. Above, we talked about different lifecycle phases (before/after/rollback) and also different selector types (assigned node properties, assigned relationship properties, deleted node labels, etc).These can be combined quite flexibly to implement various kinds of business logic in neo4j, data quality cleanups, and also dependent transactions. We’d love to hear how other people are using them at the Neo4j community forum, so please drop by and let us know.Have a look for yourselfBecause APOC is open source, you can of course always have a look for yourself at how it’s done.;Sep 5, 2018;[]
https://medium.com/neo4j/how-to-automate-neo4j-deploys-on-google-cloud-platform-gcp-6e123eccfd5e;David AllenFollowFeb 21, 2019·6 min readHow to Automate Neo4j Deploys on Google Cloud Platform (GCP)EDIT MAY 2020: Neo4j official documentation has published updated versions of this documentation.— — — — — — — —Neo4j already provides some documentation on its site for how to do deployments of Neo4j to common clouds, including Google Cloud. But in this article, I’ll provide sample shell scripts that can do this automatically for you.These are useful when you want to integrate Neo4j into your CI/CD pipeline and be able to create/destroy instances temporarily, and also just to spin up a sample instance. But really, if you can automate Neo4j deployment, then any other piece of software can make a Neo4j instance whenever it needs, which is extremely handy.If you have any questions, feedback, or want to discuss drop by this thread on the Neo4j Community site!Neo4j and Google Cloud Deployment ManagerRequirementsBefore we begin, you’ll need the gcloud command line interface program, which you can download and install with directions here. The gcloud CLI is the main way you can automate all things with GCP.It will also be necessary to authenticate your gcloud CLI, to make sure it can interact with your GCP projects.Google Cloud Deployment ManagerNeo4j provides Deployment Manager templates for Neo4j Causal Cluster (highly available clusters), and VM images for Neo4j Enterprise stand-alone. So first thing’s first, pick which one you would like to deploy. We’ll cover both in this article.Deployment Manager is really just a recipe for GCP that tells it how to deploy a whole set of interrelated resources. By deploying all of this as a stack we can keep all of our resources together, and delete just one thing when we’re done.ApproachHow to deploy a cluster is very simple: we just submit a new Deployment Manager job, pointing to the right template URL to tell GCP what to deploy. We then will provide various parameters to control how much hardware we’re using and so on.We’ll need to specify several common parameters, which you’ll see in the scripts below. Here’s an explanation of what they are.Machine Type: This is the GCP machine type you want to launch, which controls how much hardware you’re giving the database.Boot Disk Size/Type: you can use these parameters to control whether Neo4j uses standard spinning magnetic platters (pd-standard) or SSD disks (pd-ssd) as well as how many GB of storage you want to allocate. Note that with some disk sizes, GCP will warn that the root partition type may need to be resized if the underlying OS does not support the disk size. This warning can be ignored, because the underlying OS will recognize any size disk you give it without trouble.Zone: Where in the world you want to deploy Neo4j. The template supports any GCP zone, but in our example we’ll use us-east1-b.Project: This simply specifies which project ID you want to deploy into within GCP.Let’s get started. Simply run any of these scripts, and it will result in a Deployment Manger stack being deployed.Deploying Neo4j Enterprise Causal ClusterIn addition to the parameters listed above, because this is a clustered deploy, take note that we’re also using a parameter for Cores” and Read Replicas” to control how many nodes are in our cluster.#!/bin/bashexport NAME=neo4j-clusterPROJECT=my-gcp-project-IDMACHINE=n1-standard-2DISK_TYPE=pd-ssdDISK_SIZE=64ZONE=us-east1-bCORES=3READ_REPLICAS=0NEO4J_VERSION=3.5.3TEMPLATE_URL=https://storage.googleapis.com/neo4j-deploy/$NEO4J_VERSION/causal-cluster/neo4j-causal-cluster.jinjaOUTPUT=$(gcloud deployment-manager deployments create $NAME \   --project $PROJECT \   --template  $TEMPLATE_URL  \   --properties  zone:$ZONE,clusterNodes:$CORES,readReplicas:$READ_REPLICAS,bootDiskSizeGb:$DISK_SIZE,bootDiskType:$DISK_TYPE,machineType:$MACHINE )echo $OUTPUTPASSWORD=$(echo $OUTPUT | perl -ne m/password\s+([^\s]+)/ print $1)IP=$(echo $OUTPUT | perl -ne m/vm1URL\s+https:\/\/([^\s]+):/ print $1 )echo NEO4J_URI=bolt+routing://$IPecho NEO4J_PASSWORD=$PASSWORDecho STACK_NAME=$NAMEAfter all of the setup where we declare parameters of what we’re deploying, the heart of the entire script is just one simple call to gcloud deployment-manager deployments create which does all of the work.We capture the result in the variable OUTPUT, which contains a lot of text telling us about our deployment. We then process that with a little bit of perl to pull out the password and IP address of our new deployment, because it will have a strong randomly assigned password.What is nice about the Google deployment process is that this command will block and not succeed until the entire stack has been deployed and is ready. This means by the time you get that IP address back, you’re ready to go, and don’t need to check if Neo4j is up, because it is!If you lose these stack outputs (IP, password and so on) they will also appear in your Deployment Manager window within the GCP console, so you can refer back to them.To delete a deployment created in this way, you just need to take note of the STACK_NAME that we deployed. I use a short script to delete deployments like this:#!/bin/bashPROJECT=my-google-project-idif [ -z $1 ]  then   echo  Usage: call me with deployment name    exit 1figcloud -q deployment-manager deployments delete $1 --project $PROJECT# OPTIONAL!  Destroy the disk# gcloud --quiet compute disks delete $(gcloud compute disks list --project $PROJECT --filter= name~$1  --uri)When you delete Neo4j stacks on GCP, they intentionally leave their GCP disks behind, to make it hard for you to accidentally destroy your valuable data. But because these disks are left behind, you may wish to uncomment that last line, which will clean up those disks if the deploy is truly temporary and the disks aren’t wanted.Deploying Neo4j Enterprise (or Community) Stand AloneThis will create a single instance of Neo4j without high-availability failover capabilities, but it’s a very fast way to get started. For this deploy, we don’t use Deployment Manager but just create a simple VM and configure its firewall/security rules.Because we’re not using Deployment Manager for this one, this also provides an example of polling and waiting until the VM service comes up, and then changing the Neo4j default password when it does. You’ll notice at the top of the script we choose a random password by running some random bytes through a hash.The launcher-public project on GCP hosts Neo4j’s VM images for GCP. In this example we’re using neo4j-enterprise-1–3–5–3-apoc, but other versions are available too. By substituting a different image name here, you can use this same technique to run Neo4j Community.#!/bin/bashexport PROJECT=my-gcp-project-idexport MACHINE=n1-standard-2export DISK_TYPE=pd-ssdexport DISK_SIZE=64GBexport ZONE=us-east1-bexport NEO4J_VERSION=3.5.3export PASSWORD=$(head -n 20 /dev/urandom | md5)export STACK_NAME=neo4j-standaloneexport IMAGE=neo4j-enterprise-1-3-5-3-apoc# Setup firewalling.echo  Creating firewall rules gcloud compute firewall-rules create  $STACK_NAME  \    --allow tcp:7473,tcp:7687 \    --source-ranges 0.0.0.0/0 \    --target-tags neo4j \    --project $PROJECTif [ $? -ne 0 ]  then   echo  Firewall creation failed.  Bailing out    exit 1fiecho  Creating instance OUTPUT=$(gcloud compute instances create $STACK_NAME \   --project $PROJECT \   --image $IMAGE \   --tags neo4j \   --machine-type $MACHINE \   --boot-disk-size $DISK_SIZE \   --boot-disk-type $DISK_TYPE \   --image-project launcher-public)echo $OUTPUT# Pull out the IP addresses, and toss out the private internal one (10.*)IP=$(echo $OUTPUT | grep -oE ((1?[0-9][0-9]?|2[0-4][0-9]|25[0-5])\.){3}(1?[0-9][0-9]?|2[0-4][0-9]|25[0-5]) | grep --invert-match  ^10\. )echo  Discovered new machine IP at $IP tries=0while true  do   OUTPUT=$(echo  CALL dbms.changePassword($PASSWORD)  | cypher-shell -a $IP -u neo4j -p  neo4j  2>&1)   EC=$?   echo $OUTPUT      if [ $EC -eq 0 ] then      echo  Machine is up ... $tries tries    breakfi  if [ $tries -gt 30 ]  then    echo STACK_NAME=$STACK_NAME    echo  Machine is not coming up, giving up     exit 1  fi  tries=$(($tries+1))  echo  Machine is not up yet ... $tries tries   sleep 1doneecho NEO4J_URI=bolt://$IP:7687echo NEO4J_PASSWORD=$PASSWORDecho STACK_NAME=$STACK_NAMEexit 0To delete an instance created like this, again we take note of the STACK_NAME and just use another utility script:#!/bin/bashexport PROJECT=my-google-project-idif [ -z $1 ]  then   echo  Missing argument    exit 1fiecho  Deleting instance and firewall rules gcloud compute instances delete --quiet  $1  --project  $PROJECT  && gcloud compute firewall-rules --quiet delete  $1  --project  $PROJECT exit $?;Feb 21, 2019;[]
https://medium.com/neo4j/an-analysis-of-twitter-influencers-in-the-field-of-data-science-big-data-48b0809027ef;John SwainFollowOct 16, 2015·16 min readAn analysis of Twitter Influencers in the field of Data Science & Big DataObjectiveThe objective of this post is to illustrate how community detection and graph analysis can be utilized to locate influential users in a given domain of interest , in this instance, related to data derived from social media, and in particular Twitter.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comThere are many real world use cases for this kind of analysis and there are a large number of tools for targeting consumers and professionals through Twitter. The real challenge is often identifying actual influencers, and this is the problem this project seeks to address.Locating Influential UsersThere are several problems which make finding influencers difficult (see a previous post).Briefly, there are two main categories of problem:There is a large volume of noise in all interesting conversations. That is to say as soon as a subject becomes popular or valuable it attracts a large number of spammers and bots, which can obscure the valuable contributors.Conventional search tools use hashtags and text phrases to search for tweets and users mentioning the topic of interest. This is useful for tracking a specific campaign which utilises a hashtag, however it is very difficult to configure to research a wider conversation.Therefore, the challenge is to cut through the noise and discover interesting conversations which indicate who is influential without having to perform narrow hashtag or boolean logic searches which limit the searches to what is already known beforehand.MethodologyOODA LOOP for Social Media AnalysisTo solve this problem, we adapted a concept from Military Doctrine called the OODA Loop.The figure below displays an adapted version of the OODA Loop as it applies to social media analysis.OODA Loop for Social Media Analysis.The first point to note is that time is identified as being the dominant parameter”. In social media time is a critical factor in a way that it is not in many other aspects of business and political/strategic endeavours. Whether the requirement is to react to unfolding events, demonstrate thought leadership or to be leading the field in breaking news the ability to observe and react to unknown events makes the application of this military technique appropriate for social media.John Boyd the originator of the OODA Loop put it this way:In order to win, we should operate at a faster tempo or rhythm than our adversaries…TempoIf you wish to locate who is influential on a given subject and (more importantly) what topics of conversation they are involved in and leading then establishing a tempo of analysis is critical. The volume and structure of the conversations taking place have a natural tempo at any given time. It could be daily or weekly or even hourly for big breaking stories. We have developed a method of analysis which allows analysis to be undertaken in sync with the tempo of social media.This will be illustrated by showing how the OODA LOOP process is applied to the Twitter conversation about Big Data and Data Science. Our focus is on the first two stages Observation and Orientation and how the feedback from Decisions affect the development of the process and increasing acquisition of knowledge. Examples will be provided of how the Actions taken might influence the process.Implicit GuidanceAll analysis must start with an examination of the current understanding of the overall objectives and the best available domain knowledge at that point.This implicit guidance directs both the first Action you take and the way you start the process of Observation. In this case my objective is to discover who the important influencers are and what the topics of conversation are, so I am only starting with the Observation phase and not taking any initial Actions before the first phase of Orientation is undertaken.Therefore, we start with a wide ranging search term for all tweets which will broadly cover the topic of Big Data and Data Science. This is the initial search term which returns all tweets which contain the combination of words and phrases:(datascience OR data science”) OR (bigdata OR big data”) OR ( data AND (algorithm OR viz OR vis OR python OR SAS OR SPSS) ) OR (machinelearning OR machine learning”) OR rstats OR analytics OR data mining” OR artificial intelligence” OR AIOver a continuous period of 10 days (14 Sept — 25 Sept) the search collected 600k Tweets. This is sufficient or the purposes of this project, and in order to complete the initial analysis. It is important to note that the intention is not to find a ‘perfect’ query at this stage but just to find a good enough one to provide sufficient information for the Observation and Orientation phases.Initial ObservationThus far, the Implicit Guidance has directed how we configure our initial Observation Phase.In technical terms this is implemented by running a small software script that collects all matching tweets and stores them in a graph database (neo4j).A graph database stores information as a graph which records basic entities and the relationships between them. In this case the Tweets, Users, Hashtags & Links.For the analysis we simplify the graph by creating a simpler set of links shown by the red connections in this diagram which indicate that User 1 Mentioned User 2 and Retweeted User 3.There is a little more technical information in this post related to how the data is collected.Using this method we record the Conversations between and about users. This is the activity that is taking place inside the Observations process.In addition to our Implicit Guidance input there are external inputs happening continuously — these are the Outside Information” and Unfolding Circumstances” shown as inputs into the Observe phase of the OODA Loop.In this context these two inputs can be categorised as follows:Outside Information - general information that we were not aware of from other sources e.g. general news sources, other research etc.Unfolding Circumstances - the continually changing and developing situations taking place outside the system which have an impact on what happens to the conversations inside the system.The output from the Observe phase is the feed forward into the Orientation Phase. In the first iteration the whole graph content is fed forward.Initial Orientation & DecisionsThe initial Orientation phase is all about making simple decisions about how to reduce noise and potentially identify some quick wins in regards to useful Actions.To illustrate the problem of noise in a Twitter network of conversations here are two images showing how a few tweet bots pollute the network.The image on the left shows a network of conversations between users where each line from one User represents a Tweet which mentions or retweets the other User i.e. the red lines illustrated in the diagram above. It is possible to make out some structure and groupings of important users from this. The image on the right hand side shows the exact same layout but before the tweet bots have been removed. There are approximately 50 thousand users in this diagram therefore, it is easy to infer from the visualisations the amount of noise a small number of bots can generate.The diagram above displays the noise in the Twitter conversation.In the first iteration all the information including the noise is passed into the Orientation phase where we need to make some quick decisions about what to filter out.The Orientation is where the critical human analysis takes place. This is where various types of knowledge and experience are synthesised to produce output which are potential Decisions or Hypothesise for direct feedback or for testing.By Synthesis we are referring to combining different elements into a new product or output. The elements that are combined shape the way that we observe, decide and act. Therefore, it is critical that this process is undertaken consciously so that these elements inform the analysis and do not create systematic bias. This is an area where tempo plays an important part with fresh information being fed into the process regularly to avoid stagnation and groupthink. It is a process of continuous evaluation of fresh information not a contemplation of existing knowledge.In this case we are combining the following:Heritage — or tradition, what kind of organisation we are and what are our strengths and weaknesses. For example, a long established consultancy firm and a technology start up would have a very different heritage which shapes their thinking about which people would be best to engage with.Culture — or style of operating and conducting business. For example if our objective is to generate marketing value, would we do that by demonstrating thought leadership with a select group of influencers or do we want to reach millions of consumers with our brand message.New Information — information is not limited to that coming in from the Observation feed forward. There may be other information arriving from other sources that needs to be considered. For example, in the field of Data Science we may learn from other sources that the BBC are running a series of programmes and features on Artificial Intelligence, this may be a material factor in the analysis.Analysis — the assessment of the current state of our knowledge. In other words analysing just the information we have at this stage in the process and not falling into the trap of making overall judgements made on partial information. Stick to the process and feed forward the output to the next phase.In order to filter out the noise and non useful information there is a pipeline of data processing which can be tuned depending on the outcome of the Orientation phase.For example, the very first stage is to go through a series of iterations to filter out the obvious noise as illustrated in the diagram above. This process is shown in the following part of the OODA Loop flow diagram below:In simple terms a series of decisions can be tested by feeding back the decision directly to the Observation process (by passing the Action & Test) and then re-evaluating the Orientation. At the start of a Twitter conversation analysis we can apply some very simple filters which we know can prove very effective at cutting through a large element of the noise.Simple filtersFor example filtering out all users where the ratio of Retweets/Tweets is over 97% removes a large number of TweetBots without removing any genuine users.In practice there are a number of these kinds of filters based on similar simple metrics with some checks and failsafes which, when combined further reduce the amount of noise. It is important to note that this process can be undertaken by a single person over the course of a few cycles with some input from colleagues with domain expertise.More Sophisticated AnalysisOnce we have removed the obvious noise we are still left with a large graph of conversations some of which are still created by more sophisticated bots and networks of bots.The following illustration shows that some of these are easier to detect than others. Visual inspection is useful as the patterns that these Communities form can be clearly seen.Suspect Communities Highlighed in Black Genuine Communities in RedTh next blog posts will cover the details of how the statistical analysis of the graph reveals these rogue Uses and how they are removed from the analysis. In short, however, there are a further series of steps in the processing pipeline to remove these Users and configure the analysis to best suit the particular use case.It is important to note that all the filtering is non-destructive. That is to say, none of the data is removed from the graph, however is merely filtered out of the view of the graph that we analyse. There are two separate and complimentary methods of graph analysis which are employed during the Orientation phase. Different filters can be applied to the graph for the different methods:Visual Analysis — visual network representation (or maps) as shown above. For large graphs (millions of Users) we cannot visualise that amount of information even if the tools existed to manipulate such large graphs visually. Therefore, it is necessary to further filter the graphs that we chose to evaluate visually. It is possible reduce very large graphs to several thousand Users and still retain most of the important information about the conversation structure between influential Users.Graph Metrics and Statistical Analysis — The eventual output of the process will be lists of Users, Topics and Communities. These may be ranked by the most important or the most interesting which are measured in various ways which will be covered later. Depending on the nature of the graph structure and the particular use case a requirement may be to calculate these metrics based on a different filter than the one used to visualise the network map. For example if the requirement was to find Users who influence the general public it would be necessary to calculate the Rankings on the whole graph. However, if the requirement was just to find the Users who were influential amongst a community of other important or influential Users i.e. an expert community, it would be sensible to filter the graph to remove Users with a very small follower count before calculating the metrics.Several techniques based on graph theory or simple filtering can be used at this stage to further reduce the graph for analysis or visualisation. Each one removes certain nodes (Users) from the overall graph and they can be used in combination.Small Users — removing Users who have a small number of followers (say less than 100) will dramatically reduce the size of a graph. This is almost always useful for the visualisation of a graph however, it is only appropriate in certain cases when calculating ranking and statistics.Degree Range — the degree of a node in a graph is the number of connections it has. These can be In Degree or Out Degree depending on the direction and can be weighted to indicate multiple connections between the same nodes. For a Twitter graph the In Degree of a User is the number of times that some other User has Retweeted or Mentioned that User. The Out Degree is the number of times a User Retweets or mentions others. In simple terms a user with a high In Degree is likely to be an important or influential Users.Giant Component — The giant component of a graph, in it’s simplest form, is the biggest part of the graph that is joined together by at least one connection. Again this is almost always useful for visualisation, however a little more care needs to be taken before deciding to solely use the Giant component for metrics.Illustration of Giant ComponentThese (and other) graph filtering techniques can be used in combination to create the appropriate set of sub graphs for the evaluation. Every conversation is different although there are certain archetypes and patterns which recur. This process is one of human evaluation and a combination of expertise, experience and domain knowledge is required in finding the appropriate way to reduce the complexity of the incoming information.Finding Communities & TribesOverviewOnce the initial filtering of the graph is working the next stage is to evaluate the formation of Communities and Tribes with in the graph.The main conceptual idea of this analysis is that communities form between Users who share common interests and that these can be detected with community detection algorithms. This makes the information in the graph self organising to a large extent.The concept of Tribes and Buyer Personas is a hot topic at the moment. Buyer Personas are a way to categorise people by their interests and values which will then correlate with a brand or organisations products and services.In our project we have taken a novel approach which does not attempt to define the fundamental characteristics of people but simply detects what people are interested in now -or at least in the current period of analysis.This is implemented in the following way using definitions of Communities and Tribes.Cycle Times and TempoBefore defining Communities and Tribes it is critical to understand the concept of cycle times. All communities have a natural tempo based on the topics of interest to that community. So a business related community will have a daily/weekly/monthly tempo a sports community will have a tempo related to the frequency of games or tournaments a conference or festival will have a lead in period and a hectic few days of activity during the event. There are natural peaks and troughs in the volume of communication. The concept of the OODA Loop is to reacting quickly to unfolding events. Where there is a predictable rhythm or tempo to the events then the analysis works best in sync with this tempo.The nearest we can get to knowing what people are interested in now is what they are interested in during the latest cycle of analysis.CommunitiesA community is a group of Twitter Users who are identified by their pattern of communication (Retweets, Mentions, Replies) over a short period of time in sync with the tempo of the analysis. In simple terms a Community is a collection of people who communicate with each other during that time period. For the purposes of the analysis techniques employed in this analysis a Community is defined by an automated community detection algorithm for the period of time that the analysis takes place.MayorsCommunities are collections of Users and have a leader. The leader of each community is the User in that Community with the highest Pagerank measure. This User is defined as the Mayor of that Community.TribesTribes are a longer lasting concept and exist beyond each cycle of the analysis. Tribes also have Mayors and the members of the Tribe are the members of each community of which that Mayor was also the Mayor of the community.Here is an example.This is a section of the network map for 15th Sept where you can clearly identify a community with hadleywickham as the Mayor.The same pattern is repeated the following day, the 15 Sept.Whilst the visual inspection (for someone familiar with the Users in the map) indicates that this is a community with a shared interest further analysis is required to confirm this.For each community detected during each cycle a technique called Topic Analysis is conducted. This examines the text contained in the Tweets within the community, that is, the Tweets just between members of the identified community (Retweets, Mentions, Replies). Topic Analysis is a statistical technique for identifying a set of topics defined by the important words contained within a set of documents. In this case the document is defined by extracting the text from the set of Tweets within the community.Here is the set of Topics (defined by the ‘terms’ within each Topic) for the first day, 15th Sept. The terms list shows the top 3 Topics identified and associated with this community of users. Below that is a simple word cloud of the most frequent words used within the Tweets in the community.Topic analysis can provide a technique for much deeper analysis but at this stage it just provides confirmation that the subjects of conversation within the Community are a coherent indication of a shared community of interest.Here is the same for the next day, 16th Sept.So, returning to the way in which the Tribes are generated. Here is the list of Users in the hadleywickham communities that were highlighted on the map for 15th and 16th Sept. With three Users highlighted.The hadleywickham Tribe is calculated as a simple superset of all the Users that are members of the hadleywickham communities. Here are the three hightlighted Users in the hadleywickham Tribe as it stood on 16th Sept.By the end of the 10 days analysis on 25th Sept the hadleywickham Tribe has accumulated 598 Users, here are the top 20.The Weight field in the right hand column indicates how many times the User has been a member of a hadleywickham Community. So hadleywickham has been a member 20 times which means that over the 10 day period there were 20 Communities of which hadleywickham was Mayor.The Weight value can be used to select only the most frequently occurring members of the Tribe.Total Number of TribesIn total there are 320k Users in the whole 10 sample used for this analysis. The analysis over the 10 day cycle detects a total of 1300 Tribes. Some of these Tribes are very small but most contain a coherent set of Users with a common interest. The ongoing process is designed to refine and improve this detection process at every iteration.This is one of my favourites, Cricket Monthly.The Cricket Monthly (@cricketmonthly) | TwitterThe latest Tweets from The Cricket Monthly (@cricketmonthly). ESPNcricinfos digital magazine. On mobile, www: http://t…twitter.comHere is the CricketMonthly Tribe.Cricket Monthly is the Mayor of this small Tribe of Users who can be identified as being interested in sports and particularly cricket analytics. This picture illustrates just how hidden away and hard to find this kind of Tribe is in a mass of conversation.A list of the members of this Tribe on Twitter was created, check for yourself that this is a small but committed group of people who are interested in sports and particularly cricket analytics.Results OverallTo recap the data was collected for 10 days from 14th to 25th of September. My next post will cover more detailed analysis of the interesting Communities and Tribes that were discovered.In the meantime here is the list of the top 20 most influential Twitter Users during this period.Top 20 Influential Twitter Users — 14th-25th September 2015What NextThe next post will cover some more detailed analysis of the events and discussions that influenced the performance of some the people in the Top 20 along with further discussion about the importance of being a connector and the concept of ‘Interestingness’Free download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Oct 16, 2015;[]
https://medium.com/neo4j/explore-the-neo4j-entity-resolution-sandbox-8c63e9056bd5;Chintan DesaiFollowMay 9, 2022·4 min readExplore the Neo4j Entity Resolution SandboxEntity resolution (ER) is the process of analyzing and disambiguating of data to identify multiple digital records representing the same real-world entity such as a person, organization, place, or other types of object.For instance, a user can have more than one user account or profile on an e-commerce website with different email addresses and slightly different forms/abbreviations of names, etc.A human may be able to tell if the records actually belong to the same underlying entity. But given the number of possible combinations and possible matching, an intelligent automated solution is required for doing so, which is where entity resolution systems come into play.Photo by Tolga Ulkan on UnsplashUse CasesEntity resolution has many use cases across many sectors.Life Science & Healthcare IndustriesLife science and healthcare organizations require data linking the most. For example, a healthcare organization can implement Entity resolution for linking a patient’s records from a number of sources, matching data from hospitals, clinics, labs, insurance claims, and social media profiles to create a unique profile of each patient. This will help curate a precise and effective treatment. Life Science organizations can use ER to connect various entities, research results, input data sets, etc. This can facilitate the R&D.Insurance and Financial Services OrganizationsFinancial services and insurance companies often struggle with siloed datasets. Various products/categories maintain their data in different systems and databases. Hence, it is difficult to reconcile a customer’s choices, track record, credit ratings, etc. on a central platform. ER can enable them to perform record linking on different data sets and produce a unified view of customers’ states and needs.Digital Marketing and Content Recommendation BusinessesEffective marketing and recommendation schemes cannot be produced using distinct data sets or fragmented data. Records linking, machine learning, and analytics can be very helpful in producing effective marketing content.Identifying redundant customers is another area in marketing and CRM that needs to be addressed. ER can be mighty effective in such use cases.Graphs Can Come in HandyGraphs can add benefits to the entity resolution process, by not just using the attributes of the entities but also taking their context into account — such as behavior, social relationships, shared attributes to others, connections to people, objects, locations, events (POLE).Example Use CaseWe have crafted a similar use case for performing entity resolution.We take the example of a dummy online movie streaming platform. For ease of understanding, we took only movies and users datasets. Movies have different genres. Users can stream and watch movies on this platform. One or more persons from a family can be using the same or different profiles on the platform.Data ModelExampleUsers can have one or more accounts on a movie streaming platform. They have slightly different names or abbreviations, and different email addresses configured with different profiles.We are performing entity resolution over users’ data to identify such users. We are also performing linking for users who are from the same account (or group/family). Later, we utilized this linking to provide effective recommendations to individual users.Readymade Neo4j Sandbox to Explore Entity ResolutionWe have a preset sandbox to walk you through this Entity Resolution use case. Neo4j Sandbox is a great — and free — online tool from Neo4j to try their graph database without installing anything locally.We have loaded an easy browser guide to walk you through the below steps.Relate: Establish connections (relationships) between entitiesExplore: Perform basic querying with Cypher on loaded dataER: Perform Entity Resolution based on similarity and do record linkageRecommend: Generate recommendations, based on user similarities or preferencesAdditional: Try a couple of preference-based similarities and recommendation examples using Neo4j GDSGet started with your own entity resolution sandbox.ReferencesEntity resolution explanation and one more interesting study:Exploring Supervised Entity Resolution in Neo4jWhile Supervised Entity Resolution (ER) can be immensely valuable, it is sometimes difficult to apply and scale in the…neo4j.comFull source code as an example to try on your local box:GitHub - neo4j-graph-examples/entity-resolution: Entity resolution, also known as Data Matching or…Entity resolution, also known as Data Matching or Record linkage is the task of finding a data set that refer to the…github.com;May 9, 2022;[]
https://medium.com/neo4j/hands-on-with-the-neo4j-graph-data-science-sandbox-7b780be5a44f;William LyonFollowMar 7, 2020·11 min readHands-On With The Neo4j Graph Data Science SandboxUsing The Neo4j Graph Data Science Library And Rule-Based Graph Data Visualization In Neo4j BloomIn this post, we explore how to use the graph algorithms available in the Neo4j Graph Data Science library along with the Neo4j Bloom graph visualization tool. We show how to run graph algorithms and use the results to style graph visualizations in Bloom, all in Neo4j Sandbox.The Neo4j Graph Data Science LibraryNeo4j Graph Data Science is a library that provides efficiently implemented parallel versions of common graph algorithms for Neo4j, exposed as Cypher procedures. Types of algorithms available include centrality, community detection, similarity, pathfinding, and link prediction. You can read more about Neo4j Graph Data Science in the docs.Neo4j BloomNeo4j Bloom is a graph exploration application for visually interacting with graph data. You can read more about Neo4j Bloom in the docs or on the Bloom landing page.The many features of Neo4j Bloom.Neo4j SandboxThe different projects available in Neo4j Sandbox.Neo4j Sandbox is a great free way to try out Neo4j without downloading or installing anything locally. It gives you access to Neo4j database, Neo4j Bloom, and plugins like Neo4j Graph Data Science, all hosted online and private to you. Once you sign in you can choose which project to spin up. Each project contains a different dataset and guided experience to show you how to query the dataset using Cypher and the Neo4j drivers, all in a private hosted environment.Today we’re interested in the Graph Data Science project so we’ll start there. Once we create a sandbox instance we can open Neo4j Browser where we’ll see a friendly interactive guide with embedded queries and images walking us through the process of using the Neo4j Graph Data Science library.The guide explores how to use PageRank, Label Propagation, Weakly Connected Components (WCC), Louvain, and Node Similarity.The DatasetThe Graph Data Science sandbox comes loaded with a dataset of Game of Thrones characters and their interactions across the many books of the fantasy series. The interactions are derived from the text of the books: any two character names appearing within 15 words of one another is counted as an interaction.The Graph of Thrones data model.The dataset is partly based on the following works:Network of Thrones, A Song of Math and Westeros, research by Dr. Andrew Beveridge.A. Beveridge and J. Shan, Network of Thrones,” Math Horizons Magazine , Vol. 23, №4 (2016), pp. 18–22Game of Thrones, Explore deaths and battles from this fantasy world, by Myles O’Neill, https://www.kaggle.com/Game of Thrones, by Tomaz Bratanic, GitHub repository.Creating A Graph Data Science GraphsThe GDS algorithms run on a subgraph or projected graph that can be specified based on labels and relationships, or a Cypher query that may create a projected graph (one that doesn’t exist in the database). For our first example, we will use the(:Person)-[:INTERACTS]-(:Person) subgraph. We load this graph using the gds.graph.create procedure.CALL gds.graph.create(got-interactions, Person, {  INTERACTS: {    orientation: UNDIRECTED  }})Finding Influential Nodes With PageRankA common question when working with graph data is to ask, What are the most influential nodes in this network?” Centrality algorithms are used to determine the importance of nodes in a network. The Graph Data Science library includes several centrality algorithms including PageRank, ArticleRank, betweenness, closeness, degree, and eigenvector centrality.The PageRank algorithmPageRank measures the transitive influence and connectivity of nodes to find the most influential nodes in a graph. As a result, the score of a node is a certain weighted average of the scores of its direct neighbors.PageRank is an iterative algorithm. In each iteration, every node propagates its score evenly divided to its neighbours.You can read more about PageRank and other centrality algorithms in the documentation.Computing PageRank With Neo4j Graph Data ScienceWhen running algorithms with the Neo4j Graph Data Science procedures, we have the option of streaming the results back, or writing the values to the graph as node or relationship properties.Here we compute PageRank over our interactions subgraph, showing the top 10 nodes with the highest PageRank score.CALL gds.pageRank.stream(got-interactions) YIELD nodeId, scoreRETURN gds.util.asNode(nodeId).name AS name, scoreORDER BY score DESC LIMIT 10-----------------------------            ╒════════════════════╤══════════════════╕            │ name               │ score            │            ╞════════════════════╪══════════════════╡            │ Jon Snow           │17.213974114507444│            ├────────────────────┼──────────────────┤            │ Tyrion Lannister   │16.862491100281478│            ├────────────────────┼──────────────────┤            │ Cersei Lannister   │13.730249913781883│            ├────────────────────┼──────────────────┤            │ Jaime Lannister    │13.522417121008036│            ├────────────────────┼──────────────────┤            │ Stannis Baratheon  │12.067919091135265│            ├────────────────────┼──────────────────┤            │ Daenerys Targaryen │11.790235754847526│            ├────────────────────┼──────────────────┤            │ Arya Stark         │11.221931967139245│            ├────────────────────┼──────────────────┤            │ Robb Stark         │10.819953513145444│            ├────────────────────┼──────────────────┤            │ Eddard Stark       │10.244713875278832│            ├────────────────────┼──────────────────┤            │ Catelyn Stark      │9.997365672886371 │            └────────────────────┴──────────────────┘Other centrality measures include degree centrality and weighted degree, which we can easily compute in Cypher.CALL gds.pageRank.stream(got-interactions) YIELD nodeId, score AS pageRankWITH gds.util.asNode(nodeId) AS n, pageRankMATCH (n)-[i:INTERACTS]-()RETURN n.name AS name, pageRank, count(i) AS degree, sum(i.weight) AS weightedDegreeORDER BY pageRank DESC LIMIT 10----------------------------------╒════════════════════╤══════════════════╤════════╤════════════════╕│ name               │ pageRank         │ degree │ weightedDegree │╞════════════════════╪══════════════════╪════════╪════════════════╡│ Jon Snow           │17.213974114507444│190     │2757            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Tyrion Lannister   │16.862491100281478│217     │2873            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Cersei Lannister   │13.730249913781883│199     │2232            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Jaime Lannister    │13.522417121008036│169     │1569            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Stannis Baratheon  │12.067919091135265│147     │1375            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Daenerys Targaryen │11.790235754847526│120     │1608            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Arya Stark         │11.221931967139245│132     │1460            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Robb Stark         │10.819953513145444│136     │1424            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Eddard Stark       │10.244713875278832│126     │1649            │├────────────────────┼──────────────────┼────────┼────────────────┤│ Catelyn Stark      │9.997365672886371 │126     │1230            │└────────────────────┴──────────────────┴────────┴────────────────┘Instead of streaming the results back we can write them to the graph, stored as node property values in Neo4j. Here we compute PageRank for each Person node in our projected graph and write the value back to the pageRank node property.CALL  gds.pageRank.write(got-interactions, {writeProperty: pageRank})Writing the values to the graph is useful because we can then use those values stored in Neo4j in our real-time queries or data visualizations.Rule-Based Node Size In Neo4j BloomNow that we’ve computed PageRank, let’s switch over to Neo4j Bloom and start exploring the graph visually. In Bloom, the perspective is the configuration for our visualization environment. The perspective defines the node labels, relationships, properties, icons, size, and colors to use. We can also define search phrases in a perspective that enable powerful natural language like searching functionality by exposing parameterized Cypher queries.Choosing a perspective when first starting Neo4j Bloom.In the perspective, we can also define rule-based styling that takes into account the values of node or relationship properties when styling the visualization. For example, now that we’ve computed PageRank for our Person nodes, let’s create a styling rule that nodes with higher PageRank score should appear larger in the visualization. To do this we select the Person category in the legend panel and create a new rule-based style.Creating a rule-based style to scale node size relative to the PageRank value.Now when we query for the Person category we see node size scaled by the PageRank score for that node. This allows us to immediately determine which nodes are most important, just by visually examining the scene.Node size scales relative to the computed PageRank value.Community DetectionNext, we’ll apply the Label Propagation algorithm to find communities in our graph. Community detection algorithms are used to determine how groups of nodes in a graph are clustered and also the tendency of clusters or partitions to break apart. The Neo4j Graph Data Science library currently includes the following community detection algorithms: Louvain, Label Propagation, Weakly Connected Components, K-1 Coloring, Modularity Optimization, Strong Connected Components, and Triangle Counting / Clustering Coefficient.Label Propagation is a fast algorithm for finding communities in a graph. It propagates labels throughout the graph and forms communities of nodes based on their influence.Label Propagation is an iterative algorithm. In the first iteration, a unique community label is assigned to to each node. Then in each iteration, this label is assigned to the most common label among a node’s neighbors. Densely connected nodes quickly spread their labels across the graph. For more details, see the documentation.Since label propagation is a weighted algorithm (it can take into account relationship weights), we need to create a new interaction subgraph which includes relationship weights:CALL gds.graph.create(  got-interactions-weighted,  Person,  {    INTERACTS: {      orientation: UNDIRECTED,      properties: weight    }  })And now let’s run the algorithm, writing the assigned communities back to the graph in a property called community. Since label propagation is an iterative algorithm and can take a few iterations to converge on stable community assignment, we specify the maximum number of iterations in the procedure call.CALL gds.labelPropagation.write(  got-interactions-weighted,  {    relationshipWeightProperty: weight,    maxIterations: 10,    writeProperty: community  })Rule-Based Node ColorNow that we’ve assigned each node to a community, we want our visualization in Neo4j Bloom to use the results of this algorithm, coloring nodes by community assignment. To do this we add a new rule-based styling to the perspective in the legend panel.Configuring a rule-based style to color nodes according to their community.Now in our Bloom scene, we see the node size styled by PageRank and the color determined by which community the node is assigned.Next, we explore the node similarity algorithm.Node SimilarityJaccard similarity is the size of the intersection of two sets divided by the size of their union.The Node Similarity algorithm is used to compute similarity scores for pairs of nodes in a graph. The similarity between two nodes is based on their respective sets of neighbors. To obtain a similarity measure between two sets, we compute the Jaccard Similarity of their neighboring nodes. Nodes are similar if most nodes that are neighbors to either node are in fact neighbors to both. Typically, Node Similarity is used on a bipartite graph, for example containing People that have a LIKES relationship to Items. Node Similarity can then be used to find the pairs of People that are the most similar in the sense that they mostly like the same items. For more information see the documentation.Let’s create a new subgraph of Person , Book , House , and Culture nodes and all relationships connecting them.CALL gds.graph.create(got-character-related-entities, [Person, Book, House, Culture], *)And then we run the node similarity algorithm on this subgraph. This will create a weighted SIMILARITY relationships in the graph, storing the Jaccard similarity for each node pair in the property character_similarity .CALL gds.nodeSimilarity.write(  got-character-related-entities,  {    writeRelationshipType: SIMILARITY,    writeProperty: character_similarity  })Rule-Based Relationship SizeWe can use this similarity score to style our graph in Bloom by making the thickness of the relationship scale relative to the similarity score of two nodes: nodes that are more similar should have a thicker relationship connecting them. To do this we add a new rule-based style to the SIMILARITY relationship, specifying the size of the node should be determined by the character_similarity relationship property value.Styling SIMILARITY relationships relative to the node similarity score in the legend panel.Having added this style rule, we can use Bloom’s natural language like querying to search for a character and all similar characters in the search bar.Person with name Daenerys Targaryen SIMILARITY PersonThe equivalent Cypher query for this search would be something like this:MATCH  (p1:Person {name:  Daenerys Targaryen })-[r:SIMILARITY]-(p2:Person)RETURN *Now in the visualization, we can see the relationships connecting characters most similar to Daenerys are thicker.We can verify this by inspecting all relationships connected to Daenerys in the Card List, and see that Barristan Selmy is indeed most similar to Daenerys.Putting It All TogetherLet’s say we want to visualize the top characters (as defined by PageRank) and see who they interacted with the most. We could write a Cypher query like this to give us the results:MATCH (p:Person) WHERE EXISTS(p.pageRank) AND EXISTS(p.community)// only take top 10 by PageRankWITH p ORDER BY p.pageRank DESC LIMIT 10 //find interactions with other charactersMATCH interactPath=(p)-[r:INTERACTS]->(other) // order by number of interactionsWITH interactPath,p,r,other ORDER BY r.weight DESC// take top ten interactions per Person WITH COLLECT(interactPath)[0..10] AS topInteractions, p RETURN topInteractionsBut how can we visualize the results of this Cypher query in Bloom? To do this we create a search phrase in the Bloom perspective drawer. Search phrases allow us to define parameterized Cypher queries that can then be used in the natural language like search by Bloom users via the search bar.Adding a search phrase in the perspective drawer.Now we can easily search for the 20 top characters as defined by PageRank, and for each character see the top 40 characters they interacted with, simply by using the search phrase in the search bar.Top 20 characters and 40 connectionsThe values 20 and 40 are passed into the parameterized Cypher query we defined in the search phrase as $numCharacters and $numConnectionsThis visualization is styled using the results of the graph algorithms we ran earlierNode size is determined by PageRank — more important nodes are larger in the visualizationNode color is determined by label propagation, a community detection algorithm. Nodes belonging to the same community are the same color.Relationships are styled according to the weight property of the relationship. Relationships are thicker when two characters have interacted more frequently.When we zoom out we can see the full subgraph:Resources To Learn More About Graph Algorithms And Neo4j BloomHere are a few resources to learn more about Neo4j Graph Algorithms and Neo4j Bloom.Neo4j Graph Data Science sandbox — use graph algorithms and Neo4j Bloom to explore a dataset of Game of Thrones characters and their interactions.Applied Graph Algorithms Online Course — use graph algorithms to add functionality to a business search application using Neo4j and React.js.Data Science With Neo4j Online Course — this course introduces you to using Neo4j as part of your Data Science and Machine Learning workflows.Download The Book Graph Algorithms: Practical Examples in Apache Spark and Neo4j” —free download of the authoritative book on using graph algorithms with Neo4j.The Neo4j Graph Data Science Library ManualNeo4j Bloom landing page and documentation.The Bloom Visual DiscoveryNeo4j Sandbox — use Neo4j Bloom to explore a financial services dataset to find potential fraud.;Mar 7, 2020;[]
https://medium.com/neo4j/how-to-use-sigmajs-to-display-your-graph-3eedd75275bb;Benoît SimardFollowApr 25, 2018·9 min readHow to use Sigmajs to display your graph ?In this article, I will show how to display and customize a graph in a browser with the help of SigmaJS, a JS library dedicated to graph drawing delivered under the MIT licence.Initially, SigmaJS has been developed for Gephi to export a graph on the web. Today, sigmajs is a stable, full features and highly configurable graph library. Moreover, the small cherry on the cake is its compatibility with touch screens.InstallationHow-toThere are four steps to follow :Step 1 : Install sigmajs binariessigmajs can be found on :NPM : you just have to type : npm sigma --save-devGithub : download the build.zip from a release like thisStep 2 : Import the JavaScript library in your web pageThe minimal script you have to load is : <script src= ./build/sigma.min.js ></script>.Personally, I prefer to import all the source files, it helps to lighten the library by removing the unused source files (ex: I don’t need SVG & Webgl renderers if I use canvas).Step 3 : Create an HTML containerYou have to create an HTML element, for example a div with an id like this : <div id=sigma-container></div>.Moreover, you also need to give it an height and width, otherwise it will have a size of 0 pixel. This is easily done with some CSS : #sigma-container { width:100%, height:100%}Step 4: Initialize sigmajsTo initialize Sigmajs, you need to call the function sigma with those arguments :A render : it contains the HTML element that sigmajs will use to display the graph, plus the type of renderer (SVG, canvas or WebGL)sigmajs’s settings : this object will override the default settings, so you can pass an empty object for now. The complete settings list is available hereAt the end, you should have something like that :<html>  <head>    <title>SigmaJS example</title>    <script src= ./build/sigma.min.js ></script>    <style>      html { height:100%}      body {height: 100%}      #sigma-container { width:100% height:100% background-color:#E1E1E1}    </style>  </head>  <body>    <div id=sigma-container></div>    <script>      // Initialize sigma:      var s = new sigma(        {           renderer: {             container: document.getElementById(sigma-container),             type: canvas           },           settings: {}         }       )    </script>  </body></html>How to choose between SVG, Canvas and Webgl ?SigmaJS can used one this technology to display your graph : svg, canvas & webgl.SVG is a markup language, directly interpreted by your browser, so it’s easy to customize nodes & edges style, you just have to add some CSS rules. But when you apply a layout algorithm on the graph, you continuously change the DOM of the SVG, and if this DOM is heavy, the result is slow and can freeze your browser.Canvas is an HTML5 element compatible with all modern browsers. It defines an area on your web page where you can draw some pixels by using a JavaScript API. The JavaScript code is directly interpreted by your browser and it decides what and when to render. Due to this, Canvas scales more than the SVG, but if you want to change the color of a shape, you need a way to specify it in the code.On the other hand, there is WebGL, compatible with almost all newest browsers (even if the API is not stable). As in Canvas, you need to write some JavaScript code for rendering your visualisation, but this time, its computation is delegated to OpenGL so to your GPU.To resume :The choice will depend on your needs, and also of the SigmaJS plugins you want to use (plugins are not compatible with every format).I suggest you to choose Canvas or Webgl if you want to really display a big graph.The SigmaJS graph structureDefinitionNow that sigma is initialized, we need to give it a graph. It structure is simple : an object with an array of nodes and an array of edges.Node definitionEdge definitionExamplevar graph = {  nodes: [    { id:  n0 , label:  A node , x: 0, y: 0, size: 3, color: #008cc2 },    { id:  n1 , label:  Another node , x: 3, y: 1, size: 2, color: #008cc2 },    { id:  n2 , label:  And a last one , x: 1, y: 3, size: 1, color: #E57821 }  ],  edges: [    { id:  e0 , source:  n0 , target:  n1 , color: #282c34, type:line, size:0.5 },    { id:  e1 , source:  n1 , target:  n2 , color: #282c34, type:curve, size:1},    { id:  e2 , source:  n2 , target:  n0 , color: #FF0000, type:line, size:2}  ]}Loading graph in sigmaSigma has a complete API to manage its graph data. I let you see the API documentation.To load a graph in sigma, you just have to call the method read on the sigma graph instance : s.graph.read(graph)Once it’s done, we need to tell sigma to draw the graph by calling its refresh function : s.refresh()Live exampleGraph layout algorithmDefinitionWhat is hard in displaying a graph is to rapidly display it in such a way that we can see all nodes and their edges without overlaps (in fact less as possible). To do it, we need an algorithm that computes the position of each nodes, and the most known for that are the force-directed algorithms.The principle is simple, you need to consider two forces :Repulsive : Nodes repulse each others. You can consider nodes like particle with the same electric charge.Attractive : Two nodes with an edge, attract themselves. You can consider an edge as a spring.Then you run an algorithm that compute on each iteration, the sum of the applied forces on each node, and move them in consequence. After a number of iteration, you will see that graph is in a stable state.Force-Atlas2SigmaJS include (as a plugin) a forced-directed algorithm called Force-Atlas2.Step 1 : import the plugin files<script src=”./build/plugins/sigma.layout.forceAtlas2/supervisor.js”></script><script src=”./build/plugins/sigma.layout.forceAtlas2/worker.js”></script>Step 2 : run itNow that the plugin is loaded, we can directly call it on the sigma instance : s.startForceAtlas2() This creates a web worker where all the algorithm iterations will be calculated.Step 3 (optional) : Stop itThe algorithm won’t stop by itself, so I recommend you to stop it after a predefined duration (10 seconds in my example) : window.setTimeout(function() {s.killForceAtlas2()}, 10000)Live examplePluginsSigmaJS has a lot of plugins, you can see the list here. I will not show you all of them, so I have done a list of my most used plugins.Edge labelsThis plugin allows you to add a label on each edge. I mainly use it to display the Neo4j’s relationship type.Import the needed script (in my case settings.js, sigma.canvas.edges.labels.def.js & sigma.canvas.edges.labels.curvedArrow.js)Add a label property on yours edgesEdge parallelEdgesIf you want parallel edges (ie. to have multiple relationship between two nodes), it’s the plugin you must have.To use it, import the needed script :utils.jssigma.canvas.edges.curvedArrow.js & sigma.canvas.edgehovers.curvedArrow.js if you have a directed graph.sigma.canvas.edgehovers.curve.js & sigma.canvas.edges.curve.js if you want the an undirected graph.sigma.canvas.edges.labels.curve.js if you have enabled label on edgesThen, add a property count that represent the index of the edge in the set of parallel edges. Inversely proportional to the amplitude of the vertex of the edge curve.Relative sizeThis plugin is really useful when you want to see which node is most connected. The size of the node depends of its degree, ie. its number of in-going & outgoing edges.Sigma and Neo4jNow that you know how to use Sigma, the next step is to build a graph visualisation from Neo4j.To do this there is two points :How to query Neo4j in your browser ?From a query result, how to build a sigma graph structureI will not explain the first point, Michael has already done this part in this excellent post.So let see the second one !Create a graph structure from a Neo4j queryResult of a query is a collection of tuple, ie. composed of rows where each row has some columns. Moreover, each cell is typed, and to display a graph we only want node, relationship and path.To create our data structure, we need to iterate over rows, then over columns and finnaly check the type. If it’s a node or relationship, we can add it to our sigma graph structure (if it’s not already present).And what about path ? A path in Neo4j driver types, is an array of segment where each segmet is composed of :a starting nodea relationshipan ending nodeSo if we have a path, we need also to iterate over it to add starting & ending node, plus the relationship.But wait, a Neo4j node (resp. relationship) is not a Sigma node (resp. relationship), so we also need to convert them.If you code it, finally you should have something like this :let graph = { nodes:[], edges:[]} this.driver.session().run( MATCH (n)-[r]->(m) RETURN n,r,m LIMIT $limit , {limit:50}).then(  (result) => {    // for each rows    result.records.forEach( record => {      // for each column      record.forEach( ( value, key ) => {        // if its a node        if ( value && value.hasOwnProperty( labels ) ) {          graph.nodes.push(convertionToSigmaNode(value))        }        // if its an edge        if ( value && value.hasOwnProperty( type ) ) {          graph.edges.push(convertionToSigmaEdge(value))        }        // if its a path        if ( value && value.hasOwnProperty( segments ) ) {          value.segments.forEach( ( seg ) => {            // add starting & ending nodes + relationship            graph.nodes.push(convertionToSigmaNode(seg.start))            graph.nodes.push(convertionToSigmaNode(seg.end))            graph.edges.push(convertionToSigmaEdge(seg.rel))          })        }      })    })  })And here we go, you have everything to display your graph from a Cypher query with SigmaJS !It’s a little borring, no ? All this code just to display a graph…​ And what if I tell you that I have already made the work for you ?Neo4j + Sigma = NeoSigTo avoid you the complexity of doing all the above work, I have created a library for that : NeoSig.To use it, you need to import the library + the Neo4j driver :<script src= https://cdn.jsdelivr.net/npm/neo4j-driver@1.6.0 ></script> <script src= https://cdn.jsdelivr.net/npm/neosig@1.2.2/docs/neosig-1.2.2.js ></script>NOTE: The library embed SigmaJS (with some customs code) but not the Neo4j driver.Then you can call the function Neo4jGraph(neo4jConfig, neo4jStyle, query, queryParams), it returns a promise with the sigma graph object.Neo4jGraph(neo4jConfig, neo4jStyle, MATCH (n)-[r]->(m) RETURN n,r,m LIMIT $limit, {limit:20}).then( function(graph) {  s.graph.read(graph)  // enable dragndrop  sigma.plugins.dragNodes(s, s.renderers[0])  // start layout  s.startForceAtlas2()  setTimeout(() => { s.stopForceAtlas2() }, Math.log(graph.nodes.length*graph.edges.length)*1000)})Where neo4jConfig is an object with :const neoConfig = {  url:bolt://localhost:7867,  user: neo4j,  password: letmein,  driver : {    // all the driver configuration (optional)  }}And neo4jStyle isonst neoStyle = {  labels: { // Map of label    Person : {      label: name, // The nodes property to display as label      color: #654321, // Color of the node      size: 10, // Size of the node      icon: { // icon object        name: f007, // Fontawesome unicode        color: #FFF, // Color of the font        scale: 1.0 // Scale ratio      }    },    Movie : {      label: title,      color: #123456,      size: 10,      icon: {        name: f008,        color: #FFF,        scale: 1.0      }    }  },  edges: { // Map of edges    ACTED_IN: { // Name of the relationship type      label: roles,      //color: #202020,      // size: 2    }  }}NOTE: By default, a node is black, with a size of 5, and the label is its Neo4j’ID  a relationship is black with a size of 1, and the label is its Neo4j type.And the final result is :You can see the code here :A big thanks to Jacomyal, Yomguithereal and Astik for the help !;Apr 25, 2018;[]
https://medium.com/neo4j/a-developers-look-ml-models-in-neo4j-7d4cbacb320c;Lauren ShinFollowJul 11, 2018·6 min readGraphs and ML: Remembering ModelsA Developer’s LogI wrote in my first article about the linear regression procedures I added for Neo4j. Today I want to explain some of the internals, and why I chose to build them the way I did.User-defined procedures must remember” information between calls in order to build and maintain a machine learning model. This pushes beyond the typical functionality of procedures in Neo4j. In the following article, I explore the key implementation details for a set of functions and procedures that create, train, test, use, and store linear models of graph data. I hope this helps your understanding of my work or for implementing similar user-defined procedures of your own.To see these user-defined functions and procedures in action, read my previous post in which I use linear regression to create a price predictor for short term rentals in Austin, TX.I take a brief writing break to illustrate the awesomeness of user-defined procedures in Neo4j.The GoalI want to perform linear regression on the data I have stored in Neo4j. There are many libraries with tools for creating linear models (Pandas and Numpy for Python, Commons Math for Java, etc.), but to use these directly I must export data from my graph into another software. In order to model my data from within Neo4j I need to extend the functionality of the graph query language Cypher.The preferred means for extending Cypher is with user-defined functions and procedures. These are written in Java, built for instance with Maven, deployed to the Neo4j database as JAR files, and called from Cypher. I need to write a procedure that performs linear regression on graph data.What makes this problem interesting?First, let’s take a look at a typical procedure in Neo4j: apoc.meta.graph. Running CALL apoc.meta.graph() will return a visual representation of the graph’s schema (your data model). For example, here is the result of this procedure call on the short term rental graph from my previous post:This procedure accesses the information stored in the graph in order to determine the underlying structure. Other procedures alter the graph, such as apoc.refactor.mergeNodes which merges multiple nodes into just one. However, procedures do not typically remember information between calls, they just produce a stream of outputs or modify data.I can create a procedure that, like apoc.meta.graph, accesses the information in the graph in order to create a linear model without storing any external information. I can pass in all data at once, perform the least squares calculations in the procedure, and return the parameters of the linear model. But if I make another call to the procedure, it will have already forgotten the model it just created.But what if I then decide to add more data to the model? What if I want to use a large amount of training data that requires too much memory to be input as the argument to one procedure?Idea #1: Serialize!My first attempt at a solution is a bit of a workaround because instead of remembering” information, the procedure stores the model in the graph so it can be accessed and updated later. The idea is to serialize the model’s Java object and store the byte array in the graph between procedure calls. Here’s a visual representation of the process of serialization and deserialization:http://blog.acorel.nl/2017/09/using-serializable-objects-in-abap.htmlNote: I am using SimpleRegression from the Apache Commons Math library. SimpleRegression performs updating calculations on incoming data points so that no individual data points are saved. Instead, it stores certain information such as mean y” value, total number of data points, etc. and updates these values with each new data point. Thus with each additional data point the model performs calculations and improves without increasing its memory usage. The result: when we serialize the SimpleRegression object the corresponding byte array is not very large (at least, it doesn’t scale with size of the data set!).I first wrote the following helper functions so that throughout my project I could convert the SimpleRegression object to byte[] and vice versa. These required imports from java.io.*.//Serializes the object into a byte array for storagestatic byte[] convertToBytes(Object object) throws IOException {    try (ByteArrayOutputStream bos = new ByteArrayOutputStream()         ObjectOutput out = new ObjectOutputStream(bos)) {        out.writeObject(object)        return bos.toByteArray()    }}//de serializes the byte array and returns the stored objectstatic Object convertFromBytes(byte[] bytes) throws IOException, ClassNotFoundException {    try (ByteArrayInputStream bis = new ByteArrayInputStream(bytes)         ObjectInput in = new ObjectInputStream(bis)) {        return in.readObject()    }}Then, next time I wanted to edit the model, I retrieved the byte array from the graph and deserialized it.try {    ResourceIterator<Entity> n = db.execute( MATCH (n:LinReg {ID:$ID}) RETURN n , parameters).columnAs( n )    modelNode = n.next()    byte[] model = (byte[])modelNode.getProperty( serializedModel )    R = (SimpleRegression) convertFromBytes(model)} catch (Exception e) {    throw new RuntimeException( no existing model for specified independent and dependent variables and model ID )}And, after editing the model, stored the new byte[] representation back in the same node.try {    byte[] byteModel = convertToBytes(R)    modelNode.setProperty( serializedModel , byteModel)} catch (IOException e) {    throw new RuntimeException( something went wrong, model cant be linearized so new model not stored )}If you’re interested, check out the full code. Note that these preliminary implementations are much different (and more convoluted!) than the final version of my linear regression procedures below.IssuesWhat if I want to, for the sake of design, separate create model, add data, and remove data procedures? Graph databases are constantly receiving updates, so I need to create a model that is as flexible as the graph. Serialization and deserialization has a significant time requirement. You may pass in multiple data points at once in order to limit the number of procedure calls (and the number of times the model is stored and retrieved), but I need some better way to store the intermediate model between procedure calls so that it may be updated as many times as I need.Idea #2: Static MapStatic variables in the Java classes implementing the procedure live as long as the database continues running. Therefore, we can store model objects by name in a static map. Models are stored in the procedure so that each step of linear regression — create, add data, remove data, etc. — is isolated into a separate procedure but alters the same SimpleRegression model. Something like an add procedure can be called once for each data point without severe performance penalties. This creates the simplified design we desire. With every step isolated the procedures are clear and the user has greater control over each step of building the linear model.The models are stored in a static ConcurrentHashMap in one of the Java classes used to implement the procedures: LRModel.java. Whenever a procedure is called that needs to access the model, it is retrieved from models by name using the method from. Using that particular map implementations allows for concurrent access from multiple threads.private static ConcurrentHashMap<String, LRModel> models = new ConcurrentHashMap<>()static LRModel from(String name) {    LRModel model = models.get(name)    if (model != null) return model    throw new IllegalArgumentException( No valid LR-Model   + name)}Now we only have to serialize and store the model before database shutdown and load it back into the procedure’s static memory when the database is restarted. Check out the full implementation on Github.LimitationsIf the database shuts down unexpectedly the static variables will be cleared and the model lost. It might be a good idea to have a backup option in which the model is serialized and saved at regular time intervals. After database failure, restart the database and rebuild the model.Serialization is not the best way to save the model because if anything is changed in the next version of Commons Math, the updated version may not recognize a serialized SimpleRegression object from before.Stats for test data are not stored between database shutdown/restart. Ideally, I would implement the updating simple regression myself instead of using Commons Math, and then store all necessary information about training and testing data instead of storing a serialized SimpleRegression.Improve my work!If you have ideas more stable than storage in static maps and serialization, let me know or implement it yourself! I challenge you to improve my work. I would love to discuss possibilities, just message me on LinkedIn or @ML_auren. Cheers!;Jul 11, 2018;[]
https://medium.com/neo4j/neo4j-4-3-blog-series-relationship-indexes-14d2b656abd2;Stu MooreFollowJun 24, 2021·7 min readNeo4j 4.3 Blog Series: Relationship IndexesWelcome to the first in a series of blogs on some of the exciting new features in Neo4j 4.3 that was released last week at NODES 2021.Relationship Property Indexes help find co-occurrence in the Panama PapersOver the coming weeks, I will be publishing technical blogs on the following new features:Relationship/Relationship Property Indexes (this blog)Relationship Chain LocksDeploying on Kubernetes with Helm ChartsServer Side RoutingRead Scaling for AnalyticsEach blog will provide an introduction into the feature, when to use it, and a worked example.Finally, a big thanks to Michael Hunger for his help in putting the example in this blog together.What Are Relationship / Property Indexes, and When Do You Use Them?The new index types in Neo4j 4.3 work in much the same way as Node and Node Property indexes do — they enable you to quickly look up where there are references to a particular Relationship Type in the graph (i.e. which Nodes are connected by the relationship). Or in the case of Relationship Property Indexes, which Relationship Types have those properties . You can specify a combination of properties too — known as a compound index — for example, indexing on properties since and from would only index relationships with both of these properties definedCreating a Relationship and /or Relationship Property Index will enable you to run more complex queries on relationships in less time. The indexes help reduce scanning across the entire database, which translates into fewer hits on the database. This won’t just benefit your queries, but anyone else using Neo4j at the same time. And in turn, reducing the hits on the database means you reduce the demands on the storage IO — something your storage team will certainly thank you for.Side Note on the Use of IndexesAdding Indexes does need to be done with some thought because they increase the write workload — creates and updates on the index when new data is written to the database. Indexes may not benefit your queries when the majority of nodes have the same relationship type or there is only a single relationship type — same goes for relationship properties.How will I know the Index is helping? We’ll explain later, but Neo4j has a handy way of showing you its plan for running the query, and this includes the use of the indexes. Neo4j’s schema-less architecture means that you don’t have to figure this all out upfront, and Indexes can always be added to improve performance. Note, Neo4j will create a LOOKUP Relationship type index by default, and you can drop it with the DROP INDEX command if it isn’t required.OK, That’s Great! Give Me an Example?I am going to use the same query that I presented in the announcement for Neo4j 4.3 at NODES — where we imagined we were exploring the Panama Papers to find occurrences of the same two officers involved in the same off-shore companies. We aren’t expecting to find just one or two companies or one or two directors, since the scale of the scandal suggests that many of the same individuals were involved in hundreds of bogus companies.To set this up, Michael Hunger and I used his commands to import the panama data here. We made a few minor changes (described below) to preserve the officer role as a relationship type (we could have chosen to create different types of relationships as well).New code used in this exampleneo4j@neo4j> USING PERIODIC COMMITLOAD CSV WITH HEADERS FROM file:///all_edges.csv” AS rowWITH row WHERE row.rel_type IN [officer of”,”director of”,”shareholder Of”, beneficiary of”, secretary of”]MATCH (n1:Node) WHERE n1.node_id = row.node_1MATCH (n2:Node) WHERE n2.node_id = row.node_2CREATE (n1)-[r:OFFICER_OF]->(n2) SET r.role = row.rel_typeReplaced the code used in Michael’s original script because he created a single relationship type OFFICER_OF for all the directors, shareholders, beneficiaries, etc.neo4j@neo4j> USING PERIODIC COMMITLOAD CSV WITH HEADERS FROM file:///all_edges.csv” AS rowWITH row WHERE row.rel_type = officer_of”MATCH (n1:Node) WHERE n1.node_id = row.node_1MATCH (n2:Node) WHERE n2.node_id = row.node_2CREATE (n1)-[:OFFICER_OF]->(n2)The QueryTo find pairs of directors who are involved in multiple shell companies we need what is termed a co-occurrence query.” This type of deep query is something that is only really practical to perform in a graph database, because you are examining every legal business entity in the graph to see if there are relationships with the same pairs of individuals.neo4j@neo4j> MATCH (o1)-[r:OFFICER_OF]->(e)<-[OFFICER_OF]-(o2)WHERE id(o1) > id(o2)AND r.role CONTAINS director”WITH o1, o2, count(*) as c order by c desc limit 100RETURN o1.name, o2.name, cCo-occurrence queries can be used in lots of other settings too, for example social media posts — do the same two people comment on the same social media posts — may find that Dwayne ‘The Rock’ Johnson and Kevin Hart do, which may imply that they are good friends.Query ResultsIf you scroll through the results you find that some of the directors are involved in anywhere from 60 to 90+ companies, while others are only involved in a couple. It is not unreasonable to expect people to be involved in a couple of companies legitimately, but 60+ is a definite red flag for suspicious activity.How Do You Create the Index on the Role?Creating the index is straightforward. Run the following in the cypher-shell:neo4j@neo4j> CREATE INDEX officerRelationshipPropertyFOR ()-[r:OFFICER_OF]-()ON (r.role)Check out the docs for the full syntax.How do we SHOW that Neo4j runs 3.4 times faster with the new index?Neo4j provides two commands to help you figure out what is going on under the hood: EXPLAIN and PROFILE. EXPLAIN will figure out the execution plan, but won’t actually run the query so won’t return any results. PROFILE on the other hand will run the query, consume the resources, and return the results. By running the query with EXPLAIN or PROFILE before we create the index, we can compare the results we get after creating the index.neo4j@neo4j> PROFILE MATCH (o1)-[r:OFFICER_OF]->(e)<-[OFFICER_OF]-(o2)WHERE id(o1) > id(o2)AND r.role CONTAINS director”WITH o1, o2, count(*) as c order by c desc limit 5RETURN o1.name, o2.name, cWhen the command runs, it will produce the results and explain the plan used. Below on the left you have the execution without the index, and on the right the plan with the index which references the DirectedRelationshipContainsIndex signaling that the Relationship Property Index was used.Without the indexCypher version: CYPHER 4.3 planner: COST, runtime: INTERPRETED*. 4065736 total db hits in 2571 msWith the indexCypher version: CYPHER 4.3, planner: COST, runtime: INTERPRETED*. 999737 total db hits in 744 ms.*I am using Community Edition which is why the INTERPRETED runtime is used, if you are running Enterprise Edition it will use the new faster PIPELINED runtime.Now That You Mention It, How Much Faster Is PIPELINED?By switching to Enterprise Edition, Neo4j will automatically use the PIPELINED runtime. The PIPELINED runtime uses algorithms to group the operators in the execution plan in order to optimize performance and memory usage. In doing so, it is able to execute the same query with the Index in 507 ms, which is 1.5x faster than INTERPRETED .Enterprise without Index: CYPHER 4.3, planner: COST, runtime: PIPELINED. 4952883 total db hits in 1847 msEnterprise with Index: CYPHER 4.3, planner: COST, runtime: PIPELINED. 1228764 total db hits in 507 msPlease note these results are provided for illustrative purposes only. They were all run on a 1GB Docker instance.It is also worth giving a quick shout-out to another handy feature in Neo4j 4.3 — EXPLAIN plan logging, covered in the next section, which can help you troubleshoot use of indexes in queries..Troubleshooting with EXPLAIN Plan LoggingEvery time a CYPHER query is run, it generates and uses a plan for the execution of the code. The plan generated can be affected by changes in the database (such as a new index being added). It is not possible to see which plan was used. If you need to see what is happening at the query plan execution level, you may want to enable the new logging feature.The new configuration setting helps DBAs troubleshoot queries and confirm which plan was used when the query plan was generated and record it in the log.Name: dbms.logs.query.plan_description_enabledDatatype:boolean, default value = falseDynamic: trueNOTE: Enabling this option will impact the performance of the database, because of the cost of preparing and including the plan, Neo4j does NOT recommend leaving this set to true.Stay TunedThat’s all for this week on Relationship / Property Indexes. Look out for the next blog in the series — Relationship Chain Locks where you can find out how not to block, when you update the Rock!;Jun 24, 2021;[]
https://medium.com/neo4j/neo4j-graphacademy-news-for-q4-2022-78d307e70f3d;Elaine RosenbergFollowJan 31·5 min readNeo4j GraphAcademy News for Q4 2022Every quarter we share the exciting new courses, additions, and changes in GraphAcademy to keep you informed about new and better ways to learn all about making use of graphs in your work.The last few months of 2022 were busy for us as we prepared for NODES 2022, but we’ve still got some exciting updates for you.Sandbox Issues ResolvedWe are aware that a number of users have experienced problems connecting to Neo4j Sandbox while attempting challenges in GraphAcademy.We’re pleased to say that after some investigation, we have managed to identify and solve the problem.If you are still experiencing problems with the integrated Sandbox window in GraphAcademy, you can also find the Sandbox credentials on the right hand side of the course overview page. Clicking on the Sandbox URL will take you to Neo4j Browser.Sandbox Credentials ExampleYou can also access the Sandbox instance by logging into sandbox.neo4j.com with your GraphAcademy user credentials.If you are still experiencing problems, please feel free to reach out to us at graphacademy@neo4j.com.Lightning QuizzesIf you don’t have time to complete every challenge but would still like to earn your badge, you can now take a Lightning Quiz to test your understanding and pass the course.Lightning QuizzesIf you enrolled more than seven days ago, you will be able to access the Lightning Quiz from the course overview page by clicking the Start Quiz button. If you get 100% of the answers correct, you will pass the course!New GraphAcademy Course: Building Neo4j Applications with TypescriptWe are pleased to announce our new Building Neo4j Applications with Typescript course.Neo4j Typescript App Development CourseThe course, estimated at two hours, is shorter than the other courses in the Developer learning path, featuring only three succinct code challenges designed to teach you everything you need to know without being overly repetitive.The Creating a Driver lesson challenges you to write the code to create a Neo4j Driver instance and use that instance to find the director of a movie in the graph, while to pass the Writing Data to Neo4j lesson you must add yourself as an actor in The Matrix.All code challenges take place using a service called Gitpod, allowing you to run and complete the tests in your browser without installing any code (Gitpod is a hosted cloud version of VS Code).Gitpod VS Code instance for the GraphAcademy Typescript CourseIn this course you will learn:The lifecycle of the Neo4j Driver and how it relates to your applicationHow to install and instantiate the Neo4j JavaScript Driver to your TypeScript projectHow to execute Read and Write TransactionsHow to define TypeScript types and interfaces to add type checking to your TypeScript codeEnroll in Building Neo4j Applications with TypeScriptUpdated GraphAcademy Course: Building Neo4j Applications with Node.jsAs with the new TypeScript course, the Node.js course has been updated to run in Gitpod. This means that you don’t have to clone the repository in order to complete the course. Everything can be done online directly from your internet browser.Check out the changesPath Finding Algorithms with Neo4j Graph Data ScienceThe Path Finding with GDS course, released in October, teaches you the various methods available for finding the shortest paths between pairs of nodes in your Neo4j graph and includes examples for both weighted and unweighted relationships.Take the Path Finding with GDS course with Neo4j GraphAcademyThis course teaches you how to find the shortest paths between pairs of nodes in the graph and includes examples for…graphacademy.neo4j.comThe course is the first in a series of applied courses for data scientists interested in improving their machine learning algorithms using graph features.In this one-hour course, you will learn:The difference between weighted and unweighted relationshipsHow to use Cypher to find the shortest paths between a pair of nodesHow to use the Neo4j Graph Data Science library to find weighted shortest pathsYour Path to Neo4j CertificationIf you have not done so already, consider becoming a Neo4j Certified Professional.GraphAcademy Certification T-Shirt 2023To become certified, you enroll in the Neo4j Certified Professional exam.We recommend that you take these courses to prepare for certification:GraphAcademy Learning PathNote that we have a limit of one t-shirt per certification so if you certified prior to 2023 and already received a t-shirt, you cannot receive another one.NODES 2022 VideosWe hope that you were able to attend our most recent online conference, NODES 2022.This conference had over 100 speakers from all over the world.Some of our favorites include:Building a Neo4j/Python OGM by Estelle ScifoNeo4j With Docker and Docker Compose Deep Dive by Christophe WillemsenHidden in the Clouds: Using Graph Technology to Understand Your Cloud Estate by Rhys EvansUsing Graph for Suspicious Bitcoin Transactions by Adam TurnerBuilding a Visual Rail Planner with NeoDash by Niels De JongYou can view all videos from our conference here.Nodes 2022 Videosneo4j.comThank you for enrolling in our GraphAcademy courses. We always appreciate your feedback so we can continue to improve our courses and educational content.Feel free to share your thoughts with the Neo4j Community.Happy learning,Adam Cowley, Elaine Rosenberg, and the Neo4j Developer Relations Team;Jan 31, 2023;[]
https://medium.com/neo4j/cypher-sleuthing-dealing-with-dates-part-4-ae162fe8c27c;Jennifer ReifFollowAug 18, 2021·10 min readCypher Sleuthing: Dealing with Dates, Part 4Image by Markus Winkler, Unsplash*Latest version with Neo4j Browser duration format changes is available at https://jmhreif.com/blog/cypher-sleuthing-dates-part4/My previous part 1, part 2, and part 3 posts on this topic introduced Cypher dates, translated formats to the Cypher-supported ISO 8601 format, calculated durations, accessed components, and translated durations to certain values.Cypher Sleuthing: Dealing with Dates, Part 1No matter what database, programming language, or webpage you might be using, dates always seem to cause headaches…neo4j.comCypher Sleuthing: Dealing with Dates, Part 2My previous part 1 post on this topic introduced Cypher dates and translated a few other date formats to the…neo4j.comCypher Sleuthing: Dealing with Dates, Part 3My previous part 1 and part 2 posts on this topic introduced Cypher dates, translated formats to the Cypher-supported…neo4j.comIf you read those, then this post is the next one in the series showing you how to convert durations across component categories. If you haven’t read the previous posts leading up to this one, feel free to catch up (recommend at least Part 3 as this post’s prequel) — though it isn’t required. 🙂In our Part 3 post, we covered three duration categories (and related functions) for converting duration values to whole values — months, days, seconds. We also looked at components and how to use them to access other values outside of those months, days, and seconds.In this post, we will combine the three duration functions along with various components to translate values from a unit in one category to a unit in another category. Then, we will take a look at a couple of fun puzzles having to do with Cypher and dates to wrap up.Rewind: Quick CatchupLet’s review a couple of principles on duration categories and components from our last post before we dive into using them together.There are 3 component groups (shown again in the screenshot below), and these form the foundation for duration units and conversions.The table can be found in the Cypher manual section.As a reminder: the column on the right is the key — values in one column can be converted to any other unit in that same cell, but not another row.For instance, I could convert a duration from days to weeks, but not to months or minutes. Notice, also, that these categories correspond to our duration functions of inMonths(), inDays(), and inSeconds(), which we will discuss here!Finally, here are the rules I discovered for the Cypher component groups.You can only convert among units in a component grouping, not across groups.There must be whole values to convert to larger units. Components do not mix whole and fraction values (i.e. 36 hours -> 1.5 days). There are specific components that handle whole values of the unit, and there are separate components that handle only fractions of the unit.Photo by Jon Tyson on UnsplashNow, per our rules, we are limited in expressing certain durations as numbers that aren’t easy to understand or read. For instance, saying that my flight leaves in 28 hours (while precise) probably triggers a mental calculation to 1 day 4 hours. Because our brains are used to allocating resources into the largest buckets first, then remainders into smaller categories, our perception of time and planning seems to operate better in these formats.As another example, most things operate on a 12-hour clock (3:00pm), except for where precision matters. In those cases, you will see a 24-hour clock (15:00). But telling a random person on the street that the time is fifteen hundred will most likely return confused looks. Think smaller numbers, larger units (1 day vs 24 hours, 2 months vs 60 days, etc).OK, so how can we translate some of these larger-number-smaller-unit values into something more easily understood?Earlier, I mentioned that the duration categories (Months, Days, Seconds) align with the duration functions (inMonths(), inDays(), inSeconds()). This is to allow conversions across component groups!Combining Duration Functions and ComponentsUsing durations functions with components means we can convert our duration values from one component category into another, and then translate among the components within that group.Note that precision of conversions may not always be 100% accurate, as certain conversions are not consistent throughout the year. However, the conversions use general standards (30 days in a month, 24 hours in a day).Let’s expand the power of converting durations across categories by combining functions and components!Example 1: Translate duration from months/days to weeks/daysMATCH (v:Vacation)RETURNduration.between(date(),v.startDate) as preciseDuration,duration.inDays(date(),v.startDate).weeks as weeks, duration.inDays(date(),v.startDate).daysOfWeek as daysOfWeekIn our example above, we are translating a duration in months and days into days and weeks. First, we need to calculate the entire value into days using the duration.inDays() method. Then, we can access the weeks and days components from that.Note that, depending on the input dates (and year), the results can vary — i.e. durations with those same dates but in a leap year or from June 30-Sept 1 that includes two consecutive months with 31 days each.Let’s take another example!Example 2: Calculate projected hours a resource works on a projectMATCH (:Employee)-[rel:ASSIGNED]-(p:Project)RETURN duration.between(rel.startDate, p.expectedEndDate) as lengthAssigned, duration.inSeconds(rel.startDate, p.expectedEndDate).hours as lengthInHoursYou might respond that this assumes an employee spent 24 hours per day and seven days per week on a project, right?You would be correct! Let’s correct this, since our lengthInHours value is now just a number and not a duration value.MATCH (:Employee)-[rel:ASSIGNED]-(p:Project)RETURN duration.between(rel.startDate, p.expectedEndDate) as lengthAssigned, duration.inDays(rel.startDate, p.expectedEndDate).weeks as lengthInWeeks, duration.inDays(rel.startDate, p.expectedEndDate).weeks * 40 as projectHoursNotice that in order to get the project hours, I took the approach to find the total number of weeks for the project, then calculate the number of hours per week worked (assuming a 40 hour work week and 100% allocation of the resource).Now, this number may not be the actual total of the completed project, but it would definitely be a good estimate number.Switching gears just a bit, infant age is always confusing to me because some people go by weeks, months, or years (e.g. baby is 14 months old… cue math calculation in brain). Let’s let Cypher do the work.Example 3: Calculate age of infantMERGE (b:Baby) SET b.dateOfBirth = date(‘2021–02–28’)RETURN b.dateOfBirth, duration.between(b.dateOfBirth, date()) as age, duration.inMonths(b.dateOfBirth, date()).months as months, duration.inDays(date(‘2021–02–28’),date()).weeks as weeksIn the code above, we set the baby’s birthdate as February 28, 2021, then in the return statement, calculate the baby’s age by precise duration, months, and weeks.In order to get months, we calculate the duration directly to months with the inMonths() function. Then, to get weeks, we first need to convert to the days/weeks category using the inDays() function, then grab the component for weeks. No more mental calculation!We could be even more specific with the components to preserve remainder values.Example 4: Calculate age of infant with remainder unitsMATCH (b:Baby)RETURN b.dateOfBirth, duration.between(b.dateOfBirth, date()) as age, duration.inDays(b.dateOfBirth,date()).weeks as weeks, duration.inDays(b.dateOfBirth,date()).daysOfWeek as daysOfWeekWe could use this in a baby tracker app to tell us that a baby is exactly 19 weeks and 1 day old. This could also be used for more precision on a product/application being live, amount of time without incidents, or many other use cases!Cypher PuzzlesIn the past couple of weeks, I have come across a couple of fun puzzles with Cypher dates that I’d like to share with you. I will include answers, but I’ll post those at the bottom, so that those who want to challenge themself without peeking first can solve the puzzles.Postgres SQL RangesThe first challenge is a calendar appointment query. Postgres received an update, which improves queries in SQL for range data. This gives us a fun opportunity to see what Cypher’s version of this looks like. Let’s take a look at the question.Available dates in the next month (source)Updated SQL solution:SELECT datemultirange(daterange(‘2021–06–01’, ‘2021–06–30’, ‘[]’))        - range_agg(appointment_dates) AS availabilityFROM appointmentsWHERE appointment_dates &&       daterange(‘2021–06–01’, ‘2021–06–30’, ‘[]’)Solve away! Answer will be posted at the bottom of this post.Weekly Progress of YearOur second challenge is to write a Cypher query that visualizes progress through the year on a weekly basis. We will stick to using regular characters to visualize the progress, so it won’t be anything fancy and no extra tools will be needed. Let’s see our task.Create a progress bar for how many weeks of the year have passed (include percentage, too)Example characters for progress visualization:28*’#’ + 22*’-’Have at it! Answer will be posted at the bottom of this post.KudosQuick shout-out to my colleague Michael Hunger who suggested both of these challenges and provided far cleaner and efficient solutions than those I was able to draft. 😁 Michael is widely revered as the founder of the APOC library, contributor of many other core aspects of Neo4j, and guru on Cypher. He may very well be the cornerstone of Neo4j developers and maintains an impressive presence on all content platforms, so if you needed help on anything Neo4j-related over the years, there’s a high probability that you have run into him.Solution: Postgres SQL RangesThere are several ways you can write this query, and even more when you start considering different data models. However, we will see two solutions that work, then I’ll include a brief explanation of the logic.Option 1:WITH date(‘2021–06–01’) as start, date(‘2021–06–30’) as endUNWIND [days IN range(0,duration.between(start,end).days) |          start + duration({days:days})] as dayOPTIONAL MATCH (a:Appointment {date:day}) WITH * WHERE a IS NULLRETURN dayOption 2:WITH date(‘2021–06–01’) as start, date(‘2021–06–30’) as endUNWIND [days IN range(0,duration.between(start,end).days) |          start + duration({days:days})] as dayWITH * WHERE NOT EXISTS { (:Appointment {date:day}) }RETURN dayIn both of our solutions, we are first setting a start and end date of the month (you could choose any, but we just picked last month) and unwinding the days between those two dates (from start to end of the duration between) as each day. Then, we take all those days and see if there is an appointment that already exists on any of them and return only the remaining days.Solution: Weekly Progress of YearJust as with the first challenge’s solution, there are some different ways to tackle this one. You could use a variety of characters and tools to create something intriguing, but we are keeping it simple, clean, and efficient.Option 1:WITH datetime().week as week, 52 as weeksRETURN reduce(r=’’, w in range(1,weeks) |         r + CASE WHEN w < week THEN ‘#’ ELSE ‘-’ END)         + ‘ ‘ + (100*week/weeks) +’%’ as progressOption 2:WITH datetime().week as week, 52 as weeksRETURN reduce(r=’’, w in range(1,weeks) |         r + CASE WHEN w < week THEN ‘>’                  WHEN w=week THEN ‘8’                  ELSE ‘<’ END)         + ‘ ‘ + (100*week/weeks) +’%’ as progressThe solutions to this problem might appear a bit more complicated, but we can break them down into manageable pieces. First, we need to find out what week of the year is the current week and note the total weeks in the year as our starting and end points. The next line returns the calculation using a reduce() function, which hops through a list of items and aggregates the current item to the current aggregate-value of all previous items. For example, if I had a list of 1,2,3, then reduce with a sum would have final results of 6 (1+2+3).Inside reduce(), we establish a result variable, and loop through each week in the number of weeks in the year (range(1,weeks)). On the right side of the pipe character, we then have our expression to aggregate our variable at each item in the list. We evaluate r — when the week number from our loop is less than the current week (in the past), we use one character (solution 2) when the week is current week, we use another character and when the week is greater than current week (yet-to-come), then we use a different character. Finally, we attach a percentage to the end of the output by calculating the current week number divided by total weeks (28/52) and multiplying the resulting fraction by 100 for the result.Wrapping Up!In this post (Part 4!), we have seen how to combine duration functions and components in order to translate durations in one component category to another one. Then, we put our new skills to the test with a couple of Cypher date challenges, stating the problems to solve, and then walking through some solutions and their logic.If you’d like to see some more Cypher sleuthing, I presented a session at NODES (Neo4j’s online developer conference) that covered a high-level overview of these date concepts and other gotchas. The recording is now available on YouTube!In the next post, we will step through the date procedures/functions that the APOC library offers and discuss which ones are replaceable with built-in Cypher functionality or still required to accomplish specific tasks related to temporal data.Until next time, happy coding!ResourcesCypher Sleuthing: Part 1Cypher Sleuthing: Part 2Cypher Sleuthing: Part 3Cypher Manual: Duration functionsCypher Manual: Duration ComponentsCypher Manual: Reduce functionCypher Manual: Range functionNODES 2021: Cypher Sleuthing presentation;Aug 18, 2021;[]
https://medium.com/neo4j/flights-search-application-with-neo4j-grandstack-and-graphql-custom-resolver-part-3-4d14d174d980;Vlad BatushkovFollowApr 20, 2020·9 min readFlights Search Application with Neo4j — GRANDstack and GraphQL Custom Resolver (Part 3)GRANDstack overview: Neo4j Database, React, GraphQL API on Apollo server powered by neo4j-graphql-js libraryIn this series of articles, I share my experience of building a web application that you can use to search for flights. The project was started for Neo4j Workshop in Bangkok, November 5, 2019. I hope this series can help you in your learning journey of modern web development tools using Neo4j, Docker and GraphQL. This article covers the last phase of project development: applying GRANDstack framework. I will share how easy it can be to use the GRANDstack toolset to build a full-stack web application. Project GitHub repo with all source code you can find here. Previous parts of this series:Dockerizing (Part 1)Cypher (Part 2)TargetThe general idea of Flights Search Application is to have a web page with a search box, where you can define a place of departure and destination, choose the date and voila — a list of available flights would appear below. Our needs are limited by only a one-way flight, no class options, and no other fancy options. A brutal one-way flight for MVP.Very basic Flights Search barArchitectureBefore jumping to the front-end, let’s briefly overview an overall design of an existing Flights Search Application. The database is provided as a Docker image, including 1 month of daily Flights scheduler of Airports all over the world from openflights.org (data was modified to Neo4j-friendly imports with my small dotnetcore console app). A detailed explanation of how to build your own Neo4j Docker Image with Database import using the neo4j-admin import tool is in my first part of this series.Architecture for the Flights Search applicationI already wrote a query to find flights. From the first view, it may look like an ugly monster, but, don’t worry, the query is not as complex as it looks.The query is explained in the article how to write your own APOC Custom Procedures & set it up in Docker using Cypher-shell. The database Schema is represented below (not totally the same, but 99%).Max De Marzi original content. Flight Search Proof of Concept Database Schema.GRANDstackNow let’s move forward. First, a few words about the general idea of GRANDstack, a rock-star mix of modern technologies to quickly build a full-stack application.It’s no secret that graphs drive modern development and the latest technology in this area is GraphQL. GraphQL APIs are aggressively replacing REST API endpoints in many companies.Why not use a native graph database like Neo4j with graph-oriented GraphQL queries? It sounds like a really smart idea. Having any UI framework aware of GraphQL, we’ll get a full-funnel graph-oriented solution.GRANDstack is a framework that combes these technologies into one piece.GraphQL + React + Apollo + Neo4j Database = GRANDstackI want to point out, that React as R”-component, actually can be replaced with any other client-side library or framework: Angular, Vue, Elm or whatever you want. The same is true for Apollo server, with some other options to replace A”.From my point of view, the fundamental component” of GRANDstack is a neo4j-graphql-js library, that translates GraphQL query to Cypher and connects G” GraphQL with ND” Neo4j Database.Web ApplicationFlights Search Application requires mostly super simple stuff on the front-end side. GRANDstack makes development easier than you can imagine.From the GitHub GRANDstack starter project, we can fork or copy a bootstrap of our future Application and implement Flights Search on top of it.How many pages do we need in the Flights Seach App?Grid-view pages: list all entities of Airports and Airlines.Map page: beauty world map to visualize Airport connections.Search for Flights: the main page with a search bar and list of result flights.Now, one by one.Grid pageLet’s build a grid-view page. The easiest page.Airports table viewI can totally reuse the provided React List page as a template for my Airports (as well for Airlines). GraphQL schema definition for type Airport will be totally based on the Airport node structure from Neo4j Database.Compare GraphQL Airport Type definition to some Airport Node in Neo4j DatabaseThe definition of Airport type has a location” property of type Point. This type, provided by neo4j-graphql-js and gives us the ability to work with Spatial data in a simple and short way (more about Spatial Type in docs).Really impressive, how many other different useful types neo4j-graphql-js library gives us absolutely for free. You can scroll Schema definitions and find various other types. For example, here we have all possible Types for grid use-cases: filtering, sorting, pagination.Use-case for filter, orderBy, firstGrid page is nothing more than just a representation of a list of Airports. React component Query from react-apollo library helps us to request the data.Filtering a table by city name Moscow” will translate GraphQL into the following Cypher query:MATCH (`airport`:`Airport`) WHERE (`airport`.code CONTAINS $filter.code_contains) AND (`airport`.country CONTAINS $filter.country_contains) AND (`airport`.city CONTAINS $filter.city_contains) WITH `airport` ORDER BY airport.name ASC RETURN `airport` { .code , .name , .city , .country ,location: { longitude: `airport`.location.longitude , latitude: `airport`.location.latitude }} AS `airport` LIMIT toInteger($first){   offset : 0,   first : 100,   filter : {     code_contains :   ,     country_contains :   ,     city_contains :  Moscow   }}Map pageHere we will use the power of GraphQL augmented schema to showcase some Cypher-related features. Here is the page with a world map to visualize how departure Airport is connected to arrival Airports:From Sheremetyevo Airport to any place in the world (almost)The UI implementation of this page is based on React Simple Maps. I will not focus on this topic in the article.From the GRANDstack perspective, there are 2 properties in GraphQL schema for Airport Type:directs” — list of all Airports connected to this Airport with Direct Flight.neighbors” — list of Airports in the same city as this Airport. But not this Airport itself.DirectsThe directs” property is built with the help of the relation directive. In Neo4j Database all Airports connected to each other with [FLIES_TO] relationship. We can simply declare this relationship in GraphQL schema, request a field and enjoy the data, provided by automatically generated Cypher query.SVO — [FLIES_TO] → Airports (only shows 25 items from 143 total)NeighborsThe neighbors” property represents a relationship that does not exist in the Neo4j Database, so there is no way to use relation directive. To solve this problem we use the cypher directive. It allowed us to create our own Cypher query and resolve this property of GraphQL schema.MATCH (a:Airport) WHERE this.city = a.city AND this <> a RETURN aSearch pageFlights Search functionality is implemented as a Neo4j Custom Procedure and now I want to call it using the GraphQL resolver and map the result object to the GraphQL schema.Let’s look at the structure of the Procedure result. In the example below a single item look like this:[{   flights : [    {       flight : {         flight_number :  SU_BKK_SVO_20200101 ,         price : 12800,        ...      },       company : {         name :  Aeroflot Russian Airlines ,         code :  SU ,         country :  Russia       }    }  ],   route : [    {       code :  BKK ,       name :  Suvarnabhumi Airport ,       city :  Bangkok ,       country :  Thailand     },    {       code :  SVO ,       name :  Sheremetyevo International Airport ,       city :  Moscow ,       country :  Russia     }  ],   stops : 0}, ...]An array of objects with more inherited array objects. JSON of Neo4j Types:[{  flights: [    {      flight: { Flight },      company: { Airline }    }  ],  route: [    { Airport }  ],  stops: Int}, ...]The main complexity of this result — it contains objects, that does not match to any existed Nodes and Relationships of Neo4j Database.The challenge is how to make this JSON result properly map to the GraphQL schema.Try to use cypher directiveI tried to use existing functionality and call a Custom Procedure using cypher directive. Automatic object mapping, unfortunately, this didn’t work.Disclaimer: In the last section, I will show my own way to solve this issue. But I would gladly look into an elegant solution using less manual work. If you faced the same issue and alredy know fast and the furious solution, please, share in comments. My approach is described only in learning purposes.GraphQL Custom Resolver and MappingInstead of using cypher directive, I do a manual database call by using a GraphQL Custom Resolver. The resolver does a call of the Custom Procedure using Neo4j javascript driver.But now all returning types are Neo4j Types and I lost all the magic of auto-mapping from neo4j-graphql-js library. Javascript driver returns objects of Neo4j Types, not GraphQL Types.Lesson learned: neo4j-graphql-js does a great job, by translating Neo4j Types into GraphQL Types (for example, _Neo4jDateTime and _Neo4jPoint ). Without neo4j-graphql-js mapping, we need to write it by ourselves. Let’s understand, how painful is this.Cypher Map projectionFirst of all, we can try Type mapping on the Neo4j side. From Neo4j Type to base GraphQL primitive types (existed scalar types).Let’s get rid of Node types, they are pretty bulky. A possible way to translate Nodes into plain JSON objects is to use Cypher map projection: x { .* }Before, getFlightsNodes returns Nodes...WITH collect({ stops: 2, route: list.route, flights: [{ flight: f1, company: a1 }, { flight: f2, company: a2 }, { flight: f3, company: a3 }] }) as transfers, transfer, direct...After, getFlightsObjects returns plain JSON objects...WITH collect({ stops: 2, route: list.route, flights: [{ flight: f1 { .* }, company: a1 { .* } }, { flight: f2 { .* }, company: a2 { .* } }, { flight: f3 { .* }, company: a3 { .* } }] }) as transfers, transfer, direct...This projection helped to get rid of Nodes, but we still have property Types to map: Int, DateTime, Point.Scalar TypesAnother technique to try is using custom GraphQL Scalar Types. For example, we have a DateTime Type structure as a result of the javascript driver.I want to have something much more simple in my GraphQL schema. So I can declare custom FlightsDataTime and FlightsInt as GraphQLScalarTypes.The GraphQL API is ready to handle the new FlightsSearchObjects query.The main goal achieved: the JavaScript driver returns a readable result from the Custom Procedure and is properly processed by GraphQL (thankful to Cypher Map projection and GraphQL Scalar Types).Is it a perfect solution? Frankly speaking, I don’t think so… But exploring the process was fun. Now we know a little bit more about the core stuff.Flights Search page works! The procedure analyzes thousands of paths between Airports in seconds and shows to user details about flights. Pretty good result for a pure Neo4j-based solution.End of Part 3. End of series. ConclusionThat’s it! It was a long trip. The workshop event was hosted in November 2019 and the last part of the article series finally published in April 2020. Wow.A lot of things in implementation were changed compared to what was discussed and showed on that Workshop. A lot. Every step of work was revisited and improved. Fork the Flights Search GitHub repo to explore the project and share your opinion.For every article, I provided a deep dive into details of implementation with problems, solutions, and workarounds I faced. It was hard work. This is why I really believe, that this educational project can help other developers like me to save time and made a correct decision.Thanks for reading. And enjoy work with Neo4j Database.Cheers!Resourcesvladbatushkov/flightsCypher queries Original .csv files from openflights.org Dotnet core C# console application to generate .csv files of…github.comneo4j-graphql.js API Reference · GRANDstackThis reference documents the exports from neo4j-graphql-js: Wraps makeExecutableSchema to create a GraphQL schema from…grandstack.ioneo4j-graphql/neo4j-graphql-jsA GraphQL to Cypher query execution layer for Neo4j and JavaScript GraphQL implementations. …github.comDesigning Your GraphQL Schema · GRANDstackThe goal of this guide is to explain how to design your GraphQL schema for use with GRANDstack and neo4j-graphql. We…grandstack.io;Apr 20, 2020;[]
https://medium.com/neo4j/exploring-the-u-s-nation-bridge-inventory-with-neo4j-part-4-5be6310b883;Michael McKenzieFollowMay 11, 2021·5 min readJared Erondu on UnsplashExploring the U.S. National Bridge Inventory with Neo4j — Part 4: Cleaning up Counties, Places, BridgesIn part 3, we began building a hierarchical structure making connections from bridges to places to counties to states. We ended with more :County nodes in the graph than anticipated. In part 4, we will identify and cleanup invalid” nodes. There is much more to this than we can cover in a series like this. We will focus on some of the basic assumption we made regarding the data itself.Invalid” NodesIn the context of our exploration, the term invalid” simply refers to nodes that are incorrect due to our interpretations or assumption about the data. It doesn’t mean the raw data itself is incorrect (that’s an entirely different discussion).We made two assumptions when we first began.Counties never change. This isn’t something one generally considers. In fact, I only learned of this when trying to figure out what was wrong. This is beyond the scope of what we’ll be reviewing.Raw data is reliable. This one hits closer to home and is what most the readers will be more familiar with. Let’s focus here.According to the encoding document the STATE_CODE_001, COUNTY_CODE_003, PLACE_CODE_004, and STRUCTURE_NUMBER_008 raw data fields should consist of two, three, five, and fifteen digits, respectively. Armed with that clarification, we will add :Invalid labels to the (:Bridge) , (:Place) , and (:County) nodes that violate these string restrictions.// Set Invalid BridgesMATCH (b:Bridge)WHERE size(b.countyCode) <> 3OR size(b.placeCode) <> 5OR size(b.code) <> 15SET b:Invalid// Set Invalid PlacesMATCH (p:Place)WHERE size(p.countyCode) <> 3OR size(p.code) <> 5SET p:Invalid// Set Invalid CountiesMATCH (c:County)WHERE size(c.code) <> 3SET c:InvalidNow we can query the path (:State)-[:HAS_COUNTY]->(:County)-[:HAS_PLACE]->(:Place)-[:HAS_BRIDGE]->(:Bridge) to get a breakdown and count of the nodes with the :Invalid label:MATCH (s:State)-[:HAS_COUNTY]->(c:County)-[:HAS_PLACE]->(p:Place)-[:HAS_BRIDGE]->(b:Bridge)WITH s, c, p, bRETURN s.code AS sCode,        NOT c:Invalid AS validCounty,        NOT p:Invalid AS validPlace,        NOT b:Invalid AS validBridge,        count(*) AS countORDER BY sCode, validCounty, validPlace, validBridgeValid” node countsBased on how we built the hierarchical tree structure starting with the (:Bridge) , if a (:County) is :Invalid , then every connected (:Place) and (:Bridge) are also :Invalid . Similarly, if a (:Place) is :Invalid , then every connected (:Bridge) are also :Invalid  however, the connected (:County) may or may not be :Invalid . It is impossible to have an :Invalid parent node with a valid child node along the (:State)-[:HAS_COUNTY]->(:County)-[:HAS_PLACE]->(:Place)-[:HAS_BRIDGE]->(:Bridge) .Reprocessing :Invalid NodesWith the :Invalid label added to the graph we can reprocess the data with our improved logic.Add processing label to the (:Row) nodes connected to the (:Bridge:Invalid) nodes:// Query `(:Bridge:Invalid)` and set a processing label on the connected `(:Row)`:CALL apoc.periodic.iterate(‘MATCH (b:Bridge:Invalid)RETURN b‘,’MATCH (b)<-[:DATA_FOR]-(row)SET row:ConnectRowToBridge‘,{batchSize:1000,parallel:true})2. Reprocess (:Row:ConnectRowToBridge) ://Reprocess (:Row:ConnectRowToBridge) nodes with updated logic: CALL apoc.periodic.iterate(‘WITH 3 AS cCodeSize,     5 AS pCodeSize,     15 AS bCodeSizeMATCH (row:ConnectRowToBridge)WITH row,     cCodeSize,     pCodeSize,     bCodeSize,     cCodeSize — size(row.COUNTY_CODE_003) AS cDiff,     pCodeSize — size(row.PLACE_CODE_004) AS pDiff,     bCodeSize — size(row.STRUCTURE_NUMBER_008) AS bDiffRETURN row.STATE_CODE_001 AS sCode,       apoc.text.repeat(0”,cDiff) + row.COUNTY_CODE_003 AS cCode,       apoc.text.repeat(0”,pDiff) + row.PLACE_CODE_004 AS pCode,       apoc.text.repeat( ,bDiff) + row.STRUCTURE_NUMBER_008 AS bCode‘,’MERGE (bridge:Bridge {stateCode: sCode,                      countyCode: cCode,                      placeCode: pCode,                      code: bCode})ON CREATE SET bridge:ConnectToPlace‘,{batchSize:10000, parallel:false})We pad the COUNTY_CODE_003 and PLACE_CODE_004 with 0’s, and the STRUCTURE_NUMBER_008 with spaces. This is just to address the string length requirement and does not address any other value validation step.With the new (:Bridge) nodes created from the updated logic we can continue our processing by creating (:Bridge)<-[:DATA_FOR]-(:Row) :CALL apoc.periodic.iterate(‘WITH 3 AS cCodeSize,     5 AS pCodeSize,     15 AS bCodeSizeMATCH (row:ConnectRowToBridge)WITH row,     cCodeSize,     pCodeSize,     bCodeSize,     cCodeSize — size(row.COUNTY_CODE_003) AS cDiff,     pCodeSize — size(row.PLACE_CODE_004) AS pDiff,     bCodeSize — size(row.STRUCTURE_NUMBER_008) AS bDiffRETURN row,       row.STATE_CODE_001 AS sCode,       apoc.text.repeat(0”,cDiff) + row.COUNTY_CODE_003 AS cCode,       apoc.text.repeat(0”,pDiff) + row.PLACE_CODE_004 AS pCode,       apoc.text.repeat( ,bDiff) + row.STRUCTURE_NUMBER_008 AS bCode‘,’WITH row, sCode, cCode, pCode, bCodeMATCH (bridge:Bridge {stateCode: sCode,                      countyCode: cCode,                      placeCode: pCode,                      code: bCode})CREATE (row)-[:DATA_FOR]->(bridge)WITH rowREMOVE row:ConnectRowToBridge‘,{batchSize:1000,parallel:false})At this stage, we can rerun the previous queries building and connecting the appropriate (:Place) and (:County) nodes. Let’s note that we do not need to update the logic for those steps since we have rectified the errors and assumptions made at the (:Bridge) level.Now we can delete all of the (:Invalid) nodes from the graph:CALL apoc.periodic.iterate(‘MATCH (inv:Invalid)RETURN inv‘,’DETACH DELETE inv‘,{batchSize:1000,parallel:false})Lessons LearnedUse your resources! Lucky for us we have the encoding document readily available for reference. That isn’t always the case. You will likely have to revisit logic the correct mistakes you have made. That’s one of the great things about graph databases and Neo4j, you can do that pretty darn easily.Validation checks are nice. It isn’t always easy to do depending on the data you are working with, but finding a way to add validation check steps into your ETL process can help save you a lot of time before running long import queries. It can also be a good way to incorporate someone with better expertise on the project to help review data as well.UI is great. Seeing data through a different lens can also help provide improved understanding to the data you are looking at. Being able to review the data in a way that relates to what is being represented by the data can help provide additional direction. This is something we’ll encounter in a later post.We now have an improved (:State)-[:HAS_COUNTY]->(:County)-[:HAS_PLACE]->(:Place)-[:HAS_BRIDGE]->(:Bridge) hierarchical structure, but a bridge is a thing that exists in a location. In part 5 of the series we look at a critical piece of the bridge data, its longitude and latitude.;May 11, 2021;[]
https://medium.com/neo4j/a-graph-neural-network-to-approximate-network-centralities-in-neo4j-2ee96705a464;Kristof NeysFollowOct 19, 2020·7 min readA Graph Neural Network to approximate Network Centrality metrics in Neo4jIn this post, we will look at how a Graph Neural Network can be deployed to approximate network centrality measures, such as Harmonic centrality, Eigenvector centrality, etc. and include them as properties in a Neo4j graph.A common challenge graph analysts face is the time complexity constraints many of the most important centrality metrics have.For instance, Cohen et al. illustrate in Computing Classic Closeness Centrality, at Scale”, that calculating the closeness centrality on a network of 24 million nodes takes an astonishing 44,222 hours, or around 5 years, to compute (assuming standard computing resources).Indeed, Betweenness centrality, when applying the Brandes algorithm, has a time complexity of O(|V||E|) and consequently large networks will require an ever-increasing runtime when computing this centrality metric. On the other hand, the degree centrality measure has a time complexity of O(|V|) and therefore computes in a rather fast runtime.Outline & Key findingsThe outline of the project Mark Needham and I did is as follows: we assume a specific graph exists in Neo4j on which the graph analyst wants to compute the Harmonic centrality measure. Once we have obtained the graph to be studied from Neo4j, using the Python driver, we load it in a Graph Neural Network (GNN). This model in turn generates the predicted Harmonic centrality values, which then gets returned to the graph in Neo4j as node properties.The key finding of our project is that a sophisticated GNN, that combines representation learning with Deep learning, succeeds in finding an approximation of the Harmonic centrality, using Degree centrality as input, with a very high Kendall coefficient of around 95%, and this at a fraction of the run time when using the exact algorithm.Harmonic Centrality measureWe selected the Harmonic centrality, a geometric type measure, for the main reason that it has been described by Boldi and Vigna, in their paper Axioms for Centrality”. It is the only centrality metric that satisfies all the axioms they developed to rank these metrics. In other words it’s the ‘Gold standard’ of centrality metrics, and of course, it forms part of Neo4j’s Graph Data Science library.To recap, the Harmonic centrality is a ‘distance’ based centrality measure and an improvement on the closeness centrality measure. The latter scores the centrality of a node based on the mean distance it has to all other nodes in the network. As a result, a high closeness centrality implies that this specific node can on average reach other nodes in fewer steps. The main issue with this approach is that nodes that are un-reachable have a distance of infinity. To address this issue, Marchiori and Latora proposed in Harmony in the small-world” to replace the average distance with the ‘harmonic mean’ of all distances. Formally, the harmonic centrality of node x is defined as:As such, any unreachable nodes, and hence any infinity term, is excluded.Real-World Network To illustrate our work we obtained a graph from Stanford Network Analysis Platform (SNAP)” of Stanford University. Specifically, for this first version of the project we used one set only: soc-Slashdot0811 (‘SDot’): a directed graph from the Slashdot Zoo signed social network from November 6 2008, consisting of 77,360 nodes, 905,468 edges, with an average degree of 4.13 and average clustering coefficient of 0.055.Graph Neural NetworkAs mentioned, in order to solve the problem of fast approximation of node centrality metrics for large networks we have is to follow a Machine Learning architecture that combines node embedding with a neural network and can broadly be categorised as a Graph Neural Network architecture.Graph and Node embeddingThe method of graph embedding aims to learn a representation of a graph, node, or edges in a low-dimensional vector. Put differently it achieves a low-dimensional vector representation of a node such that it preserves a node’s ‘topological information’. These methods typically require two matrices: the graph adjacency matrix and a feature matrix, which contains a number of features for each node. With these two matrices Graph embedding methods then use a set of weight matrices through which they learn a specific mapping for each node. Hence, each node is then mapped to an F dimensional vector which represents each node’s features as well as their connectedness, taken from the adjacency matrix.The approach we will follow in this project will be the Structure2vec method as developed by Dai et al. in Discriminative embeddings of latent variable models for structured data”. In the Structure2Vec method, each node is codified by taking into account two main inputs:i) any information on the specific nodesii) the embedding of all its neighbours.The model architecture consists of two parts: each node is ‘codified’ into an embedding vector which is computed using only the degree centrality metric (ranked and normalised) — this is the Structure2vec component, which subsequently forms the input feature for second component, the Neural Network, that then produces an approximation of the chosen centrality metric, which, as mentioned earlier, in this first version of the project is the Harmonic centrality.We follow the approach used by Mendonca et al. in Approximating Network Centrality Measures Using Node Embedding and Machine Learning”, where the authors have made their TensorFlow code public at https://github.com/MatheusMRFM/NCA-GEThe second component of the model is the Neural Network, which applies a Deep Learning architecture consisting of four layers. Briefly, the Deep Learning model works as follows: once we have constructed the embedding matrix, H, we perform the regression operations, with three hidden layers obtaining the following prediction of the centrality measure, Y:Again, we have three hidden layers (fully connected), which create the weight matrices where each layer uses the Rectified Linear Unit (Relu) activation function.Training the ModelFinally, to train the model we generated a synthetic graph data set, applying the Barabasi-Albert model where we generated 900 training and 100 test graphs where the parameters to generate the graphs were randomly generated. The bounds on the number of nodes were set to be between 1000 and 100. The bounds of the edges to be attached from a new node to existing nodes were set in a range of between 3 and 10. On average each graph consisted of 558 nodes with an average degree of 13 and an average clustering coefficient of 0.08. Finally, both input and output centrality metrics are ranked and normalised. After training the model we tested it on the real-world network, Sdot, and achieved a Kendall Tau coefficient of around 95%.WorkflowNow, with the details of the theory and structure out of the way, we can move on to the workflow, which consists of four steps.Step 1: upload the Sdot data set to Neo4jSince the ‘Sdot’ set is not pre-loaded in Neo4j, and to make a complete representation of the workflow, we first upload the graph into Neo4j by applying the following Cypher code.Step 2: Obtain graph from Neo4j and load into GNN modelThe second step, and where the real work starts, is to load the graph into the Graph Neural Network from Neo4j. For that, we use the Neo4j Python driver to fetch the graph from Neo4j and return it in a .csv format, ready for processing.Step 3: Run Graph Neural Network model and obtain predicted valuesNow that we have the ‘Sdot’ graph as a CSV file, we can run the Graph Neural Network model on our local machine, which computes the approximated Harmonic centrality values.Step 4: Set predicted Harmonic centrality measure as a Node property of the graph in Neo4jHaving computed the approximate Harmonic centrality measures we use once again the Neo4j Python driver to wite back the obtained values to Neo4j and attach them as a Node property to the ‘Sdot’ graph, as shown in the code below:Querying Neo4jFinally, we run a quick Cypher query to check that indeed the data is there:and indeed — we find what we are looking for:ConclusionAs stated in the introduction, by making use of the Neo4j Python drivers and by deploying a Graph Neural Network we have found an efficient way of approximating a centrality measure in less than 10 seconds, where the exact algorithm can easily take hours. We estimate that the entire workflow can be computed in less than a minute whilst achieving high approximation accuracy.In my next post, we will compare the harmonic centrality scores computed by the Graph Neural Network with those computed by the algorithm in the Graph Data Science Library.;Oct 19, 2020;[]
https://medium.com/neo4j/how-to-get-started-with-the-neo4j-graph-data-science-python-client-56209d9b0d0d;Tomaz BratanicFollowJun 3, 2022·11 min readHow to Get Started With the Neo4j Graph Data Science Python ClientLearn the basic syntax of the newly released Python client for Neo4j Graph Data ScienceData scientists like me love Python. It features a wide variety of machine learning and data science libraries that can help you get started on a data science project in minutes. It is not uncommon to use a variety of libraries in a data science workflow.With the release of version 2.0 of Neo4j Graph Data Science (GDS), a supporting Python client has been introduced. The Python client for Neo4j Graph Data Science is designed to help you seamlessly integrate it into your data science workflow. Instead of having to write Cypher statements to execute graph algorithms, the Python client provides a simple surface that allows you to project and run graph algorithms using pure Python code.GitHub - neo4j/graph-data-science-client: A Python client for the Neo4j Graph Data Science (GDS)…graphdatascience is a Python client for operating and working with the Neo4j Graph Data Science (GDS) library. It…github.comSince the Python client for GDS is relatively new, there are not many examples out there yet. Therefore, I’ve decided to write this blog post to help you get started with the GDS Python client syntax and show some common usage patterns through a simple network analysis.The Neo4j Graph Data Science Python client can be installed using the pip package installer.pip install graphdatascienceAn important thing to note is that the Python client is only guaranteed to work with GDS versions 2.0 and later. Therefore, if you have a previous version, I suggest you first upgrade Neo4j Graph Data Science to the latest version.All the code for this blog post is available in a Jupyter Notebook on GitHub.blogs/gds_python_intro.ipynb at master · tomasonjo/blogsJupyter notebooks that support my graph data science blog posts at https://bratanic-tomaz.medium.com/ …github.comNeo4j environment setupIf you want to follow along with the code examples, you need to set up a Neo4j database. I suggest you use a blank project on Neo4j Sandbox for this simple demonstration, but you can also download a Neo4j Desktop application and set up a local database.Neo4j Sandbox has GDS already installed. However, if you use Neo4j Desktop, you have to install Neo4j Graph Data Science manually.Install Neo4j Graph Data Science Desktop. Image by the author.Setting up the GDS Python client connectionWe start by defining the client connection to the Neo4j database. If you have seen any of my previous blog posts that use the official Neo4j Python driver, you can see that the syntax is almost identical.We have instantiated the connection to the Neo4j instance. If you are using Neo4j Enterprise, you might have multiple databases available in Neo4j. If we want to use any database other than the default one, we can select the required database using the set_database method.gds.set_database( databaseName )Lastly, we can verify that the connection is valid and the target Neo4j instance has GDS installed by using the gds.version() method.print(gds.version())The version() method should return the version of the installed GDS. If it returns anything else, make sure that you entered the correct credentials and Neo4j Graph Data Science is installed.Executing Cypher statementsThe Python client allows you to execute arbitrary Cypher statements using the run_cypher method. The method takes two parameters to input.The signature of the run_cypher method. Image by the author.The first and mandatory parameter is the Cypher query you want to execute. The second method parameter is optional and can be used to provide any query parameters.The run_cypher method can be used to import, transform, or fetch any data from the database. We will begin by populating the database with the Harry Potter network I created in one of my previous blog posts.Network of interactions in the first Harry Potter book. Image by the authorThe network contains characters in the first book, and their interactions, which are represented as relationships. The CSV with the relationship is available on my GitHub, so we can use the LOAD CSV clause to retrieve the data from GitHub and store it in Neo4j.The import script uses the run_cyphermethod to execute the Cypher statement used to import the Harry Potter network. To demonstrate how Cypher parameters work with the run_cyphermethod, I’ve attached the URL of the file as a Cypher parameter. While the Cypher query is represented as a string, the Cypher parameters are defined as a dictionary.If you have done any data analysis in Python, you have probably used the Pandas library in your workflow. Therefore, when fetching data from a database using the run_cyphermethod, the method conveniently returns a populated Pandas DataFrame. Having the data available as a Pandas DataFrame makes it much easier to integrate the data from Neo4j into your analytical workflow and use it in combination with other libraries.In this example, we will retrieve the degree (count of relationships) for each character in the network using the run_cypher method.The degree_df.head() method will visualize the first five rows in your Jupyter Notebook.First five rows of the degree_df DataFrame. Image by the author.Since the data is available as a Pandas DataFrame, we can easily integrate it into our analytical workflow. For example, we can use the Seaborn library to visualize the node degree distribution.ResultsNode degree distribution plot. Image by the author.We can easily observe that most nodes have less than 15 relationships. However, there is one outlier in the dataset with 83 connections, and that is, of course, Harry Potter himself.Projected graph objectThe central concept of the GDS Python client is to allow projecting and executing graph algorithms in Neo4j with pure Python code.Furthermore, the Python client is designed to mimic the GDS Cypher procedures so that we don’t have to learn a new syntax to use the Python client.As you might know, before we can execute any graph algorithms, we first have to project an in-memory graph. For example, let’s say we want to project a simple directed network of characters and their interactions with the Python client.Mapping between Cypher procedure and Python client method. Image by the author.If you are familiar with Cypher procedures of Graph Data Science, you will be able to pick up the Python client syntax easily. For the most part, we remove the CALLclause before the GDS procedures, and we get the Python client syntax to project graphs or execute algorithms.In our case, we want to project a network of characters where the interaction relationships are treated as undirected. Therefore, we must use the extended map syntax to define undirected relationships.Mapping between Cypher procedure and Python client method. Image by the author.When dealing with map objects, or dictionaries as they are called in Python, we have to add quotes around map keys. Otherwise, the keys would be treated as variables in Python, and you would get a NameError as the key variables are not defined. So, apart from adding quotes and removing the CALLclause, the syntax to project an in-memory graph is identical.When projecting a graph with the Python client, a client-side reference to the projected graph is returned. We call these references Graph objects. Along with the Graph object, the metadata from the procedure call is returned as Pandas Series.We have passed the projected graph reference to the G variable and stored the metadata information as the metadata variable. The metadata variable contains information that you normally get as the output of the procedure.Metadata of the projected graph. Image by the author.There are 119 nodes and 812 relationships in our projected graph.The Graph object, available as the variable G, has multiple method that can be used to inspect more information about the projected graph. For a complete list of the methods consult with the official documentation.For example, we can return the projected graph name using the name() method, inspect the memory usage using the memory_usage() method, or even calculate the density of the graph using the density() method.Running graph algorithmsNow that we have the projected graph ready and available as the reference variable G, we can go ahead and execute a couple of graph algorithms using the Python client.We will begin by executing the weighted variant of the PageRank algorithm. The stream mode of the algorithm returns the result of the algorithm as a stream of records.Mapping between the PageRank Cypher procedure and Python client method. Image by the author.Similar to before, when we were projecting an in-memory graph, we need to remove the CALLclause in the Python client for all algorithm executions. We reference the projected graph by its name with the Cypher procedure statement. However, using the Python client, we pass the Graph object as the reference to the projected in-memory graph instead of its name. Lastly, any algorithm configuration parameters can be specified as keyword arguments in the Python client.We can use the following Python script to execute the streammode of the weighted PageRank algorithm.The stream mode of any algorithm in Neo4j Graph Data Science returns a stream of records. Python client then automatically converts the output into a Pandas DataFrame.First five rows of the pagerank_df DataFrame. Image by the author.If you have ever executed the stream mode of the graph algorithms in Neo4j Graph Data Science, you might be aware that the result contains internal node ids as a reference to nodes instead of actual node objects. The pagerank_dfDataFrame contains two columns:nodeId: Internal node ids used to reference nodesscore: PageRank scoreWe can retrieve the referenced node objects using the nodeId column without constructing a Cypher statement by using the gds.util.asNodes() method. The gds.util.asNodes() method takes a list of internal node ids as input and outputs a list of node objects.The node_object column now contains the referenced node objects. Node objects are defined in the underlying Neo4j Python driver. You can reference the official documentation if you want to examine all the possible methods of the node object.In this example, we will extract the nameproperty from node objects and then visualize a bar chart of the top ten characters with the highest PageRank score.ResultsTop ten characters with the highest PageRank score. Image by the author.An additional benefit of having the graph algorithm output available in the Pandas DataFrame is that if you are not experienced with Cypher aggregations, you can simply skip them and do your aggregations in Pandas.As opposed to the stream mode of algorithms, the stats, mutate, and write modes do not produce a stream of results. Therefore, the results of Python client methods are not Pandas DataFrame. Instead, those methods output the algorithm metadata in Pandas Series format.For example, let’s say we want to execute the mutate mode of the Louvain algorithm.The mapping from Cypher to Python is identical as in the PageRank example. The Graph object replaces the name of the projected graph, and additional algorithm configuration parameters are specified using the keyword arguments.The metadata of the Louvain algorithm is the following:Louvain algorithm metadata. Image by the author.We can observe that the algorithm identified ten communities, while the modularity score is relatively low.The mutate mode of the algorithm stores the results in the projected in-memory graph. One of the Graph object methods is the node_properties() method, which can be used to inspect which node properties are present in the in-memory graph.The result of the node_properties() method verifies that the communityId node property was added to the projected graph.Sometimes we want to retrieve the node properties from the projected in-memory graph. Luckily, there is a gds.graph.streamNodeProperty()method that fetches node properties from the projected in-memory graph and outputs them as a Pandas DataFrame.The first parameter of the gds.graph.streamNodeProperty()method is the referenced Graph object. As the second parameter, we define which property we want to retrieve from the in-memory graph.ResultsFirst five rows of the louvain_df DataFrame. Image by the author.Again, we get the internal node ids in the nodeId column. We could use the gds.util.asNodes() method to fetch the node objects that the internal node ids reference. Unfortunately, the column with the retrieved node properties has a generic name propertyValue. In our case, it would make sense to name the column with the results communityId. However, we can do that manually if we need to.Like mentioned before, the added benefit of dealing with Pandas DataFrames as algorithm output is that you can apply all your Python skills to transform or manipulate the results. In this example, we simply grouped the DataFrame by the communityId column and count the members of each community.Count of members per community. Image by the author.Helpful methodsIn the last part of this post, we will go over some of the helpful methods. The first one that comes to mind is listing all of the already projected in-memory graph with the gds.graph.list() method.Results of the gds.graph.list() method. Image by the author.Sometimes there are already projected in-memory graphs present in the database. If you don’t have a reference to the projected graphs in the form of a Graph object, you cannot execute any graph algorithm. To avoid having to drop and recreate projected graphs, you can use the gds.graph.get()method.When using the shortest path algorithms, you need to provide source and target nodes ids. You could use Cypher statements or you could use the gds.find_node_id() method.Find node id method syntax. Image by the author.The gds.find_node_id()takes in two arguments. The first argument defines the node label we are searching for. In our example, we are searching for the Characternode label. The second parameter specifies the node properties used to identify the particular node. The node properties are defined as a dictionary or map of key-value pairs, similar to the inline MATCH clause. The only difference is that we must add quotes around the key values of properties since otherwise, we would get a NameError in Python.The last useful method I will present here is the drop() method of a Graph object. It is used to release the projected graph from memory.ConclusionThe Neo4j Graph Data Science Python client is designed to help you integrate Neo4j and its graph algorithms into your Python analytical workflows. The syntax of the Python client mimics the GDS Cypher procedures. Since not all graph algorithms are documented to be used as Python client method, you need to take into account the following guidelines when translating a Cypher procedure to a Python client method:When specifying a map or a dictionary as a parameter to any method, make sure to add quotes around the keysInstead of referencing the projected graph by its name, you need to input the Graph object as the first parameter of graph algorithmsAlgorithm specific configuration parameter can be specified using keyword argumentsThe streammode of graph algorithms outputs a Pandas DataFrameOther algorithm modes like stats, write, and mutate output the metadata of the algorithm call as a Pandas SeriesI am very excited about the new Python client and will be definitely using it in my workflows. Try it out and if you have any feedback please report it to the official GitHub repository of the Python client.As always, the code is available on GitHub.;Jun 3, 2022;[]
https://medium.com/neo4j/monitoring-the-cryptocurrency-space-with-nlp-and-knowledge-graphs-92a1cfaebd1a;Tomaz BratanicFollowJan 25, 2022·10 min readMonitoring the Cryptocurrency Space with NLP and Knowledge GraphsLeverage the power of Diffbot’s APIs and Neo4j to monitor and analyze articles revolving around cryptocurrencyEvery day, millions of articles and papers are published. While there is a lot of knowledge hidden in those articles, it is virtually impossible to read all of them. Even if you only focus on a specific domain, it is still hard to find all relevant articles and read them to get valuable insights.However, there are tools that could help you avoid manual labor and extract those insights automatically. I am, of course, talking about various NLP tools and services.In this blog post, I will present a solution of how you can combine the power of NLP with knowledge graphs to extract valuable insights from relevant articles automatically.There are multiple use-cases where this solution would be applicable. For example, you could create a business monitoring tool to survey what the internet says about your own company or perhaps your competitors. If you are an investor, you could identify potential investments by analyzing the news revolving around the companies or cryptocurrencies you might be interested in. Not only that, but you could feed the extracted information to a machine learning model to help you spot great investments in either stocks or crypto.I am sure there are more applications I haven’t thought of yet.In my previous post, I’ve already alluded to how you could manually develop your data pipeline.Making Sense of News, the Knowledge Graph WayHow to combine Named Entity Linking with Wikipedia data enrichment to analyze the internet news.medium.comWhile it could take months to build your data pipeline that effectively crawls the internet articles and process them with NLP models, I’ve found a Diffbot solution that could help you solve that in a matter of hours.Diffbot | Knowledge Graph, AI Web Data Extraction and CrawlingAccess a trillion connected facts across the web, or extract them on demand with Diffbot - the easiest way to integrate…www.diffbot.comDiffbot has a mission of constructing the world’s largest knowledge graph by crawling and analyzing the entire internet. It then provides APIs endpoints to search and retrieve relevant articles or topics. If you wanted to, you could also use their APIs for data enrichment as their knowledge graph contains information about various organizations, people, products, and more.On top of that, they also offer the Natural Language Processing API that extracts entities and relationships from the text. If you have read any of my previous blog posts, you already know that we will use Neo4j, a native graph database, to store and analyze the extracted information.AgendaRetrieve articles that talk about cryptocurrencyTranslate foreign articles with Google Translate APIImport articles into Neo4jExtract entities and facts with Diffbot’s NLP APIImport entities and facts into Neo4jGraph analysisI’ve prepared a Jupyter Notebook that contains the code to reproduce the steps in this article.blogs/DiffbotNLP + Neo4j.ipynb at master · tomasonjo/blogsJupyter notebooks that support my graph data science blog posts at https://bratanic-tomaz.medium.com/ …github.comRetrieve Articles About CryptocurrenciesAs mentioned, we will use the Diffbot APIs to retrieve articles that talk about cryptocurrencies. If you want to follow this post, you can create a free trial account on their page, which should be enough to complete all the steps presented here. Once you login to their portal, you can explore their visual query builder interface and inspect what is available.Diffbot’s visual query builder. Image by the author.There is a lot of data available by Diffbot’s Knowledge Graph API. So not only can you search for various articles, but you could use their KG APIs to retrieve information around organizations, products, persons, jobs, and more.This example will retrieve the latest 5000 articles with a tag label Cryptocurrency.I have constructed the search query in their visual builder and simply copied it to my Python script. That’s all the code required to fetch any number of articles that are relevant to your use case.Translate Foreign Articles with Google Translate APIThe retrieved articles are from all over the world and in many languages. In the next step, you will use Google Translate API to translate them to English. You will need to enable the Google Translate API and create an API key.Make sure to check their pricing, as it ended up a bit more than expected for me to use their translation API. I’ve checked pricing on other sites, and it is usually between $15 to $20 to translate a million characters.Before we move on to the NLP extraction part, we will import the articles into Neo4j.Import Articles into Neo4jI suggest you either download Neo4j Desktop or use the free Neo4j AuraDB cloud instance, which should be enough to store information about these 5000 articles. First of all, we have to define the connection to Neo4j instance.The imported graph model will have the following schema.Article graph schema. Image by the author.We have some metadata around articles. For example, we know the overall sentiment of the paper and when it was published. In addition, for most of the articles, we know who wrote them and on which site. Lastly, the Diffbot API also returns the categories of an article.Before continuing, we will define unique constraints in Neo4j, which will speed up the import and subsequent queries.Now we can go ahead and import articles into Neo4j.I won’t go into much detail and explain how the above Cypher query works. Instead, multiple blog posts deal with an introduction to Cypher and graph imports if you are interested. There is also a GraphAcademy course about importing data, that covers the basics.Take the Importing CSV Data into Neo4j course with Neo4j GraphAcademyIf you find yourself stuck at any stage then our friendly community will be happy to help. You can reach out for help…graphacademy.neo4j.comWe can examine a single to blog post to verify the graph schema model.Sample article visualization. Image by the author.Before we move on to the analysis part of the post, we will use the NLP API to extract entities and relationships, or as Diffbot calls them, facts.The Diffbot website offers an online NLP demo, where you can input any text and evaluate the results. I’ve input a sample content of an article we have just imported into Neo4j.Diffbot NLP demo. Image by the author.The NLP API will identify all the entities that appear in the text and possible relationships between them even as a graph. In this example, we can see that Jack Dorsey is the CEO of Block, which is based in San Francisco and deals with payments and mining. Jack’s coworker at Block is Thomas Templeton, who has a background in computer hardware.To process the entities in the response and store the to Neo4j, we will use the following code:This example will import only entities that have allowed types such as organization, person, product, and location, and their confidence level is greater than 0.7. Diffbot’s NLP API also features entity linking, where entities are linked to Wikipedia, Crunchbase, or LinkedIn, as far as I have seen. We also add the extra entity types as additional labels to the Entity node.Next, we have to prepare the function that will clean and import relationships into Neo4j.I have omitted the import of the properties that are defined in the skipProperties list. To me, it makes more sense to store them as node properties rather than relationships between entities. However, in this example, we will simply ignore them during import.Now that we have the functions for importing entities and relationships prepared, we can go ahead and process the articles. You can send multiple articles in a single request. I’ve chosen to batch the requests by 50 pieces of content.By following these steps you have successfully constructed a knowledge graph in Neo4j. For example, we can visualize the neighborhood of Jack Dorsey.Visualization of a subgraph of the KG constructed with NLP techniques. Image by the author.The NLP extraction picked up that Jack Dorsey is the CEO of Block and has working relationships with Alex Morcos, Martin White, etc. Of course, not all extracted information is perfect.I find it funny that the NLP identified Elon Musk as an employee of Dogecoin, which is not that far from the truth anyhow. I haven’t played around with confidence levels of facts, but you could increase the threshold to reduce the noise. However, this is a game between precision and recall.This is just a sample subgraph. It is hard to decide what exactly to show as there is so much information available.Graph AnalyticsIn the last part of this post, I will walk you through some example applications that you could use with a knowledge graph like this. First, we will evaluate the timeline of the articles.MATCH (a:Article)RETURN date(a.date) AS date,       count(*) AS countORDER BY date DESCLIMIT 10ResultsThere is between 150 to 450 articles per day about cryptocurrencies around the world which backs my initial statement about that volume being too much to read. Next, we will evaluate which entities are most frequently mentioned in articles.MATCH (e:Entity)RETURN e.name AS entity,       size((e)<-[:MENTIONS]-()) AS articlesORDER BY articlesDESC LIMIT 5ResultsAs you would expect from articles revolving around cryptocurrencies, the most frequently mentioned entities are:cryptocurrencybitcoinEthereumblockchainThe sentiment is available on the article level as well as entity level. For example, we can examine the sentiment regarding bitcoin grouped by region.MATCH (e:Entity {name:bitcoin})<-[m:MENTIONS]-()-[:ON_SITE]->()-[:HAS_REGION]->(region)WITH region.name AS region, m.sentiment AS sentimentRETURN region, avg(sentiment) AS avgSentiment,        stdev(sentiment) AS stdSentiment,        max(sentiment) AS maxSentiment,        min(sentiment) AS minSentiment,        count(*) AS articlesORDER BY articles DESCLIMIT 5ResultsThe sentiment is on average positive, but it heavily fluctuates between articles based on the standard deviation values. We could explore bitcoin sentiment more. Instead, we will examine which persons have the highest and lowest average sentiment in and also present in most articles in North America.MATCH (e:Person)<-[m:MENTIONS]-()-[:ON_SITE]->()-[:HAS_REGION]->(region)WHERE region.name =  North America RETURN e.name AS entity,       count(*) AS articles,       avg(m.sentiment) AS sentimentORDER BY sentiment * articles DESCLIMIT 5UNIONMATCH (e:Person)<-[m:MENTIONS]-()-[:ON_SITE]->()-[:HAS_REGION]->(region)WHERE region.name =  North America RETURN e.name AS entity,       count(*) AS articles,       avg(m.sentiment) AS sentimentORDER BY sentiment * articles ASCLIMIT 5ResultsNow, we can explore the titles of articles in which, for example, Mark Cuban appears.MATCH (site)<-[:ON_SITE]-(a:Article)-[m:MENTIONS]->(e:Entity {name: Mark Cuban})RETURN a.title AS title,        a.language AS language,        m.sentiment AS sentiment,        site.name AS siteORDER BY sentiment DESCLIMIT 5ResultsWhile the titles themselves might not the most descriptive, we can also examine which other entities frequently co-occur in articles where Mark Cuban is mentioned.MATCH (o:Entity)<-[:MENTIONS]-(a:Article)-[m:MENTIONS]->(e:Entity {name: Mark Cuban})WITH o, count(*) AS countOfArticlesORDER BY countOfArticles DESCLIMIT 5RETURN o.name AS entity, countOfArticlesResultsNot surprisingly, various crypto tokens are present. Also, the Dallas Mavericks appear, which is the NBA club that Mark owns. Does Dallas Mavericks support crypto, or do reporters like to state that Mark owns the Dallas Mavericks, that I don’t know. You could proceed with that route of analysis, but here, we’ll also look at what facts we extracted during NLP processing.MATCH p=(e:Entity {name:  Mark Cuban })--(:Entity)RETURN pResultsFacts retrieved about Mark Cuban. Image by the author.Next, we will quickly evaluate the article titles where Floyd Mayweather appears, as the average sentiment is quite low.MATCH (a:Article)-[m:MENTIONS]->(e:Entity {name: Floyd Mayweather})RETURN a.title AS title, a.language AS language, m.sentiment AS sentimentORDER BY sentiment ASCLIMIT 5ResultsIt seems that Kim Kardashian and Floyd Mayweather are being sued over an alleged crypto scam. The NLP processing also identifies various tokens and stock tickers, so we can analyze which are popular at the moment and their sentiment.MATCH (e:Entity)<-[m:MENTIONS]-()WHERE (e)<-[:STOCK_SYMBOL]-()RETURN e.name AS stock,        count(*) as mentions,        avg(m.sentiment) AS averageSentiment,       min(m.sentiment) AS minSentiment,       max(m.sentiment) AS maxSentimentORDER BY mentions DESCLIMIT 5ResultsI have only scratched the surface of the available insights we could extract. For the end, I’ll just add two visualizations and mention some applications you could develop.Evaluate market structure. Image by the author.For example, you could analyze the market by looking at relationships like COMPETITORS, ACQUIRED_BY, SUPPLIERS, etc. On the other hand, you could focus your analysis more on the persons in the graph and evaluate their influence or connections.Evaluate connections between persons. Image by the author.ConclusionI’ve only scratched the surface of possible analysis with these types of the data pipeline. As mentioned, you could monitor the news regarding your company, your competitors, the whole industry, or even try to predict future events like acquisitions. Not only that, but you could also use extracted data to fuel your machine learning models for your desired use case, like predicting crypto trends.As always, the code is available on GitHub.;Jan 25, 2022;[]
https://medium.com/neo4j/language-buffet-using-neo4j-with-graalvm-part-2-e5d2df990805;Jennifer ReifFollowFeb 12, 2021·10 min readLanguage Buffet: Using Neo4j with GraalVM, Part 2We are continuing our journey through different use cases with GraalVM and Neo4j. In the last blog post, we covered a few of the different ways we could use Neo4j and GraalVM together. As a refresher, the list is shown below.Polyglot clients — access Neo4j from languages using an official driver (like the Java driver). Use libraries in the source language for connecting to Neo4j from target GraalVM language implementations.Library-sharing — access non-Java libraries to use within programs in other languages. For instance, pull in Python’s ratelimit library or Javascript’s colors into a Java program that interacts with Neo4j.Polyglot procedures — extend Neo4j and the Cypher query language by writing procedures and functions in any language and packaging them as a Neo4j database plugin. Write extensions in JVM languages (Java, Kotlin, Scala, Groovy, etc) and non-JVM languages. Execute language-specific code within a Cypher procedure (i.e. run Python inside a Cypher statement).Polyglot Cypher — use Cypher as the query language in various programs (by implementing Cypher in GraalVM’s Truffle language framework). Embed Cypher code in your Python or Javascript program for executing against Neo4j.The previous post also covered the first scenario to connect to Neo4j from a variety of languages by using GraalVM’s polyglot capabilities and the official Neo4j Java driver. This round, we will go a little more complex and walk through the 3rd item for building a Neo4j database extension that will run various language code within a Cypher procedure call.Let’s get started!Extend Neo4j and Cypher with Custom Procedures and FunctionsMany databases provide the ability to write custom code for handling functionality that isn’t provided out-of-the-box, and Neo4j is no exception. When certain capabilities are not available or are complex in Cypher, users can write their own procedures and functions, package them, and add them as a plugin to the database. Things like the APOC library, GDS, and more were built this way!However, traditionally, these extensions are written in a JVM-based (Java virtual machine) language like Java, Groovy, Scala, etc. With GraalVM, we are granted a shared polyglot environment that allows a Java-based source (like Neo4j) to understand a non-JVM target (like Python). GraalVM translates various languages that have been implemented with their Truffle language framework, allowing interaction between languages.SetupFirst, if you don’t already have it, we will need Neo4j. Unfortunately, Neo4j Desktop and Sandbox both have pre-defined environments that make it difficult to run a GraalVM environment in conjunction, so the easiest approach is to download Neo4j server community edition for this example.Next, if you installed GraalVM and accompanying languages with the last blog post, feel free to skip to the next section. Otherwise, we will walk through the steps again here.GraalVM is another JDK (Java Development Kit) install, which is a bundle of tools for developing Java applications. If you’re familiar with this, feel free to use however you are most comfortable to handle java versions and JDKs. If you’re new to JDKs, an article I found explains the components of the Java environment. For managing all the options on my machine, I really like using SDKMAN!. It automatically syncs classpaths and seamlessly allows me to change versions and providers with a command or two. The commands to install the GraalVM JDK with SDKMAN! are listed below.#List available Java vendors and versions in SDKMAN!% sdk list java#Install one for GraalVM (my current version)% sdk install java 20.3.0.r11-grl#Switch Java versions% sdk use java 20.3.0.r11-grl#(optional) Set it as the default JDK for your system% sdk default java 20.3.0.r11-grl#Verify Java version on your system (and results for my environment)% java -versionopenjdk version 11.0.9” 2020–10–20OpenJDK Runtime Environment GraalVM CE 20.3.0 (build 11.0.9+10-jvmci-20.3-b06)OpenJDK 64-Bit Server VM GraalVM CE 20.3.0 (build 11.0.9+10-jvmci-20.3-b06, mixed mode, sharing)Note: when you install a version of Java, it may prompt you to set it as default in the install. However, if it doesn’t or you choose to set it as default later, I included the command to do that.Ok, those are the base requirements to install — GraalVM and Neo4j. There are a couple of other setup needs to run various languages with that. Though you can use standard language environments, I’ve opted for the built-in GraalVM languages, as I assume those have less setup overhead. To install each of the GraalVM-supported languages, we can use the GraalVM Updater (gu) tool. Commands for using gu to install each language are shown below.#See what’s there alreadygu list#Pythongu install python#Javascript (included)#Rgu install r#Rubygu install rubyNote: gu is included in the base install of GraalVM. If you haven’t installed any other languages before you run the gu list command shown first in the code block above, you may notice that a couple of things are already there. That’s because these are built into the GraalVM general install.For the R install, there are a couple other dependencies listed in the documentation that are needed. My Mac already had these installed on my system, but depending on your operating system and version, you might want to verify them.With Ruby, there are a couple of extra dependencies that need to be installed, as well. Most of these were already installed on my Mac, but you can verify these for your operating system and version. After those are complete, the first command in the code block below runs a script to connect openssl and libssl.I also had some issues with the recommendation to use a Ruby manager. It moved the path around where I couldn’t execute Ruby. I ended up uninstalling my Ruby manager and remapping TruffleRuby. In the end, the two commands below should help you see if your environment looks similar to mine. Note that SDKMAN! is in my path for TruffleRuby.#After installing deps, make the Ruby openssl C extension work with your system libssl)<path to your GraalVM JDK>/languages/ruby/lib/truffle/post_install_hook.sh% truffleruby -vtruffleruby 20.3.0, like ruby 2.6.6, GraalVM CE Native [x86_64-darwin]% which truffleruby/Users/jenniferreif/.sdkman/candidates/java/current/bin/trufflerubyYou can check that all the desired languages are installed by running the gu list command again to see all the languages you now have. Let’s start Neo4j with the command bin/neo4j start, and finally, we are ready to get our project up and running!Extension ProjectI have used Maven for this project, but you can use Gradle or something else, if you prefer. The dependencies are pretty straightforward, as we just need to include Neo4j and the GraalVM SDK in the pom.xml file. We also have a couple of interesting additions in the build section of the pom, so let’s look at those.<build>  <plugins>    <plugin>      <artifactId>maven-dependency-plugin</artifactId>      <executions>        <execution>          <phase>prepare-package</phase>          <goals>            <goal>copy-dependencies</goal>          </goals>          <configuration>   <outputDirectory>${project.build.directory}/lib</outputDirectory>          </configuration>        </execution>      </executions>    </plugin>    <plugin>      <artifactId>maven-shade-plugin</artifactId>      <version>3.2.2</version>      <configuration>   <createDependencyReducedPom>false</createDependencyReducedPom>      </configuration>      <executions>        <execution>          <phase>package</phase>          <goals>            <goal>shade</goal>          </goals>        </execution>      </executions>    </plugin>  </plugins></build>If you read the previous blog post, you may remember the first dependency plugin. This packages up the dependencies during the prepare-package phase of the build and drops them into the /target/lib folder of our project.The second dependency is not one we had in the last example. It generates a jar file with our custom code, along with dependencies specified with <scope>compile</scope>. Before we get to packaging, though, let’s look at the rest of the code!Custom Procedures and FunctionsFor the polyglot procedures, the project provides the code for you and others to write procedures and functions in other languages and run them via the provided procedure (polyglot.run). We could even expand or adjust this to run entire scripts in other languages, as shown in this article.The src/main/java folder houses a polyglotProcedures folder containing two Java programs. One is the PolyglotUtil class that defines the polyglot.run procedure, and the other is the TypeConverter (absconded from David Allen’s similar project). We will start with the PolyglotUtil, then walk through the TypeConverter.public class PolyglotUtil {  @Context  public GraphDatabaseService db  @Context  public Log log  …}The PolyglotUtil class starts by defining the context for connecting to our Neo4j graph database and sets up logging. Next, we are ready to define any procedures and functions we want to be able to call.@Procedure(value = polyglot.run”)@Description(polyglot.run(language, code) — Executes the code given. Throws things otherwise.”)public Stream<Output> execute(@Name(language”) String language, @Name(code”) String code) throws IOException {    try (var context = org.graalvm.polyglot.Context.newBuilder().allowAllAccess(true).build()) {    var bindings = context.getPolyglotBindings()    bindings.putMember(db”, db)        Value v = context.eval(language, code)    log.info(Check value equals  + v)    Object result = convert(v)    //Map these to a generic output as a type hack around the uncertainty of what comes back    //Neo4j procs require a stream of concrete types    return Stream.of(new Output(result))  } catch (Exception exc) {    exc.printStackTrace()    throw exc  }}Here, I have set up a single procedure called polyglot.run() that takes 2 parameters — 1 for the language of the code we want to run, 1 for the actual code we want to execute. First, we need to label it as a procedure (@Procedure annotation) and define the documentation for it (@Description annotation). We need the return type to be a Stream of an Object type (in this case Output) because Cypher needs a stream of a generic object type to process and return results. We’ll cover more on this in a minute when we get to the next code block.Inside the procedure call, we will put all of our code in a try/catch block that ensures GraalVM can build context for the rest of our code. Within the try, we get the polyglot bindings from the context and put our database (Neo4j) in that. This allows everything to communicate. The next line of code defines a variable v that’s a Value type, which is a GraalVM type that allows us to translate between various language’s data types. We set the variable equal to the evaluation of the code passed into the procedure (language and code string itself). The log statement following simply outputs the variable to check that the value matches our expectations.Next, we try to convert the value variable to an Object type called result. Our convert() method is in the TypeConverter class that we’ll discuss in a minute, so more on this shortly. Then, the last line in the try block maps the result to a stream of Output, which is explained in the comment and defined in the code below. Lastly, we do a catch to grab any exceptions and give the stacktrace.public class Output {  public Object result  public Output(Object thingy) {    result = thingy  }}Cypher expects procedures to return a stream of concrete types, so we need to define a concrete type that contains our generic object coming from the GraalVM code execution. Because the return from our GraalVM code execution could be a variety of things — Java Decimal, Python dict, Javascript hash, etc., we need to accept all of those and yet convert them to a concrete type that Cypher can expect.Cypher will actually specify this with a helpful error message that you can see if you comment out the Output class and tweak the return on the procedure to return a Stream of result, instead of Output. When you execute, Cypher should show an error that it expects return results of a specific type and even recommends a class of Output to solve the problem (exactly what we used here).Ok, now to our TypeConverter class! This class is taking our GraalVM Value variable and ensuring that the data type is something Neo4j can understand.public class TypeConverter {  public static final List<String> stringList = Arrays.asList(class”, constructor”, caller”, prototype”, __proto__”)  public static Object convert(Value v) {    if (v == null || v.isNull()) return null    if (v.isProxyObject()) {      System.err.println(Warning: proxy objects are not yet supported from guest languages for neo4j serialization”)      return null    }    if (v.isHostObject()) {      return v.asHostObject()    }    Set<String> memberKeys = v.getMemberKeys()    if (!memberKeys.isEmpty()) {      Map<String,Object> result = new HashMap<>()      for(String key : memberKeys) {        if (!stringList.contains(key) &&            !v.getMember(key).canExecute()) {          System.out.println(Recursing on  + key)          result.put(key, convert(v.getMember(key)))        }      }      return result    }    if (v.isBoolean()) return v.asBoolean()    if (v.isNumber()) return v.asDouble()    if (v.isString()) return v.asString()    System.err.println(Unsupported guest language values cannot be mapped, and will be returned as null”)    return null  }}In the above code, we have a series of if statements that check if the GraalVM Value is an Object (null, proxy, host, or map), boolean, number, or string and tries to convert it to the Java type of that.Now it’s time to deploy our code and test it out!Deploy Custom Code as Database PluginIn your preferred IDE or on the command line, build your project (or a clone of this one), followed by the mvn clean package command. This packages everything up and drops a .jar file of our project into the target directory. Copy the created .jar into the /plugins folder of your Neo4j database. You will need to find where you installed Neo4j, and the plugins folder should be in there.If Neo4j was already running, we will need to restart it (bin/neo4j restart). Otherwise, we can start the database with bin/neo4j start. After a few seconds, it should start, but I like to verify by opening another command line window, going to the Neo4j directory, then cd logs and run tail -f neo4j.log to check nothing gives an error code. The place where I run the start command doesn’t always show a clear error/shutdown message when things go wrong, so viewing the log file makes this a little more visible.Now we can access Neo4j Browser by opening a web browser and going to localhost:7474. If you are not familiar with Neo4j Browser, then the top input bar is where we type and run Cypher queries and procedures. In that command line, execute the procedure with CALL polyglot.run(arg1, arg2). You can run a variety of languages and code for the arguments, but I have included a few examples below to get you started.//returns the string hello” in a result paneCALL polyglot.run(‘js’, ‘hello”’)//prints hello” to neo4j.log outputCALL polyglot.run(‘js’,’print(hello”)’)//executes the math and returns the resultCALL polyglot.run(‘python’, ’CALL polyglot.run(‘python’, ‘import math totalEntities = 3000 callsNeeded = int(math.ceil(totalEntities / 100)) callsNeeded’)’)Languages that the first parameter accepts include anything implemented with GraalVM Truffle, including llvm, R, js, python, ruby.Wrapping Up!You can use this project as-is to run a variety of custom procedures, functions, or code snippets in the Cypher call. Or, you can use this as a template to build your own procedures and functions that accept script files or write additional code in other languages. Feel free to try it out and let us know what you need or would like to see in this project!As with the previous GraalVM project, we are looking for feedback to better understand what is needed or used in this project area. We’d be happy to hear from you either via Github (liking the project or creating issues/feature requests) or via the Neo4j Community Site (getting help or letting us know what you like/dislike). Happy coding!ResourcesGithub projectGraalVM documentationLearn Neo4j: developer guidesAsk questions: Neo4j Community SiteMichael Simons GraalVM/Neo4j blog postDavid Allen magnolia project;Feb 12, 2021;[]
https://medium.com/neo4j/how-to-build-a-json-restful-api-with-neo4j-php-and-openapi-e45bf0a8956;Ghlen NagelsFollowJan 29, 2022·9 min readHow to Build a JSON RESTful API with Neo4j, PHP, and OpenAPIBuilding a web app doesn’t need to be complicated. Choosing the correct technologies and architecture is already 50 percent of the solution. A RESTful API can be the backbone of any application. It is simple, understandable by almost any programmer in any language, and easily programmable.Some of the most mature tools on the market make it a breeze: OpenAPI helps define, document, and test the API without even programming a single line of code. PHP is probably the most mature web programming language in existence. It has excellent frameworks, a very active community, and continues to improve year over year. Then, of course, there is Neo4j, the best graph-based database on the market!Want to know the real kicker? All of these tools are free!This blog post will reveal how to build an API by example. It is based on the preceding live stream, which is available as a video here. We will create a simple API to manage Users and make them friends. Very simplistic, but valuable to show off the many functionalities of the tools above!The first step about documenting the API can be seen as optional. If you immediately want to get into the nitty-gritty of API development, start at step 2: Set up Neo4j.Define the API (Optional)Without falling into the classic waterfall development cycle, starting by documenting small pieces of your API can be a great way to get started. A structured approach through OpenAPI leverages these key advantages:An architect/group can define the API, regardless of the underlying programming language.The declarative YAML file is understood by many tools. There is automatic validation middleware, API mocking, automatic client generation in many programming languages, integration with postman…The documentation step allows for very fast iterations as it produces visible results quickly.This blog post is about the actual programming, but the API definition is here.Set Up Neo4jSetting up Neo4j is one of the easiest things you can do if you use Neo4j AuraDB. Neo4j AuraDB is a managed cloud solution with automatic backups, high availability, and scaling built-in. There is already good documentation on how to do this:Building a Web App with Neo4j AuraDB and PHPIf you have decided to leverage the power of the worlds’ leading graph database as a PHP developer or team, get started…medium.comCreating an account - Neo4j AuraThis page takes you through all the steps needed to create a Neo4j Aura account. Navigate to the Neo4j Aura Console in…neo4j.comBuckling Down: The Actual ProgrammingInstall a framework and the driverIn this example, we will be using the slim framework. This is mainly because we teach the basics of REST APIs using Neo4j and PHP. This example also works with Laravel, Symfony, or any other ecosystem you can imagine.It even works without a framework. Naturally, you will need to adhere to their principles when defining controllers, routes, and dependency injection with your tool of choice.Installation is the easiest step of them all:composer create-project slim/slim-skeleton:4.4.0 [my-app-name]Change the directory to your app and install the Neo4j client and phpdotenv for convenience:cd [my-app-name] && composer require laudis/neo4j-php-client vlucas/phpdotenvYou can now run the API locally using this command:composer startIt is now reachable through the browser on localhost port 8080: http://localhost:8080. In a display of true minimalism, the app will simply show hello world.” The framework is called slim for a reason.The world’s most awesome app. Hello world!Create the driver and sessionCreating a driver for Neo4j is easy as one-two-three. A session can be created with a driver via this one-liner:Create a session from a basic driverTo add it to dependencies, navigate to app/dependencies.php and add the line. The page should now look like this:final dependency injection configurationThe problem is that the NEO4J_URIneeds to be loaded into the server environment. This can be difficult and cumbersome, which is why I am a big fan of phpdotenv, a library to load .env files in the server environment. This allows decoupling of the sensitive authentication URI from the codebase, as you would typically keep it in the .gitignore. A lot of frameworks already have this functionality.Go to public/index.php and add this line before the line $response = $app->handle($request) .Dotenv\Dotenv::createImmutable(__DIR__.’/../’)->safeLoad()The end result should look something akin to this:The environment get loaded at line 78 in the real fileBootstrap the user controllerLet’s create a controller with the basic REST methods for managing users. First, create a directory src/Application/Controllers, then create UserController.php with the following methods and constructor:Scaffolding for all methods in the controllerBy adding the session as a parameter in the controllers’ constructor, we automatically tell the application to inject it. By defining a Session key in the dependencies’ definition, we provided enough information to the application to successfully inject the parameter of this type.Now that you have created all the methods, we can wire them in the app/routes.php file.app/routes.php file⚠️ Note: Don’t forget to import the UserController!This ensures the application routes the requests to the correct methods in the controller.Performing read queriesWe will start by implementing the route GET /users. This is arguably the easiest route to implement. It does not require any parameters, as it simply returns a list of all the users in the system regardless.The Cypher query is pretty straightforward:Simple read query for matching usersThis query matches all nodes with the label User and returns their id, first, and second name. But the Cypher language captures this pretty well 👌We can run the query on the session like this:How to run a query in a sessionWrapping our query in NOWDOC (i.e. <<<ENDTAG query ENDTAG) has several advantages. The language can be easily deduced by an IDE to get code completion. It also allows straightforward formatting and prohibits string interpolation. This is important for later as Cypher and PHP use dollar signs to signify variables.The driver will automatically interpret the results and return the rows and columns in a list and map respectively. Since all objects in the driver are json serializable, we can effortlessly return the results:How to write results to a responseUsing parameters in the driverThe next implementation introduces a little more complexity: parameters. GET /user expects and id in the query to return it. The driver effortlessly handles it like this:Example of using parameters in a queryBecause we are using NOWDOC, the $id variable is pure Cypher syntax. The driver expects an iterable object to hold the parameters. The classic example is, of course, an array. The query parameters look like this: [ id  =>  some-id ] . At runtime, Neo4j will correctly substitute the id variable with ‘some-id’ in this example.This example assumes the input is already valid. You would either use something like an OpenAPI PSR validator in the real world or do it yourself. But that’s beyond the scope of this blog post.After we have the query result, there is still some work to do. The user might not exist after all! Since the result is a list, we can use the isEmpty method.How to return a 404 in SlimThe slim framework will correctly catch the exception and return it in a 404 response.Writing the result to the response is, once again, easy as pie:How to write a single result rowUsing objects as parametersIterables and JsonSerializable objects can be used as parameters in the driver. For this reason, we will create our own UUID object and use it as a parameter when creating a user.The Uuid class jsonserializes to a string and is placed in src/Domain/Uuid.php:A JSON serializable UUID exampleThis makes the POST /user endpoint a breeze:How to return a created userThe REST principles should return a 201 CREATEDresponse with a Location header to navigate the created user easily.Finishing off the user endpointsThe only endpoint to finish off is DELETE /user . Once again, the theme is simplicity, ease, and dare I say it, elegance:How to quickly delete a user⚠️ Watch out! Detach deleting removes all relationships attached to the user and the user itself. If you want to make sure there are no relationships on the node do a simple delete. Only using the DELETE keyword will fail if there are still relationships attached to the node.REST is a very minimalistic principle. We should only return the essential information. Because of this, a successful deletion should return with 204 NO CONTENT.Leveraging the Power of Graphs and Neo4jAs shown in the previous examples, Neo4j and PHP with a framework are a match made in heaven. With just a few lines of code, we essentially created a full API managing the databases’ users.While the powerful Cypher query language is ideal for creating and querying nodes, it becomes exceptional when relationships are thrown into the mix. The following section is all about relationships, fast-performing queries, and ditching these pesky foreign keys in traditional SQL databases 👍Creating the Friends ControllerThe FriendsController will be responsible for forging friendships that may last a lifetime or end up becoming estranged. It will be able to query friends of friends. It can even calculate the minimum distance of friends between two people. Let’s go!Controller scaffolding src/Application/Controllers/FriendsController.php:Followed up by the routing in app/routes.php:The final routes.php fileSummarize a query resultPUT /user/friend requires producing the same state in the application on subsequent reruns with the same parameters. This principle is known as idempotency. This is because friendship is a binary relationship. You are either friends, or you are not. You will never hear anyone say: Timmy is my best friend, we have been friends over 100 times!Cypher has a powerful keyword for this: MERGE. Merging a pattern only creates it if it does not exist. In this case, it will only create a relationship as the query already matched both users, meaning they must exist.How to merge a relationshipBecause relations are unidirectional in Cypher, but can be queried bidirectionally, we don’t need to create the relationship twice. It is more efficient to make sure the merged direction is always the same. For this reason we sort both friends by their ids before passing them as parameters:How to solve unidirectional relationshipsTo keep our good form going strong, we will return the GET /user/friends endpoint to query the relationship:How to set the location endpoint correctlyIn order to fully comply with the PUT request, we differentiate between a 201 CREATED and 200 SUCCESS response. The CREATED response should only be returned if the relationship was actually created.Luckily, the driver supports result summaries. This means we can easily query whether there were actual updates in the database:How to return the correct response when merging somethingThis efficiency definitely beats using two queries to detect beforehand whether or not the relationship existed already! 🙌We can now start filling the database with HTTP requests. I filled mine with PHP and Neo4j being friends. My screen on Neo4j Bloom looks like this:It’s official: PHP is friends with Neo4j 😃Aggregating relationshipsWhen listing friends in the GET /user/friends endpoint, we can leverage Cypher to reduce code:A simple relationship aggregationBy using the collect function, we can collect” all rows into a single one, by wrapping them in a list.Once again, the controller implementation is a short as it is simple. 👌Querying relationship distanceWe don’t need any application code to traverse paths in the database. Neo4j already provides the shortest path function!Easy shortest path functionThis query essentially says: Match user a and b with their respective ids and define their path as a set of nodes with at least one relationship FriendOf. Then let the shortest path function do the heavy lifting to find the actual shortest path.Because it is possible to have no connection between users at all, it’s best to check against an empty result set:How to test if the results are emptyWrapping Things UpThe only endpoint left to implement is DELETE /user/friend, an exercise. The end result is also available on GitHub in the neo4j-examples organization here.GitHub - neo4j-examples/friends-php-clientThis project is an example of how to create a JSON API with Neo4j and PHP Please refer to the openapi.yml file or the…github.comOther resourcesIf you want to learn more about the driver, there is an older blog post that goes more in-depth here:For a lot of the Neo4j PHP libraries:Neo4j PHP CommunityProjects for using Neo4j from PHP Php client and driver for neo4j database PHP 88 16 Symfony Bundle for the Neo4j Graph…github.comFor other blog posts:Connect to Neo4j with PHPLeverage the power of Neo4j from your PHP applications with the new Neo4j PHP clientmedium.comBuilding a Web App with Neo4j AuraDB and PHPIf you have decided to leverage the power of the worlds leading graph database as a PHP developer or team, getting…neo4j.comFor the PHP Video (look at me, mom!)For the actual repository used in the example and live stream:GitHub - neo4j-examples/friends-php-clientThis project is an example of how to create a JSON API with Neo4j and PHP Please refer to the openapi.yml file or the…github.com;Jan 29, 2022;[]
https://medium.com/neo4j/describing-a-property-graph-data-model-d203c389546e;Michael SimonsFollowMar 22·8 min readDescribing a Property Graph Data ModelPhoto by Pietro Jeng on UnsplashProblem StatementAlmost all client applications we build at Neo4j are interested in knowing what’s in the database. Not the actual data, but the shape of it, its schema.Since we have no standard way of describing it yet, it leads to a situation where each application has its own representation of what the schema looks like.Some tools store a schema in memory, some store it in light persistence places (like web browser local storage) and some store it in Neo4j itself. Different approaches exist as well for inferring a schema: Some tools do sampling, and some do full-store scans.For most applications that are not just based on explorative work, one thing holds true: You can’t have no schema. There will always be a schema.Either your database provides a schema or your application assumes and enforces it, either through object mapping such as GraphQL schemata or OGMs or implicit assumptions: Your data model implies a schema.ExplorationsWhen starting this project we gathered all stakeholders within Neo4j to take inventory of:how they infer the existing data modelin what format they store ithow they store itwhat problems they have run intowhat they would changeWhat became clear early in the project was that we need to be able to serialize the data model so that it can be transported and persisted. This would also create the possibility for applications to hand off model instances between each other for tighter integrations and a better user experience (more on that further down).The GoalThe goal of this project is to validate the prospect of a JSON based data model. Therefore we built a plugin (procedure) that introspects the database it is called on and creates JSON response that validates against a JSON schema. Finding or creating the perfect inferring algorithm was a non-goal of the project.Note: This post is a great example of how hard naming can be: We have a JSON-Schema that allows tooling to validate concrete instances of the content of a database scheme (represented as JSON). The latter is referred to as data model in this post.The SchemaThe schema is currently published at https://unpkg.com/@neo4j/graph-json-schema/json-schema.json and can be used as a $schema reference both for validating and when authoring a schema by hand.The JSON schema is based on the idea to separate tokens (node labels and relationship types) and concrete instances of nodes and relationships model elements (object types). Think like classes and concrete object instances.This approach basically serves multiple purposes already: Having an easy-to-read catalog of all labels and types inside a database instance and a normalized combination thereof.For the movie graph contained in Neo4j the set of tokens looks like this:{  graphSchemaRepresentation : {    graphSchema : {      nodeLabels : [       {  $id :  nl:Person ,  token :  Person  },       {  $id :  nl:Actor ,  token :  Actor  },       {  $id :  nl:Director ,  token :  Director  },       {  $id :  nl:Movie ,  token :  Movie  }     ],      relationshipTypes : [       {  $id :  rt:ACTED_IN ,  token :  ACTED_IN  },       {  $id :  rt:DIRECTED ,  token :  DIRECTED  }     ],      nodeObjectTypes : [],      relationshipObjectTypes : []   } }}The following snipped show — based on the previous tokens — two concrete node object types and one relationship (the actual movie graph is bigger and the content has been edited for brevity){  graphSchemaRepresentation : {    graphSchema : {      nodeLabels : [],      relationshipTypes : [],      nodeObjectTypes : [       {          $id :  n:Person ,          labels : [{  $ref :  #nl:Person  }],          properties : [           {              token :  born ,              type : {  type :  integer  },              nullable : false           },           {  token :  name ,  type : {  type :  string  },  nullable : false }         ]       },       {          $id :  n:Movie ,          labels : [{  $ref :  #nl:Movie  }],          properties : [           {              token :  title ,              type : {  type :  string  },              nullable : false           },           {              token :  release ,              type : {  type :  date  },              nullable : false           }         ]       }     ],      relationshipObjectTypes : [       {          $id :  r:ACTED_IN ,          type : {  $ref :  #rt:ACTED_IN  },          from : {  $ref :  #n:Actor:Person  },          to : {  $ref :  #n:Movie  },          properties : [           {              token :  roles ,              type : {  type :  array ,  items : {  type :  string  } },              nullable : false           }         ]       }     ]   } }}A graphy visualization of the JSON schema instance for the movie graphs data model looks like this:A graphy visualization of the movie graphs data modelThe AlgorithmThe proof of concept uses an algorithm similar to what Neo4j offers in Neo4j-GraphQL:Use official kernel APIS to retrieve labels and relationship types in useUse existing db.schema.nodeTypeProperties for retrieving all properties for all combinations of labels. Group by sorted combinations of labels to create node object instancesUse existing db.schema.relTypeProperties for retrieving all relationship properties plus a full store scan for evaluating the correct start and end node of the existing types. Group by type and the target of the relationship to create relationship instances. This step does sampling by default and compares only 100 concrete relationships per type and property by default (concrete relationship here means (n:LabelA) -[:TYPE]-> (b:Label2) and not only :TYPE )The set-based approach with grouping on both relationship type and the target is superior to what is currently available as db.schema.visualization as relationships with the same type won’t be merged into each other.The schema makes heavy use of references and thus requires all items to have IDs. In the printed examples above the IDs have been derived from the tokens and can be easily shared and remembered. The algorithm can however also generate ids for you. It will use Time-Sorted Unique Identifiers (TSID) for it. These are readable and sort nicely. The IDs in the schema have no other meaning than being identifiers. The value is derived from the tokens.All tokens that require a quotation and/or sanitization when used in Cypher statements will be quoted and sanitized by default. So everything in the scheme is safe to build Cypher statements with (in case you are interested in that topic, you might want to have a look at the schema name support of the Cypher-DSL).Our proof of concept can also visualize the data model itself, without materializing the JSON schema as such:Visualisation of the concrete data modelUse CasesNeo4j WorkspaceAll of the workspace’s sub-applications: Import, Query, and Explore will use the graph schema for different purposes.ImportThe Neo4j importer tool is a place where users can create their data model and map/import data from CSV files to fit the model. In this case, the model would be stored in the schema format for users to be able to revisit and diff existing models. For incremental imports, inferring the existing schema in the database to create the import model/mapping would save time.QueryIn Query, there’s a sidebar that informs the user on what labels, relationship types and property keys exist in the database that’s connected. This reassures the user that they’re connected to the expected database, and also access to execute queries with a simple mouse click.ExploreThe Explore feature of Workspace relies on knowing the database schema for assisting the user when creating perspectives and scenes and authoring search queries. It is adding styling metadata on top of the core graph model.Cypher EditorThe Cypher editor can provide better and faster smart-sense capabilities based on the preloaded schema without going back and forth to the database. Such capabilities include for example autocompletion (as used to) for labels and types, but also properties and while writing DML statements hints when mandatory properties are missing.Neo4j GraphQL IntrospectorAs the algorithm used in the project is mostly identical to what is in the Neo4j GraphQL introspector today, Neo4j GraphQL could seamlessly switch to something that is maintained in the core product itself, not suffering from network latencies, potentially using faster kernel APIs without sacrificing any of its capabilities our users like.Neo4j-OGMNeo4j-OGM for Java is a Graph-Object-Mapping framework. It basically defines a schema for Neo4j through annotated Java classes. Those Java classes are mostly written by hand or sometimes generated by modeling tools. The proposed schema can be used to steer source generators for Java such as JavaPoet to generate those classes, and thus, turn responsibilities the other way around. While often generated sources are not always to the liking or needs of power users, it would offer a huge benefit to the 80% cohort who just wants their graph mapped to a class graph reassembling the existing database’s shape.General Mapper GenerationAll official language drivers, Neo4j-OGM and Spring Data Neo4j support pluggable mapping mechanisms from records to objects in the corresponding language. Such a mapper can be generated in its simplest form when the shape of a record or at least the shape of nodes in such a record is known. While the code generated with the help of a schema is not sophisticated code at all (it won’t be a general OGM approach that will take care of recreating the local graph based on relationships etc), it will be helpful nevertheless as it takes away from users. It can also be faster than solutions based on dynamically introspecting fields of classes client slide.Try Out the Proof-of-Concepts ProcedureThe plugin generating the JSON instance is called Neo4j Schema Introspector” and its source code is available on GitHub: neo4j/graph-schema-introspector.In case you just want to run it inside your Neo4j installation, you can grab the early access release from the release pages. Look out for the single jar-file named graph-schema-introspector-1.0.0-SNAPSHOT.jar.If you are familiar with Java development and have Java 17 installed, you can follow the introductions in the README to build it yourself.Download that file to a location you can find and copy it like this to your Neo4j installation:cp ~/Downloads/graph-schema-introspector-1.0.0-SNAPSHOT.jar \  ~/Applications/Neo4j/neo4j-enterprise-5.5.0/plugins Your paths as well as the version and edition of Neo4j might vary, but the plugin will work with both community and enterprise editions in the range of the 5.x Neo4j series. After you copied the file you need to restart Neo4j.If you are doing your experiments in Docker, follow this flow:mkdir -p $HOME/neo4j/pluginscp ~/Downloads/graph-schema-introspector-1.0.0-SNAPSHOT.jar $HOME/neo4j/pluginsdocker run \--publish=7474:7474 --publish=7687:7687 \--volume=$HOME/neo4j/plugins:/plugins \neo4j:5.5.0The introspector can be run like this:Pretty Printing the Schema as JSONCALL experimental.introspect.asJson({prettyPrint: true})Visualizing the Schema (Only in Browser)CALL experimental.introspect.asGraph({})Visualizing the Schema Analog to Existing Visualization (Only in Browser)CALL experimental.introspect.asGraph({flat:true})There are more options for those procedures, please refer to the README for them.Working With the JSON SchemaUtility LibrarySince JSON is a serialized format, there can’t be any hot reference paths inside it (as shown in the examples above) but references by strings ($ref -> $id).This makes it hard to work with programmatically and to accommodate that we’ve started to build a utility library graph-schema-json-js-utils in TypeScript.With this utility library, you can seamlessly go from JSON to a JS graph (i.e. the $refs are connected) and back to JSON.See the README for example code or install via npm install @neo4j/graph-schema-utils .To validate a graph schema in the JSON format, the utility library also comes with a validation function. This makes it easy to be able to trust input before you start using it.FeedbackWe would appreciate your feedback on both the JSON schema and how the data model materializes as well as on the introspector algorithm and the utilities around it.You can do this as comments under this post and of course as a ticket on https://github.com/neo4j/graph-schema-introspector/issues.Thanks to my colleague Oskar Hane for working together with me on this topic and story.;Mar 22, 2023;[]
https://medium.com/neo4j/discover-aura-free-week-27-graphconnect-talks-c63c4ea7a1c5;Michael HungerFollowMay 31, 2022·8 min readDiscover Aura Free: Week 27 — GraphConnect TalksThis week we’re looking at a sneak preview of our conference sessions at GraphConnect 2022, which takes place next week (June 6–8) in Austin, Texas.GraphConnect 2022GraphConnect 2022. GraphConnect 2022. Join us for the largest gathering of graph developers in person on June 6 to June…graphconnect.comIf you haven’t decided to come, now is your chance. If you drop an email to michael@neo4j.com we are giving away 10 free conference tickets for our viewers/readers.I sometimes reminisce about past GraphConnects — which go all the way back to 2012 — with our Neo4j Flickr photo albums.Neo4j Graph DatabaseFlickr Fotoswww.flickr.comIf you’d rather watch our livestream, it is right here. Otherwise follow the blog post for all the details.You can find our data and write-ups on our GitHub repository and all videos on the Neo4j website.But let’s get started by setting up our AuraDB Free database.Create a Neo4j AuraDB Free InstanceGo to https://dev.neo4j.com/neo4j-aura to register or log into the service (you might need to verify your email address).After clicking Create Database you can create a new Neo4j AuraDB Free instance. Select a Region close to you and give it a name, e.g. graphconnect.Choose the blank database” option, as we want to import our data ourselves.On the credentials pop-up, make sure to save the password somewhere safe. The default username is always neo4j.Then wait three to five minutes for your instance to be created.Afterwards, you can connect via the Query Button with Neo4j Browser (you’ll need the password), or use Import to load your data with the Data Importer and Explore for Neo4j Bloom.Once your AuraDB instance is running, the connection URL neo4j+s://xxx.databases.neo4j.io is available and you can copy it to your credentials — you might need it later.If you want to see examples for programmatically connecting to the database, go to the Connect” tab of your instance and pick the language of your choice.The DataThe conference agenda isn’t available at the website, but we just snagged it from the conference planning doc :)So here are the two CSVs one with the talk details, and one with the schedule.We put them in google sheets to clean them up and could have loaded them from there (by publishing the sheets to the web: Sessions, Schedule) using LOAD CSV, but we wanted to use our built-in data importer for modeling and loading.The basic structure of the sessions is:IDCompanyIndustryTopicAudienceTypeUseCaseTitleAbstractExperienceFirstNameLastNameFullNameJobTitleBiographyIn the schedule we have:DateTitleStartTimeEndTimeTypeStartEndThe Data ModelWe want to extract the Session and the Presenter, as well as attributes like: UseCase, Audience, Experience, Topic, Company into their own nodes.The data importer allows us to load the CSV’s and map them to our graph model, one by one.Mapping each attribute requires time to create the extra node with a singular attribute and then link it to the session via that attribute and the session’s title.To save you some work, we’ve provided a zip file with model and data that you can load directly into the data importer.We can use a trick for merging the additional CSV by declaring a separate node with the same label Session and mapping the CSV file to it, while using the matching value for Title as the ID.Data ImportWe now just have to click Run Import and supply our password to get the data imported in a few seconds.If we head over to Query, i.e. Neo4j Browser, we can look at all our data there:The final bit of data import is some post-processing:As the UseCase data is stored as a comma-separated string property, we need to split that into a list and then turn the list into rows with UNWIND. Then we can create UseCase node for each entry uniquely with MERGE and connect them to our Session.MATCH (s:Session)UNWIND split(s.UseCase,, ) AS ucWITH * WHERE trim(uc) <> MERGE (u:UseCase {name:uc})MERGE (s)-[:FOR_USECASE]->(u)We also want to turn the Date and the Start, End, StartTime, EndTime properties into proper datetime values instead of strings.MATCH (s:Session)SET s.Date = date(s.Date)SET s.Start = localdatetime(s.Start)SET s.End = localdatetime(s.End)SET s.StartTime = localtime(s.StartTime)SET s.EndTime = localtime(s.EndTime)Data ExplorationWe can see a single session with its related context.MATCH (s:Session)WITH s LIMIT 1MATCH path = (s)--()RETURN pathLet’s look at some breakdowns across the attributes, e.g. which UseCase has the most sessions:MATCH (uc:UseCase)<-[:FOR_USECASE]-()RETURN uc.name, count(*) as cORDER BY c DESCData Science is leading with App Development as a close second. Note that a single session can have multiple use cases.╒═════════════════════╤═══╕│ uc.name             │ c │╞═════════════════════╪═══╡│ Data Science        │44 │├─────────────────────┼───┤│ Apps & APIs         │39 │├─────────────────────┼───┤│ Cypher              │28 │├─────────────────────┼───┤│ Best Practices      │28 │├─────────────────────┼───┤│ Language-specific   │26 │├─────────────────────┼───┤│ Graph Visualization │23 │├─────────────────────┼───┤│ Cloud               │22 │├─────────────────────┼───┤│ Aura                │19 │├─────────────────────┼───┤│ Knowledge Graphs    │17 │├─────────────────────┼───┤│ Operations          │16 │├─────────────────────┼───┤│ Security            │15 │├─────────────────────┼───┤│ Full Stack          │12 │├─────────────────────┼───┤│ GraphQL             │5  │├─────────────────────┼───┤│ Use Case            │4  │├─────────────────────┼───┤│ Telecom Networks    │1  │├─────────────────────┼───┤│ Digital Twin        │1  │├─────────────────────┼───┤│ Fraud detection     │1  │├─────────────────────┼───┤│ Knowledge Graph     │1  │└─────────────────────┴───┘Or what is the overlap between use cases and industries (we need to exclude General as industry):MATCH (uc:UseCase)<-[:FOR_USECASE]-(s)-[:FOR_INDUSTRY]->(i:Industry)WHERE NOT i.Industry in [General]RETURN uc.name, i.Industry, count(*) as cORDER BY c DESCLIMIT 5Great to see that there are a lot of data science and healthcare related talks.╒══════════════════╤═══════════════════════╤═══╕│ uc.name          │ i.Industry            │ c │╞══════════════════╪═══════════════════════╪═══╡│ Data Science     │ Healthcare            │7  │├──────────────────┼───────────────────────┼───┤│ Data Science     │ Financial             │4  │├──────────────────┼───────────────────────┼───┤│ Data Science     │ Biotechnology         │3  │├──────────────────┼───────────────────────┼───┤│ Knowledge Graphs │ Healthcare            │3  │├──────────────────┼───────────────────────┼───┤│ Data Science     │ Aerospace and Defense │3  │└──────────────────┴───────────────────────┴───┘We can also compute percentages of all talks for a given use case per industry:MATCH ()-[:FOR_INDUSTRY]->(i:Industry)WITH i, count(*) as allMATCH (n:UseCase)<-[:FOR_USECASE]-(session)-[:FOR_INDUSTRY]->(i)WHERE NOT i.Industry IN [General]RETURN n.name as useCase, i.Industry, count(*) as count, count(*)*100/all as percent, allORDER BY count DESC LIMIT 10This gives us a better understanding of focus areas rather than the absolute numbers.╒═════════════════════╤═══════════════════════╤═══════╤═════════╤═══│ useCase             │ i.Industry            |count  │percent  │all╞═════════════════════╪═══════════════════════╪═══════╪═════════╪═══│ Data Science        │ Healthcare            │7      │77       │9              │├─────────────────────┼───────────────────────┼───────┼─────────┼───│ Data Science        │ Financial             │4      │100      │4              │├─────────────────────┼───────────────────────┼───────┼─────────┼───│ Data Science        │ Biotechnology         │3      │100      │3              │├─────────────────────┼───────────────────────┼───────┼─────────┼───│ Knowledge Graphs    │ Healthcare            │3      │33       │9              │├─────────────────────┼───────────────────────┼───────┼─────────┼───│ Data Science        │ Aerospace and Defense │3      │100      │3              │Or between Topic and Industry…MATCH (t:Topic)<-[:ON_TOPIC]-(session)-[:FOR_INDUSTRY]->(i:Industry)WHERE NOT i.Industry IN [General]RETURN t.Topic, i.Industry, count(*) as countORDER BY count DESC LIMIT 5…which allows to see us the healthcare applications.╒═════════════════╤═══════════════════════╤═══════╕│ t.Topic         │ i.Industry            │ count │╞═════════════════╪═══════════════════════╪═══════╡│ Knowledge Graph │ Healthcare            │3      │├─────────────────┼───────────────────────┼───────┤│ Knowledge Graph │ Aerospace and Defense │2      │├─────────────────┼───────────────────────┼───────┤│ GDS-UC          │ Telecom               │2      │├─────────────────┼───────────────────────┼───────┤│ GDS-UC          │ Healthcare            │2      │├─────────────────┼───────────────────────┼───────┤│ GDS-UC          │ Financial             │2      │└─────────────────┴───────────────────────┴───────┘ScheduleWe can also look at the schedule for one day, e.g. by sorting by start time and aggregating the titles into a list.MATCH (s:Session)WHERE date(s.Start) = date(2022-06-06)RETURN localtime(s.Start) as start, collect(s.Title) as titlesORDER BY start ASCOptionally we could create Nodes for Times and Rooms, if we wanted to represent them in our graph model.We can also just see if we can fill our day with talks from specific use cases.MATCH (s:Session)-[:FOR_USECASE]->(uc)WHERE uc.name IN [Best Practices,Aura]WITH distinct sWHERE date(s.Start) = date(2022-06-06)RETURN localtime(s.Start) as start, collect(s.Title) as titlesORDER BY start ASCAh, nice, that is our day then:╒══════════╤═══════════════════════════════════════════════╕│ start    │ titles                                        │╞══════════╪═══════════════════════════════════════════════╡│ 10:00:00 │[ Accelerating ML Ops with Graphs and Ontology-││          │Driven Design ]                                │├──────────┼───────────────────────────────────────────────┤│ 10:50:00 │[ Introduction to Neo4j AuraDB: Your Fastest Pa││          │th to Graph ]                                  │├──────────┼───────────────────────────────────────────────┤│ 11:10:00 │[ Node Art , Taming Large Databases ]          │├──────────┼───────────────────────────────────────────────┤│ 11:30:00 │[ Tracking Data Sources of Fused Entities in La││          │w Enforcement Graphs ]                         │├──────────┼───────────────────────────────────────────────┤│ 13:00:00 │[ Discovery and Insights with Graph Visualizati││          │on Using Neo4j Bloom , New! Monitoring and Admi││          │nistration with Neo4j Ops Manager ]            │├──────────┼───────────────────────────────────────────────┤│ 13:45:00 │[ Introducing Workspaces, a New Experience for ││          │Neo4j Developer Tools , Trucks on a Graph: How ││          │JB Hunt Uses Neo4j ]                           │├──────────┼───────────────────────────────────────────────┤│ 14:30:00 │[ The Inside Scoop on Neo4j: Meet the Builders ││          │, Knowledge Graphs for Pharma: Powered by Data ││          │& AI Using KG4P , Getting the Most From Todays││          │ Java Tooling With Neo4j ]                     │├──────────┼───────────────────────────────────────────────┤│ 15:15:00 │[ Neo4j Drivers Best Practices ]               │└──────────┴───────────────────────────────────────────────┘RecommendationNow we could look at recommending talks by shared attributes, so the more attributes a talk shares with a talk I like, the more similar it is (of course we can also do the opposite, sorting ASC to get the least overlap and more diversity).Or we fix any attribute we like to stay the same by adding or excluding relationship types.We also don’t want the recommended talk to overlap in terms of start time, so we add this as an exclusion.After finding our top five recommendations, we can fetch the speaker and their company as additional information to return.MATCH (s:Session {Title:Using Graph Analytics to Solve Cloud Security Problems})// overlap across attributesMATCH (s)--(attr)--(s2:Session)WHERE date(s.Start) <> date(s2.Start)      OR localtime(s.Start) <> localtime(s2.Start)// compute occurrence frequencyWITH s2, count(*) as freq, collect(attr) as overlapORDER BY freq DESC LIMIT 5// fetch additional data for the sessionMATCH (s2)<-[:PRESENTS]-(sp:Speaker)-[:WORKS_AT]->(c:Company)RETURN s2.Title, s2.Start, sp.FullName, c.Company, freq, overlapVisualizationFor visualization, we can look at Explore— aka Neo4j Bloom.We open it from our Aura console and generate a perspective to be used.The we can e.g. query all the Data Science sessions, their speakers, and companies.We can also take our recommendation query from above and turn it into a saved search phrase in Bloom so that we can visualize each recommendation. We can pick the parameter from suggestions from the actual title data and return paths instead of scalar data.I hope you’re coming to GraphConnect! Don’t forget our 10 free ticket offers (email to michael@neo4j.com) or use code Community50” for 50 percent off your ticket price. Register here.You can use similar graph structures for other conferences or curriculum schedules.Happy Graphing!;May 31, 2022;[]
https://medium.com/neo4j/proudly-releasing-efficient-graph-algorithms-in-neo4j-b61ab0ce55b2;Michael HungerFollowAug 3, 2017·5 min readProudly Releasing: Efficient Graph Algorithms in Neo4jI am very happy to announce the first public release of the Neo4j graph algorithms library.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comYou can use these graph algorithms on your connected data to gain new insights more easily within Neo4j. You can use these graph analytics to improve results from your graph data, for example by focusing on particular communities or favoring popular entities.We developed this library as part of our effort to make it easier to use Neo4j for a wider variety of applications. Many users expressed interest in running graph algorithms directly on Neo4j without having to employ a secondary system. We also tuned these algorithms to be as efficient as possible in regards to resource utilization as well as streamlined for later management and debugging.These algorithms represent user-defined procedures which you can call as part of Cypher statements running on top of Neo4j.KudosBig thanks goes to Martin Knobloch and Paul Horn from our good friends at Avantgarde Labs in Dresden who did all the heavy lifting. From reading tons of papers and tuning and parallelizing implementations, to providing performance testing and implementers documentation — most of the work you see in this library is theirs.Tomaz Bratanic also helped immensely with documenting the library, providing explanations and examples on small graphs and detailing syntax information for all graph algorithms.Our documentation is still a work in progress, so please bear with us! However, please let us know if the existing sections are helpful or you have ideas on how to improve the documentation.The Graph AlgorithmsThe graph algorithms covered by the library are:Centrality:PageRankBetweenness CentralityCloseness CentralityPartitioning:Label Propagation(Weakly) Connected ComponentsStrongly Connected ComponentsUnion-FindPath-Finding:Minimum Weight Spanning Tree<All Pairs — and Single Source — Shortest PathMulti-Source, Breadth-First SearchMost of the graph algorithms are available in two variants: One that writes the results (e.g., rank or partition) back to the graph, and the other, suffixed with .stream which will stream the results back for further sorting, filtering or aggregation.To select which part of the graph to run the graph algorithm on, you can provide a label and relationship type as first parameters. Then, add configuration options depending on the algorithm. For instance, iterations and damping-factor for PageRank.Example: PageRank on DBPediaHere we run PageRank on DBPedia (11M Page-nodes, 125M Link-relationships):CALL algo.pageRank(Page, Link, {write:true,iterations:20}) +------------------------------------------------------------+ | nodes    | iter | loadMillis | computeMillis | writeMillis | +------------------------------------------------------------+ | 11474730 |   20 |      34106 | 9712          | 1810        | +------------------------------------------------------------+ 1 row 47888 ms CALL algo.pageRank.stream(Page, Link, {iterations:5}) YIELD node, score WITH * ORDER BY score DESC LIMIT 5 RETURN node.title, score +--------------------------------------+ | node.title                 | score   | +--------------------------------------+ |  United States             | 13349.2 | |  Animal                    | 6077.77 | |  France                    | 5025.61 | |  List of sovereign states  | 4913.92 | |  Germany                   | 4662.32 | +--------------------------------------+ 5 rows 46247 msGraph ProjectionOne really cool feature is the ability to load a projection of a (sub-)graph of your data into the graph algorithm by passing Cypher statements to select nodes and node-pairs and choosing the cypher” graph loader. The compiled Cypher runtime of Neo4j 3.2 (Enterprise) benefits this load strategy.Here is an example for Union-Find:CALL algo.unionFind(MATCH (p:Page) WHERE p.title CONTAINS  Europe  RETURN id(p) as id ,MATCH (p1:Page)-[:Link]->(p2:Page) WHERE p1.title CONTAINS  Europe  AND p2.title CONTAINS  Europe  RETURN id(p1) as source, id(p2) as target ,{graph:cypher,write:true})+-------------------------------------------------------------+| loadMillis | computeMillis | writeMillis | nodes | setCount | +-------------------------------------------------------------+ |       2975 |             2 |          30 | 23467 |     4322 | +-------------------------------------------------------------+ 1 row 3013 msPerformance TestingFor several of the algorithms (PageRank, union-find, label-propagation, strongly-connected-components), I ran preliminary tests on medium and larger datasets that have also been used in other publications.The table below contains database size as well as node and relationship counts. For each graph algorithm, you see the runtime (in seconds) of the first and second run.Comparing them with other publications, those runtimes look quite good. But of course the real confirmation comes from you running the algorithms on your own datasets on your own hardware.first and second run of each algorithm runtime for load, compute and write-backlog-scale graph-algorithms-testingAnd here is the obligatory chart. Please note that this is log-scale to fit larger and smaller datasets in one chart.We provide two releases, one for Neo4j 3.1.x and one for Neo4j 3.2.xInstallation is easy: just download the jar-file from the release link below, copy it into your $NEO4J_HOME/plugins directory and restart Neo4j.Note: For Neo4j 3.2.x you will also have to add this line to your $NEO4J_HOME/conf/neo4j.conf config file: dbms.security.procedures.unrestricted=algo.*We would love to hear your thoughts on the new graph algorithm library!Please try out this library on your data and let us know how it worked. Raise GitHub issues if you run into any problems and don’t forget our #neo4j-graph-algorithm channel in the neo4j-users Slack if you have questions.ImplementationThe library is currently limited to handle 2 billion nodes by design, but in future versions we will remove those limits as we more tightly integrate this work.Our general approach is to load the projected data from Neo4j into an efficient data structure, compute the algorithm and write the results back. All three steps are parallelized as much as possible to maximize CPU and I/O utilization.We use a composed Graph-API interface to provide the algorithms access to the graph data, which is loaded into different representations by GraphFactory instances. Both the loading and writing back of results happens in parallel batches.Feel free to have a look at the code, give us feedback or even add your own algorithm implementation based on the existing infrastructure.We welcome any pull request with new algorithms, bug-fixes or other improvements.Happy computing,MichaelFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”ReferencesReleasesGitHub Repository (please star if you like it)DocumentationGitHub issuesNeo4j Slack community;Aug 3, 2017;[]
https://medium.com/neo4j/real-time-dashboard-of-bitcoin-transactions-with-neo4j-and-neodash-ecfda0ba2c9b;Tomaz BratanicFollowMar 21, 2022·8 min readReal-Time Dashboard of Bitcoin Transactions With Neo4j and NeoDashImporting Bitcoin transactions via Python Websocket into Neo4j and monitoring them with a NeoDash dashboardCryptocurrencies have become more and more integrated into our daily life. Many cryptocurrencies like Bitcoin and Ethereum are decentralized networks based on blockchain technology. They have a wide array of use-cases and have been implemented by various companies and governments.One remarkable attribute of many cryptocurrencies is that all the transactions are available publicly. In addition, some sites offer freely accessible endpoints to retrieve crypto transactions in real-time. That makes developing a monitoring tool to analyze the crypto transactions or track how value flows throughout the network incredibly easy.This post will present a simple architecture to listen, store, and analyze Bitcoin transactions in real-time. We will be using the Blockchain.com WebSocket API to listen for new Bitcoin transactions. It is a free endpoint and does not require any authorization. Most transactions can be represented as an exchange of value between two or more entities.If you are like me, the first thing you think about when you hear someone presenting a data format that contains entities and their relationships, is a graph. Therefore, it makes sense to store Bitcoin transactions in a graph database. Not only are you able to calculate various data statistics, but more importantly, you can analyze the flow of value within the network and more easily identify significant actors.In this example, we will be using Neo4j, a native graph database, to store the retrieved information. Lastly, we will be using a simple dashboard tool called NeoDash, that you can connect to Neo4j and seamlessly develop various visualizations for more straightforward data analysis.Overview of the service architecture. Image by the author.Before we begin with the code, we first have to review the structure of Bitcoin transactions. For example, you might be used to dealing with transactions having a single sender and recipient. However, this is not the case with Bitcoin. Instead, any Bitcoin transaction can have multiple senders and recipients.An example Bitcoin transaction with multiple senders and recipients. Image by the author.In Bitcoin terminology, the senders are called inputs, while the recipients are called outputs. Since a single transaction can have multiple inputs and outputs, we model the transaction as an intermediate node. This model allows us to capture the transactions in the original form as we can append multiple incoming and outgoing links to a transaction node.I’ve also learned another intriguing characteristic of the Bitcoin network. For example, let’s say that person A and B sent you each 0.5 BTC. So now you own 1 BTC in total that came from two separate transactions. You’ve got a good friend Jimmy, and you want to send him 0.8 BTC. The transaction data structure will the following:Bitcoin transaction chain, where the second transaction return change” to the original sender. Image by the author.So first, you can only send what you received from other transactions as input. It can’t be more or less. I didn’t know this before, but Håkan Löfqvist explained it with a simple example. Every BTC you receive from a transaction is like a dollar note. The only difference is that the value of a note could be any number of BTCs. You first received two bills in the above examples, each worth 0.5 BTC. Now you want to pay 0.8 BTC for a ticket to a movie theater. The cashier will take both of your notes and return you the 0.2 BTC change. I guess this data structure guarantees the integrity of the whole blockchain, but I haven’t delved much into it.Anyhow, the transaction will appear to have a total value of 1 BTC, even though the 0.2 BTC was returned as change. I’ve introduced another term in my graph model to deal with this. Even though the total value of the transaction is 1 BTC, the value flow is only 0.8 BTC. By the transaction value flow, I will be referring to the worth of BTC that wasn’t returned as change”.Here is an example of a Bitcoin transaction with a total value of 1.2 billion USD, but the actual flow value is only 50 USD. As most of the value was returned to the original sender, it is not insightful information when analyzing Bitcoin network flows. However, it’s still interesting to know that someone has a single BTC note worth more than a billion dollars.The real-time dashboard code is available as a GitHub repository.Developing a Real-Time Bitcoin DashboardNow that we got the theory out of the way, we can delve into developing a real-time Bitcoin dashboard. We will begin by defining the Neo4j graph model. We have two options for modeling Bitcoin transactions.Modeling transaction outputs as explicit nodes in the graph. Image by the author.Modeling transaction outputs as explicit nodes is an option. With this approach, we preserve the original structure of the Bitcoin blockchain and its information. It allows us to quickly retrieve how many Bitcoin notes (outputs) each address has and how many of them have been spent. The address is the ID of where the Bitcoins are being held.However, I’ve decided to use a simpler graph model as I was only interested in analyzing the flow of Bitcoin transactions through the network and less about how many spent or unspent outputs each address has.Bitcoin transaction graph model. Image by the author.The graph consists of addresses and transactions. The transaction nodes contain the transaction hash and the timestamp and some preprocessed information like the total and the flow value of the transaction. The original input and output contributions are stored as relationship properties to allow multiple inputs and outputs with various contributions. The direction of the relationships indicates the flow of value.The GitHub repository contains the define_schema.sh bash scripts that sets up unique constraints and indexes in Neo4j.We will take a look at the Python microservice that imports Bitcoin transactions in Neo4j. The service uses the WebSocket client library to listen for transactions through the Blockchain WebSocket.Since the transactions themselves don’t provide the USD conversion value, I’ve added a function that updates the Bitcoin conversion ratio once per hour and uses it to calculate the transaction and flow values in USD.Since the WebSocket randomly drops connection every five to ten thousand transactions, I’ve included the code to handle any WebSocket errors and reconnect automatically.The only missing thing is the code that is executed when a new transaction is transmitted through the WebSocket. You can examine the structure of the response on the Blockchain website. The data contains information about the inputs and outputs of the transaction as well as its hash and timestamp. For a more straightforward analysis, we will also preprocess the total and flow values of the transactions and the USD equivalent of BTC values.The Bitcoin values are represented as Satoshis, so we need to divide the value by 100 million to get transaction values in BTC. Google says there are around 300–400 thousand Bitcoin transactions per day, which amounts to approximately five transactions per second. At first, I thought I would need to utilize some sort of batch import to handle this volume, but it turns out we can import one-by-one transactions without any problems. I’ve left the service running for 48 hours at this point in time, and no errors appeared with this approach.The whole project consists of three docker services and their configuration as a docker-compose file. After you have defined the unique constraints in Neo4j with the define_schema.shscript, you can simply execute docker-compose upcommand to begin storing real-time Bitcoin transactions in Neo4j.Finally, I’ve used the NeoDash tool to prepare a simple dashboard to monitor overall transaction statistics as well as having the option to drill down specific addresses or transactions in the database. The NeoDash is available as a Neo4j Desktop application and a standalone docker instance. In our case, we will utilize the standalone docker instance. I’ve added the dashboard settings and the instructions on how to set it up in the readme of the repository.If you have used the docker-composecommand, the NeoDash will be available on port 80. The first tab of the dashboard provides overall transaction statistics, largest transactions, and incoming address flows for the last 24 hours.Dashboard that represents overall Bitcoin transactions. Image by the author.Interestingly, the count of transactions drops a bit during the European night. For example, let’s say you have identified a specific address you want to investigate further. On the second tab of the dashboard, you have the option to input the address id as a parameter, which will allow you to drill down a specific address.Drill down a specific address. Image by the author.I’ve picked one address that came up in the list with the highest incoming flow. It seems that an address received ten thousand BTC on the first of March and in less than 30 minutes forwarded it forward to the next address.Lastly, I’ve prepared a dashboard tab that allows you to inspect a single transaction.Inspect a single transaction in NeoDash. Image by the author.I didn’t realize that this kind of volume is moved through the Bitcoin network. For example, I’ve identified a couple of transactions with more than 500 million USD value flow. Since the data is stored in Neo4j, you have the complete flexibility of Cypher as well as the graph algorithms features in the Neo4j Graph Data Science library to search for interesting transaction patterns.For instance, I’ve identified a transaction worth 15 thousand BTC and then analyzed how the value dispersed through the network.Path of the 15 thousand BTC through the Bitcoin network. Addresses are blue and purple are the transactions. Image by the author.It seems that on March 1, 2022, someone initiated the value flow by sending 15 thousand BTC to another account. Note that the USD equivalent of the transaction flow is more than 650 million USD. The Bitcoins were then sent through 6 different accounts to their final destination in the span of six hours.ConclusionBy core, all the transactions in the Bitcoin network are publicly available and easily accessible in real-time for us to analyze them. So, if you want to do some academic research or dive into forensic investigation, I hope this post and the accompanying code can help you get started. Test it out and let me know if you have any ideas on improving the code or any interesting use cases by adding an issue to the repository.We haven’t yet used any advanced Cypher techniques or graph algorithms to examine the Bitcoin transactions, so stay tuned, as that will probably be my next post.As always, all the code is available on GitHub.P.S. I’ve later found that a project that imports the whole Bitcoin history into Neo4j as well if you are interested.;Mar 21, 2022;[]
https://medium.com/neo4j/will-it-graph-identifying-a-good-fit-for-graph-databases-part-1-506eda46c26e;Ljubica LazarevicFollowJul 19, 2021·7 min readWill It Graph? Identifying a Good Fit for Graph Databases — Part 1Graph Databases vs. Relational Databases: What’s the Difference?How do graph databases work under the hood and how is this different from relational databases?In this Will It Graph blog series, we’re going to explore the differences between native graph databases and relational databases and help you understand how to identify the right database system for your application. (Or listen to the GraphStuff.FM podcast!)Will It Graph? Identifying A Good Fit For Graph Databases | GraphStuff.FM: The Neo4j Graph Database…How do you know when the application youre building is a good fit for a graph database? How do graph databases work…graphstuff.fmRelational Databases: Normalization and JOINA lot of developers are familiar with the traditional relational database, where data is stored in tables within a well-defined schema. Each row in the table is a discrete entity of data. One of these elements in the row is typically used to define its uniqueness: the primary key. It could be a unique ID or maybe something like a social security number for a person.We then go through a process called normalization to reduce data repetition. In normalization, we’re moving references, something like an address for a person, into another table. So we get a reference from the row representing the entity to the row representing the address for that person.If, for example, somebody changes their address, you wouldn’t want multiple versions of that person’s addresses everywhere and have to try and remember all the different instances of where that person’s addresses exist. Normalization makes sure you have one version of the data, so you can make the updates in one place.Then when we query, we want to reconstitute this normalized data. We do what’s called a JOIN operation. In our main entity row, we have the primary key that identifies the ID for the entity, let’s say the person. We also have what’s called a foreign key that represents a row in our address table. We join the two tables through their primary and foreign keys, and use that to look up the address in the address table. This is called a JOIN and these JOINs are done at query time and at read time.When we’re doing a JOIN in a relational database, it’s a set comparison operation where we’re looking to see where our two sets of data overlap (in this case, the sets are the person table and the address table). At a high level, that’s how traditional relational databases work.Native Graph Databases: Connections and Index-Free AdjacencyLet’s have a quick peek at a native graph database and how it works. We spoke about the discrete entity in a relational database being a row within a table. In a native graph database, that row would be the equivalent of a node. It’s still a discrete entity, so we still have this element of normalization.A node would be an entity. If we were having person nodes, we would have one node for one person. And we would have some degree of uniqueness in it, let’s say the social security number. The key difference, however, is when we are connecting this person node to another discrete entity, for example, an address, we create a physical connection (aka relationship) between those two points.The address would have a pointer that says, what is the outbound part of the relationship that connects to the node? We then have another pointer for the inbound part of the relationship pointing to the other node. So, effectively, we’re collecting a set of pointers, and this is a manifestation of the physical connection between those two entities. That is a big difference.In a relational database, how you would reconstitute the data is it joins on read, which means at query time, it would go off to try and figure out how things map together.In a graph database, since we already know these two elements are connected, we don’t need to look up the mapping at query time. All we’re doing is follow the stored relationships to the other nodes. This is something we call index-free adjacency.This concept of index-free adjacency is key to understanding the performance optimizations of a native graph database compared to other database systems.Index-free adjacency means that during a local graph traversal, following these pointers (relationships) that connect the nodes in my graph, the performance of the operation is not dependent on the overall size of the graph. It depends on the number of relationships connected to the nodes that you’re traversing.When we talk of a JOIN being a set operation (intersection), we’re using an index in a relational database to see where those two sets overlap. This means that the performance of the JOIN operation starts to slow down as the tables get bigger. In big O notation terms, this is something like logarithmic growth using an index — something like O(log n) and also grows exponentially with the number of JOINs in your query.Whereas traversing relationships in the graph is more of linear growth based on the number of relationships in the nodes that we’re actually traversing, not the overall size of the graph.This is the fundamental query time optimization that graph databases make that give us index-free adjacency. From a performance perspective, that is really the most important thing to think about when we think of a native graph database.What does this really mean? There’s a number of really powerful things in this paradigm shift of how we’re storing the data. The key one here is that we don’t need to hypothesize about how connections are being made between discrete entities. Either they exist or they don’t exist.Graph Databases for Beginners: Why Graph Technology Is the FutureThe world of graph technology has changed (and is still changing), so were rebooting our  Graph Databases for…neo4j.comThis is really powerful because we don’t have to try and predict the number of different JOINs or traversals we’d need to make between different elements, which we would have to do in a traditional relational database system.This gives us a number of interesting opportunities. One of them is it makes it a lot easier for us to query patterns.We could be looking for a specific start point, such as looking for a specific person based on a specific social security number.But what if we’re looking at patterns such as a person node being connected to an address that is connected to a series of other nodes? We can declare a pattern and just search for that pattern. That’s a lot easier in this setup.If we’re doing something that would be very JOIN-heavy in a traditional database system, it is a lot faster in a graph database.But at What Cost?That sounds good, but obviously there has to be some trade-off, right? We don’t get anything for free in the world of computing. Building a database is really an exercise in optimizing for different trade-offs. So it’s important to understand the trade-offs that you’re making when you’re optimizing for this idea of graph-native index-free adjacency performance.Read vs. WriteOne of those trade-offs is the optimizations for read time versus write time. Think of a relationship (pointer, connection, edge) in the property graph model as a first-class citizen. They’re explicitly part of the data model. When we compare it to a relational database, a relationship is like a materialized JOIN.In a relational database, we materialize JOINs on read at query time, which means we’re taking that performance hit to see where the JOINs are with each query.Whereas in a graph database, we’re just basically following pointers, going to offsets in memory to find the other nodes that are connected as we traverse the graph.This means that, at write time, we have to materialize and store these relationships. So in a graph database, we may have different write performance than we might expect in a relational database where we’re not materializing these JOINs at write time.Why Haven’t Graph Databases Been Around for Forever?Graph databases are a relatively new thing. The reason why graph databases have very performant systems that can hold billions or even trillions of nodes and relationships and run at high speed is due to the advancement in hardware.Graph databases like to have memory, and RAM is cheap and plentiful these days. This allows us to tap into the real power of graph databases.ConclusionNow you know how relational databases and graph databases work differently. So if your application is JOIN-heavy in a traditional relational database and suffering from SQL strain, perhaps it’s time to consider switching to graph databases.White Paper: Overcoming SQL StrainNever Write Another Join! Solve Your RDBMS Problems With a Graph Database Relational databases work best for problems…neo4j.comIn the next part of the blog series, we’ll show you how to identify graph-shaped problems and some examples of the good and poor fits for graph databases.Will It Graph? Identifying A Good Fit For Graph Databases | GraphStuff.FM: The Neo4j Graph Database…How do you know when the application youre building is a good fit for a graph database? How do graph databases work…graphstuff.fm;Jul 19, 2021;[]
https://medium.com/neo4j/introducing-neuler-the-graph-algorithms-playground-d81042cfcd56;Mark NeedhamFollowApr 12, 2019·6 min readIntroducing NEuler — The Graph Algorithms PlaygroundUntil now the only way to run Graph Algorithms on Neo4j has been to learn Cypher. The Graph Algorithms Playground changes all that.Note: This article assumes that you have used Neo4j Desktop before. We added some info that should get you through anyhow, but if you have questions, please refer to this great intro to Neo4j Desktop by Jennifer Reif.It’s been almost two years since the release of the Neo4j Graph Algorithms library, which made it easy for developers to run algorithms like PageRank, Louvain Modularity, and Weighted Shortest Path, on their graph data.The PageRank algorithm in the Neo4j Graph Algorithms LibraryThese algorithms were made available as procedures, and can be executed directly from Cypher queries.Online Meetup #54: Desktop Graph Analytics: For the ThroneIn January my colleague Irfan and I were chatting, and we thought it’d be cool if we could open up the power of graph algorithms to users that don’t want to spend their days writing Cypher code.We presented our initial efforts building the Neo4j Euler (NEuler) Graph App (aka the Graph Algorithms Playground)in episode 54 of the Neo4j Online Meetup, and showed how the app could be used to analyse the Game of Thrones universe.Over the last month we’ve taken user feedback on board, and added support for path finding and similarity algorithms.How do I install it?First you should have Neo4j Desktop installed. Then in the default Project, create and start a Graph.How to setup a Graph in Neo4j DesktopThe Graph Algorithms Playground (NEuler) is one of the Graph Apps included in the Graph Apps Gallery, so you can use the one-click-install (Windows, OSX) from that page.Installing the Graph Algorithms PlaygroundThis only works on Mac and Windows at the moment.If like me, you’re using Linux, you’ll need to paste https://bit.ly/install-neuler into the ‘Install Graph Application’ form. That always works.You can access this section via the icon with four squares on the sidebar of the Neo4j Desktop.Version 0.1.10 or later is required for the remainder of this postMake sure you have at least version 0.1.10 of the app installed, otherwise some functionality like the example dataset will not yet be there. If you had installed it before, check that your Desktop is not in offline-mode. It has to be online to automatically install updates.Once you’ve done that the Graph Algorithms Playground will be available to add to any of your projects, which you do via the (+) Add Application button in your Project, see below.Presuming you added a new, empty graph (database) to your project and having started the graph, you can start the Graph Algorithms Playground graph app.Let’s explore what’s in the box.Exploring the Graph Algorithms PlaygroundOnce you launch the Graph Algorithms Playground you’ll be faced with this screen, which describes the categories of algorithms available in the app.NEuler now has support for 4 types of algorithmsIf you want to learn more about the intricacies of the algorithms in each of these categories, this is the part of the post where I shamelessly plugin the Graph Algorithms Book that Amy Hodler and I have been working on.O’Reilly Graph Algorithms BookYou’ll can download your complimentary copy of this book by going to neo4j.com/graph-algorithms-bookLoading sample graphsAnyway, enough of that, back to NEuler. I think the best way to understand graph algorithms is to play around with sample datasets that we know well and inspect the results that the algorithms return.To help with that we’ve added a section of the app from which you can load sample graphs:NEuler with the Sample Graphs” sectionAt the moment we’ve only got Game of Thrones, but we’ll add more over time. If we click ‘Load’ on the Game of Thrones dataset, we’ll see the following screen:Load Game of Thrones into Neo4jThis dataset contains the interactions between characters across the seasons, and was compiled by Andrew Beveridge. You can explore it in Neo4j separately with :play gotClicking the ‘Yes load it!’ button will execute those Cypher queries, so don’t do this on a production database!Once we’ve got that dataset loaded, we can start to explore the data in preparation for the new season that starts on Sunday (or Monday if you’re in the UK like me!).Centrality AlgorithmsOne of the simplest algorithms is Degree Centrality, which on this dataset indicates how many interactions a character has had. From the screenshot below we can see that in Season 1 — Ned, Tyrion, and Catelyn were the most directly important.Degree Centrality on Season 1 of Game of ThronesWe can do the same for the other seasons by selecting another ‘Relationship Type’ and running the algorithm again.We can also view charts showing the results of the centrality algorithms. The chart below shows the output from running the PageRank algorithm on the Season 2 data.PageRank of characters in Season 2This algorithm finds the characters who are the most transitively important i.e. which characters have been interacting with other important characters.What about the code?And if we want to try these algorithms out on our own, the ‘Code’ tab shows the queries and parameters that can be used in the Neo4j Browser to achieve this:The code behind that runs the PageRank algorithmThe ‘Send to Neo4j Browser’ option will generate a Browser Guide and open it up in the Neo4j Browser.Combining community detection and centralityOne of the other cool features of the Graph Algorithms Playground is visualising the output of the algorithms.This is most fun when combining community detection and centrality algorithms. In the example below we see the combination of the PageRank centrality and Louvain community detection algorithms.Season 2 Visualisation using Louvain for communities and PageRank for node importanceIf we tick the ‘Store results’ checkbox of these algorithms, we can then choose the ‘Node Size’ and ‘Node Color’ based on those results. In this visualisation we can see the communities from Season 2 of the show.I thought we must have made a mistake because of the disconnected component containing Daenerys on the right of the diagram, but Irfan reminded me that in Season 2 they were away from everyone else and only interacted with each other!In SummaryWe hope you like this addition to the Graph App Gallery. Enjoy playing around with the app and let us know in the comments if you like it. You can also share screenshots of the algorithm results on your data on Twitter, with the tags #Neo4j and #Neuler.If you have questions regarding your Neo4j experience, you can always head to the Neo4j Community Forum.Don’t forget to grab the free copy of our O’Reilly book about Graph Algorithms on Apache Spark and Neo4j”;Apr 12, 2019;[]
https://medium.com/neo4j/create-neo4j-procedure-with-kotlin-in-20-minutes-75fa374c46e4;Emad PanahFollowJun 23, 2018·6 min readCreate a Neo4j Procedure with Kotlin in 20 minutesGraph Database: Neo4jIf you already know about Neo4j and Graph databases you can skip ahead to the use-case for writing our procedures.There is special type of NOSQL databases known as a Graph Store. These are powered by one of the old and strong data structure concepts in computer science, that is Graph Theory. Graph theory has seen a great usefulness and relevance in many problems across various domains. The most applied graph theoretic algorithms include various types of shortest path calculations, geodesic path, centrality measures like PageRank, Eigenvector centrality, closeness, betweenness, HITS, and many others.There are many proven solutions for Graph Databases out there. Neo4j is a high-performance NOSQL graph database that is very easy to install and it is open source to use. It provides an ACID-Compliant transactional backend for your applications. The source code, written in Java and Scala, is available on GitHub. Neo4j provides you super connected data structures in graphs, that makes complicated data patterns easier to implement. Some of the key features are mentioned as the following (according to neo4j.com):1. Neo4j’s First Mover Advantage is Connecting Everyone to Graphs2. Biggest and Most Active Graph Community on the Planet3. Highly Performance Read and Write Scalability, Without Compromise4. High-Performance Thanks to Native Graph Storage & Processing5. Rock-Solid Reliability for Mission-Critical Production Applications6. Easier than Ever to Load Your Data into Neo4j7. Whiteboard-friendly Data Modeling to Simplify the Development Cycle8. Superb Value for Enterprise and Startup ProjectsOur History with Neo4jThese features met our needs and we’ve started using Neo4j in several of our projects and educating team members in graph database since 2015, thanks to simplicity of graphs and Neo4j.We have servers that are running on Neo4j 2.2, which were working fine up until now. After using Neo4j in real world projects before, this time we needed to use features inside of the graph engine for better performance and design. In SQL Server we used stored procedure and user-defined functions to explore a new level of data query in the database engine. So, what about Neo4j? Can we use inside procedure to explore some extraordinary functionality?User Defined ProceduresFortunately, Neo4j 3.0 introduced new a window to performance. You can write your own custom procedure or function easily in Neo4j just like we do in SQL Server. This feature is called User Defined Procedures”.Procedures are the preferred means for extending Neo4j. Examples of use cases for procedures are:1. Provide access to functionality that is not available in Cypher, such as manual indexes and schema introspection.2. Provide access to third-party systems.3. Perform graph-global operations, such as counting connected components or finding dense nodes.4. Express a procedural operation that is difficult to express declaratively with Cypher.In Neo4j, procedures are written in Java and packaged into .jar files. They can be deployed to the database by dropping a jar file into the $NEO4J_HOME/plugins directory on each standalone or clustered server. The database must be re-started on each server to pick up new procedures.KotlinIf we can write a stored procedure in Java, why can’t we write them in Kotlin as well? As you might know, Kotlin is a new programming language from JetBrains.Kotlin compiles to JVM bytecode, JavaScript or native code. It is very interesting for people who work with Java today. But it could appeal to all programmers who use a garbage collected runtime, including people who currently use Scala, Go, Python, Ruby, and JavaScript. Kotlin has simple and a clean an concise syntax.Here is a class in Java and Kotlin:Java Code:public class Bean {  private final String name  private final int age  public Bean(String n, int a) {    name = n    age = a  }  public String getName() {    return name  }  public int getAge() {    return age  }}Kotlin Code:class Bean(val name: String, val age: Int)As you can see, the syntax is lean and intuitive. Kotlin programs can use all existing Java frameworks and libraries, even advanced ones. It can be learned in a few hours by simply reading the language reference docs. Kotlin imposes no run-time overhead. The language has strong commercial support from several established company.Consequently our mission has been determined: we need to write a stored procedure for Neo4j in Kotlin.Use Case: NLP Analysis of User FeedbackIn our last research on Trinity Engine for the Dorium project, we needed to gather social impact indicators from various research centers. That helped us to rate social economic impacts of certain sustainable projects in Africa. But also informed about volunteer voting and observation system in different areas. In order to analyze comments and other user’s response texts, we needed to run some NLP as basis for other techniques.If you are familiar with Natural Language Processing and working with Neo4j, you can see that Cypher makes language processing easier than others, for example with following code you can create a chain of words:WITH split(there is a different good leader in world”,” ) as wordsUNWIND range(0,size(words)-2) as iMERGE (w1:Word {name:words[i]})MERGE (w2:Word {name:words[i+1]})CREATE (w1)-[:NEXT]->(w2)It creates following graph in the database :It’s been working fine until there is no repeated word in sentence, if we change sentence to the following, there is some mistake in word graph as you can see in following picture:WITH split(there is a different good leader and bad leader in world.”,” ) as wordsUNWIND range(0,size(words)-2) as iMERGE (w1:Word {name:words[i]})MERGE (w2:Word {name:words[i+1]})CREATE (w1)-[:NEXT]->(w2)Word chain with loopThe command just adds one leader word to graph and it’s because of the MERGE command in our query which guarantees uniqueness per label and property-key.MERGE either matches existing nodes and binds them, or it creates new structures and binds it. It’s like a combination of MATCH and CREATE that additionally allow you to specify what happens, if the data was matched or created.The SHA256 User Defined Procedure and FunctionSo, we need to add a unique identifier to our words. Therefor they will be always be created, even with the same name. For this reason, I’ll create a procedure in Neo4j that accept a String and converts it into a SHA256 Hash.Kotlin Code for our procedure & functionclass Sha256Hash {class HashResult(dx: String){  @JvmField var result:String = dx}fun sha256(data: String): String {    val bytes = data.toByteArray()    val md = MessageDigest.getInstance(SHA-256”)    val digest = md.digest(bytes)    return digest.fold(”, { str, it -> str + %02x”.format(it) })}@Procedure(name=”dor.sha256 )@Description(Convert data from string to SHA256 String”)fun sha256Proc(@Name(data”) data: String): Stream<HashResult> {   return Stream.of(HashResult(sha256(data)))}@UserFunction(name=”dor.sha256 )@Description(Convert data from string to SHA256 String”)fun sha256Function(@Name(data”) data: String): String {   return sha256(data)}}So after building the project with Maven and copying the resulting JAR file to the Neo4j plugin folder we can use our new features like this:Example UsageWe can then later use it with:call dor.sha256(1000”)You’ll get following hash as result for 1000 :40510175845988f13f6162ed8526f0b09f73384467fa855e1e79b44a56562a58We can provide our code either as procedure which returns a stream of data, or as a function which returns a single value an can be used in any computation in Cypher.return dor.sha256(1000”)Creating a continuous chain of wordsWITH split(there is a different good leader and bad leader in world.”,” ) as wordsUNWIND range(0,size(words)-2) as iMERGE (w1:Word {name:words[i],hash:dor.sha256(words[i]+i)})MERGE (w2:Word {name:words[i+1],hash:dor.sha256(words[i+1]+(i+1))})CREATE (w1)-[:NEXT]->(w2)And this is the result graph :As you can see in query we use index of word in words array as entry with title for hashing and the index will make a hash unique so merge clause works correctly.You can see entire code in following Github repository.;Jun 23, 2018;[]
https://medium.com/neo4j/getting-started-with-kettle-and-neo4j-32ff15b991f9;Matt CastersFollowDec 4, 2018·3 min readGetting Set Up with Kettle and Neo4jNeo4j is an awesome piece of technology. The ability to use cutting edge graph algorithms like shorted path, community detection, centrality while running transactional operations is only possible in any meaningful and performing way on a fully native graph database like Neo4j.So how do you get started? Well, you need to load data into Neo4j first and this is where a data integration tool like Kettle might come in.What is Kettle?Kettle is an open source data integration (ETL) platform with the ability to visually design your work. It has been around for over 17 years and in that time it became quite mature, stable, high performing and feature rich.From Kafka to Neo4jFor example, here is a visual representation of a Kettle transformation” which does the following:Receive messages from KafkaDo look-ups in MongoDBWrite nodes and relationships to Neo4jThe technical aspect of setting up a transformation like this takes a few minutes at most so you can focus on things like data quality, the graph model, the requirements, data accuracy, performance and so on.As you can imagine doing this without the need for coding, scripting or anything like that can be very time-saving for the set-up and the maintenance of your solution later on.So how do you get started with Kettle itself and what about the Neo4j plugins?Kettle download and installationYou can download Kettle (also known as Pentaho Data Integration Community Edition — waaay too long to say everytime so we just say Kettle”) from SourceForge, the Pentaho project, latest version 8.1 (right now, check for updates) and look for PDI-CE : pdi-ce-8.1.0.0–365.zip → Warning: it’s about 1GB bundled with all plugins!Make sure you have the right Java 8/9 runtime environment properly installed for your computer system. Java from Oracle or OpenJDK is recommended. Kettle runs fine on Windows, OSX or Linux.Now unzip the downloaded archive somewhere. It will give you an extra data-integration/ folder. This is all you need to do for as far as Kettle is concerned.Install the Neo4j plugins for KettleYou can get the latest version of the plugins at neo4j.kettle.be It points to the community project where the Neo4j Kettle plugins were first developed and where our improvements are done. From the releases download the latest version archive: Neo4JOutput-<version>.zipUnzip this where you placed your Kettle distribution in the data-integration/plugins/ folder.Now the fun startsNow you can start up the Kettle GUI called Spoon. The naming is a silly pun on Kettle and anything kitchen related.on Windows start Spoon.baton OSX start spoon.sh or the app: Data Integration.app”on Linux start spoon.shYou will notice a welcome page with useful links:The Spoon Welcome pageYou can find the Neo4j plugins when you create a new transformation:The Neo4j steps category in SpoonNext stepsHere are a few things you can do to read up on Kettle and Neo4j and some pointers on where to get help:Visit the Neo4j plugins wikiLook at examples for the Neo4j Kettle pluginsKettle documentationRead a book: Pentaho Data Integration Quick Start GuideJoin the Neo4j communityAsk your Neo4j contact for professional services for help with your Neo4j projectsStay tuned for the next story in which I’ll be going over a few concrete examples of data loading into Neo4j.Enjoy Kettle!Matt;Dec 4, 2018;[]
https://medium.com/neo4j/heres-how-you-can-access-advanced-hands-on-neo4j-training-for-free-9a71c0774434;Karin WolokFollowApr 28, 2020·2 min readHere’s How You Can Access Advanced, Hands-On, Neo4j Training — for Free!You spoke. We listened.Many of our Neo4j Certified Professionals expressed interested in taking their skills to the next level, so… we’re making it happen!What?We’re hosting 8 live, hands-on, advanced, virtual training sessions to all our Neo4j Certified Professionals, at no cost, over the course of the year.After these live sessions, the recordings will be made available as on-demand videos for all Neo4j Certified Professionals.Trainings will be on a variety of topics, like Cypher Query Tuning, Graph Data Science, APOC, GRANDstack, and more!When?Links and more info will be available soon, but for now, mark your calendars! (….and Get your Neo4j Certification if you’re not already Certified!!!)June 23: Cypher Query Tuning10:00–14:00 EDT | 14:00–18:00 GMTJune 24: Neo4j Graph Algorithms for Data Science10:00–14:00 EDT | 14:00–18:00 GMTOctober 6–8: TBD10:00–14:00 EDT | 14:00–18:00 GMTOctober 13–15: TBD10:00–14:00 EDT | 14:00–18:00 GMTHow?These sessions can only be accessed by Neo4j Certified Professionals.Already Certified in Neo4j?Links will be published and updated on the Private Certified Professionals group on the Neo4j Community Site. Your access will be automatically granted based on the association of your email address and your Neo4j Certification. We will also be emailing you with the links prior to the scheduled sessions.If you love this opportunity and want even more, you could also check out what it takes to become a Neo4j Ninja, where you’ll be rewarded for helping others in the community by gaining exclusive access to monthly live webinars, demos, and ask-me-anything and feedback sessions with Neo4j staff.Not yet Certified?Take the Neo4j Certification exam today! It’s about 1-hour long and totally free!Questions? Comments? Concerns? Ideas? Email us at Community@Neo4j.com;Apr 28, 2020;[]
https://medium.com/neo4j/introducing-the-building-neo4j-application-with-typescript-on-graphacademy-2f932543395d;Adam CowleyFollowNov 28, 2022·3 min readIntroducing the Building Neo4j Application with TypeScript on GraphAcademyIf you are a seasoned Neo4j user, you may already know about GraphAcademy, Neo4j’s free, self-paced, hands-on online training platform. We offer a wide range of courses completely free of charge, teaching everything from Neo4j Fundamentals to how to develop software that connects to Neo4j.In the past few weeks, we have released the brand new Building Neo4j Applications with TypeScript course. The course aims to teach you how to interact with Neo4j using TypeScript — teaching you everything you need to know to use the Neo4j JavaScript driver effectively in your TypeScript project.How Is It Different From the Node.js Course?This course takes a slightly different format from the other Developer courses on GraphAcademy in that the aim is not to build a working API. Instead, the course focuses on using Neo4j & TypeScript together.The course is much shorter and should take around two hours to complete. If you fancy a challenge, you could try and complete it within your lunch break!Online-only ChallengesThe course features three challenges that must be completed within an online IDE provided by GitPod.You don’t need to install software or download a repository to complete the course. Instead, simply log in with your GitHub, GitLab, or Bitbucket account, and the repository will be set up for you.All the necessary configuration is passed along as environment variables so you can get going immediately.You are still asked to execute a Cypher statement against a Neo4j Sandbox instance to pass the challenges. The Sandbox instance is created for you when you enroll in the course.Table of ContentsInstallation — Instructions for installing the dependency and creating a new Driver instance to share across your project.Creating a Driver Instance — A code challenge where you must create a driver instance and execute a Cypher query to find a value from the database.Read and Write Transactions — How to execute a Cypher statement against a Neo4j database.Reading Data from Neo4j — A quiz where you must complete a code block to read data from Neo4j.Writing Data to Neo4j — A code challenge where you must create a new Node and Relationship in a Neo4j Sandbox instance.The Neo4j Type System — Information on the Neo4j type system and any considerations you need to make when working with these values in TypeScriptType Checking — Information on how to use TypeScript generics to trigger type checking by the TypeScript interpreter.Take the Course NowYou can enroll now by heading to the Neo4j & TypeScript course on GraphAcademy and clicking Enroll Now.If you fancy a more considerable challenge, you can take the Building Neo4j Applications with Node.js course, which has also been recently revamped to use Gitpod IDE to complete the code challenges.You can contact me on Twitter or use the feedback widgets on each page if you have any comments or feedback.;Nov 28, 2022;[]
https://medium.com/neo4j/mesh-into-neo4j-7c52e3ada6b5;Tom NijhofFollowFeb 2, 2022·4 min readMeSH into Neo4jTo make a knowledge graph it is useful to have a vocabulary in place, an ontological. The Medical Subject Headings is one such an ontological, which includes many of the medical terms that are currently being used.It can be downloaded as an RDF file (N-triples), making it easy to import to Neo4j with neosemantics (n10s).Installing n10s in neo4j desktopThe next three commands will import the 2021 MeSH graph directly into Neo4j. It will take a moment before all 2 million nodes and 4 million relations are loaded in.CREATE CONSTRAINT n10s_unique_uri ON (r:Resource) ASSERT r.uri IS UNIQUECALL n10s.graphconfig.init()CALL n10s.rdf.import.fetch( https://nlmpubs.nlm.nih.gov/projects/mesh/rdf/2021/mesh2021.nt , N-Triples )Exploring the dataBefore I start I will set the caption to rdfs__label for resources, so the nodes have a name. For ns0_Term I will use ns0__prefLabel.Naming nodes within Neo4j desktopLets start with the most sexy thing to do, reading the documentation of RDF data structure of medical terms used to sort medical papers.Did I say ‘sexy’? I meant nerdiest.I will not go over the full structure, instead will select just two elements I think are interesting to start with feel free to disagree.Code in grey blocks such as this one, is the cypher query you can use in neo4j. It is not needed but might be useful if you want to know how I got the results, or they can serve as an example that my cypher is not optimized, up to standard ect.Terms, descriptors, and conceptsDescriptors, concepts, and terms are very closely related. Descriptors are the broadest within descriptors you have concepts (at leased one that is the preferred one). Concepts have terms, these terms hold synonyms to the concepts. Each concept has one preferred term, while the descripter also has one preferred term out of all (see picture below).MATCH (q:ns0__Term)<-[]-(n:ns0__TopicalDescriptor)-[]->(p:ns0__Concept)-[]->(z:ns0__Term) WHERE (n.rdfs__label =  Calcimycin )  return n, p, q, zRelation between descriptor (pink), concepts (green), and terms (blue)Terms are very useful for labeling text. Concepts can define a part which is smaller than the whole descriptor. The descriptor holds the connection to the rest of the graph (tree, other descriptors, SCR, Qualifiers, ect.). I will mainly focus on the descriptors for graph algorithms.Tree StructureAll TopicalDescriptor have a link to a tree-number (ns0__treeNumber) and to another TopicalDescriptor (ns0__broaderDescriptor).These two hold very similar information but have one use case where they differ: multiple tree locations.A descriptor can be in more than one tree at the same time (like the descriptor eye”). Eye has tree number A01.456.505.420 as subcategory of face, and A09.371 as subcategory of Sense Organ. This can give problems, because these two tree numbers do NOT have the same subcategories!Eyebrows are part of the eye as part of the face, but are NOT part of the eye as part of a sense organ.Tree overview of Eye in online MeSH BrowserIf we use ns0__broaderDescriptor to go back from Eyebrows to the broadest description, we come upon a mistake. The broader description of Eyebrows is Eye, which has two broader descriptions (namely sense organs and face). As Eyebrows is not a sense organ, this shouldn’t be correct.MATCH (n:ns0__TopicalDescriptor)-[:ns0__broaderDescriptor*]->(p:ns0__TopicalDescriptor) WHERE n.rdfs__label =  Eyebrows  return n, pSense Organs is found as broader description of EyebrowsThe other way is to go via the tree numbers. This will mean Eyebrows is only connected to one of the two tree numbers of Eye and does NOT have Sense organs as broader description.MATCH (n:ns0__TopicalDescriptor)-[:ns0__treeNumber]->(t:ns0__TreeNumber)-[:ns0__parentTreeNumber*]->(p:ns0__TreeNumber)<-[:ns0__treeNumber]-(d:ns0__TopicalDescriptor) WHERE n.rdfs__label =  Eyebrows  return n, t, p, dGoing via the tree number gives only Body Regions” and Integumentary System” as broadest descriptorFor this reason I will use ns0__treeNumber to find hierarchical relationships rather than ns0__broaderDescriptor.;Feb 2, 2022;[]
https://medium.com/neo4j/neosemantics-4-0-is-out-26a380dd7b51;Ljubica LazarevicFollowMay 18, 2020·4 min readNeosemantics 4.0 is out!Don’t miss the live stream session on neosemantics 4.0 and the new Graph App on 19 May at 7am PDT/ 10am EDT/ 2pm GMT/3pm BST/ 4pm CEST!Nicolas Picard on unsplashIntroductionNeosemantics (n10s) is a Neo4j plugin that enables the use of the Resource Description Framework (RDF) in Neo4j and is part of the Neo4j Labs program.RDF is a W3C standard model for data interchange. Some key features of neosemantics are:Store RDF data in Neo4j in a lossless manner (imported RDF can subsequently be exported without losing a single triple in the process)On-demand export property graph data from Neo4j as RDFFor those of you who want to get an idea of the kinds of things you can do with neosemantics, check out Jesus Barrasa’s collection of blog posts on the subject. The OpenPermID post, based on the Refinitiv data, is an excellent example.Neosemantics 4.0 is now available!It has been nearly 5 months since the last 3.5.x release. Based on in-depth interactions with Neo4j users at our community forum, as well as through GitHub, as well as the monumental release of Neo4j 4.0, it was time for a new and improved version of neosemantics.Over the past few weeks, we have quietly released the first generally available (GA) version of neosemantics, compatible with Neo4j 4.0. It includes some great new features that we are really excited about:Support for Neo4j multi-database through the RDF endpointPersisted Graph ConfigurationsModel Validations based on Shapes Constraint Language (SHACL)Simple Knowledge Organisation System (SKOS) importThey’re all described in detail, in the manual.We also thought it was a good time to introduce some significant API changes in the library to improve usability for developers. The main two are — a good restructuring of the naming scheme and a change to the mode of operation of the plugin.Now the Graph Configuration is persisted, instead of passing it on every individual request. This has arisen as we are seeing neosemantics being used in multi-step workflows, involving numerous imports/exports/updates. Neosemantics is not just a data migration tool anymore, which was the original use-case when it was created back in 2016.For those of you using the 3.5 version of neosemantics, we’ve added a guide in the manual to aid transition to 4.0.Under the hood, as usual, all the parsing and serialising of RDF is done using the amazing RDF4J framework, which we’ve also upgraded in this last release to v.3.2.0.There’s also an app for thatIn parallel with the release of neosemantics 4.0, we’ve also released a neosemantics GraphApp (built by Adam Cowley) to help you get familiar with some of the main features of the neosemantics toolkit through a nice and friendly user interface.Neosemantics Graph AppYou can install and run the GraphApp within Neo4j Desktop.The Neosemantics Graph App in the Graph GalleryWhat’s next?As you may have read in some of the posts we’ve published on using neosemantics (Mark Needham’s post on COVID-19 taxonomy graph, Jesús Barrasa on how to enrich your Neo4j Knowledge Graph by querying Wikidata), one of the things we find more useful is querying public SPARQL endpoints like Wikidata, UniProt and many others.In the next release of the GraphApp we’re planning to include a user-friendly interface to simplify the process of connecting to such public SPARQL endpoints. This will make the import of RDF data into Neo4j a two-click process. We’ll include a catalogue with some of the most popular ones but here’s a call to action — If you manage or are aware of a data set/data platform accessible via SPARQL, and you want it included in the catalogue of the app, please give us a shout through the community portal and we’ll have it included.Get involved!Please keep sharing your experiences through the community portal and submit your issues on GitHub.Thank you from the neosemantics team!Don’t miss the live stream session on neosemantics 4.0 and the new Graph App on 19 May at 7am PDT/ 10am EDT/ 2pm GMT/3pm BST/ 4pm CEST!Useful Links:Neosemantics homepageNeosemantics ManualGitHub issuesNeo4j Community Forum on RDF/Linked DataNeosemantics Graph App article (Graph App Install page);May 18, 2020;[]
https://medium.com/neo4j/recent-updates-to-neo4j-bloom-7bee848a91e8;Jeff GagnonFollowJul 14, 2021·7 min readRecent Updates to Neo4j ✿ BloomGraph databases are the foundation for intelligent transformations increasingly showing their value in numerous domains, including:supply chain management,cyber security,investigative analysis,public health,and predictive analytics, just to name a few.Visualizations are a powerful way to bring this value into focus for a wider audience. Bloom is Neo4j’s graph visualization tool, and has been maturing over the past couple of years.In this post, I’d like to go over a number of the key changes from the past two versions of Bloom, and discuss how they might be helpful to analysts and data scientists, as well as developers involved in these specialties.To do this, I’ll focus on three areas of improvement: performance, usability, and analytic features.Key TakeawaysNeo4j Bloom helps analysts and non-technical users in numerous domains better understand & leverage the power of Neo4jNeo4j Bloom continues to evolve with recent improvements to performance, usability and analytic capabilitiesWe want your feedback!Photo by Omar Flores on UnsplashPerformanceUsing the Reactive DriverThe first set of changes I’ll highlight are some performance enhancements. In version 1.6, Bloom started taking advantage of Reactive drivers. Bloom uses Neo4j’s JavaScript driver for retrieving data from the database.Fetching Large Amount of Data Using the Neo4j Reactive Driver: The Bloom CaseEfficiently fetch and process large amounts of data using a reactive approach for our large scale data visualization…medium.comReactive drivers were introduced in Neo4j 4.0, and Bloom, a React based JavaScript application, now uses the latest JavaScript Reactive driver for Neo4j.This lets us improve the responsiveness of queries, and also reduces database load. You can read more about how Bloom manages queries in stages in this blog. The switch to this driver should result in overall faster performance when interacting with Bloom, particularly on larger data sets.Search PerformanceAlso related to search, Bloom 1.6.0 saw improvements in performance related to suggestions offered when using the near-natural language search box.As search terms are entered into the search box, Bloom attempts to identify database schema matches and suggest those in the context box below. Changes were recently implemented to improve the speed with which these suggestions appear, by making some changes to how we cache index data and the logic that identifies potential suggestions.For example, previously Bloom would search for terms matching the characters entered even if none of the prior characters resulted in a match — now Bloom avoids searching for values when it’s established that the first few characters don’t match anything in the database.Finally, another performance enhancement centers on Bloom’s physics-based visualization layout. Particularly for large graphs shown in Bloom, changing the focus to particular nodes (for example, when using the Fit to selection” action or adding nodes by expanding) should now be a smoother and quicker visual experience. Visualization and graph layout continue to be important areas of work as we strive to build the best experience possible.UsabilityPhoto by Alvaro Reyes on UnsplashIn addition to performance enhancements, a number of usability enhancements have been added to Bloom in recent versions.Perhaps the most obvious change for users will be the automatic generation of a perspective when you load Bloom for the first time. Perspectives are a unique feature of Bloom that allow users to define just the elements of the underlying database they want to include in their visualization.Perspectives - Neo4j BloomThis chapter describes Perspectives in Neo4j Bloom and how to work with them. In Neo4j Bloom, a Perspective defines a…neo4j.comThis can be very useful, especially when dealing with a large and diverse underlying data model — some users may be interested in certain categories and properties (for example, nodes of category Customer and Address along with a lives_at relationship) while others may be interested in different types of nodes and relationships (such as Product and Customer nodes, and a transaction relationship). Perspectives also allow users to retain styling and filtering rules.Prior to version 1.7.0, Bloom would show the Perspective Gallery before loading a scene.Bloom perspective galleryIf a perspective has never been created on the database being opened, Bloom will now automatically generate a perspective by sampling the database and displaying some subset of data to start exploring. Specifically, Bloom will load one category into the auto-generated perspective for every label found in the database.For each label, Bloom will sample one node and add the property keys and types on those category to the perspective.As Bloom loads more nodes through user interactions, it will re-scan the properties and add any new ones found to the perspective — this is true even for user-generated perspectives.Also check out the Bloom manual for more details:Visual Tour - Neo4j BloomThis chapter presents a visual overview of the UI of Neo4j Bloom. The sidebar contains a set of drawers where you can…neo4j.comIn addition to auto-perspective, a few more straightforward usability improvements have recently appeared. Namely:You can now set the session timeout value in the settings drawer.The powerful Search Phrases feature has also been updated with auto-save, so Cypher queries and other details of search phrases will automatically be saved as you type.Finally, a keyboard shortcut has been added to quickly clear the scene in Bloom if you want to start with a blank canvas — Cmd+Delete on Mac or Ctrl+Delete on Windows/Linux.Styling & RenderingBloom has recently also benefited from some improvements to better enable the graph analytics work it supports. One aspect of the underlying graph model that can sometimes be confusing is the fact that nodes might have more than one label.Styling OrderIn Bloom, we use categories to define the label we use to identify nodes of a particular type. For example, you might have a node with both a User and a Troll label. In Bloom, you could interact with this node using either (expressed as a category).If you decide to style nodes, for example, based on the User label, then all nodes having the label User will take on the style you specify (perhaps you make them yellow, like in the first image below).Similarly, you could apply a different style to all nodes with the Troll label (let’s make them blue).Prior to version 1.7, this would mean the color applied to a node with both labels would depend on the order in which the styling rules were applied.Now, users can drag categories in the perspective drawer to set the order in which styles will be applied to nodes with multiple labels.Categories above will take precedence over those below. As you can see below, a node can have both a User and a Troll label. In the first image we have User above Troll, so all nodes with the User label are yellow.User” is above Troll” so all nodes with label User” are yellow, even if they also have label Troll”In the second image, we place Troll above User. Now, all of the nodes with label Troll appear using the default styling for Troll, blue in this case. User nodes that don’t also have a Troll label remain yellow.Troll” is above User” so all nodes with label Troll” are blue, even if they also have label User”FilteringIn another recent change, users can now also filter the scene using the funnel icon just below the search box.This lets you reduce the graph on scene according to your needs without having to dismiss nodes and relationships — elements that are filtered out simply go grey so long as the filter is applied, making it easier to find what you’re looking for.With the Dismiss button at the bottom of the filter pane, you can also choose to dismiss nodes that have been greyed out, or filtered from the scene.Graph with filtered out elementsBetween PredicateA between condition has also been added to the styling rules and the filter pane, enabling styling and filtering based on property values that fall between to user definable parameters.Style/Filter between”There have been a number of other changes and enhancements along the way, and lots of other upgrades and refinements are planned.These are just some of the perhaps more interesting and useful recent changes to Bloom, and we look forward to bringing you more (and hearing what’s important to you!). Please provide feedback to us using the built-in Feedback mechanism in Bloom or the link below.Feature Requests | Neo4j BloomInstead of double clicking a node to get extra data, it would be helpful to have either: Tooltips available as you role…neo4j-bloom.canny.io;Jul 14, 2021;[]
https://medium.com/neo4j/nodes-2021-is-coming-and-we-need-you-c93be99ebce4;Ljubica LazarevicFollowFeb 23, 2021·4 min readNODES 2021 Is Coming, and We Need You!The Neo4j Online Developer Expo and Summit is back for the third year running on June 17, and we want to hear your graph story.*UPDATED 26 March 2021**Need some help?Check out this GraphStuff.FM podcast on putting together a technical presentationAsk any questions on our Discord server in #nodesOr alternatively ask on the forumWhat is NODES?Neo4j Online Developer Expo and Summit (NODES) is our key developer conference. First launched in 2019, it is a multi-track developer extravaganza where Neo4j users from around the world come to share what they’ve been building, applications of Neo4j to solve graphy problems, community-developed projects and much more.Last year we attracted over 13,000 registrations to our conference, supported by over 30 community groups from across the globe. This year, NODES is going to be even bigger. Be a part of it!Why Submit?There are many reasons for submitting a talk to nodes, including:The opportunity to tell your story: Did you learn something new? Or there were challenges that you overcame? Let us know the lessons you learned in your own words.Inspire the community: We have many amazing community champions who provide insight, develop best practices and share their experiences.Boost your reputation as a graphista: Show the world your graph-wielding credentials on a global stage.The FormatThis will be a live, multi-track event, consisting of full and lighting-length talks. Sessions will be technically-focussed and we welcome talks from getting started, through to full-stack implementations. If you are not able to do your talk live, we have provisions for streaming recorded sessions.We will have two types of sessions:30-minute talks, with additional time for Q&A10-minute lightning talksfotografierende on UnsplashWhat Kind of Thing Should I Submit?Maybe you’ve built a new connector. Perhaps you’ve developed a graph-based stack. Or, you’ve gone down a well-traveled track. But you did it your way and are sharing your experiences. Whether it’s old or new, it’s unique to you, and we want to hear about it! It could be about that time you did a major data migration. Or that insightful discovery you found applying data science approaches. Maybe you’ve built a new framework or connector and want to tell the world about it.With lots of exciting areas to explore, we’re looking forward to see talks on all topics graphy. Don’t be afraid to be technical!Tell me about my potential audienceThe NODES audience will be largely technical. They are varied, including software developers, engineers, data scientists, architects. Some bring years of Neo4j experience, others will be taking their first steps on their graph journey. Think about what subset of this audience you are presenting to, and what thoughts you’d like them to be leaving with.Looking for inspiration?You X Ventures on UnsplashSo you’re keen to submit a paper, but you’re not quite sure on what? Perhaps the following will help get those creative juices flowing.Check out the talks from previous NODES conferencesWe have a wonderful collection of talks from NODES 2019 and NODES 2020. Check out the schedules and playlists below:NODES 2019 Schedule and PlaylistNODES 2020 Schedule and PlaylistNODES 2020 Extended PlaylistGet inspiration from questions and discussions from the community forum and Stack overflowDoes a question commonly come up? Has there been queries on best practices? Maybe it’s time to explore that killer platform integration. Why not explore the postings from users and community members:Community forumStack overflowAsk the communityAnother option might be to ask the community for suggestions. There may even be opportunities for a collaborative talk!I Need a Little Help…Not to worry, help is at hand!Check out this GraphStuff.FM podcast on putting together a technical presentationAsk any questions on our Discord server in #nodesOr alternatively ask on the forumHow Long Have I Got?The closing date is April 5 at 23:30 UTC. Get your submissions in quick!We look forward to receiving your submission!;Feb 23, 2021;[]
https://medium.com/neo4j/exploring-the-u-s-national-bridge-inventory-with-neo4j-part-3-connecting-bridges-to-states-f989a1919e00;Michael McKenzieFollowOct 29, 2020·9 min readArthur Ravenel Jr. Bridge, Charleston, United States by David MartinExploring the U.S. National Bridge Inventory with Neo4j — Part 3: Connecting Bridges to StatePart 1 gave the background to the United States National Bridge Inventory (NBI), a publicly available database that represents an annual snapshot of all bridges in the 50 states, Washington DC, and Puerto Rico.Part 2 concluded with the raw data from the delimited files being stored directly in the Neo4j graph database. Now the deeper dive can begin!The Recording and Coding Guide for the Structure Inventory and Appraisal of the Nation’s Bridges is what we’ll use to make sense of the data we have imported. There are nearly 140 properties stored on each (:Row) that we can use.Using this document, we’ll extract context from the data stored on each (:Row) to build out our graph. Due to the amount of information being stored, many of the fields are encoded so as to reduce the size of the files. Hence this document unlocks the additional information we need.We know that each (:File) represents data for a particular state, and that each (:File) contains multiple(:Row) nodes of data where each (:Row) represents information about a particular bridge. Therefore, our next steps will be to create and connect (:State) and (:Bridge) nodes. The (:State) and (:Bridge) nodes will need to be associated with their (:File) and (:Row) nodes, respectively.The remainder of this post will focus on there four fields:STATE_CODE_001COUNTY_CODE_003PLACE_CODE_004STRUCTURE_CODE_008(:File)-[:FILE_FOR]->(:State)Each (:File) node has the property name. The first two letters of each file name represents the abbreviation of that state. For example, the (:File {name: AK10.txt }) node represents data for the state with the abbreviation AK”, which is Alaska”.To simplify processing and allow us to monitor which (:File) nodes have already been connected to a (:State) we’ll add a temporary label first:MATCH (file:File)WHERE NOT (file)-[:FILE_FOR]->()SET file:ConnectFileToStateWith this temporary label in place, we create (:State) nodes, where needed using MERGE, and connect it to its (:File).CALL apoc.periodic.iterate(MATCH (file:ConnectFileToState)RETURN file,REMOVE file:ConnectFileToStateWITH fileMERGE (state:State {abbreviation: left(file.name,2)})WITH file, stateMERGE (file)-[:FILE_FOR]->(state),{batchSize:1000, parallel:false})Since we are working with a relatively small number of objects (1,456 (:File) and 52 (:State) nodes) we can create the (:State) nodes and build the relationships in the same query.With (:State) created, the only property on the node is abbreviation. Next we reference the encoding document and the field STATE_CODE_001 to add context and properties to these nodes.The STATE_CODE_001 field is three digits in length, where the first two digits represent the state and the third represents a region. We’ll ignore the region for the time being. Using this encoding scheme we add code and name properties to each state node using a CASE statement:MATCH (state:State)SET state.code =    // These are ordered by the state code      CASE state.abbreviation        WHEN  AL  THEN  01         WHEN  AK  THEN  02         //WHEN  03  THEN    //this is not referenced. kept for numeric continuity        WHEN  AZ  THEN  04         WHEN  AR  THEN  05         WHEN  CA  THEN  06         //WHEN  07  THEN    //this is not referenced. kept for numeric continuity        WHEN  CO  THEN  08         WHEN  CT  THEN  09         WHEN  DE  THEN  10         WHEN  DC  THEN  11         WHEN  FL  THEN  12         WHEN  GA  THEN  13         //WHEN  14  THEN    //this is not referenced. kept for numeric continuity        WHEN  HI  THEN  15         WHEN  ID  THEN  16         WHEN  IL  THEN  17         WHEN  IN  THEN  18         WHEN  IA  THEN  19         WHEN  KS  THEN  20         WHEN  KY  THEN  21         WHEN  LA  THEN  22         WHEN  ME  THEN  23         WHEN  MD  THEN  24         WHEN  MA  THEN  25         WHEN  MI  THEN  26         WHEN  MN  THEN  27         WHEN  MS  THEN  28         WHEN  MO  THEN  29         WHEN  MT  THEN  30         WHEN  NE  THEN  31         WHEN  NV  THEN  32         WHEN  NH  THEN  33         WHEN  NJ  THEN  34         WHEN  NM  THEN  35         WHEN  NY  THEN  36         WHEN  NC  THEN  37         WHEN  ND  THEN  38         WHEN  OH  THEN  39         WHEN  OK  THEN  40         WHEN  OR  THEN  41         WHEN  PA  THEN  42         //WHEN  43  THEN    //this is not referenced. kept for numeric continuity        WHEN  RI  THEN  44         WHEN  SC  THEN  45         WHEN  SD  THEN  46         WHEN  TN  THEN  47         WHEN  TX  THEN  48         WHEN  UT  THEN  49         WHEN  VT  THEN  50         WHEN  VA  THEN  51         WHEN  WA  THEN  53         WHEN  WV  THEN  54         WHEN  WI  THEN  55         WHEN  WY  THEN  56         WHEN  PR  THEN  72       END,      state.name =       CASE state.abbreviation      //need to incorporate longer state names        WHEN  AL  THEN  Alabama         WHEN  AK  THEN  Alaska         //WHEN  03  THEN    //this is not referenced. kept for numeric continuity        WHEN  AZ  THEN  Arizona         WHEN  AR  THEN  Arkansas         WHEN  CA  THEN  California         //WHEN  07  THEN    //this is not referenced. kept for numeric continuity        WHEN  CO  THEN  Colorado         WHEN  CT  THEN  Connecticut         WHEN  DE  THEN  Delaware         WHEN  DC  THEN  District of Columbia         WHEN  FL  THEN  Florida         WHEN  GA  THEN  Georgia         //WHEN  14  THEN    //this is not referenced. kept for numeric continuity        WHEN  HI  THEN  Hawaii         WHEN  ID  THEN  Idaho         WHEN  IL  THEN  Illinois         WHEN  IN  THEN  Indiana         WHEN  IA  THEN  Iowa         WHEN  KS  THEN  Kansas         WHEN  KY  THEN  Kentucky         WHEN  LA  THEN  Louisiana         WHEN  ME  THEN  Maine         WHEN  MD  THEN  Maryland         WHEN  MA  THEN  Massachusetts         WHEN  MI  THEN  Michigan         WHEN  MN  THEN  Minnesota         WHEN  MS  THEN  Mississippi         WHEN  MO  THEN  Missouri         WHEN  MT  THEN  Montana         WHEN  NE  THEN  Nebraska         WHEN  NV  THEN  Nevada         WHEN  NH  THEN  New Hampshire         WHEN  NJ  THEN  New Jersey         WHEN  NM  THEN  New Mexico         WHEN  NY  THEN  New York         WHEN  NC  THEN  North Carolina         WHEN  ND  THEN  North Dakota         WHEN  OH  THEN  Ohio         WHEN  OK  THEN  Oklahoma         WHEN  OR  THEN  Oregon         WHEN  PA  THEN  Pennsylvania         //WHEN  43  THEN    //this is not referenced. kept for numeric continuity        WHEN  RI  THEN  Rhode Island         WHEN  SC  THEN  South Carolina         WHEN  SD  THEN  South Dakota         WHEN  TN  THEN  Tennessee         WHEN  TX  THEN  Texas         WHEN  UT  THEN  Utah         WHEN  VT  THEN  Vermont         WHEN  VA  THEN  Virginia         WHEN  WA  THEN  Washington         WHEN  WV  THEN  West Virginia         WHEN  WI  THEN  Wisconsin         WHEN  WY  THEN  Wyoming         WHEN  PR  THEN  Puerto Rico       ENDEach (:State) now has three properties code, name, and abbreviation.(:Bridge)Using the fields STATE_CODE_001, COUNTY_CODE_003, PLACE_CODE_004, and STRUCTURE_NUMBER_008 we’ll create (:Bridge) nodes. Before we do that let’s create an index on the four properties we’ll add to (:Bridge).CREATE INDEX bridgePlaceCountyState FOR (b:Bridge) ON (b.stateCode, b.countyCode, b.placeCode, b.code)Note that we are not using the same field names for the property names on (:Bridge). This decision was made as a way to designate what is raw data” versus processed data”. We’ll see why this is important later.For reference, below are the field name descriptions for COUNTY_CODE_003, PLACE_CODE_004, and STRUCTURE_NUMBER_008 from the encoding document for reference:To monitor which (:Row) nodes have not been connected to a (:Bridge) we add another temporary label:CALL apoc.periodic.iterate(MATCH (row:Row)WHERE NOT (row)-[:DATA_FOR]->()RETURN row,SET row:ConnectRowToBridge,{batchSize:10000, parallel:false})Now we iterate over the temporary node label :ConnectRowToBridge to create a bridge if it doesn’t exist. This process happens more quickly since we created an index for the MERGE in this query.CALL apoc.periodic.iterate(MATCH (row:ConnectRowToBridge)RETURN row,WITH row,     row.STATE_CODE_001 AS stateCode,     row.COUNTY_CODE_003 AS countyCode,     row.PLACE_CODE_004 AS placeCode,     coalesce(apoc.text.replace(trim(row.STRUCTURE_NUMBER_008),  ^0* ,   ),row.STRUCTURE_NUMBER_008) AS bridgeCodeMERGE (bridge:Bridge {stateCode: stateCode,                      countyCode: countyCode,                      placeCode: placeCode,                      code: bridgeCode})ON CREATE SET bridge:ConnectToPlace,{batchSize:10000, parallel:false})Note that when a new (:Bridge) is created another temporary processing label, :ConnectToPlace, also gets added. This series of processing labels that get added and removed help identify where we are when new data is imported.(:Row)-[:DATA_FOR]->(:Bridge)Next we iterate back over the (:Row) nodes using the temporary processing label, :ConnectRowToBridge, to find the rows and connect them to their respective bridge:CALL apoc.periodic.iterate(MATCH (row:ConnectRowToBridge)RETURN row,WITH row,     row.STATE_CODE_001 AS stateCode,     row.COUNTY_CODE_003 AS countyCode,     row.PLACE_CODE_004 AS placeCode,     coalesce(apoc.text.replace(trim(row.STRUCTURE_NUMBER_008),  ^0* ,   ),row.STRUCTURE_NUMBER_008) AS bridgeCodeMATCH (bridge:Bridge {stateCode: stateCode,                      countyCode: countyCode,                      placeCode: placeCode,                      code: bridgeCode})CREATE (row)-[:DATA_FOR]->(bridge)WITH rowREMOVE row:ConnectRowToBridge,{batchSize:1000,parallel:false})NOTE: Since we are building this from the ground up and need to include all files and rows from 1992 to 2019, this process can take a bit of time depending on your RAM configuration. As new data is released for 2020 and beyond this will go much more quickly because we are utilizing the temporary processing labels that only get added when new items are created.At this stage we have the following schema:Now we will create paths between (:State) and (:Bridge) nodes by running a series of sequential queries creating nodes and then relationships.(:Place)Recall that we added a temporary label, :ConnectToPlace, when we created a new (:Bridge). We iterate over each bridge using that temporary label to create (:Place) nodes where they do not exist. To keep places with the same code from different states separate we use three of the bridge properties: stateCode, countyCode, and placeCode.CALL apoc.periodic.iterate(MATCH (bridge:ConnectToPlace)RETURN bridge,MERGE (place:Place {code: bridge.placeCode,                     countyCode: bridge.countyCode,                     stateCode: bridge.stateCode})ON CREATE SET place:ConnectToCounty,{batchSize:10000, parallel:false})(:Place)-[:HAS_BRIDGE]->(:Bridge)Now that we have created (:Place) where it didn’t exist, we iterate back over each (:Bridge) using the same temporary label, :ConnectToPlace, to connect (:Bridge) and (:Place).CALL apoc.periodic.iterate(MATCH (bridge:ConnectToPlace)RETURN bridge,WITH bridgeMATCH (place:Place {code: bridge.placeCode,                     countyCode: bridge.countyCode,                     stateCode: bridge.stateCode})CREATE (place)-[:HAS_BRIDGE]->(bridge)WITH bridgeREMOVE bridge:ConnectToPlace,{batchSize:10000, parallel:false})(:County)Just like what we did for (:Place), we take a similar approach using the temporary label, :ConnectToCounty, we added above.CALL apoc.periodic.iterate(MATCH (place:ConnectToCounty)RETURN place,MERGE (county:County {code: place.countyCode,                       stateCode: place.stateCode})ON CREATE SET county:ConnectToState,{batchSize:10000, parallel:false})(:County)-[:HAS_PLACE]->(:Place)We created (:County) nodes where they didn’t exist before. Time to connect (:County) to (:Place):CALL apoc.periodic.iterate(MATCH (place:ConnectToCounty)RETURN place,WITH placeMATCH (county:County {code: place.countyCode,                       stateCode: place.stateCode})CREATE (county)-[:HAS_PLACE]->(place)WITH placeREMOVE place:ConnectToCounty,{batchSize:10000, parallel:false})(:State)-[:HAS_COUNTY]->(:County)Lastly we iterate over each (:County) using the temporary label, :ConnectToState, to connect (:County) to (:State).CALL apoc.periodic.iterate(MATCH (county:ConnectToState)RETURN county,WITH countyMATCH (state:State {code: county.stateCode})CREATE (state)-[:HAS_COUNTY]->(county)WITH countyREMOVE county:ConnectToState,{batchSize:10000, parallel:false})With these new nodes added and connected, our updated graph model looks like:This updated schema also results in a tree-like” structure branching out from each (:State). For example Delaware looks likewhere the blue, red, and yellow nodes represent (:State), (:County), and (:Place), respectively.How did we do?We just completed a series of steps using the raw data stored on our nodes, briefly consulting our encoding document, and adding new nodes and relationships to our schema.Let’s run a simple query to see what we have for (:State {abbreviation:  DC }). We would assume that our tree” for DC” would consist of a singular (:State) connected to a singular (:County) connected to a singular (:Place). Is that what we have?Uh…..NOPE!There is obviously something amiss. Let’s run a query to determine the number of counties associated to each state.Comparing these results to the number of counties listed for each state according to Wikipedia we see that we have more counties in our graph than there should be.Looks like we have some more work to do…In the next part of the series we explore our assumptions about the data and the resulting errors we made in creating our nodes. Additionally, we’ll discuss the fact that counties may change over time. All that and more still to come…;Oct 29, 2020;[]
https://medium.com/neo4j/discover-auradb-free-week-13-exploring-a-kaggle-hr-attrition-dataset-e577f226e94f;Michael HungerFollowNov 16, 2021·7 min readDiscover AuraDB Free: Week 13 — Exploring a Kaggle HR Attrition DatasetNeo4j has been used in several HR applications and use-cases, such as for sales hierarchies, skills-management, recruiting applications, learning paths, internal role recommendations, and more.Photo by Alex Kotliarskyi on UnsplashSo when I found IBM Attrition Dataset as one of the trending datasets on Kaggle, I wanted to give it a try.HR Attrition data based on IBM attritionSurvival analysis and Prediction on HR attritionwww.kaggle.comThe dataset is meant for attrition prediction, but we don’t take it that far — we just import and model it into AuraDB free and explore some of the visual aspects of the data.If you’d rather watch our session, here is the video.AuraDB Free GA LaunchJust recently Neo4j AuraDB Free was launched as GA, so you can use Neo4j in the cloud without a credit card, even for long-running small projects or learning.My colleague David Allen wrote a nice blog post that gives you some hands-on tips on getting started.Announcing Neo4j AuraDB FreeWere excited to announce that everyone can now use Neo4j AuraDB Free, and get started without a credit card in a…neo4j.comDatasetThe dataset is a CSV with 32 columns with all kinds of employee data:attritiondepartment, role, job level, mode of worksalary, salary increase percentage, stock optionsovertime, job satisfaction, job involvementeducation degreehire date, years at company, years since last promotion, years with mgr, source of recruitingleaves, absenteeism, work accidentsworkplace, distance, travel frequencygender, marital statusnumber of companies worked for, working yearsImporting Attrition DataWe put that CSV into a GitHub Gist and use a shortlink for accessing the raw file https://git.io/JX0dU.First few records:load csv with headers from  https://git.io/JX0dU  as rowreturn row limit 5{   null : null,   DistanceFromHome :  2 ,   OverTime :  Yes ,   BusinessTravel :  Travel_Rarely ,   Date_of_termination : null,   Status_of_leaving :  Salary ,   Absenteeism :  2 ,   Gender :  Male ,   Attrition :  Yes ,   Source_of_Hire :  Job Event ,   YearsAtCompany :  0 ,   Mode_of_work :  OFFICE ,   Leaves :  4 ,   Department :  Research & Development ,   Job_mode :  Contract ,   TotalWorkingYears :  7 ,   PercentSalaryHike :  15 ,   MonthlyIncome :  2090 ,   Age :  37 ,   JobInvolvement :  2 ,   JobSatisfaction :  3 ,   JobLevel :  1 ,   Work_accident :  No ,   Date_of_Hire :  21-01-2021 ,   PerformanceRating :  3 ,   YearsSinceLastPromotion :  0 ,   JobRole :  Laboratory Technician ,   Higher_Education :  Graduation ,   TrainingTimesLastYear :  3 ,   MaritalStatus :  Single ,   YearsWithCurrManager :  0 ,   NumCompaniesWorked :  6 ,   StockOptionLevel :  0 }Unfortunately, there is no id column to identify employees, so we need to use the linenumber() function, and as it starts at 2 (probably 1 is the header row), subtract 1.To import our data, this time we just import all attributes of a row into a single Employee node and will later extract other nodes as we progress through the data.As all the CSV values are returned as strings, we’d have to convert them to numbers or booleans as needed, but we can also do that later. For instance:toFloat(3.14”)toInteger(42”)boolean: value = Yes”date(‘2020–01–01’)This import statement using MERGE is idempotent, so we can run it time and again, without new nodes being created if they already exist with that id.It creates 1470 nodes which is the number of rows in our dataset.LOAD CSV WITH HEADERS FROM  https://git.io/JX0dU  AS rowWITH linenumber()-1 AS id, rowMERGE (e:Employee {id:id})ON CREATE SET e += row,   e.DistanceFromHome = toInteger(row.DistanceFromHome)We want to set the Attrition fact as a label on the node, we picked :Left because it’s harder to misspell :)MATCH (e:Employee)WHERE e.Attrition = YesSET e:LeftExtract DepartmentThe first node we want to extract is the Department. We will merge department on name to prevent duplicates and create the relationship to employee.// see some departmentsmatch (e:Employee)return e.Department limit 5// create unique departments and connect themmatch (e:Employee)merge (d:Department {name:e.Department})merge (e)-[r:WORKS_IN]->(d)MATCH p=()-[r:WORKS_IN]->() RETURN p LIMIT 25Now we can inspect the departments and their employees, e.g. by opening Neo4j Bloom on this database and running the Employee Department search phrase and styling the employee by Attrition.We can also look at the percentage leavers and see that R&D — despite being the largest — has the lowest percentage of leavers.match (d:Department)<-[:WORKS_IN]-(e)return d.name, count(*) as total, sum(case when e:Left then 1 else 0 end) as leftorder by total desc// compute leaver percentagematch (d:Department)<-[:WORKS_IN]-(e)with d.name as dep, count(*) as total, sum(case when e:Left then 1 else 0 end) as leftreturn dep, total, left, toFloat(left)/total as percentorder by percent desc╒════════════════════════╤═══════╤══════╤═══════════════════╕│ dep                    │ total │ left │ percent           │╞════════════════════════╪═══════╪══════╪═══════════════════╡│ Sales                  │446    │92    │0.2062780269058296 │├────────────────────────┼───────┼──────┼───────────────────┤│ Human Resources        │63     │12    │0.19047619047619047│├────────────────────────┼───────┼──────┼───────────────────┤│ Research & Development │961    │133   │0.1383975026014568 │└────────────────────────┴───────┴──────┴───────────────────┘Extracting Role and Job-Related DataNext we can extract the role and some of the related job data.We could just store it on the relationship to the department but wanted to connect other information to the core concept of employment, so we turn it into a node.This time we use CREATE to get one instance of a role per employee.MATCH (e:Employee)CREATE (j:Job {name:e.JobRole})SET j.JobSatisfaction=toInteger(e.JobSatisfaction),    j.JobInvolvement = toInteger(e.JobInvolvement),    j.JobLevel = toInteger(e.JobLevel),    j.MonthlyIncome = toInteger(e.MonthlyIncome)MERGE (e)-[r:WORKS_AS]->(j)We can now color the role by job satisfaction (red-yellow-green) in Bloom and size it by salary.This allows us to see pairs of red-red (unsatisified-left), green-green (satisfied-retained) and the critical red-green (unsatisfied-not yet left) nodes between employees and their roles. And probably people with higher salaries are more likely to stick around and endure dissatisfaction.We forgot to create the relationship between role and department, but fortunately we can just spell out our graph pattern and close the triangle that you can also see in the data model below.MATCH (d:Department)<-[:WORKS_IN]-(e:Employee)-[:WORKS_AS]->(j:Job)MERGE (j)-[:ROLE_IN]->(d)We could use the Job Level in conjunction with the roles to create an hierarchy of roles, but as we don’t know who reported to whom, we can’t tell much about the real org-level.Data ModelSo far we ended up with this data model, but there are more and different approaches to extract relevant information into separate nodes.Some of the attributes, like role, salary etc. we could also have modeled as relationship properties the WORKS_IN relationship of Department, but we wanted to show and highlight the roles as first class entities in our model.Extracting EducationTurning Education into a node was straightforward but not as insightful.match (e:Employee)merge (edu:Education {name:e.Higher_Education})merge (e)-[r:HAS_DEGREE]->(edu)MATCH (edu:Education)RETURN edu.name, size( (edu)<--() ) as countORDER BY c DESCWe find 4 types of education, pretty evenly distributed.╒═════════════════╤═══╕│ edu.name        │ c │╞═════════════════╪═══╡│ Post-Graduation │387│├─────────────────┼───┤│ Graduation      │367│├─────────────────┼───┤│ PHD             │358│├─────────────────┼───┤│ 12th            │358│└─────────────────┴───┘We now can also start looking for patterns, like people who have similar education like leavers and which ones are most frequent.MATCH (e:Left)-[:HAS_DEGREE]->(edu)<-[:HAS_DEGREE]-(e2)return edu.name, e2:Left as hasLeft, count(distinct e2) as c order by c descBut again, those numbers are pretty close, so it’s not that predictive.╒═════════════════╤═════════╤═══╕│ edu.name        │ hasLeft │ c │╞═════════════════╪═════════╪═══╡│ Post-Graduation │false    │323│├─────────────────┼─────────┼───┤│ Graduation      │false    │309│├─────────────────┼─────────┼───┤│ PHD             │false    │301│├─────────────────┼─────────┼───┤│ 12th            │false    │300│├─────────────────┼─────────┼───┤│ Post-Graduation │true     │64 │├─────────────────┼─────────┼───┤│ Graduation      │true     │58 │├─────────────────┼─────────┼───┤│ 12th            │true     │58 │├─────────────────┼─────────┼───┤│ PHD             │true     │57 │└─────────────────┴─────────┴───┘Temporal DataWe wanted to see how recent the data is, so we returned the min- and max hire date unfortunately the date strings are not ISO formatted, so the results were not useful and we had to convert them into date values. The temporal APIs are pretty broad — my colleague Jennifer Reif wrote a 5-part deep-dive series on it:Cypher Sleuthing: Dealing with Dates, Part 1No matter what database, programming language, or webpage you might be using, dates always seem to cause headaches.medium.comBecause the dates are not ISO formatted, we can’t use the built-in functions date( 2021-11-08 ) but need to make use of the APOC utility library, and its apoc.temporal.toZonedTemporal, which can use a supplied format.call apoc.help( temporal )match (e:Employee)set e.Date_of_Hire = date(apoc.temporal.toZonedTemporal(e.Date_of_Hire, dd-MM-yyyy ))match (e:Employee) return min(e.Date_of_Hire), max(e.Date_of_Hire)Now we see that the dataset is current and that the earliest employee is from 1969 :)╒═════════════════════╤═════════════════════╕│ min(e.Date_of_Hire) │ max(e.Date_of_Hire) │╞═════════════════════╪═════════════════════╡│ 1969-06-19          │ 2021-06-25          │└─────────────────────┴─────────────────────┘As the dataset also contains the YearsAtCompany information, we can compute the date until which they are employed and set it as a new attribute using the built-in duration arithmetics.match (e:Employee)set e.Employed_Until = e.Date_of_Hire + duration({years:toInteger(e.YearsAtCompany)})match (e:Employee) return min(e.Employed_Until), max(e.Employed_Until)The first employee left in 1994 and the dataset seems to be from June 2021.╒═══════════════════════╤═══════════════════════╕│ min(e.Employed_Until) │ max(e.Employed_Until) │╞═══════════════════════╪═══════════════════════╡│ 1994-06-19            │ 2021-06-30            │└───────────────────────┴───────────────────────┘Similarity Network and PredictionsYou would need the graph data science library for computing similarity networks or node classification based on attributes and then use them to identify employees similar to the leavers who have not left yet and try to help them with their careers.In Neo4j Sandbox, Desktop, or soon AuraDS you can project the isolated employee data into a in-memory graph with rescaled, normalized attributes that form a vector of information about each employee.Those vectors can either be used to compute a similarity network with k-nearest-neighbors or node-similarity. That network can then be analyzed for tightly knit clusters that identify similar people and see the risk of churning per cluster. For high risk clusters, the people who have not yet left can be identified and worked with.Alternatively those vectors, our extracted nodes, and the similarity network can be used to compute node embeddings that are then utilized in training a node-classification algorithm to predict attrition.ResourcesNeo4j AuraDB FreeKaggle DatasetGitHub GistShortLink to Raw CSV5-part series on temporal data in Neo4j;Nov 16, 2021;[]
https://medium.com/neo4j/author-7cf794d895a4;Guillermo FernandezFollowApr 28, 2020·22 min readNEO4J PROV-db connector / CSV converter. Application to the detection of epidemiological patterns of COVID19 and the implementation of a PROVn model for the monitoring of infections, proactive detection of vulnerabilities, management of confinement and clinical evolution of cases to assist in the progressive and selective reduction of quarantine.GIT:Ama-Gi/prov-neo4j-covid19-trackNEO4J PROV-db connector / CSV converter. Application to the detection of epidemiological patterns of COVID19 and the…github.comAuthor:Phd Candidate, Guillermo Gabriel Fernández Amado. Sys Engineer & Sociologist (Arg), History Phd std. (Spa), Certified neo4j developer, Developer of Banc Sabadell AML (Cat).A pandemic proliferates through a network of connections. Fighting against a spreading pandemic is a massive, comprehensive issue that affects health, services supply chain and assistance social systems, as well as many other economic structures.Graphs are perfectly suited for handling connected data, from tracing connections through complex networks to understanding dependencies between entities.Tracing connections between people when they spend time together is a key aspect of how graphs are useful for responding to the spread of COVID-19, as well as identifying clusters of activity.Another aspect is drug research and repurposing. By understanding how existing compounds work, which genes they affect and how genes are connected and related to each other, we open up opportunities to identify and reuse existing drugs.At this moment a solution that Provenance (W3C) uses to manage epidemiological emergencies is lacking.We are going to be able to create a COVID19 PROVn model, to be geo-referenced, base of each person case infected, record their evolution and time history using the PROV framework, and manage the information revenue using python and visualizing by Linkurious.In this model, the Agents are people (infected, vulnerable and potentially infected, with symptoms, etc.).Activities are previous actions that person did, as a retrospective of what he did in the last days (air travel, train, work, visits to people, meetings, assistance, health facilities, food, etc.) Entities will be potentially dangerous elements of infection (plane, train, facility, office, hospital, residence, apartment, route, building, supermarket, meeting, school, company, etc.).Only in this way, we will be able to know in a retrospective way, the previous steps, recolected by surveys (done by the police, doctors, government, calls, etc.) of the affected agents and thus, for example, anticipate quarantine, and confine of agents, activities and / or entities, proceeding to inform the competent authorities of each case.It provides a triple level of analysis, starting with a layer of contagion prevention and emergency action detecting communities and places of epidemiological risk. A second layer that provides information and analysis for a correct and efficient management of the orders of confinement, evolution and monitoring of each case. Finally a third layer that would be used as a knowledge base for medical and genetic research.This model has three layers, one for incident management using activities like (incident, confine), a second for monitoring (evaluation, tracing) and the medical and genetics layer (to be expanded). The model is instantiated by a python program covid19_example.py, and use provconvert application to read a CSV file and make a use case from each row and load the csv to neo4j using PROVn notation.This PROVn framework is also compatible with POLE principles. Using Neo4j we can quickly and efficiently identify and notify Persons, Objects, Locations and Entities to be shared with health officials and sanitary/policital resources to identify and map areas of concern and ring/risk patterns.The POLE data model — Person, Object, Location, Event — is commonly applied to security and investigative use cases such as policing, anti-terrorism, border control, and social services. It’s also a great fit for the graph and graph algorithms. POLE data model can support police and social services investigations and generate real-time insights using the Neo4j browser as well as some sample visualisations. Some of the cases are to be found in police forces, government (tax / social service) agencies, immigration authorities, etc. They all have that same requirement of being able to analyse and link different entities together.This database is built based on the combination of different data sources:User-provided data by surveys of affected persons.Official epidemiological data from the Epidemic Steering Committee of each region, departament or sanitary administration.Social media data from participating partners, using a voluntary App downloaded by participants of a muestral epidemic program.Other resorces by APIs or ETL processes.Use Cases:It provides a unique traceability model between the epidemiological alert trigger event, the confinement orders, identification of risk locations, identification of potential affected and infected individuals, evolution of their clinical state and result of PCR or similar tests, traceability of movements and allocation of agents of compliance with specific protocol, and its impact on the health system, its diagnosis, analysis, evolution of clinical history and knowledge base with all this repository of events and data that provides information to genetic research of the virus, its immunization, antibodies and possible mutation.This project aids to contact tracing and smart quarantine in the context of the current coronavirus pandemic, identify people at risk using actual and potential contact tracing, suggest who should be informed or quarantined, visually explain why someone is at risk, find quarantine offenders. In resume, build intelligent systems upon the combined power of collaborative knowledge graphs and machine learning:Pandemic workflow analysis: w/load real time data & get historical traceability.Algoritm community & GIS distance for determine vulnerability/risk area with infectious potential.Machine Learning algoritms applied to miminize impact based on behavioural patterns in descalate confinement and aids the researching of virus genoma.Aids on Mitigation Strategy Political Programs.Get efficient medical resources affectation of personnel, viral test kits, beds, respirators.Clear definition of roles and responsibilities in extreme situations.Manage monitoring of patient evolution and potential viral load of entities.Scientific Investigation of mutations of virus strains. Will provide information that can be applied in analysis and the use of AI algorithms to assist in other anti-epidemic activities.Propitiate complex scientific analysis and interdisciplinary collaboration about the pandemic phenomenon.Simulate scenarios using agent-based modeling (Netlogo) generating PROV documents to feed back the graph base.Correlate vaccins, clinic history and epidemiological deceases, aids to an effiient inmunization response of population groups and helps to medical programs.Help government to localize, predict and prepare for quarantine or treatment of potentially infected individuals as well as disinfection of unsafe areas to prevent the rapid spread of the virus to the community.Update interpersonal contacts, places and trips in the last days. It allows knowing the risk of infection at any geographic level or entity (institution) using community detection algorithms. Through daily updated information, governments are adequately prepared to face different situations and strategies of de-escalation and de-confinement.Once the data — structured and unstructured — is converted into neo4j knowledge graph, the customizable and extensible platform helps to extract actionable insights and handle complex tasks like contextualize, explore, analyze, understand, and act upon vast amounts of information using the latest advances in graph analytics, and machine learning.What is PROV and why it is useful for pandemic modeling?Inspired in Stefan article: https://medium.com/neo4j/getting-started-with-provenance-and-neo4j-b50f666d8656Provenance describes the origin and history of a thing”. The term provenance originally has been used for paintings and their owner history over time. If there are some missing Provenance information, for example, a decade where you can’t tell who was the owner, the painting value drops immediately. Today we use the term provenance to track processes and responsibilities in general. For example to track the production of food or to track the history of a file.W3C — PROV standard: In 2003 the W3C adopted the official PROV standard to describe provenance structures. I don’t want to explain the complete standard only the key concepts: Entities, Activities, Agents, and Relations.PROV and Neo4jTo handle PROV graphs it is useful to store the information in some kind of database instead of a single file. Neo4j provides the required features to store PROV data in a property graph:Nodes — Mapped to Entities, Activities, and AgentsRelations — For example, used, wasGenerartedBy …Properties — Predefined PROV — as well as custom properties can be attached to nodes as well as to relations (for example startAtTime, endAtTime)Within all the information in the graph database, the analyst benefits from Cypher powerful query mechanisms to extract information for analyze and answer questions like:What is the origin of this contagious spread?Who was responsible for the confine and control of evolution of patient?Is this patient under good treatment and quarentine conditions (based on evolution of the illness and clinical diagnostics) ?When an individual is confirmed to be infected it could mark a risk indicator to all of the patient social contacts to allow other confine and protocol application.When a flight, bus, building, hospital, residence, etc is confirmed to have passengers or residents infected , all passengers on that ambit, entity and zone with and their contacts are also found promptly.When a place is confirmed to have people infected all people who arrive at the same time with the infected patient are immediately found.All the evolution in the illness upon, their states, updating and risks are in continuous and online update in the database, for aids to a real time decision taking.A first COVID19 PROV model (Alpha):This metagraph PROVn is inspired in POLE principles:Person is the different Agents:Person affected (‘personRole’ field in csv): sanitary, police, citizen, etc. Also person status information (contact like mails, telephone, other persons, friends, family, coworkers, etc.), incident roles (victim, offender, negligence, accident, etc.)Agency (‘typeAgency’ field in csv) that Persons belongs to: Hospital, Administration, Ministery, Company, Industry, Education, University, Transport, Union, etc.Evaluation Agent (‘agentRole’ field in csv) helping Persons: sanitary, police, administrative, etc. Also agent contact information.Researchers (‘researcherRole’ field in csv) helping medical diagnosis, cure, inmunization and doing genetic investigation of virus patterns: biologist, doctor, geneticist, laboratory, pharmacist, etc.Objects are fisical affected object Entities of different types (‘EntityTypes’ field in csv): personal home, flight, funeral, hospital, highway, hotel room, residence, airport,club, clinic, highway, family home, friend home, tobacco, supermarket, bus station, neighborhood, route, dentist, bikeroad, train, parks, subway, uber, etc.Locations are addresses with lat/lon GPS geographical information for visualization geographic maps in Likurious, like properties or attributes for Entities or Activitities or Events, for example, ‘regionName’ field in csv for example): regions in quarentine cases, confine addresses and confine latency, buildings, hospitals addreses, personal addresses, food emergency banks, banks, detected origin incident addresses, etc.Events are administrative entities of various types along lifecycle of each case: origin incident issue, incident types (cohabit, family, plane, park, funeral, medical review, driving, social proximity, cowork, sick, visit, medical control or test, party, friend, hospital, doctor, kiosk, supermarket, bakery, pharmacy, bank, bus, plane, walking, motorcycle, bycing, bus, subway or train travel, car carpooling, etc.) urgent call, ambulancy, police control stops, urgent surgery rooms, tests, protocols, genetics resumes, tracing resumes, report resumes, person risk, order resumes, evaluation resumes, clinic history, diagnostics resume, etc…Entitities, and Objects in PROVn framework, are not isolated items or terms. They are in the same metagraph under ‘Entity’ nodes category, and Locations are properties of that Entities or nodes, also could be Activities properties.You see the graph is huge and complex. With many different steps and activities. But it could be more big and extended to a geographic zone allowing big-data treatment and patterns analysis for stoping the spread and manage desescalate of confinement.This could be useful for pandemic spread analysis and for transparency public sanitary politics, global pandemic process pattern modeling and simulation and finally genomic researh and lifecycle of evolution of patient cases.Evolution in PROV N Model (version 1): template_block.provnModel COVID19 graph use case generated by:provconvert -bindver 3 -infile template_block.provn -outfile block.pdfdocumentprefix tmpl <http://openprovenance.org/tmpl#>prefix var <http://openprovenance.org/var#>prefix vargen <http://openprovenance.org/vargen#>prefix prov <http://www.w3.org/ns/prov#>prefix ex <http://www.example.org#>prefix prov <http://purl.org/dc/terms/>prefix foaf <http://xmlns.com/foaf/0.1/>prefix dcterms <http://purl.org/dc/terms/>bundle vargen:b// Entitiesentity(var:clinicHistory, [prov:name=’var:clinicHistoryTitle’])entity(var:report, [prov:name=’var:reportName’])entity(var:region, [prov:name=’var:regionName’])entity(var:entity)entity(var:order, [prov:name=’var:orderName’, prov:status=’var:orderStatus’])// Activitiesactivity(var:evaluation, [prov:resume=’var:evaluationResume’, prov:status=’var:evaluationStatus’])activity(var:incident, [prov:resume=’var:incidentResume’, prov:type=’var:incidentType’, prov:lat=’var:incidentLat’, prov:lon=’var:incidentLon’])activity(var:confine, [prov:resume=’var:confineResume’, prov:lat=’var:confineLat’, prov:lon=’var:confineLon’])// Usage and Generation// used(var:confine, var:entity, -)wasGeneratedBy(var:order, var:confine, -)wasGeneratedBy(var:entity, var:confine, -)// Agents and ResponsibilitywasAssociatedWith(var:confine, var:person, -)agent(var:person, [prov:type=’var:personType’, prov:name=’var:personName’, prov:risk=’var:personRisk’, prov:tel=’var:personTelephone’, prov:email=’var:personEmail’, prov:address=’var:personAddress’, prov:lat=’var:personLat’, prov:lon=’var:personLon’])agent(var:agency, [prov:type=’var:typeAgency’, prov:name=’var:nameAgency’])agent(var:agent, [prov:type=’var:agentType’, prov:name=’var:agentContact’])agent(var:researcher, [prov:type=’var:researcherType’, prov:name=’var:researcherContact’])actedOnBehalfOf(var:person, var:agency)// wasAttributedTo(var:order, var:agent)// Rolesused(var:incident, var:report, -, [prov:role=’var:reportData’])used(var:incident, var:region, -, [prov:role=’var:regionsToAggregateBy’])// used(var:incident, var:entity, -, [prov:type=’var:entityType’] )wasAssociatedWith(var:incident, var:person, -, [prov:role=’var:personRole’])wasAssociatedWith(var:evaluation, var:agent, -, [prov:role=’var:agentRole’])wasAssociatedWith(var:tracing, var:researcher, -, [prov:role=’var:researcherRole’])wasAssociatedWith(var:incident, var:entity, -, [prov:type=’var:entityType’])// Derivation and Revisionentity(var:evolution)wasDerivedFrom(var:evolution, var:report, [prov:type=’var:evolutionData’])wasDerivedFrom(var:revision, var:evolution)entity(var:revision)wasDerivedFrom(var:revision, var:order, [prov:type=’var:revisionData’])// Plansactivity(var:tracing)// agent(var:agent, [prov:type=’var:agentType’, prov:contact=’var:agentContact’])entity(var:protocol, [prov:title=’var:protocolResume’])// wasAssociatedWith(var:tracing, var:agent, var:protocol)wasDerivedFrom(var:tracing, var:protocol)wasGeneratedBy(var:evolution, var:tracing, -)// TimewasGeneratedBy(var:order, var:evaluation, -, [prov:orderTime=’var:orderTime’] )wasGeneratedBy(var:revision, var:evaluation, -, [prov:revisionTime=’var:revisionTime’, prov:lat=’var:revisionLat’, prov:lon=’var:revisionLon’] )activity(var:tracing, -, -, [prov:tracingTimeStart=’var:tracingTimeStart’,prov:tracingTimeEnd=’var:tracingTimeEnd’] )// Alternate Entities and Specializationentity(var:genetics)wasDerivedFrom(var:genetics, var:clinicHistory, [prov:type=’var:geneticsType’])entity(var:diagnostics)entity(var:tests)specializationOf(var:diagnostics, var:clinicHistory)specializationOf(var:tests, var:clinicHistory)alternateOf(var:tests, var:diagnostics)wasDerivedFrom(var:tests, var:evolution)wasDerivedFrom(var:diagnostics, var:report)endBundleendDocumentCSV example with some data: csv1.csvPython provdbconnect (to neo4j) and provconvert (to json):Inspired in Luc article: https://lucmoreau.wordpress.com/2017/03/30/prov-template-a-quick-start/The process involves the creation of a provn json file, using the csv data and the provn model template of the covid19 framework, using only this source code:Steps:1-Integrate PROV template template_block.provn and csv1.csv read made with the following python code tobindings.awk:function ltrim(s) { sub(/^[ \t\r\n]+/, ”, s) return s }function rtrim(s) { sub(/[ \t\r\n]+$/, ”, s) return s }function trim(s) { return rtrim(ltrim(s)) }BEGIN {printf({\”var\”:\n{)OFS=FS=”,”}NR==1 { # Process headerfor (i=1i<=NFi++)head[i] = trim($i)next}NR==2 { # Process typesfor (i=1i<=NFi++)type[i] = trim($i)next}NR==line{first=1for (i=1i<=NFi++) { # For each fieldif (first) {first=0} else {printf ,”}if (type[i]==”prov:QUALIFIED_NAME”) {printf \”%s\”: [{\”@id\”: \”%s\”}]”, trim(head[i]), trim($i)} else if (type[i]==”xsd:string”) {printf \”%s\”: [ \”%s\” ]”, trim(head[i]), trim($i)} else {printf \”%s\”: [ {\”@value\”: \”%s\”, \”@type\”: \”%s\”} ]”,trim(head[i]), trim($i), trim(type[i])}}printf \n”}END {printf(},\n”)printf(\”context\”: {\”ex\”: \”http://example.org/\ }\n )printf(}\n”)}2-The python code calls a binary provconvert (https://github.com/lucmoreau/ProvToolbox/wiki/provconvert), and the bindings.json was automatically created:3-from Python to binary call (iterate for csv row data):os.system(‘cat csv1.csv | awk -v line=’+str(line)+’ -f tobindings.awk > bindings.json’)os.system(‘provconvert -bindver 3 -infile template_block.provn -bindings bindings.json -outfile block.json’)os.system(‘provconvert -bindver 3 -infile template_block.provn -bindings bindings.json -outfile block’+str(line)+’.pdf’)4-This execution results in this json data (example of one row from csv):5-Each row generate a graphic use case and a bundle (load) in neo4j:Examples: row 3 and row4Graph use case 3:Graph use case 4:Some explorations:The CSV is a simple excel spreadsheet, but it could be replaced by an API REST in JSON format. We consider CSV the most simple and universal data format for mixing and using several and different types of resource data repositories that could be connected to the central servers in an online and real time fashion.List (and possible deletion) of the load bundle (batch) data:In neo4j we can have a first global look to explore the CSV loaded and depured from ETL bundles, with a simple python script: covid19_example.pyNodes:Entity, Agent and ActivityEdges:wasAssociateWith, used, wasDerivedFrom, alternateOf, specializationOf, actedOnBehalfOf, wasGeneratedByIt seems that we have supernodes (super contagious person, city, bad protocol, village, place, hospital, etc etc). Here we can see a unique high used protocol and high connected city with a lot of cases. Supernodes are a key concept in epidemiological and contagious disease transmission studies.This allows to make a ranking for those Agents thats have more cases inside, on in his relationships, connected data:Activities (blue in PROV model): incident, confine, evolution, tracingSome of them below (green in neo4j):These are the Agents (orange in model): agency, person, agent, researcherSome of them below (red in neo4j):Global visualization of 200 (all kind) of connected items:These are the Entities (yellow in model): entity (epidemic alert issue), region, order (confine), revision (of person&confine pattern), evolution (of revision&person&confine pattern), protocol (applied to tracing evolution ot the epidemiological pattern case), report (of origin incident, confine evolution and clinic changes), tests (used in evolution tracking), diagnostics (of the clinic case), clinic history (unified and updated resume of patient), and genetics (genetic material and knowledge base from patient, virus type, mutation, etc.).Some of them below (blue in neo4j):An particular edge (wasAssociatedWith):Other edge (wasGeneratedBy):SpecializationOf:With Linkurious it is possible to understand the intermedition of two differents aggrupation of information of any kind. You can find the ‘bridge data’ between two communities or gropups or ring patterns, in this case we can see the common location of two patterns: Castellon city.Or could be possible to examine and count all cases in a particular city:Linkurious visualization of a Shortest Path between a Hotel and a Car travel (red link). Pattern ring shows and could explode information about Locations involved (hospital, city), Persons affected and status (infected, tested, quarentined, asyntomatic, etc), under different Risks (bigger icon, highest risk), Incident resume and Confine order N°, etc. With this kind of exploration and on demand visualization usin Linkurious, it could be possible to detect and establish a change in the sanitary protocols, risks, extend people to test or confine, link different cases, etc.It is possible also to select different shortest paths between different patterns that links two nodes, of any kind. So it is posible to investigate the ring pattern between two Persons, Agents, Agencies, Researches, Locations, Events, Incidents, Entities, Objects, and a mix of them, for example a Person with a Location, a Incident with a Object or Location, a Person with an Object, etc.Below we can see the link (and shortest path) between an Hotel (origin or potential confine place) and a Incident of infection detected in a car travel in a police control stop.Visualization of different icons (nodes), colors and stypes of links (edges), for a quick understand of entities and relationships.The betweenness algorithm measures centrality in the graph — a way of identifying the most important nodes in a graph. It does this by identifying nodes which sit on the shortest path between many other nodes and scoring them more highly. We can see the people here which are potentially important in the graph by using this measure — they sit on the shortest path between the most other people via the any relationship (ignoring relationships direction, as it’s not very important here). Information and resources tend to flow along the shortest paths in a graph, so this is one good way of identifying central nodes or ‘bridge’ nodes between communities in the graph.Below an example of betweeness of intitutional Agents:But also applied to more Entity betweenesses, for example to detect those Entities of any kind that function like a bridge beween all kind of relationships, or a particular one:Betweenness centrality is used to identify influencers or important epidemic cases, Agents, Activities or Entities. For example an Agent in a betweeness centrality position allows to consider asymptomatic patients cases to be classified as ‘hypertransmitter’ and this kind detection prevent the spread of the infection, more dificult than identification of symptomatic patients. Studies show that ‘influencers’ in networks are not necessarily in relevant social positions, but instead can be found in brokerage positions of the network. Identify of such influencers could stop the pandemic spread.We can explore the graph for the top result from the previous query out to 2 levels and see how well connected this Agent is. We get even more results if we look farther out than 2 hops. This is like an ego graph:And this is another inverse-ego graph begining for a Control Agent, growing up 5 hops to the any Agent:We can see an Evaluation Agent, a Tracing Agent, a Infected Person and a Institution Agent, all in a Agent related infection-control-prevention ring.It’s useful also get some ‘real time’ results using a graphical visualization and analysis tool, in an interactive manner, thanks to Linkurious, Bloom or Hume graphical graph exploration software.For example in Linkurious is possible to expand the relationships of a particular data (3, 4, 4, 2 more links to more connected data…upper right node number)We can search a list of persons infected, locations and all kind of object that has the string ‘Ma’:And expand a particular Person to visualize all the data connected to that person in n hoops or levels of connected data (key concept like onion layers, exponential friends of friends, aritmetic infected of infected people, etc).Zooming data shows helps in focus in a particular kind of interest data or infectious ring pattern. Here we ca see tree person infected in the same hospital.List of two infected people isolated, with no contact or patterns in common:List of all types of nodes (Entities, Agents and Activities) involved in a case of infection with a risk bigger than 6, thas has a relationship not so far than 6 hops. This could be effective for isolate not only a Person, but a Place, Objects, Activities. This is useful in a desescalation administration scenario.Using the longitude and latitude properties in our location nodes, we can do a distance-based search to find infected or vulnerable people within 6 km of a certain address, or of a certain entity or incident that is even in a list of vulnerable places.MATCH (a:Activity)-[r]->(b:Agent) where exists (a.`prov:lat`) and exists (a.`prov:lon`) with a,b where exists (b.`prov:lat`) and exists (b.`prov:lon`) with a,b, a.`prov:lat` as lat_a, a.`prov:lon` as lon_a, b.`prov:lat` as lat_b, b.`prov:lon` as lon_b with a,b,point({ longitude: lon_a, latitude: lat_a }) AS Point_a, point({ longitude: lon_b, latitude: lat_b }) AS Point_b with a,b, Point_a, Point_b , distance (Point_a,Point_b) as distance_ab where distance_ab < 6000000 return a.`meta:identifier_original`, Point_a,b.`meta:identifier_original`,Point_b,distance_ab order by distance_ab ascNow we can explore a series of queries to simulate research on ‘vulnerable’ or ‘at risk’ individuals in the graph, being possible to find complex logic patterns, like finding people who arrived in a particular flight N°, and have been involved any Activity associated to that flight in almost any 5 hops that has no infected people reported yet, and neither associated to any activity related to a plane, and not belongs to any high education institution like university, resulting in discovering of one person that went to a particular park and is in that particular vulnerability path that conects those two (aparent disconected) persons that have not interacted in a direct manner. This allows to explore the graph out through a wider social and interaction circle. A small change to the query allows us to see not only ambient or travel or institution membership or related friends of individuals who are associated with an infectious focus, but also ‘friends of friends’ or ‘relations of relations’ who are associated with other focuses as well.MATCH p=(r:Entity{`meta:identifier_original`: ‘ex:flight_4432’})<-[*..5]-(a:Activity)-[:wasAssociatedWith]->(x:Agent) where not (a:Activity{`prov:type`: ‘plane’})-[:wasAssociatedWith]->(:Agent{`prov:type`: ‘infected’})-[:actedOnBehalfOf]->(:Agent{`prov:type`: ‘University’}) return pIt turns out there are connections between them, of different lengths. There are actually multiple paths by which some of them are connected.If the confine and treaceability protocol includes a survey of last days interaction, we could have more information in the graph database that might expand the vulnerability map to other geographic areas, based on their family relationships rather than their direct social relationships. We’ll look for people who are not directly related to an infection, and neither is their relative, but their relative has potential asyntomatic friends or contacts. There’s a high chance that the more persons are being exposed to those potentially infected, putting them at risk.In graph theory, a clustering coefficient is a measure of the degree to which nodes in a graph tend to cluster together. Evidence suggests that in most real-world networks, and in particular social networks, nodes tend to create tightly knit groups characterised by a relatively high density of ties. Here we can see a complex use case about how could have a clustering coefficient to identify potential communities of asyntomatic people with risk of being infected:I want to calculate the coefficient for those Agents that has relation with any Agent, Activity or Entity classified by a risk >10 and in a proximity of 4 hops with another Agent, and update 17 nodes with each coeffiient:CALL algo.triangleCount(‘START a=node(*) MATCH (a:Agent)-[*]->(b) where a.`prov:risk`>10 RETURN distinct id(a) AS id’, ‘MATCH (p1:Agent)-[*1..4]-(p2:Agent) RETURN id(p1) AS source, id(p2) AS target’,  {concurrency:1, graph:’cypher’,write:true, writeProperty:’triangles’, clusteringCoefficientProperty:’coefficient’}) YIELD loadMillis, computeMillis, writeMillis, nodeCount, triangleCount, averageClusteringCoefficientAnd the ranking of potential asyntomatic infected order by coefficient:With another community detecction algoritm, it is possible to generate comunnities acording to this coefficient, or any other.Next steps on the research of PROVenance for COVID19:Using NetLogo plugin for PROVENANCE:NetLogo plugin for data provenanceAgent-Based Models (ABMs) are useful tools to study emergent collective behaviour of individual entities (or agents) in…ssc2019.uni-mainz.deAgent-Based Models (ABMs) are useful tools to study emergent collective behaviour of individual entities (or agents) in social, biological, economic, network, and physical systems. Provenance information can support ABMs by explaining individual agent behaviour. Provenance provides information about entities, activities, and people involved in producing a piece of data or thing, which can be used to form assessments about its quality, reliability, or trustworthiness. How the simulation models themselves have been generated has received little attention. Although efforts have been dedicated to making simulation models accessible and facilitating their reuse, such as the ODD protocol, these focus on the product, i.e., what the model looks like, rather than the process, i.e., how the model has been generated. ODD+P (Reinhardt et al. 2018) has been proposed as a solution. However, the need of a NetLogo plugin to help documenting data provenance in ABMs.Researchers who use agent-based models (ABM) to model social patterns often focus on the model’s aggregate phenomena. However, aggregation of individuals complicates the understanding of agent interactions and the uniqueness of individuals. Its necessary to develop a method for tracing and capturing the provenance of individuals and their interactions in the Net Logo ABM, and from this the creation of a dependency provenance slice”, which combines a data slice and a program slice to yield insights into the cause-effect relations among system behaviors.https://www.researchgate.net/publication/261089317_Dependency_Provenance_in_Agent_Based_ModelingResume:We described a new tool to stop the spread of COVID-19, analyzing the types of contagious and confine patterns, for minimize social distancing also. If we could completely stop all interactions between people, this would be over very quickly. Unfortunately, to keep a country operational, we have people that still need to be moving around. Some of these people may be asymptomatic carriers or they may encounter someone who is. Anyone of us going to the grocery store, the doctors, to work may contribute to the virus’s transmission without even knowing it. To combat this, health officials need to trace the movements of anyone that tests positive, then all the people they encountered, and all the people encountered, and so on. This is difficult and time consuming. To assist with this, we need the support of thirty-party contact tracing app and online decentralizated survey to load data to this PROVn neo4j framework.Also, research in Dependency Provenance in epidemiological spread and virus mutation using ABM for model and simulate scenarios provided by real time data by neo4j, is a must.In resume, next and desired steps in the evolution of this proyect needs helping us in:GIS layer visualization (for ex. Linkurious/neo4j)Define a final COVID19 PROV Template and Bindings (and CSV/API structure)Collaboration in provide CSVs/APIs data with real cases from Sanitary Institutions, Google Maps, etc.Standardize collect data for: Mobile Apps and Surveys of affected peopleFinantial support for continue reserching in this proyect.Links:Graphs4good author presentation 27/03/2020:2020-graphs4good-graphhack-projects showcase: Covid19_provenance_n: Social Provenance and Graph Spread Using Neo4j & PROVhttps://neo4j.com/blog/2020-graphs4good-graphhack-projects/;Apr 28, 2020;[]
https://medium.com/neo4j/hacktoberfest-in-neo4j-land-c73b9798db6f;Michael HungerFollowOct 4, 2019·2 min readHacktoberfest in Neo4j LandNow that in some countries fall has arrived with colorful leaves, stormy weather and fog, your indoor activities in October can revolve around contributing to open-source projects that could use some help.Luckily for all of us, Digital Ocean has been running Hacktoberfest” for 6 years now.DFThey also published a FAQ for successful participation in the event, both for maintainers and submitters.Hacktoberfest is a month-long celebration of open source software run by DigitalOcean and DEV.Hacktoberfest is open to everyone in our global community. To participate, four pull requests must be submitted to public GitHub repositories. You can sign up anytime between October 1 and October 31.The initiative is encouraging maintainers to add that label to their project issues and incentivizing active participants with some cool swag — T-shirts and stickers to the first 50,000 participants!They also have a detailed page on Getting started”, Rules & Values” and fighting spam.Like last year, for the Neo4j Ecosystem, we have a number of projects that are participating in this cool idea.SDN RX the brand new reactive Neo4j Spring Integration [Hacktoberfest Issues]Neo4j-Streams the Neo4j Apache Kafka Integration library & plugins [Hacktoberfest Issues]APOC — Awesome Procedures On Cypher the standard” library for Neo4j covering everything from data integration, graph refactoring to a lot of utility functions [Hacktoberfest Issues]Neo4j-GraphQL-Java — an graphql-to-cypher translation library for Java [Hacktoberfest Issues]GRANDstack Starter — the project template to quickly launch a project with GraphQL React Apollo and Neo4j Database [Hacktoberfest Issues]If you encounter other projects in the Neo4j Ecosystem, please ping us and we’ll add it to the list. If you maintain a project in our Ecosystem and want to be highlighted, please add those labels to your issues, and let us know too: devrel@neo4j.comHappy October-Hacking!;Oct 4, 2019;[]
https://medium.com/neo4j/solving-sudoku-with-neo4j-a5258d545daf;Nathan SmithFollowJan 1, 2020·7 min readSolving Sudoku with Neo4jIn my last blog post, we looked at a Sudoku graph as a way to explore the recently release K1 Coloring algorithm for Neo4j. Playing with a Sudoku grid could give us a good feel for graph coloring, but the K1 Coloring algorithm doesn’t always converge to the optimum solution for a graph. In this post, we’ll build an algorithm for Sudoku that will find the optimum solution.I experimented with several ideas for a Neo4j Sudoku solver with mixed results. Then I turned to the wikipedia page for Sudoku solving algorithms. The page introduced me to several ways to think about a Sudoku algorithm. I found that the approach of modeling the puzzle as an exact cover problem worked well for Neo4j.We’ll create nodes for each possible move in Sudoku. A move represents placing a specific number in a cell. For example, put a 4 at row 1 column 3” is a move. I will give my moves attributes for row, column, block, number, and status. The status can be Yes”, No”, or Maybe” reflecting whether I have included the move in the solution, ruled it out, or it is undetermined.//Create movesUNWIND RANGE(1,9) AS rowUNWIND RANGE(1,9) AS columnUNWIND RANGE(1,9) AS numberCREATE (m:Move {row:row, column:column, number:number, status: Maybe })SET m.block = 3*((m.row-1)/3) + (m.column-1)/3 + 1The rules of Sudoku set up certain constraints. Each number must appear in a row exactly once. Each number must appear in a column exactly once. Each number must appear in a block exactly once. Each cell can contain only one number. We create nodes representing each of these constraints. Then, we create edges connecting each move to the constraints it could fulfill. You will need to have multi-line statements enabled to run the code below. If you’re not sure how to do that, you can find more information here.//Create row requires number constraintUNWIND RANGE(1,9) AS rowUNWIND RANGE(1,9) AS numCREATE (c:Constraint {row:row, number:num, type: Row requires number. })WITH cMATCH (m:Move)WHERE m.row = c.row and m.number = c.numberMERGE (m)-[:MATCHES]->(c)//Create column requires number constraintUNWIND RANGE(1,9) AS columnUNWIND RANGE(1,9) AS numCREATE (c:Constraint {column:column, number:num, type: Column requires number. })WITH cMATCH (m:Move)WHERE m.column = c.column and m.number = c.numberMERGE (m)-[:MATCHES]->(c)//Create block requires number constraintUNWIND RANGE(1,9) AS blockUNWIND RANGE(1,9) AS numCREATE (c:Constraint {block:block, number:num, type: Block requires number. })WITH cMATCH (m:Move)WHERE m.block = c.block and m.number = c.numberMERGE (m)-[:MATCHES]->(c)//Create cell requires number constraintUNWIND RANGE(1,9) AS rowUNWIND RANGE(1,9) AS columnCREATE (c:Constraint {row:row, column:column, type: Cell requires number. })WITH cMATCH (m:Move)WHERE m.row = c.row and m.column = c.columnMERGE (m)-[:MATCHES]->(c)When the puzzle is solved, each constraint will be connected to exactly one move with a status of Yes.” Therefore, whenever we set a move status to Yes,” we should set the statuses of the move’s neighbors two-hops away to No.”At the start of the puzzle, some cells are already filled in. The code below will update the statuses of some moves to reflect the initial clues for a puzzle. As we work through the puzzle, it will be important to know the order in which we changed move statuses. Our search for a solution might go down a dead end, and we would need to backtrack. We’ll use a property called search” to keep track of the search order.//Activate cluesMATCH (m:Move {row:2, column:1, number:1}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:4, column:1, number:2}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:6, column:1, number:8}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:7, column:2, number:7}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:9, column:2, number:6}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:3, column:3, number:5}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:5, column:3, number:6}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:9, column:3, number:3}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:1, column:4, number:2}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:2, column:4, number:9}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:5, column:4, number:4}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:8, column:4, number:6}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:3, column:5, number:4}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:9, column:5, number:1}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:1, column:6, number:5}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:2, column:6, number:7}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:4, column:6, number:9}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:2, column:7, number:6}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:5, column:7, number:1}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:2, column:8, number:2}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:6, column:8, number:7}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:7, column:9, number:4}) SET m.status = Yes, m.search = 0MATCH (m:Move {row:8, column:9, number:9}) SET m.status = Yes, m.search = 0//Update conflicted movesMATCH (m:Move {status:Yes})-[:MATCHES*2]-(m2:Move) SET m2.status = No, m2.search = m.searchTo solve our puzzle, we’ll follow a version of Algorithm X created by Donald Knuth. This is a depth-first search, where we try to follow a branch all the way to a solution. If we hit a dead end, we backtrack to the last fork and follow it. Here are the steps.Find the constraint node connected to the smallest number of moves with Maybe” status.If there are Maybe” moves connected to the constraint in step 1, choose one of the Maybe” moves and set the status to Yes.” Mark the constraint and the maybe nodes with the next search number in case we need to backtrack later. If there are no maybe moves connected to the constraint in step 1, skip to step 4.Set all the Maybe” moves two hops away from the move selected in step 2 to No.” Mark these nodes with the search number you used in step 2. Repeat the algorithm from step 1.If there are no Maybe” moves found at step 2, check to see if there are any Maybe” moves left in the graph. If not, you have solved the puzzle!If there are no Maybe” moves found at step 2, but there are Maybe” moves left in the graph, you have hit a dead end, and we need to backtrack. Find the highest search number and set all the moves with that number back to Maybe.” Find the constraint with the highest search number and pick a different connected Maybe” move to try and continue from step 2. If there are alternate Maybe” moves, remove this search number from all nodes and repeat step 5 to backtrack further.I will show you how to run the Cypher steps individually. You can check out this jupyter notebook to see how I used Python to put the steps together.Step 1 of the algorithm looks like this.//Find move to searchMATCH (c:Constraint)WHERE NOT (c)<-[:MATCHES]-(:Move {status: Yes })OPTIONAL MATCH (c)<-[:MATCHES]-(m:Move {status: Maybe })WITH c, mORDER BY id(c), id(m)WITH c, collect(m) AS maybesORDER BY size(maybes), id(c)LIMIT 1RETURN id(maybes[0]) AS move, id(c) AS constraintIf we got a result back, we’ll use the next query to make the updates for this search pass. Use the IDs that you received from step 1 as the parameters $moveId and $constraintId.//Update movesMATCH (m:Move)WITH max(m.search) + 1 AS newSearchMATCH (m)-[:MATCHES]->(c:Constraint)WHERE id(m)=$moveId and id(c)=$constraintIdSET m.status =  Yes ,m.search = newSearch,c.search = newSearch,c.moveId = $moveIdWITH m, newSearchMATCH (m)-[:MATCHES]->(:Constraint)<-[:MATCHES]-(m2:Move {status: Maybe })SET m2.status =  No ,m2.search = newSearchRepeat these two code blocks until the first block returns no results. When you get no results back, check to see if there are any Maybe” moves left in the graph.MATCH (m:Move {status:Maybe}) RETURN count(*) AS maybeCountIf you have zero Maybes” left, you have solved the puzzle! Many easier Sudokus will be solved without having to backtrack.If you do need to backtrack, we need to find the most recent search number, undo the decision, and take a different track.//Undo and search for alternate branch at last depthMATCH (m:Move)WHERE EXISTS(m.search)WITH m.search AS search, collect(m) AS movesORDER BY m.search DESCLIMIT 1FOREACH(move in moves | set move.status =  Maybe )WITH searchMATCH (c:Constraint {search:search})OPTIONAL MATCH (c)<-[:MATCHES]-(m2 {status: Maybe })WHERE (id(m2) > c.moveId OR m2 is null)RETURN id(c) AS constraintId, id(m2) AS moveId, searchIf this code returned a moveId, you can remove the highest search number from the moves and constraints. Then you can plug the constraintId and the moveId into the Update moves” code above and continue.MATCH (n) WHERE n.search = $search SET n.search = null, n.moveId = nullIf the Undo and search for alternate branch at last depth” code doesn’t return a moveId, we have exhausted the search at this depth. We remove the last search number from the graph and run the Undo and search for alternate branch at last depth” code again until we find a possible branch to explore.//Remove last search IDMATCH (n) WHERE n.search = $search SET n.search = null//Undo and search for alternate branch at last depthMATCH (m:Move)WHERE EXISTS(m.search)WITH m.search AS search, collect(m) AS movesORDER BY m.search DESCLIMIT 1FOREACH(move in moves | set move.status =  Maybe )WITH searchMATCH (c:Constraint {search:search})OPTIONAL MATCH (c)<-[:MATCHES]-(m2 {status: Maybe })WHERE (id(m2) > c.moveId OR m2 is null)RETURN id(c) AS constraintId, id(m2) AS moveId, search;Jan 1, 2020;[]
https://medium.com/neo4j/analyzing-roland-garros-and-us-open-tennis-tournaments-via-neo4j-9be55e3044a4;Ali Emre VarolFollowMay 16, 2022·11 min readAnalyzing Roland Garros and US Open Tennis Tournaments Via Neo4jPhoto by Kevin Mueller on UnsplashNeo4j: Simple Joyful TraversalsIn my opinion, one of the most decent and outstanding sports is tennis. I love watching the buttery slice series, tweeners, wedding proposals, imitations, and impersonations my favorite copycat is Djokovic. One unforgettable funny moment in a tennis tournament was the marriage proposal made to Steffi Graf and her response.OutlineIntroduction and MotivationDatasetThe GraphAnalysis- Finals of the tournaments- Champions and runners-up of the tournaments- Players who were the runners-up in the previous year and the winners the following year- Players who lost before QF in the previous year but won the tournament the following year- Players who have been champions at least twice- Tournament winning streaks- Players who have been runners-up at least twice and streaks- Sweepers in the finals- Route to trophyConclusionReferencesIntroduction and MotivationThe ATP and WTA organize four major tennis tournaments called the Grand Slams each year. The Grand Slam tournaments and their planned dates are as follows:Australian Open (January)Roland Garros (French Open) (May — June)Wimbledon (June — July)U.S. Open (August — September)Grand Slam tournaments last two weeks, and in the second week — when the fourth round matches, quarterfinals, semifinals, and finals occur — the quality of play is generally better.In 2022, Roland Garros and the U.S. Open were held between May 16 and June 5 and August 29 to September 11, respectively. In this blog post, I will review the Roland Garros and U.S. Open tournaments between 2000 — 2021 with the help of Neo4j. I wanted to analyze all four, but I will only explore these two tournaments due to the constraints of free AuraDB.Learning to play tennis is as tricky as learning tennis terms. Many of these terms also consist of words that are difficult to understand. They are the words we rarely hear in daily life. Because it’s more enjoyable to watch tennis tournaments once you’ve mastered tennis terminology. I will share as many terms as we need in this blog post. Without much ado, let’s hit the ball 🥎.Tennis is a four-point game in which a two-point lead must win. These four points are:No points are scored: Love1 point scored: 15 points2 points scored: 30 points3 points scored: 40 points4 points earned: Set point (set over)For a tennis player to win a game, he/she must win by at least a two-point lead. If the score is tied at 40 to 40 (deuce), it extends until one player wins by a two-point lead (an advantage point and a point). If the player with an advantage point loses the next point, the score will be deuce again.A set is won when a player has won a minimum of six games with a two-game advantage over his opponent. For example, the possible score for a six-game set could be 6–0 or 6 –1 or 6 –4 but not 6 –5. A player must win two consecutive games before winning a set in a scenario where the score is tied at 5–5. For example, a player may win a set with a score of 7–5 or 8–6.In Grand Slams, winning the men’s and women’s singles events requires going through seven rounds and matches. Men have to win three sets of a possible five to win a match, and women have to win two sets of a possible three.Rounds of Grand Slams start with 128 players for single (R128) and 64 players (R64) for double. After each round, the number of players remaining is halved. For example, 32 players (R32) remain for men’s single after two rounds. 16 (R16) remaining after the third round. And then, eight players. After that, quarter-final (QF), semi-final (SF), then final (F).One of my primary motivations for writing this article is that I love watching tennis matches, particularly Grand Slams. The other is to showcase Neo4j’s abilities in analyzing sports competitions and tournaments.DatasetFor graph generation, we will use the singles dataset curated by Jeff Sackmann in the tennis_wta and tennis_atp repositories. Jeff’s repositories include CSV files containing all the matches on the Women’s WTA tournaments between 1920 and 2022 and the Men’s ATP tournaments from 1968 to 2022. Strictly speaking, he always keeps the repositories up-to-date. Great thanks to Jeff Sackmann for curating the datasets.JeffSackmann - Overviewgithub.comAs I mentioned above, we will use the WTA and ATP datasets between 2000 and 2021. I merged and filtered them out using Pandas and saved them to my repository for simplicity.blogposts/medium/tennis at main · iamvarol/blogpostsgithub.comThe GraphFirst off, if you’re a developer and are not familiar with Neo4j, you should start here to acclimate yourself. In short, Neo4j is one of the industry-standard graph databases that offers alternative solutions for developers. Products include Neo4j Desktop, AuraDB, AuraDS, Bloom, Graph Data Science, etc.The graph data model is shown below. Generally speaking, it tells us a player can win or lose a match, and matches in tournaments are lined according to rounds (from R128 to F). The annual tournaments are likewise arranged according to their years.The node labels for the graph include Player (id, name, gender, hand, ioc), Match (id, year, round, score), Set (id, score, number), and Tournament (id, name, year, type). The relationships for the graph include MATCH_WINNER, MATCH_LOSER, IN_TOURNAMENT, IN_MATCH, NEXT_TOURNAMENT, and NEXT_MATCH.It is time to set up restrictions complying with the data model. We will create unique node property (id) constraints for Player, Match, Set, and Tournaments. These constraints will prevent the creation of duplicate nodes in the graph generation phase.On the other hand, in Neo4j, when we define a constraint, we also set out an index implicitly. We get an index on the label and properties that will reduce time in the graph creation phase.I used separate code snippets for WTA and ATP tournaments to build the Graph, but both are in the same sense, only by changing the relevant parameters.In my opinion, when creating a graph, the critical part is to create a data model and logic that can be easily queried — that is, traversed. To illustrate this with the example, we have players, sets, matches, and tournaments.Each tournament consists of matches in different rounds. Each match has sets played by the players. The player wins the match if he/she wins enough sets. The player who wins their match in all rounds becomes the tournament’s champion.To load the CSV file and create the nodes, we will use the apoc.periodic.iterate procedure from the APOC library, which is great for processing large amounts of data in one transaction. APOC is the abbreviation of the Awesome Procedures On Cypher, an add-on library for Neo4j. It provides a lot of practical procedures and functions to facilitate and speed up transactions.We establish a NEXT_TOURNAMENT relationship separately for Roland Garros and U.S. Open by year.We create a NEXT_MATCH relationship for the matches in each tournament based on the rounds.Thanks to the NEXT_TOURNAMENT and NEXT_MATCH relations, we will have the opportunity to make inquiries between tournaments and matches now.After running all the graph-related code snippets, we will have more than 45K nodes and more than 75K relationships. The below visualization only shows 20 percent of the nodes and relationships. When we run the analysis code snippets, we will see that none of the processing of the responses takes more than seconds. Therefore, we can safely conclude that AuraDB is an impressive Graph DB for storing and processing, even though I used the free plan.A visualization from the GraphAnalysisThe chart below shows the 20 countries with the highest participation according to the players’ involvement. When this chart is examined, the U.S., France, and Spain are the top three countries, respectively.Finals of the tournamentsIf we set the match round to F Match {round: F }, we can traverse to finals in tournaments.Champions and runners-up of the tournamentsAfter pivoting the above table, we can list the winners of the tournaments by year in male and female categories as seen below. The kings are Rafael Nadal (13) and Roger Federer (5) in Roland Garros and U.S. Open, respectively. Justine Henin(4) and Serena Williams (5) are the queens of Roland Garros and U.S. Open, respectively.ChampionsNovak Djokovic and Roger Federer share the same position in Roland Garros four times when we check the runners-up. Kim Clijsters, Dinara Safina, and Simona Halep are in the same place in the women’s singles in Roland Garros two times. Novak Djokovic (6) and Serena Williams (4) are listed as top runners-up in the U.S. Open tournaments.Runners-upWhen we evaluate Roland Garros and U.S. Open together, Rafael Nadal is clearly ahead in men’s singles, while Novak Djokovic is the leading runner-up.And interestingly, Serena Williams dominates women’s singles with both her championships and her runners-up. This dominance is due to Serena Williams’ performance at the U.S. Open.Players who were the runners-up in the previous year and the winners the following yearWhen we examine runners-up in the previous year who became champions the following year, Novak Djokovic (3) in men’s singles and Serena Williams (2) in women’s singles come to the fore.Players who lost before QF in the previous year but won the tournament the following yearThe table below shows the players who did not reach the quarterfinals the previous year and who became the champions the following year.Particularly noteworthy players here are those who were eliminated in the first round last year (R128) and became champions the following year:Dominic ThiemJelena OstapenkoStan WawrinkaSerena WilliamsFrancesca SchiavoneJustine HeninAlbert CostaJennifer CapriatiPlayers who have been champions at least twiceWith the help of the query below, we find players who have been champions at least twice in a tournament.Tournament winning streaksTournament winning streaks are essential in evaluating the players. After applying a function to the above dataframe, we can find out the winning streaks in Roland Garros and U.S. Open between 2000 and 2021.Players who have been runners-up at least twice and streaksWith the help of the query below, we find players who have been runners-up at least twice in a tournament.After tweaking the results, we can find out the runners-up streaks in Roland Garros and U.S. Open between 2000 and 2021. As listed below, Roger Federer was the runners-up three times in 2006, 2007, and 2008.Sweepers in the finalsThere are different analyses we can do by considering the sets. Undoubtedly, the most important of these will be to find the champions without losing any sets throughout the tournament — that is, sweepers. When we look at the list below, it is seen that women champions are generally more talented in this regard 👏.Route to TrophyI stated that the NEXT_MATCH and NEXT_TOURNAMENT relations would help us a lot in graph queries. With the help of these relationships, we answered the above questions very easily and quickly. Finally, we’ll use these relationships to look at the opponents the champions face in each round and their match scores on their journey to the trophy.As the champion of the Roland Garros 2021, Novak Djokovic started his first match with Tennys Sandgren in the series leading to the final.Then, respectively, he won the games he played with Pablo Cuevas, Lorenzo Musetti, Ricardas Berankis, Matteo Berrettini, and Rafael Nadal and advanced to the finals. He became the 2021 Roland Garros champion by defeating Stefanos Tsitsipas in the final.The champion of the Roland Garros 2021, Novak Djokovic’s JourneyConclusionIf I had to describe Neo4j in three words, they would be simple joyful traversals.”I’m sure the above analysis can be done somehow with SQL queries since SQL is a powerful language that has been used for a long time. At the same time, it is clear that too many JOINS that hold relationships between tables will be used to do the above analysis with SQL.The above analysis will be a burden, as each join will make the query complex and time-consuming to complete. However, how easily we do this using Cypher Query Language can be seen.To make such a good comparison, I highly and kindly recommend that you read this article recently published by Michael Hunger. Also, you can check out my previous Neo4j-related blogposts.The notebook we’ve worked through can be found here. I hope you fork it and modify it to meet your needs. Pull requests are always welcome!Thank you for reading! You can reach out to me on LinkedIn, GitHub, and Twitter!10 Things You Can Do With Cypher That Are Hard With SQLSQL has been around for a long time its a powerful query language, but there are some things that just make more…neo4j.comCreating Clinical Knowledge Graph by Spark NLP & Neo4jThe first end-to-end clinical knowledge graph creation using Spark NLP and Neo4j.medium.comExploring the European Natural Gas Network as a Knowledge GraphIn this blog post, we’ll use Neo4j to turn the European Gas Network into a knowledge graph to analyze the data.medium.comReferenceshttps://github.com/JeffSackmann/tennis_wtahttps://github.com/JeffSackmann/tennis_atphttps://www.rolandgarros.com/en-us/https://www.usopen.org/index.htmlhttps://www.markhneedham.com/blog/2020/01/23/quick-graph-australian-open/;May 16, 2022;[]
https://medium.com/neo4j/graph-data-processing-30451b5b576f;Iryna FeuersteinFollowMay 13, 2018·5 min readGraph data processing with Neo4j and Apache SparkRecently I was confronted with the task how to process and analyze linked data and run graph algorithms with Apache Spark running on Microsoft Azure. The suggested solutions were to use GraphX and/or Gremlin API.As I’m working with the graph database Neo4j for quite a long time and really like it’s simpleness, power and performance I started to look for connectors between Apache Spark and Neo4j and found three possible solutions which suited my situation.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comThese are:1. Using Neo4j-Spark-Connector in conjunction with Scala.2. Using SparkR and neo4r library for connecting to the database and running cypher queries with R.3. Using SparkR and RNeo4j library (pretty much similar to the second way).I’ll go briefly through those possibilities by an example used by the GraphX tutorial. I have updated the GraphX tutorial first, as the structure of the trip data offered by Ford GoBike has changed in meanwhile. The full Notebook hosted by Databricks can be found here: [notebook].Neo4j-Spark-ConnectorConfigurations and setupFirst of all, I have created a Scala notebook on Databricks. For using Neo4j in Scala environment we need to attach the neo4j-spark-connector to our Spark cluster. For that purpose:1. Please go to Workspace -> Shared -> Right click -> Create Library.2. Select Maven Coordinate as a source and 3. Add neo4j-contrib:neo4j-spark-connector:2.1.0-M4 (or any other version you would like to use)Documentation can be found here: https://docs.databricks.com/user-guide/libraries.html#create-a-library.One more thing to configure is the credentials for database access. You may use following credentials or change them to your own:spark.neo4j.bolt.url bolt://f899de9d.databases.neo4j.io:7687spark.neo4j.bolt.user public-userspark.neo4j.bolt.password ford-go-bikeTo add spark configuration you have to edit the cluster used to run your Spark Jobs.After adding the above configuration press confirm and restart the cluster.DataYou can have a look at the data at this Neo4j Cloud instance using the credentials mentioned above user=public-user and password=ford-go-bike.The schema of the subgraph we are interested in is very simple:Subgraph of the graph usedWe have nodes of type Station which are connected via edges (relationships in Neo4j) of type TRIP. Each Station has a name, a unique ID sid and longitude and latitude saved as properties. On each trip edge, we also save the duration of the trip and the direction of the trip edge shows which of the stations was actually the start and which one the end station.If exploring the data on the cloud instance on yourself be careful about displaying the trip relationships. As there so many of them between each station, your browser may have problems with rendering the results. It might be helpful to deactivate the Connect result nodes check-box in the Neo4j browser settings and show only the edges you are interested in.CodingNow you are ready to start your coding. Let us get some statistics first:import org.neo4j.spark._ import org.graphframes._// connection credentials come from your // spark context configured aboveval neo = Neo4j(sc)// loading your dataval graphFrame = neo.pattern((Station”,”name”),                             (TRIP”,”duration”),(Station”,”name”))                           .partitions(3).rows(1000)                           .loadGraphFrame// getting some statisticsdisplay(graphFrame.degrees)The result of the code above is a table of indegrees of every station node in the graph. Luckily it can be nicely formatted in the Databricks notebook, so the displayed result looks likeIndegrees of all station nodes in the graphAnd we can see at one glance, that there is one station at which a vast majority of the trips end. So we could be interested to investigate this station next.Executing graphFrame.vertices.count afterwards returns the number of station nodes (vertices) in the graph, which is equal to 172.You can run graph algorithms like PageRank too.val pageRankFrame = graphFrame.pageRank.maxIter(5).run()val ranked = pageRankFrame.verticesranked.printSchema()val top5 = ranked.orderBy(ranked.col(pagerank”).desc).take(5)display(top5)As a result we get following five kind of central” stations:Five most popular stations reached from other popular stationsOr simple execute your Cypher queries:val query = match (start:Station)-[t:TRIP]->(end:Station)              with start.name as from,                  end.name as to,                  count(t) as trips,                  round(avg(toInteger(t.duration))/60)                      as averageDuration,                   round(distance(                     point({longitude:toFloat(start.longitude),                             latitude:toFloat(start.latitude)}),                      point({longitude:toFloat(end.longitude),                             latitude:toFloat(end.latitude)})                       )/1000) as distance              return from, to, trips, averageDuration, distance                  order by distance desc limit 5”neo.cypher(query).partitions(5).batch(10000).loadDataFrameThe above query calculates the distance between all start and end stations in kilometres and the average duration of the trips between them. The five longest trips are then displayed.The longest trips and their duration in minutesCypher and RBoth libraries for Neo4j connection in R are quite similar, so I’ll just describe the neo4r driver here. You can refer to the notebook referenced above, to find out how to use the RNeo4j library and pick up the one you like better.InstallationsFirst we have to install all packages needed:install.packages(devtools”)library(devtools)devtools::install_github(neo4j-rstats/neo4r”)library(neo4r)Now we are ready to connect to the database and display the schema (change the url and credentials to your valid data):con <- neo4j_api$new(url = http://52.86.4.26:34781 ,                      user = neo4j”,                      password = cheaters-garages-cardboard”)con$get_version()[1]  3.4.0 To execute cypher queries one have to just call the APIcypher <- match (start:Station)-->(end:Station)            return start.name, end.name limit 20”triplist <- call_api(cypher, con)After loading the data you need, you can just process them with R as usuallibrary(igraph)triplist %>% convert_to(igraph”) %>% plot()Sample of the Ford GoBike data as a graphSummaryThis blog post was meant as a short overview how to start working with graph data in Spark environment. If you have any questions, feel free to contact me on Twitter.Free download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;May 13, 2018;[]
https://medium.com/neo4j/whats-cooking-part-5-dealing-with-duplicates-a6cdf525842a;Ljubica LazarevicFollowJun 25, 2019·14 min readWhat’s cooking? Part 5: Dealing with duplicatesIt’s time for part 5 of the BBC goodfood Series. In this post we’ll start to look at some of the approaches we can use to tackle duplicates in our data.Photo by Scott Webb on UnsplashYou can read the previous parts of the series to get up to date:Part 1: Importing BBC goodfood information into Neo4jPart 2: What can I make with these ingredients?Part 3: A segue into graph modellingPart 4: Similarities**UPDATED — now includes equivalent examples using the Neo4j Graph Data Science library***IntroductionWelcome back, dear foodies! We’ve done some interesting work on our BBC goodfood journey:We’ve explored the BBC goodfood data set and thought about how we might model and import itWe’ve thought about how we can start to query that data set, leveraging relationships between ingredients and recipes to find potential recommendationsWe’ve started looking at similarities between recipes based on common ingredientsHowever, we can avoid it no longer! We have some data quality issues we need to address, namely duplicate ingredients.In this post we’re going to:Look at the kinds of challenges we face when dealing with duplicatesInvestigate some approaches we can use to identify the duplicate ingredientsIntroduce potential approaches that might be useful when thinking about entity resolutionThis is by no means a set of exhaustive approaches. There will undoubtedly be more and different ways to attack this challenge. However, you will be armed with some ideas!Overview of the situationLet’s have a look at an example. I fancy some almonds about now, so why not investigate that://All the ingredients that relate to almondsMATCH (i:Ingredient) WHERE i.name CONTAINS almond RETURN i.name as Almonds ORDER BY AlmondsWhich returns the following:Now whilst there’s a big difference between almond butter and almond milk, we do have a situation where we’re dealing with:Plurals: e.g. almond and almondsActive versus passive tense: e.g. flaked almond and almond flakesAdditional words: e.g. blanched almond and whole blanched almondsAnd I suspect there will be some more hurdles we’ll need to navigate down the road!As a check point, let’s see how many total ingredients we currently have, duplicates and all. Running:MATCH (n:Ingredient)RETURN COUNT(*)tells us we have 3077 ingredients. Let’s see what we’re left with after a bit of work.Duplicate resolution versus entity resolutionThe other interesting aspect we’re going to touch on here is entity resolution. Whilst we’ll cover this in more depth in a later post, we will have some opportunities in this current journey to address some obvious candidates. So what might we mean by this?https://unsplash.com/photos/iASD5_HpTZcFor example, I think we can all comfortably agree that almond and almonds are probably one and the same, and it’s a duplicate of sorts. And we’d also say the same around cherry tomato and cherry tomatoes.But let’s look more closely at cherry tomato… is it a cherry, or is it a tomato? Ah — now we step into the interesting world of entity resolution. There may be many reasons why we may wish to determine whether cherry tomato is a tomato or a cherry (or just embrace it as an entity in it’s own right!):We’re thinking about replacement ingredients — if we don’t have any cherry tomatoes, but we happen to have some vine tomatoes in the pantry, we don’t want to give up on that lunch we’d set our hearts onWe’re thinking about better understanding recipe similarity (such as Mark’s post on similarity) — we’ll better understand any recipes lurking under different namesWe want to build a taxonomy of ingredients — vine tomato, cherry tomato, plum tomato — they’re all tomatoesThere will, undoubtedly, be much conversation around what entity resolution is and isn’t:one view point may well be that almond and almonds is entity resolution (and not a duplicate), and that cherry tomato is not a vine tomatoanother view point may well be that all tomatoes are a tomato.Whilst the view of how we deal with cherry tomato will be a topic for a future post, defining the specifics will inevitably come down to the business rules.Getting started — it’s all about the business rules…To be able to think about how we’re going to resolve the duplicates we have, we have to make a number of decisions around how we classify why two entities are duplicates of each other. Some of these will be obvious data quality challenges, for example erroneous special characters, or extra padding provided by white spaces. Some are slightly less obvious, such as plural versus singular, or passive versus active tense. Some will be hard to spot and will rely on context, e.g. Is a Cherry Tomato a type of Cherry or a type of Tomato?These decisions that we take and make will be based on our knowledge of the domain we are working in. There will be other domains where we cannot make assumptions on plural words, for example! These set of decisions and requirements we pull together to determine how we handle duplicates are commonly known as business rules — i.e. the business decisions being made on how technology will be applied.So, let’s go about defining some business rules. For now, we’re going to leave the Cherry Tomato dilemma — we’ll have a look at some approaches we can use in the next post. Having a look at the data, here are some examples we’re defining as the rules to clean the data:plurals refer to the same thing: e.g. tomato, tomatoesThe different plural types: cherry, cherries, tomato, tomatoes, onion, onionsDealing with active versus passive: flaked almonds, almond flakesDealing with capitalisation issues — we will lower-case everythingDealing with interesting spellings: We’ll try and match things together that ‘sound’ the same, e.g. herbs and herbesDealing with punctuation: we’ll treat hyphens as spacesDealing with stop words: we’ll assume they’re not important for the ingredient nameThe other rule we’re going to apply is that we don’t mind about the word ordering in the ingredients, e.g. almond flakes, flaked almonds, or light brown soft sugar, and light soft brown sugar.Whilst this is not an exhaustive list of business rules, this should help us think about some of the approaches we can do to automate de-duplicating our ingredients.Dealing with duplicates — tokenising the ingredientsTo make our lives easier, it would make sense to tokenise the ingredient words, for example, split out cherry tomato into two words. We’ll create nodes for this, and we’ll have something looking like this:This makes it significantly easier to deal with the various aberrations we described above. ‘How so?’ I hear you ask! Tokenising the ingredient allows us to:Forget about word ordering: ‘ginger and garlic paste’ and ‘garlic and ginger paste’ can now be treated the sameTackle plural words that may appear in different parts of the Ingredient nameMake it generally easier to de-duplicate Ingredients by de-duplicating constituent componentsSo let’s get started! First of all, we have some weird character import going on (due to improper encoding), so let’s fix that:MATCH (i:Ingredient) WHERE i.name CONTAINS( â€™ ) OR i.name CONTAINS( â€˜ ) OR i.name CONTAINS( â€¨ )SET i.name = replace(replace(replace(i.name, â€™ ,    ), â€˜,  ),  â€¨ ,   )We also have some errant characters, so we’ll remove those too:MATCH (i:Ingredient)WHERE i.name CONTAINS(() OR i.name CONTAINS())SET i.name = replace(replace(i.name,  ( ,   ),  ) ,   )Now we’ve cleaned up those initial data issues it’s time to tokenise. The code to do this:MATCH (i:Ingredient)WITH i, split(lower(i.name),  ) AS namesFOREACH (n IN names| MERGE (in:IngredientName {name:n}) MERGE (in)-[:IS_COMPONENT_OF]->(i)    )What this will do is take each ingredient, split the name by space, and then create a new node for each component, and connect it back to the original ingredient name. Note that we’re using MERGE, which means we’ll reuse any existing components. Also, we’re using lower() to remove any issues around case.We also have some hyphenated strings, so let’s deal with those too:MATCH (i:IngredientName)-[:IS_COMPONENT_OF]->(in)WHERE i.name CONTAINS -WITH i, in, split(i.name, -) AS namesFOREACH (n IN names| MERGE (i2:IngredientName {name:n}) MERGE (i2)-[:IS_COMPONENT_OF]->(in)    )DETACH DELETE iA slight difference from the previous query. Here we want to delete the hyphenated entity once we’ve processed it (which we do with DETACH DELETE i).So that’s the basic tokenisation done. Now we’re going to do some processing on IngredientName to help us with our de-duplicating efforts. It’s worth mentioning there are many approaches we could be using to do the cleaning we’re about to do: you may be using an ETL tool or programatically cleaning the data. As we’re doing this work in Neo4j, I’ve decided that’s where I’ll do the cleaning.Firstly, let’s get rid of some stop words, such as in, and, etc. We’re not going to be very precise with this, we’ll assume anything that is 2 characters in length is a stop word:MATCH (i:IngredientName) WHERE length(i.name) <3  DETACH DELETE iAnd ditch some other ones, such as and, with etc.MATCH (i:IngredientName) WHERE i.name IN [and, the, this, with] DETACH DELETE iNow we’re going to deal with plurals. We are assuming those are going to be +s, +es, +oes. We’ll cover +ies shortly:MATCH (i1:IngredientName), (i2:IngredientName)WHERE id(i1)<>id(i2) AND (i1.name+s = i2.name OR      i1.name+es=i2.name OR      i1.name+oes=i2.name)WITH i1, i2MATCH (i1)-[:IS_COMPONENT_OF]->(in1:Ingredient),       (i2)-[:IS_COMPONENT_OF]->(in2:Ingredient)MERGE (i1)-[:IS_COMPONENT_OF]->(in2)DETACH DELETE i2Something to bear in mind — this is not a particularly graphy query to begin with. It’s essentially a Cartesian query, so at some point there may well be too much data to compare against, and we would have to think about how to do this in a different way. The WHERE id(i1)<>id(i2) part stops us from comparing the same node against itself.Now we need to do a little more ‘heavy’ lifting, we’re going to do a bit of string manipulation to tackle +ies plurals, and active versus passive. This is quite expensive, and the above approach may cause us to run into problems. To handle this we’re going to reduce the initial data set that we want to compare against. For this I’ve decided to use Sorensen Dice Similarity.Sorensen Dice Similarity FormulaSorensen Dice Similarity is an approach to determine how similar two samples are.Here we use the APOC implementation as a way of fuzzy-matching text:MATCH (n1:IngredientName),(n2:IngredientName)WHERE id(n1) <> id(n2)WITH n1, n2,      apoc.text.sorensenDiceSimilarity(n1.name,n2.name) as sorensenDSWHERE sorensenDS > 0.6 AND left(n1.name,2)=left(n2.name,2)with n1, n2WHERE length(n1.name) <> length(n2.name) AND (left(n1.name, length(n1.name)-1)+ies = n2.name OR      n1.name+d = n2.name OR      left(n1.name, length(n1.name)-1)+d = n2.name)WITH n1, n2MATCH (n2)-[:IS_COMPONENT_OF]->(i)MERGE (n1)-[:IS_COMPONENT_OF]->(i)DETACH DELETE n2We’re also checking the start of both words to make sure that they start with the same characters, further cutting down the amount of data we’re going to check. A word of warning we need to specify some sort of threshold for Sorensen Dice Similarity. In this example we’re trying to cut down how much data we bring in to query. There may be other situations where the temptation to overfit based on the sample data we’re using could cause unexpected outputs later. Do keep an eye out for that.Last but not least, we’re now going to try and mop up the last few rogue items. There are misspellings/different spellings, such as herbs and herbes. We’re going to use APOC again, this time using Double Metaphone. Double Metaphone is a type of phonetic algorithm used for indexing words based on their sounds.Again, we’re using Sorensen Dice Similarity to cut down how much data we pull in:MATCH (n1:IngredientName),(n2:IngredientName)WHERE id(n1) < id(n2)WITH n1, n2,      apoc.text.sorensenDiceSimilarity(n1.name,n2.name) AS sorensenDSWHERE sorensenDS > 0.92 CALL apoc.text.doubleMetaphone([n1.name, n2.name]) YIELD value WITH n1, n2, collect(value) AS valWHERE val[0] = val[1]WITH n1, n2MATCH (n2)-[:IS_COMPONENT_OF]->(i:Ingredient)MERGE (n1)-[:IS_COMPONENT_OF]->(i)DETACH DELETE n2For those of you who are decomposing the queries to see the inner workings, you may well have noticed that words such as con and cone are being classed as the same. We have to decide whether this level of aggressive ‘de-duplication’ is acceptable or not, which will again come back to business rules. In this scenario, I’m taking the decision that it doesn’t matter, as this is on tokenised words, and we’re leaving the original words in tact, and it may well be the case that con and cone are for completely different ingredients. In this case the rate of intersection via IngredientName will be so low, we can forget about it.First pass de-duplicationSo, let’s have a look what this tokenising and clean up work has done for us so far. If we now use our cleaned up tokens, we can now match the following:MATCH (i:Ingredient)WITH i, [(i)<-[:IS_COMPONENT_OF]-(in:IngredientName) | in] AS componentsMATCH (i)-[:IS_COMPONENT_OF*2]-(i2)WHERE i.name < i2.nameWITH DISTINCT i, components, i2WHERE size((i2)<-[:IS_COMPONENT_OF]-()) = size(components) AND all(in IN components WHERE (in)-[:IS_COMPONENT_OF]->(i2))RETURN i.name, collect(i2.name)Which gives us something like this:Not bad! That’s 358 duplicate ingredients we’ve found already (replace the last line with RETURN count(*) ). That’s just by tokenising the ingredient names, doing some clean-up, and then finding intersection matches.One challenge we still need to overcome. We haven’t quite solved our previous issue where there’s an extra word, e.g. ‘gluten free white flour’/’gluten free flour’. Let’s have a look at how we might approach that next.Introducing community detection for detecting duplicatesMark introduced the Jaccard algorithm to find similarity between recipes. We’re going to use it again, this time to build relationships between ingredients, determining similarity based on their relationships to IngredientName. Those tokenised words are definitely very useful!Once we’ve added more structure to the Ingredients, we’re then going to use Louvain to determine ingredient groupings. In this context, this will hopefully help join together all the ingredients that are the same, even with the odd missing word.Louvain is a community detection algorithm. Based on the structure of the graph and how entities are connected, it will attempt to map specified node types to specific groups, if they are deemed to have similar connectivity. In this example, Ingredients grouped together are assumed to be the same ingredient.In true Blue Peter style — here’s one I prepared earlier — The node in the centre is the IngredientName node, and the nodes with numbers are the identified communitiesWe won’t go over Jaccard again, since that was covered previously. What we do need to decide is what our cutoff value is going to be:Too high and we’ll miss some true duplicatesToo low and we’ll treat non-duplicates as duplicates.Again, we still need to be aware of the overfitting problem.Here’s the Jaccard query. Note that in the Graph Data Science (GDS) library (the successor to the Graph Algorithms library), does this in a separate way. We’ll have both approaches in this post. If you’re using the Graph Algorithm library, you will do the following:MATCH (p:Ingredient)<-[:IS_COMPONENT_OF]-(n)WITH {item:id(p), categories:collect(id(n))} as itemsListWITH collect(itemsList) as itemsToProcCALL algo.similarity.jaccard(itemsToProc, {   writeRelationshipType: SIMILAR_TO,   similarityCutoff: 0.8, write:true})YIELD nodes, similarityPairs, write, writeRelationshipType, writeProperty, min, max, meanRETURN nodes, similarityPairs, write, writeRelationshipType, writeProperty, min, max, meanUsing the GDS library, we’re going to load the graph into memory first. Whilst you can use anonymous graphs when running the procedures, if you first load the graph into memory, you’ll be able to quickly try out different similarityCutoff values:CALL gds.graph.create.cypher( similarity , MATCH (n) WHERE n:Ingredient OR n:IngredientName RETURN id(n) AS id , MATCH (i:Ingredient)<-[:IS_COMPONENT_OF]-(in:IngredientName) RETURN id(i) AS source, id(in) AS target )And now we’ll we can run our similarity procedure nodeSimilarity which is based on Jaccard Similiarty score:CALL gds.nodeSimilarity.write( similarity ,{similarityCutoff:0.8, writeRelationshipType: SIMILAR_TO , writeProperty: score })Once we’re finished with our in-memory graph, we’ll remove it:CALL gds.graph.drop( similarity )A SIMILAR_TO relationship will be joined between Ingredients that meet the cutoff value. We’re going to tell Louvain to work with the Ingredient node, and to use the SIMILAR_TO relationship type to determine the communities. Using the Graph Algorithms library, the query is thus:CALL algo.louvain.stream(Ingredient, SIMILAR_TO, {}) YIELD nodeId, communityWITH algo.getNodeById(nodeId) AS ingredient, communityMERGE (e:Entity {id:community})MERGE (e)-[:CONTAINS_MEMBER]->(ingredient)For the GDS snippet, again we will load a graph into memory, don’t forget to drop it when you’re done:CALL gds.graph.create.cypher( community , MATCH (i:Ingredient) RETURN id(i) AS id , MATCH (i1:Ingredient)-[:SIMILAR_TO]->(i2:Ingredient) RETURN id(i1) AS source, id(i2) AS target )And the GDS query:CALL gds.louvain.stream( community ) YIELD nodeId, communityIdWITH gds.util.asNode(nodeId) as i, communityIdMERGE (e:Entity {id:communityId})MERGE (e)-[:CONTAINS_MEMBER]->(i)Here we yield a community ‘id’ and the node (via its internal id) that belongs to it. We in turn take that, create a new Entity node, and merge all the associated Ingredient nodes to it.We can try experimenting with the Jaccard cutoff value. Trying a cutoff value of 0.7 gives us:Jaccard Similarity with a cut off of 0.7whereas a cutoff value of 0.8 gives us:Jaccard Similarity with a cut off of 0.8What should be clear here is that we’re approaching the borders of entity resolution. Depending on what is our definition of entity resolution, we might want to categories all types as sugar in one category, or we may not. This will all come down to what is acceptable and what our business rules are.There are a few erroneous results with a lower cutoff value, and they’re not unreasonable. As we’re still not looking at Ingredients with full context, there’s no way to know that the colour of the muscovado sugar might be important. Or that explicitly stating that gluten-free flour is white is not necessary. Also, don’t forget about the overfitting challenge as well.I’m going to leave it here for now — we’ve covered a lot of ground around. Some approaches we might use to tackle duplicates, we’ve hinted at some options around entity resolution. Don’t worry, we’re going to be back soon looking at some more ‘graphy’ ways to tackle that subject.Looking back at where we’ve got to:with ‘pure’ cleaning only, we’ve managed to bring down the number of ingredients from 3077 to 2719using more fuzzy approaches, depending on the cutoff thresholds, we’ve further brought that number down to 2673SummaryWe’ve shown you in this post some principles around how we might prepare and then look for potential duplicates in our data set. We’ve highlighted the importance of determining what the business rules are to enable us to programatically implement them. Last but not least, we’ve also touched on thinking about the overfitting problem.By tokenising ingredients and building out more structures, we have started to explore ‘graphy’ ways to find duplicates. It is also starting to come together as to how we might do more extensive entity resolution.But our work here is still not finished. In the next post we’re going to have a look at how might we bring some more context in based on the data that we have and see if we can do even better.;Jun 25, 2019;[]
https://medium.com/neo4j/s3-spark-and-neo4j-3474265ca97;joydeep bhattacharjeeFollowAug 26, 2018·5 min readS3, Spark, and Neo4jsource: https://www.pexels.com/photo/nature-france-water-summer-34318/In the modern pipeline, you will invariably have data in S3 in either CSV or other formats. So it is quite inevitable that you will have to pull files from S3, do some manipulations in your spark dataframe and then push them to the database.In this post, we will pull jsonlines files from S3, create a dataframe out of them and then push to Neo4J which is our graph database. In case you are not using graph databases for your data modeling, I will highly recommend that you try it out. You can take a look at the advantages of using a graph database in this nice article by DZone.What Are the Major Advantages of Using a Graph Database? - DZone DatabaseA graph database is a data management system software. The building blocks are vertices and edges. To put it in a more…dzone.comFor this purpose, we are going to use jsonline files. Jsonlines are basically records of JSON strings and look like below. I am choosing Jsonlines and not CSV files for this post because there is the added challenge of parsing the JSON string. In case you are interested in CSV files then have a look at a YouTube video that I had uploaded on a similar topic.Another thing that we will try is to keep the operations as lazy as possible and not do any actual computation till the last moment.ConfigurationI have Neo4J of version 3.4.0 downloaded in my machine. I can now start like below.➜  ./bin/neo4j consoleActive database: graph.dbDirectories in use:  home:         /Users/joydeep/Documents/neo4j-community-3.4.0  config:       /Users/joydeep/Documents/neo4j-community-3.4.0/conf  logs:         /Users/joydeep/Documents/neo4j-community-3.4.0/logs  plugins:      /Users/joydeep/Documents/neo4j-community-3.4.0/plugins  import:       /Users/joydeep/Documents/neo4j-community-3.4.0/import  data:         /Users/joydeep/Documents/neo4j-community-3.4.0/data  certificates: /Users/joydeep/Documents/neo4j-community-3.4.0/certificates  run:          /Users/joydeep/Documents/neo4j-community-3.4.0/runStarting Neo4j.2018-08-26 04:00:47.627+0000 INFO  ======== Neo4j 3.4.0 ========2018-08-26 04:00:47.684+0000 INFO  Starting...2018-08-26 04:00:50.260+0000 INFO  Bolt enabled on 127.0.0.1:7687.2018-08-26 04:00:54.985+0000 INFO  Started.2018-08-26 04:00:56.122+0000 WARN  Low configured threads: (max={} - required={})={} < warnAt={} for {}2018-08-26 04:00:56.134+0000 INFO  Remote interface available at http://localhost:7474/An interesting thing to notice is that apart from the usual HTTP port on 7474, you also have the Bolt port (7687) which is used to pass Cypher queries to Neo4j. Neo4j is implemented in Java and accessible from software written in other languages, in our case, Scala, using the Cypher query language.Now lets take a jsonline file in s3. The files look like below.{ FieldA :  12 ,  FieldB :  376 }{ FieldA :  18 ,  FieldB :  35 }{ FieldA :  50 ,  FieldB :  190 }So what we will try to do is that we will take those FieldA and FieldB, map to a dataframe in spark and then push it to neo4j.Since you are trying to get the data from S3, you will need to have the AWS keys provided as environment variables.export AWS_SECRET_ACCESS_KEY=<mysecret>export AWS_ACCESS_KEY_ID=<mykey>Now to parse the CSV in Spark, we open the spark-shell. You will need to add these dependencies.aws-java-sdk:1.7.4hadoop-aws:2.7.5.1neo4j-spark-connector:2.1.0-M4So the spark-shell command will look like this. The spark-shell version that I am using is 2.11.8.$ spark-shell --packages com.amazonaws:aws-java-sdk:1.7.4,ch.cern.hadoop:hadoop-aws:2.7.5.1,com.fasterxml.jackson.core:jackson-annotations:2.7.9,com.fasterxml.jackson.core:jackson-core:2.7.9,com.fasterxml.jackson.core:jackson-databind:2.7.9.4,org.wso2.orbit.joda-time:joda-time:2.9.4.wso2v1,neo4j-contrib:neo4j-spark-connector:2.1.0-M4The Spark Pipeline.Now lets get to the code. Create an RDD out of using the textFile method.val lines =  sc.textFile( s3a://bucketname/pathtofile.jsonl )Notice that we are using the s3a file convention. Below is the rationale.The difference between s3 and s3n/s3a is that s3 is a block-based overlay on top of Amazon S3, while s3n/s3a are not (they are object-based).The difference between s3n and s3a is that s3n supports objects up to 5GB in size, while s3a supports objects up to 5TB and has higher performance (both are because it uses multi-part upload). s3a is the successor to s3n.source: https://stackoverflow.com/a/33356421/5417164Now that you have the RDD, we will need to parse the JSON on each line. We can create this handy function to parse the JSON. We will import the required libraries, then create a function getJsonContent that takes the FieldA and FieldB columns from the parsedJson. We can then extract the string out of it using the extract[String] method. We can then return the parsed values.import org.json4s.{DefaultFormats, MappingException}import org.json4s.jackson.JsonMethods._import org.apache.spark.sql.functions._def getJsonContent(jsonstring: String): (Integer, Integer) = {   implicit val formats = DefaultFormats   val parsedJson = parse(jsonstring)     val value1 = (parsedJson \  FieldA ).extract[String].toInt   val value2 = (parsedJson \  FieldB ).extract[String].toInt   (value1, value2)}Now that we have the function we can easily pass that to lines and transform that to a dataframe.val newNames = Seq( id ,  count )val df = lines.map(getJsonContent).toDF(newNames: _*)Now that you have the dataframe, you can create the nodes and edges in Neo4j.import org.neo4j.spark._import org.graphframes._Neo4jDataFrame.mergeEdgeList(sc, df, ( Event ,Seq( id )),( HAS ,Seq.empty),( Count ,Seq( count )))which means will use the Neo4jDataFrame.mergeEdgeList method on the Spark context sc and dataframe df to create connections between Event and Count with connection of HAS and the values are taken from id and count of the dataframe respectively. Notice that we the processing also happens only now. We are able to leverage the full power of Sparks lazy processing.The ResultNow once you are done check the nodes and connections in Neo4j with Cypher queryMATCH p=()-[r:HAS]->() RETURN p LIMIT 25graph rending in neo4jScala offers a convenient and easy way for S3 file processing. This post is aimed at helping beginners use S3 files and Scala with ease. If you found this useful, do leave a comment, we would love to hear from you and share the post with your friends and colleagues.I have recently completed a book on fastText. FastText is a cool library open-sourced by Facebook for efficient text classification and creating the word embeddings. Do check out the book.fastText Quick Start Guide: Get started with Facebooks library for text representation and…Perform efficient fast text representation and classification with Facebooks fastText libraryKey FeaturesIntroduction…amzn.to;Aug 26, 2018;[]
https://medium.com/neo4j/sandbox-spring-cleaning-38689661dece;Michael HungerFollowFeb 19, 2021·5 min readSandbox Spring CleaningAll new Neo4j Sandboxes to explore and learn about graphs with major version updates.Photo by Ostap Senyuk on UnsplashAs the first spring flowers start to bloom in some regions, we’re excited to announce that our Sandbox infrastructure got an overhaul as well.TLDRGo check out the new Sandboxes at https://sandbox.neo4j.comNeo4j Sandbox Project SelectionStarting today, the Sandbox back-end runs on self-contained Docker images on AWS Fargate managed by a Lambda-Function-based API which allows easier and more robust scaling of the service. We use Neo4j itself as a provisioning database.We were able to shut down an auto-scaling ECS cluster with 111 32-GB EC2 instances that ran thousands of sandboxes before.Big Thanks to Rafal Janicki, Shreyans Ghandhi, and Max Andersson for all the hard work and to the team for testing and feedback.This Was Also a Great Opportunity for Us to Upgrade EVERYTHING!So here comes the rundown. :)Sandbox UISandbox now runs Neo4j 4.2 (Enterprise) with all the new features like multi-database, fine grained permissions, reactive drivers, and more. See our Developer Guides for more details on what’s new in these versions.Neo4j Graph Database - Developer GuidesNeo4js primary product and focus is our graph database that stores data in the form of nodes and relationships. It…neo4j.comSandbox also comes with the latest versions of the APOC utility library.Awesome Procedures On Cypher (APOC) - Neo4j LabsIt can be installed with a single click in Neo4j Desktop, is available in all Neo4j Sandboxes and in Neo4j Cloud…neo4j.comBrand new version 1.5 of the Graph Data Science Library with machine learning workflows and new pathfinding algorithms.Neo4j Graph Data Science - Developer GuidesGraph Data Science techniques can be used as part of a variety of different applications and use cases. Graph queries…neo4j.comEach Sandbox now also comes with the Graph Data Science Playground (NEuler) the interactive guide to get started with the library.Graph Data Science Playground (Neuler) On SandboxTo support your visual exploration, you can use the new 1.5 version of Neo4j Bloom.What’s new in Bloom 1.5?The latest version of Neo4j Bloom is out. Let’s a look at the new features and improvementsmedium.comThe latest addition to Sandbox is Neosemantics, the RDF and linked data library for Neo4jneosemantics (n10s): Neo4j RDF & Semantics toolkit - Neo4j Labsneosemantics runs as an extension to your Neo4j database. Downloading the appropriate release for your Neo4j database…neo4j.comThese features and libraries within Neo4j Sandbox are also used in our free online training classes.GraphAcademy - GraphAcademyNow is the perfect time to show your employer, customers, and colleagues that you are a Neo4j expert. We currently have…neo4j.comSandbox UI FeaturesSome Sandbox features that you might have missed in the last few months:You can now easily add your Sandbox as a Remote Database Connection to Neo4j Desktop.And you can invite collaborators to share your Sandbox with.Driver Code ExamplesWe also updated all the code examples so that each Sandbox has an interesting query for its dataset in the runnable source code for all our officially supported languages:JavascriptPythonJavaWith these coming soon:.NetGoCode Examples for Neo4j DriversThe GraphQL integration allows you to spin up a CodeSandbox (see screenshot) with a full GraphQL API to be consumed by your front-end App, service, or JAMstack site generator. It infers the GraphQL schema directly from the data in your Neo4j Sandbox instance.CodeSandbox for GraphQL ServerSandbox Datasets Available on GitHubWe have also made the datasets for each Sandbox available on GitHub together with the example queries, model image, Bloom perspectives, source code examples, and more.Check them out at github.com/neo4j-graph-examplesneo4j-graph-examplesneo4j-graph-examples has 20 repositories available. Follow their code on GitHub.github.comWhat’s Next?The content of your own Sandboxes will be available as dump-files that can be easily imported into Neo4j Aura (Cloud) and Neo4j Desktop. Both through the UI and in the email you get when a sandbox is terminated.In the next release of Neo4j Desktop you will be able to use these repositories directly as starting points for your local Neo4j projects.With the new capabilities available we will extend our Free Online Training to use more of the recent features in the sandbox for the graph databases and graph data science.FeedbackOf course we love to hear your feedback, so please share here, on Twitter or in our community forums how you like the new capabilities or if there are any issues.Neo4j Online CommunityWelcome to the Neo4j Community, a global forum for online discussion on how (graphs)-[:ARE]->(everywhere)community.neo4j.comWe had to do quite some work updating the datasets and browser guides to the new Cypher syntax, so if you spot anything we missed please let us know or send a PR to the repositories above.;Feb 19, 2021;[]
https://medium.com/neo4j/neo4j-drivers-are-go-9697baf4d116;Nigel SmallFollowJul 19, 2018·3 min readNeo4j Drivers are Go!First alpha release of the official Neo4j Go driver available.Depending on where you’re from, it could be said that 2016 was a year with some pretty significant events. When I look back to that year, one of the most exciting moments for me came in April, when we finally released Neo4j 3.0. This release represented the culmination of around 18 months’ work, during which we built our new binary protocol — Bolt — as well as the first batch of official drivers for Neo4j.The database has a long and distinguished history within the Java ecosystem, but tooling and support for other languages were less readily available a few years ago. There were some solid community-authored HTTP drivers around, but many non-Java users still felt like second class citizens in the graph database world. So, the primary goal in offering official drivers was to reach out to these other users and show them that Neo4j wasn’t just 4 Java”.On top of the obvious inclusion of a Java driver for Bolt, we also built a driver for .NET (because Windows users are people too), JavaScript (because I hear it’s quite popular in some quarters, plus we have a browser application to feed) and Python (because of data scientists… and me). We could have gone further: PHP and Ruby were both potentials that didn’t quite make the cut. Not least because both languages had great community support but also because we had to draw the line somewhere.By now, I can hear you thinking, that’s all fascinating, but why the trip down Memory Lane?”The Prototypical Neo4j-Go-HackerWell, the reason for all the nostalgia is because today marks our first step beyond those first four languages. Today, we are releasing the first alpha of our newest language driver: an official Neo4j driver for Go.Go wasn’t even on our radar back in 2016, but has gone from strength to strength since. We actually started receiving so many requests for a Go driver over the past couple of years that it became the obvious next language to add to the official catalogue.And so as of today you can now go get github.com/neo4j/neo4j-go-driver (after installing the underlying connector).The full set of installation instructions can be found in the project README. And once installed, you’ll be able to create a NewDriver, start a Session, and run a Transaction, just like with all our other drivers. We’ve ported the same uniform API that we use everywhere else to make the whole experience simpler, easier, and more familiar.Here is the mimium viable snippet with the new driver:Note that we’re starting with a 1.7 release though, not 1.0. There will be no 1.0 to 1.6 releases for the Go driver because we’re starting with the 1.7 feature set. This will mean that by GA, you’ll get all the same features as we give you for the other languages, including routing and full type system support. Routing won’t be there for this first alpha but it’ll follow in one of the upcoming alphas or betas over the next few weeks.We plan to go GA for the Go Driver along with the Neo4j 3.5 release later this year. That will also be when we release the other 1.7 series drivers. If you want to find out more about any of this, reach out in the #neo4j-golang channel of our community Slack if you find any problems with the library, pop an issue into GitHub.Now, off you Go!;Jul 19, 2018;[]
https://medium.com/neo4j/meet-the-graph-gallery-3666a127efee;Michael HungerFollowOct 12, 2018·2 min readMeet the Graph GalleryGraph Examples on your DesktopThe developer relations team is excited to launch the Graph Gallery”, a new Graph App for Neo4j Desktop. It allows you to browse and search Graph Examples (also known as Graph Gists) provided by the Neo4j Community across a variety of use cases and industries.With a single click, you can launch any of those examples as a Browser Guide in the Neo4j Browser of your currently running database. There you can, step by step, insert the graph data and run the queries of the example and learn more about the use case and how people use Neo4j to solve problems.Thanks a lot to Cristina Escalante and Alisson Patricio from our partner SilverLogic for building the app. We are enormously grateful to all the Neo4j Community Contributors that created Graph Examples over the years either as part of challenges or just through their willingness to help and share their experience.InstallationGo to the Apps TabEnter as Package URL: https://r.neo4j.com/graph-gallery-appPress Install”Add the graph-app to your projects.How to use itMake sure that you have a running empty database created in Neo4j Desktop.After launching the app you can browse the examples or search for certain keywords or authors, then look through the results and choose one you like. Hit the Play as a Browser Guide” button to launch the example as a guide in your Neo4j Web Browser.We recorded a short video that also demos the installation and usage of the Graph AppIn December we plan to run another competition for graph examples, and this time you’ll know that your contributions will not just win you prizes but also appear in the Graph Gallery on every Neo4j user’s desktop.Please let us know what you think and report back your experiences. If you run into any issue, please raise them here on GitHub, and feel free to fork the project to build your own Graph App.In the next weeks we will present more graph apps from Neo4j partners and employees and how they were built to show you how easy it is to start Graph App development.Have fun exploringMichael for the Developer Relations Team;Oct 12, 2018;[]
https://medium.com/neo4j/football-transfers-graph-e8ba7347169e;Mark NeedhamFollowJul 24, 2019·5 min readFootball Transfers GraphExplore a Neo4j Graph of football (soccer) transfers in the Summer 2019 WindowThe Football Transfer window is currently open, which means players are moving around the globe for increasingly exorbitant fees.Transfer Markt — The best place to keep track of transfersThe best place to keep track of what’s going on is the Latest Transfers page of the transfermarkt website, which captures details of the players, clubs, and fees involved.Irfan and I were trying to work out where the money was flowing based on the transfers, so we decided to create a Neo4j Graph to help us out.The scraping code to get the data from the transfermarkt website is in the mneedham/football-transfers repository. The transfers.json file contains JSON documents for all the transfers that happened since June 2019.Loading the dataOne line of the transfers.json file looks like this:{    season : 2019/2020 ,    player :{       href : /antoine-griezmann/profil/spieler/125781 ,       name : Antoine Griezmann ,       position : Centre-Forward ,       age : 28 ,       image : https://tmssl.akamaized.net//images/portrait/medium/125781-1533626871.jpg?lm=1533626889 ,       nationality : France    },    from :{       href : /atletico-madrid/startseite/verein/13 ,       name : Atl\u00e9tico Madrid ,       country : Spain ,       league : LaLiga ,       leagueHref : /primera-division/transfers/wettbewerb/ES1 ,       image : https://tmssl.akamaized.net//images/wappen/tiny/13.png?lm=1519120744    },    to :{       href : /fc-barcelona/startseite/verein/131 ,       name : FC Barcelona ,       country : Spain ,       league : LaLiga ,       leagueHref : /primera-division/transfers/wettbewerb/ES1 ,       image : https://tmssl.akamaized.net//images/wappen/tiny/131.png?lm=1406739548    },    transfer :{       href : /jumplist/transfers/spieler/125781/transfer_id/2552096 ,       value : \u00a3108.00m ,       timestamp :1563058800   }}We have players, from and to clubs, and the transfer itself. We’ll import that data into the following graph model:Football Transfers Graph ModelWe can execute the following query, that uses APOC’s Load JSON procedure, to create players, transfers, leagues, and clubs:CALL apoc.load.json( https://github.com/mneedham/football-transfers/raw/master/data/transfers.json )YIELD valueWITH value, apoc.text.replace(value.transfer.value,  £ ,   ) AS transferValueWHERE transferValue <>  ?  AND transferValue <>  - MERGE (p:Player {id: value.player.href})SET p.name = value.player.nameMERGE (from:Club {id: value.from.href})SET from.name = value.from.nameFOREACH(ignoreMe IN CASE WHEN value.from.leagueHref =    THEN [] ELSE [1] END |  MERGE (fromLeague:League {id: value.from.leagueId})  SET fromLeague.name = value.from.league  MERGE (from)-[:IN_LEAGUE]->(fromLeague))MERGE (to:Club {id: value.to.href})SET to.name = value.to.nameFOREACH(ignoreMe IN CASE WHEN value.to.leagueHref =    THEN [] ELSE [1] END |  MERGE (toLeague:League {id: value.to.leagueId})  SET toLeague.name = value.to.league  MERGE (to)-[:IN_LEAGUE]->(toLeague))MERGE (t:Transfer {id: value.transfer.href})SET t.value = CASE       WHEN transferValue contains  k         THEN toFloat(apoc.text.replace(transferValue,  k ,   ))             * 1000       WHEN transferValue contains  m         THEN toFloat(apoc.text.replace(transferValue,  m ,   ))             * 1000000       ELSE 0.0 ENDSET t.date = date(datetime({epochseconds:value.transfer.timestamp}))MERGE (t)-[:OF_PLAYER]->(p)MERGE (t)-[:FROM_CLUB]->(from)MERGE (t)-[:TO_CLUB]->(to)After we’ve done that we’ll run the following query to create a relationship from a league to the country that it belongs to:CALL apoc.load.json( https://github.com/mneedham/football-transfers/raw/master/data/leagues.json )YIELD valueMATCH (l:League {id: value.league})MERGE (c:Country {name: value.country})MERGE (l)-[:IN_COUNTRY]->(c)Graph of the top 10 transfersNow we’ve ready to query the graph.Top 10 transfersThe following query finds the top 10 transfers by value, and returns the player and clubs involved:MATCH (t:Transfer)-[:OF_PLAYER]->(player),       (from)<-[:FROM_CLUB]-(t)-[:TO_CLUB]->(to)RETURN player.name, from.name, to.name,       apoc.number.format(t.value) AS priceORDER BY t.value DESCLIMIT 10Top 10 TransfersAtlético MadridAtlético Madrid and Real Madrid appear on several of these transfers.We can aggregate the queries involving these teams to see how much money has been involved in their transfers.Money In, Money OutThe following query finds the clubs that have spent and received the most money during the transfer window:MATCH (club:Club)WITH club,     apoc.coll.sumLongs(       [(club)<-[:FROM_CLUB]-(t) | t.value]) AS moneyIn,     apoc.coll.sumLongs(       [(club)<-[:TO_CLUB]-(t) | t.value]) AS moneyOutRETURN club.name,        apoc.number.format(moneyIn) AS in,        apoc.number.format(moneyOut) AS outORDER BY moneyIn + moneyOut DESCLIMIT 10Money In and Money OutAs we guessed, the Madrid clubs are at the top of the list. Surprisingly there aren’t any English clubs in the top 10.What about if we only look at the money spent on transfers?Money OutThe following query finds the clubs that have spent and received the most money during the transfer window:MATCH (club:Club)-[*2]->(country:Country)WITH club, country,     apoc.coll.sumLongs(       [(club)<-[:TO_CLUB]-(t) | t.value]) AS moneyOutRETURN club.name, country.name,  apoc.number.format(moneyOut) AS outORDER BY moneyOut  DESCLIMIT 10Money spent on transfersThe Spanish teams still dominate the top 3 positions, but interestingly Aston Villa have spent the most money of the English teams so far. Presumably that will change by the end of the summer.Next let’s go a level up, and see which countries money is flowing between.Money Flow by CountryMoney FlowThe following query finds the total fees spent moving players from teams in one country to another, excluding transfers between clubs that play in the same country.We also return the most expensive transfer between those countries:MATCH (t:Transfer)-[:OF_PLAYER]->(player),      (fromCountry)<-[:IN_COUNTRY]-(fromLeague),      (fromLeague)<-[:IN_LEAGUE]-(from)<-[:FROM_CLUB]-(t),      (t)-[:TO_CLUB]->(to)-[:IN_LEAGUE]->(toLeague),      (toLeague)-[:IN_COUNTRY]->(toCountry)WITH *ORDER BY fromLeague, toLeague, t.value DESCWITH fromLeague, toLeague, sum(t.value) AS totalFees,      fromCountry, toCountry,      collect({player: player.name, fee: t.value}) AS transfersWHERE fromCountry <> toCountryRETURN fromCountry.name, toCountry.name,        apoc.number.format(totalFees) AS total,        transfers[0].player AS player,       apoc.number.format(transfers[0].fee) AS fee,        size(transfers) AS numberOfTransfersORDER By totalFees DESCLIMIT 10Money flow between countriesThe most money has transferred from Portugal to Spain, although this is a bit skewed by the transfer of João Felix, which accounts for almost 60% of the money flow.Next StepsWe hope you enjoy the dataset and if you have any questions or suggestions on what we should do next let us know in the comments or send us an email to devrel@neo4j.com.;Jul 24, 2019;[]
https://medium.com/neo4j/why-you-should-ask-your-colleagues-to-keep-your-day-free-for-june-17th-2a97370d8021;Ljubica LazarevicFollowJun 4, 2021·4 min readWhy You Should Ask Your Colleagues to Keep Your Day Free on June 17We’re bucking the recent trend of pre-recorded talks with a fully immersive, fully interactive experience for NODES 2021. Tell your colleagues to leave you alone that day, because you will not want to miss out!SummaryWe are turning the virtual conference scene on its head. NODES 2021 will be a live, fully immersive, fully engaging experience, and to get the most out of it, skip watching the post-event recordings, and keep June 17 free so you can be there on the day.It’s Been a Tough 15 Months for Many of UsPandemic challenges aside, we’ve all had a hard time at different points over the past year. Our ways of working have been turned upside down. We’ve had to make big changes to our jobs and how we communicate with our colleagues.Magnet.me on UnsplashUnderstandably, many virtual events have gone down the pre-recorded route to combat many challenges, such as managing time zones worldwide and accommodating varying internet qualities.However, after many months of mostly online meetings, catching up with videos, and general disconnect, we’re all feeling the effects of zoom fatigue.For NODES 2021, we’re going to do something different. We’re bucking the trend, and we’re going live. The first NODES event, held in 2019, was a live event, and we’re going back to our roots.First of All, A Shout-Out to Our Community PartnersWe’re thrilled to have a great group of community partners supporting us to make NODES a more engaging event. Thank you all for your work to create a more inclusive culture in technology.Why Is NODES 2021 Different?94% of all the talks at NODES will be delivered live. We will have over 50 talks across five tracks, with presenters from the Americas, Asia, and Europe.Our five tracks are in a streaming set-up — if you want to watch all of the talks in the Best Practices track, you won’t need to lift a finger.We have an interactive Q&A panel made up of the speakers, community partners, and the Neo4j team to answer all your questions.We have scheduled breaks to allow you to pace yourself through the conference, take a breather, grab some snacks, and get to know your fellow attendees.We know that not everybody will be able to join in live for the whole day. Pre-NODES is back again this year with community guests from around the world, so no matter where you are, you can join us on the day. The Pre-NODES show will start at 7 a.m. UTC/12:30 p.m. IST/3 p.m. SGT.We are planning something very special for the main break, and you will not want to miss it. Watch this space for more information!The text and audio channels that will be available during NODESDon’t Forget Our Discord Server!As well as being able to use the chat room in each of the tracks on the streaming platform, we will also have dedicated text and voice channels on our Discord Server. The streaming tracks set-up this year will offer many ways to keep the conversation going throughout the day and beyond. The channels will become available shortly before the event.The Discord server isn’t only just for NODES! It’s a great place to meet fellow graphistas, join in with training classes, take part in exclusive sessions, and be a part of the graphy conversation.How to Get ReadyHopefully, you’ve been persuaded to join us on the day of NODES and take part in our fun activities. So on to the next question — how to get ready?!John Tuesday on UnsplashFirst of all, don’t forget to register!Plan your day! You can check out the agenda here. Think about what talks you’d like to watch live. While not everybody will be able to watch all of the content on the day-of, with Pre-NODES starting the show, followed by the main event, we hope there is something for everyone.Join us on the Discord server. Don’t forget to say hello in #introduce-yourself 😁Think about your environment. Can you relocate to a comfortable sofa? Perhaps you can stream the conference from your TV or tablet, or make other changes to make it feel different from the usual work from home” day.Make a day of it! Get your snacks and drinks ready for the day. Active in the community? Why not reach out to your network and form virtual viewing parties? Discord can accommodate private group chats for your online gathering.Finishing NotesWe know not everybody will be able to attend on the day, and that’s OK! It will still be possible to access all of the talks at a later date, but you will need to register to get access right after the event.Happy conferencing!;Jun 4, 2021;[]
https://medium.com/neo4j/create-a-data-marvel-develop-a-full-stack-application-with-spring-and-neo4j-part-1-350f0f7e6609;Jennifer ReifFollowNov 28, 2018·6 min readCreate a Data Marvel : Develop a Full-Stack Application with Spring and Neo4j — Part 1*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)My dad and I are now both developer advocates, and we have put together joint presentations the last couple of years to submit to conferences. Our goal is to present our joint project/presentation at least once together. This year, we wanted to leverage both of our amazing technologies to show something easy, powerful, and fun.My dad works with all things Spring, and I work with Neo4j. The two technologies are well-established and thriving, so our integration would encompass the seamless and simple development experience of Spring with the data structure of a graph database using Neo4j. There is also a great integration project that allows each technology to work with the other easily. Spring Data Neo4j allows Spring to understand the nodes and relationships of the graph database and allows Neo4j to connect and pass data to/from a full Spring application.This gave us a great foundation to build upon and a solid starting point. With my dad’s expertise in all things Spring and my growing knowledge in Neo4j, the only thing lacking was to find an intriguing plot for the basis of our presentation.Finding a data setTo me, the most fun demos are centered around an interesting data set that also showcases the technologies and their capabilities. So, first, we had to find a data set. This took some time. There were already a few popular data sets that Neo4j often used in demoing our technology, but we wanted something fresh and new to tackle. We also knew we didn’t want to use something too small or simple because showing something real-world is always more helpful and inspiring to others.* Note: medium-sized datasets also make it more likely to come across real data issues to work around to get it to function. First rule of data is that it is messy and often inconsistent when pulling from various sources.Dad found that Marvel publishes a large set of its comic data to an API that developers can access. With the recent release of the latest Avengers movie in May and our love of superheroes, it easily topped the list.* Note: You can find out more information about this API by going to developer.marvel.com.Now, would it work? APIs can be frustrating and fraught with gaps in or inconsistent data. While we later found that this API was no exception, we needed to do a little research up front to see if the data provided enough and had what we needed.API evaluationThe initial look at Marvel’s API showed that they had good documentation for connecting to various endpoints. There was substantial data to return and several connections among the entities for leveraging Neo4j nicely (relationships between entities are a main component in a graph!). I could even use their online documentation to test their endpoints with various parameters and verify the data results returned.Marvel’s general organization of the information, documentation, and site also bode well. This was a serious enough endeavor for Marvel that they invested the time and energy to create this developer portal and include docs, test endpoints, and other info. Needing an API key also showed that they cared who was hitting their data and how much, providing an exchange of their comic data for your email and a bit about your intended use. Our goal was to utilize their data in a personal project (non-commercial use) to show the best of two technologies in a fascinating use case, so it seemed like a fair trade to us. :)* Note: I also found that Marvel uses graph technology. One of their representatives even spoke at Neo4j’s GraphConnect conference a few years back! You can view the full recording of Peter’s presentation about the complicated data and chronology of the Marvel universe on vimeo.Constructing the data modelWe had completed our initial evaluation of the API, and it looked promising so far. At this point, I needed to create the data model for Neo4j so that we could import the data into a sensible structure.Since end points are often laid out like relational table structures (high-level categories with specific fields or columns returned), it was up to me to decide how each entity was related to each other. In the Marvel data, the main data entities include characters, comics, stories, events, series, and creators.There were a couple of different approaches I could take with those entities. I could focus on one central entity and how it relates to each of the other entities. I could also make some entities have relationships with multiple entities. For instance, comic issues could include characters, and series could contain comic issues, as well as feature certain characters.Neo4j allows the user to determine the best data model for the particular use case. Even if multiple users had the same business project, each party could come up with different data models, and Neo4j could support and handle each one! This allows you to build the data model that is best for your data and the usage — not based upon the structure and requirements of the database itself.After several iterations and whiteboard sessions”, I came to the data model in the image below. This model gave me some complexity due to multiple entities and types of relationships, but it also gave each entity only one relationship with one other entity. If the data became too complex, I could always ignore some of the entities.In this model, I decided to make the Comic Issue entity the center of the data model and relate all the other objects to that central node. In my mind, the other entities (like creator and series) made the most logical sense directly connected to a comic. Also, the Marvel API documentation mentions how each entity relates to the comic and retrieves some of each other entity when pulling a comic, so that further solidified my choice.What I LearnedWe now have a sensible data model for our Neo4j graph database, and we can look forward to importing the data from the API into the graph!It took some time and a few iterations to determine the best entities to include, the properties we wanted, and the relationship structure that worked best for this project. Below is a list of my key takeaways from the steps we have covered so far (API evaluation and data model creation).It took time to research the API structure and understand what we could or could not get from it.It took some thought and testing in the interactive documentation to come up with a data model that made sense, was interesting, and also didn’t overcomplicate the data.I learned a LOT from modeling a real data set. I wasn’t playing with a pre-existing model or something that had already been translated to graph before. I had to step through the process, just as any other project developer would for a new project.Next StepsIn the next posts, I will walk through the next phases of this project covering the data import and application development using Spring Data Neo4j, plus other details around the project and its current state. Stay tuned for more info!ResourcesFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifNeo4j Data ModelingDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j Guide;Nov 28, 2018;[]
https://medium.com/neo4j/neodash-2-0-a-brand-new-way-to-visualize-neo4j-ec8dee689e9b;Niels de JongFollowDec 14, 2021·5 min readNeoDash 2.0 — A Brand New Way to Visualize Neo4jVisualizing graph data is often key to the success of a Neo4j project. As part of my job, I always find that the best way to convince people of the value of graphs is to let them see and interact with their data. Next to a neat graph visualization, tables or traditional charts are an immensely powerful way to make your data visible. Put this all together in an interactive dashboard, and you’ve got an amazing way to tell a graph story.I developed NeoDash as an open-source dashboard builder to help you speed up this process. With some simple Cypher queries, NeoDash lets you directly visualize your Neo4j data as graphs, bar charts, tables, maps, and more. You can group your visualizations together in a dashboard, add interactivity, and share your dashboards with others.This post will go over the big changes in the new 2.0 release.Want to try out the application right away instead? Use NeoDash from your browser, or install it into Neo4j Desktop.Check out the project’s Github repository for details on other types of deployments, as well as pointers for extending the application. Enjoy!What’s New?The 2.0 release of NeoDash version comes with a ton of changes. Firstly, the application was rewritten completely (from scratch) to be more stable and performant. You’ll now be able to run NeoDash to report on your production database in real-time.The next sections will zoom in on the four main features that come with the new dashboard builder:A fancy new dashboard editorMore powerful visualizationsEasier customization & interactivityImproved saving/sharing of dashboardsA New Dashboard Editor for Neo4jThe NeoDash editor is a web-based application for building and viewing dashboards. It connects directly to your Neo4j database using the Neo4j JavaScript driver.Your dashboard can consist of a number of pages, each of which can have several reports. For each report, you specify a Cypher query that is used to create a visualization. For an interactive introduction to the editor, check out the video below:A big improvement to the 2.0 editor is the in-built Cypher editor for writing the report queries. This editor has live syntax highlighting, as well as checks for validating your query.Other new features include integrated documentation with query examples (see screenshot below), as well as easier customization of reports. More on this later.Powerful VisualizationsNeoDash 2.0 can render your query results in tables, bar charts, line charts, maps, and more. The new release comes with a canvas-based graph renderer (react-force-graph) that can handle drawing thousands of nodes/relationships, as well as support easy custom styling of nodes and relationships.The bar and line chart visualizations are now based on Charts, which supports a variety of styling options and increased interactivity.Drawing geodata? Try out the improved map report, which lets you draw any Neo4j Spatial Values using OpenStreetMap.Customization & InteractivityStarting with NeoDash 2.0, many report types come with an ‘advanced settings’ option that allows you to customize the visualizations. Customizations can be purely aesthetic (the color scheme used by the visualization) or impact the way that the results are processed. The image below shows how advanced settings can be used to create a stacked bar chart:customized stacked bar chartInteractivity can be added to the dashboard by means of ‘Parameter Select’ cards. These will allow dashboard viewers to dynamically specify parts of the Cypher queries that populate the reports.Check out the video above for instructions on how to set up these types of interactions.Saving and Sharing DashboardsOnce you’re done building your dashboard, you can save it directly to the Neo4j database. This will create a Neodash_Dashboard node that holds a text representation of the dashboard you put together.From the ‘Load Dashboard’ screen, everyone with access to the same database can load your dashboard into NeoDash.If you want to show off your new dashboard, you can generate a direct link to a standalone dashboard. These can act as a nice proof-of-concept for a Neo4j front-end. A standalone dashboard will automatically connect to a Neo4j database, disable editing, and hide the NeoDash editor. Users can then view all reports and interact with the data, but won’t be able to see the queries underneath.An example standalone dashboard generated from the Neo4j movie database. Use the sharing button on the editor sidebar to generate a standalone dashboard link.Wrapping UpYou can try out NeoDash yourself in your web browser, or by installing it into Neo4j Desktop. Suggestions and feedback are more than welcome in the Github Repository.Keep in mind that reporting is only a part of a graph visualization journey: Visualization tools like Neo4j Bloom will let data analysts go a lot deeper into exploring and analyzing graphs.If you’re running graph algorithms on huge datasets, you may need an offline tool such as Gephi to render high-volume graph visualizations.If you’re interested to learn more about the different types of Neo4j visualization tools, check out my blog post here.15 Tools for Visualizing Your Neo4j Graph DatabaseDiscover the best graph visualization tools you can use to visualize your graph database, including for development…neo4j.comHappy dashboarding!;Dec 14, 2021;[]
https://medium.com/neo4j/create-a-data-marvel-part-4-how-to-design-the-application-874ba6ea08a5;Jennifer ReifFollowDec 20, 2018·9 min readCreate a Data Marvel — Part 4: How To Design the Application*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)Throughout the last few blog posts in this series, you have seen how Mark Heckler and I began this coding project, chose a dataset, drafted a graph data model (Part 1), and imported the data (Part 2 and Part 3) from an API to a Neo4j database. This post will now focus on the application development and the choices and options available to us, as well as what we decided and why.We already knew we wanted to work with Spring and Neo4j before we began the project, but we needed to figure out how best to approach integration between the application and the data layers. First, we needed to consider our limitations and goals for this demo.We planned to submit it to conferences or meetups for presentation, so the whole kit-and-kaboodle needed to be explained in 30–60 minutes (90 minutes, if we had luxury). We also wanted to be able to live-code as much as possible and really show people how simple it is to use both technologies and what each brought to the table. This means that we needed to code as much as possible within the 30–60min time frame. Plus, we knew we would need some introduction to our project and data source.Could we show what we wanted and give the audience a guidebook for start-to-finish development and inspire them to want to walk out of the session and build it themselves? This is really the crux of designing good conference content. We needed to find a short, sweet, and simple way to build an application and easily connect to a graph data source. There were a few different paths we could take.We could write a Spring application and use a driver (such as Java) to connect from the application to a Neo4j instance. This means some configuration and setup code to connect the driver to the database and instantiate it in the application.Our other option (which we took) was to use the integration project of Spring Data Neo4j. This project was built by employees and community members to join the capabilities and ease of Spring Data with the power of graphs in a Neo4j backend. It includes pre-configured setup, as well as additional built-in functionality that seemed perfect for our needs. We will talk a bit more in detail about what this project gives us.The Joys of Spring DataSpring Data (per their documentation) aims to provide a familiar and consistent, Spring-based programming model for data access while still retaining the special traits of the underlying data store.” Now, this simply means that it was designed to plug and play easily with any datastore and not change the datastore’s unique structure or interactions.Spring Data needs to be extremely flexible to work with a variety of data sources — including relational, document, key-value, graph, and others. This is something the project accomplished, providing simple integrations with sources such as JDBC, JPA, MongoDB, Redis, Couchbase, Elasticsearch, and many others! For a full list of supported sub-projects within Spring Data’s high-level umbrella project, you can check out the documentation.Spring Data also provides features for various object-mapping methods per datastore needs and includes its fantastic repository! It also makes creating domain classes simple with several annotations. True to its goals, the project also includes simple integrations or more complex options, as well, based on needs.If you are familiar with the Spring ecosystem, you may know one of its goals is to reduce (and sometimes eliminate) boilerplate code. One helpful feature for this is that some methods can be derived for you without writing a query. Spring Data (no matter what data source it’s using) can derive queries for you based on repository method names. This sounds like Spring’s typical magic”, but we will look at this more in detail later.This impressive list of Spring Data integrations includes Neo4j using the project name Spring Data Neo4j and includes a variety of helpful functionality. We will look at what Spring Data Neo4j (SDN) can add to our demo next!The Simplicity of Spring Data Neo4jSpring Data Neo4j combines the Spring functionality of passing and grouping data with the power of property graph data storage and retrieval in Neo4j. The project supports the property graph data model, which organizes data into nodes and relationships. SDN also maps a graph to objects using the Object-Graph-Mapper (OGM) with annotations for mapping POJO entities.Another important functionality is its support of the Cypher Query Language. This allows the developer to write Cypher queries in the application and pass the query and data between layers over all the common Neo4j transport methods — binary protocol (BOLT), HTTP, and embedded.* Note: SDN also is able to be used with Neo4j’s causal clusters (multi-server cluster).Finally, Spring Data Neo4j helps us avoid writing so much boilerplate code, therefore streamlining the development process and allowing us to focus on the data and the application functionality! Remember that Spring Data also can derive dynamic queries based on the repository method names. This also means we don’t need to write a block of extra code for queries. We will show an example in upcoming paragraphs.Just to show what all is involved in the Spring Data Neo4j package and what it provides out-of-the-box, the diagram below outlines each layer and how communication passes between the application and the database.The Obvious ChoiceOk, so using Spring Data Neo4j completely made sense for what we needed. It would allow us to easily build a simple application with Spring magic” and reduce hassle in connecting to our Neo4j data source. It would handle mapping from our domain objects to our Neo4j property graph model elements (nodes and relationships) using a few simple annotations, and it could shortcut some of our basic methods by automatically deriving those queries.All-in-all, this integration could help us focus on the get-up-and-running steps within our allotted 30–60 minutes of presentation time! Let’s start building!Creating the ApplicationAnother added benefit to the Spring development ecosystem is the Spring Initializr. This page is a simple form that helps you design a Spring application with the needed dependencies, naming, language (Java, Kotlin, or Groovy), version, and format (maven or gradle). Based on your needs and choices, it generates the skeleton project for you with some basic folders and an application class outline. This is super helpful for hitting the ground running and creating a project before audience members’ eyes in mere seconds (go ahead, try it out!).Once you choose your project details, you can click Generate Project, and it will create the project and download it to your local device. You can then open the project in your favorite editor and start coding!For today’s post, we will generate a dummy project (leave the default group and artifact fields) with our 4 dependencies for the Marvel comic demo and step through the development.We want to include Lombok to trim repetitive code for getters, setters, and constructors. Lombok helps us move a little faster and automatically configures your classes with the appropriate getters, setters, and constructors based on simple annotations you can provide instead. You will see these in our code.We also need Neo4j so that Spring knows to connect to that as our data source, so we add that in, as well. The Web dependency handles several web client and server components with all supporting tasks, and Thymeleaf helps make our front end webpage pretty using templates.With all this info plugged in shown in the image below, we are off to a great start!The ProjectWe like to use JetBrains’ IntelliJ IDEA editor and typically use Maven projects, so we open the pom.xml file in IntelliJ, and the project structure opens up for us.If you open the pom.xml file at the bottom, you will see Spring Boot is incorporated, as well. Spring Boot makes it incredibly simple to create applications that you can run with very little Spring configuration. Certain niceties such as metrics and health checks are also available, should you want/need them. The painless bootstrapping ability and dependency versioning management are the core of Spring Boot’s popularity.That’s what the spring-boot-starter-parent handles for us. If you look on down into the dependency tree, you will see that we have spring-boot-starter-data-neo4j. That’s our Spring Data Neo4j module that includes Spring Boot!We also see Thymeleaf, the web package, and Lombok. Everything looks good, and we have all of our dependencies.Outside of the pom.xml, we have a bare-bones folder structure. Under the src folder, we have our main folder, and the test folder. We will not work much with the test folder at this time, but it is important to create tests for your applications and add them in this folder.The main folder contains all of our core domain model classes and repository interfaces. It is also where we have our resources folder to put html and other files later. This main folder is where the bulk of our code will go for this project.What I LearnedBefore I stepped through this part of the process, I knew how to build an application, but I didn’t know exactly why or what Spring was handling behind-the-scenes. This helped me understand how and why to make certain choices in design.I also relied much more on my dad’s wealth of knowledge on the Spring ecosystem and shortcuts, but we were both new to the Spring Data Neo4j integration. Below is a list of my key takeaways from the first step of the application development process.It took some research and pair programming sessions to figure out the best integration solution. I relied some on my dad’s depth of knowledge in the Spring topic.We had to plan out what we wanted to focus most on and what we wanted audience members to see and learn from it. Deciding to focus on code and building the solution was an easy choice for us, but determining what needed for context and what/how to trim other content was not so easy.While we both had certain knowledge in Spring or in Neo4j, neither of us had dealt with the Spring Data Neo4j integration project. We had to read docs, look at some examples, and experiment with it some. It was fun and rewarding, as well as not overwhelming.Next StepsThis part of the project often seems trivial and less important than data in the database or coding the application itself, but it’s still a critical piece of the puzzle. Knowing how and why two technologies integrate is part of every developer’s daily life these days.Having an integration project that is built to shortcut some of the hassles of bridging gaps and connecting two different technologies often makes life much easier. One other benefit is that a team may be familiar with one technology, so the bridge project take something new and combines it with existing knowledge on the comfortable topic. It is easier to work with each technology’s strengths, as well as on-board new developers.The next posts will step through the code in the application and show how it retrieves data from Neo4j and displays it nicely on a page. Stay tuned!ResourcesFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j GuideProject Lombok docsPrevious parts of this blog series: Part 1, Part 2, Part 3;Dec 20, 2018;[]
https://medium.com/neo4j/graph-modeling-labels-71775ff7d121;David AllenFollowOct 22, 2020·10 min readGraph Modeling: LabelsThis article is the latest in a series on advanced graph modeling the other articles in the series deal with keys, relationships, super nodes, and categorical variables.Today we’re going to talk about labels in Neo4j, what they are, how to use them, how they get abused, and how to avoid that. Neo4j’s data model is fundamentally called the Labeled Property Graph (LPG) Model”, and labels are pretty important.We will cover:What is a label? What do they mean?How Neo4j Treats LabelsHow should I use them in my models?What is a label? What do they mean?Labels are a kind of naming that can be applied to any node in the graph. They are a name only — and so labels are either present or absent. From graph database concepts:Labels are used to shape the domain by grouping nodes into sets where all nodes that have a certain label belongs to the same set.If you’ve ever used Cypher you’ve seen them: CREATE (p:Person { name:  David  }) specifies a label Person”. It can be any UTF-8 string that you like. By using back-ticks to escape your string in Cypher, you could have a(:`Longer Spaced Label`)Or you could even use table flipping guy (╯°□°)╯︵ ┻━┻ or emoji as your labels, though these wouldn’t be very convenient to type in Neo4j Browser, they can make for some fun with visualizations like Neo4j Bloom. 😉Set membershipThe quote above mentioned grouping nodes into sets”, and this is really the key concept to understand about what labels do for you. Consider this simple, unlabeled graph:An unlabeled graphNow what if we were to divide all of the nodes into two groups?Venn diagram of two kinds of nodesOK, that’s already more clear. So now a label in Neo4j is just a set membership marker”. Once we apply real Cypher labels, we get automatic coloring Neo4j browser shows us the same thing as the Venn diagram above, just that it’s easier to see what’s going on.Multiple set membershipSets can have subsets, so we could further divide our (:Job) nodes into (:Job:Technology) or (:Job:Healthcare) if we wanted. Any node can be in any number of sets that you want. This gets harder to represent visually with colors, but the concept is no different than the Venn diagram above.How Neo4j treats labelsEvery node, when stored on disk, has a few slots where Neo4j can store an identifier that lets it know which labels the node has. There’s a pre-allocated certain amount of storage (I believe it’s 4 slots). As labels are applied to a node, the database puts that label reference into one of those slots to provide a form of semi-free indexing, and for database statistics, to inform the Cypher query planner.Semi-Free” indexingBack to my toy database I was using for the images above, I created 10,000 extra nodes in addition to my handful of Job” and Person” nodes. Then I ran an EXPLAIN on the query MATCH (n { name:  A  }) RETURN n. What did the database do?Note the AllNodesScan at the topAt the very top, AllNodesScan is exactly what it sounds like. The database looks through every single node in the entire database (10,005 estimated) to find the ones we wanted. That’s pretty inefficient. But what if I we ask it to explain its plan for finding the (:Person) with the name A”? EXPLAIN MATCH (p:Person { name:  A }) RETURN pNodeByLabelScanThe result is a NodeByLabelScan which needs to consider an estimated 10 rows. This is exactly what’s meant by using labels as indexes — every time you specify one in a read query, the database has far less to look at to get the job done for you.The reason I’m saying they’re semi-free and not totally free is of course the database still has to maintain the label store. Writing those labels (like writing & maintaining an index) is more work than not doing it, so by applying labels to nodes you do more up-front work at write time, in order to win at read time.As a form of indexing, compare these two options and notice how they’re equivalent, but how the label is easier to read (it’ll probably also be more performant)./* Option 1: bad */CREATE (n:Everything { name:  A , nodeType:  Job  })CREATE INDEX nodeType_idx ON :Everything(nodeType)/* Option 2: much better, ultimately the same thing */CREATE (n:Job { name:  A  })Database StatisticsNeo4j maintains a count store” for holding count metadata for a number of things. The count store is used to inform the query planner so it can make educated choices on how to plan the query. Getting counts from the count store is just fetching a single number, so it goes very fast. For you (or the cypher query planner), the count store is handy. You can read more about it in this knowledge base article.In that EXPLAIN plan above, Cypher knew roughly how many nodes there were going to be because of the labels and the count store. In Halin, you can see the basics of the count store quite simply — the same stats the database sees.OK with all of that out of the way, it’s time to use this information and talk about how to build better labeled property graph data models.How should I use labels in my models?Sometimes the simplest advice is the best: 9 times out of 10, if you attach one and only one label to every node, you’ll do just fine, and don’t need further advice. It really can be that simple. The rest of this article deals with more complicated situations, and how to think in terms of using data modeling principles.The general principles are simple: Use labels to indicate semantic class of information and set membership, and use them to get the speed-ups of semi-free” indexing described above. To make this more concrete, let’s look at a few best and worst practices. Summarized, they are:Label every node, no exceptionsAlways have a query use case for a labelMultiple labels should be semantically orthogonalAvoid label overloadLabel every nodeThat means, use at least one label and avoid unlabeled nodes wherever possible. Unlabeled nodes are semantically indistinct What is a node like that even supposed to mean? And they’re harder to differentiate from other nodes.If you just stick with strictly one label per node, in 95% of cases, you’re good to go! Almost all of the rest of this article deals with the issues surrounding multiple labels per node. But if you don’t need them, you can stop here, you’re done.Always have a query use case for a labelDon’t design a data model if you don’t know what queries you want to ask of the database. If you have an idea that you would like to use label Foo, make sure it connects to a real question you need to ask of the database.One of the most important facets of data modeling:Data models exist to facilitate answering questions from the databases — they are not for creating pristine semantic models of domainsThis is a major mistake a lot of folks make, a source of a lot of data model errors and pain: Trying to create a great semantic model of a domain, instead of focusing on what questions they need the database to answer. Creating great semantic models is arguably futile, because a model is a map and the map is not the territory. We are data modelers, not philosophers, and we have questions we need this database to answer, yesterday.If you can’t figure which query will use the label you have in mind, don’t use it. You can always apply more labels later. (The YAGNI principle)Multiple labels should be semantically orthogonalSemantically orthogonal” is a fancy term that means that labels should have nothing to do with one another. Imagine we had a tiny report about an international business, showing the number of products it sells in different markets.The business region” (USA vs. EU) is visually and semantically orthogonal to the count (Customer vs. Product).There’s no real relationship between the concepts of customer & geography this is what might make them good intersecting set labels.And so labeling nodes (:Customer:USA), (:Customer:EU), (:Product:USA), (:Product:EU)might make for good data partitions. Notice how with orthogonal labels, we support 2 different use cases: we have an entire partitioned graph by geography. We can pull an entire sub-graph like this:MATCH (n:USA) RETURN nAnd yet the set intersection is also highly selective, and makes for faster queries:MATCH (p:Product:EU) RETURN pThese properties come from the orthogonality of the label semantics.Don’t overdo it — Avoid label overloadAbove we mentioned that Neo4j pre-allocates slots for about 4 labels. This much you get for semi-free” as we were describing but if you go over that limit, Neo4j starts having to allocate extra space just to store your pile of labels. And remember that usually you get your fastest query speedups by using the most specific label when you query so if you have 10 labels on a node, you’re usually just asking the database to do lots of unnecessary work that isn’t going to speed up your query.As a general rule of thumb, past 4 labels per node, expect overall performance to get worse, not betterIn the majority of well-thought out models I’ve seen, one or two labels per node is sufficient. Some exotic cases with good reasons might need three or four but above this, you should really question whether the model makes sense and is tuned to your actual query needs.Avoid class hierarchiesAn idea that people sometimes get about labels is to use them to model class hierarchies. This is often called inheritance” or IS-A” relationships. If you understood the section above about semantic orthogonality” you should immediately spot that class hierarchies are not semantically orthogonal.Imagine you have a zoo of information like this:It’s tempting to now go create a lot of nodes that are labeled (:Bat:Mammal:Animal), (:Pelican:Oviparous:Animal) and so on. Using our set membership” argument above, folks who do this will point out how easy it is to MATCH (a:Animal) as a set, sweeping in all of the crocodiles & whales, and so forth.This is generally not a good idea for a lot of reasons:Neo4j doesn’t enforce co-label constraints, i.e. which labels can occur together. Which means the database won’t give you any help in avoiding mammalian crocodiles, or non-animal whales, which are actually nonsense under your semantic model.It will tend to overload the number of labels as your hierarchy grows.You can do the same anyway with set intersection. If you just labeled everything with Mammalian or Oviparous, you could always get all of the super-class members (Animal) anyway, by MATCH (n) WHERE n:Mammal OR n:Oviparous.Most of the time, people like this model, but when they look into their actual query patterns, they don’t really have a use case for matching abstract super-classes in a real world query, and it’s just over-complicated. Class hierarchies are often a case of someone creating a semantic model as opposed to focusing on how to answer questions.If a node has an IS-A relationship somewhere (for example, a whale IS-A mammal) — use a relationship to a different node indicating Mammal. It should not be a label.Avoid composition relationshipsAlternatively, there are HAS-A” relationships, which can indicate ownership. Again, the word relationship” is in here, so if in your model you have two distinct classes of things (for example a Person” and a Car”) it can sometimes be tempting to model the relationship as a label, such as (:Person:CarOwner).Notice the name of that label, CarOwner a pattern to look for are the noun verbers”. (noun=Car verb=own). A noun verber” term in a data model is expressing a relationship. The better way to go with that is to use a relationship.Often you’ll find composition relationships won’t be semantically orthogonal. The good example above (:Customer:EU), (:Customer:USA) wasn’t a noun verber” example. It wasn’t composition (EU doesn’t have” a customer, the business does), and it also wasn’t inheritance (EU isn’t a customer), which is part of why it was orthogonal.ConclusionFollowing these guidelines is a good way to get the best balance of clarity and performance in your model, and should help whether your model is just a simple toy, or a giant enterprise model.Happy graph hacking!This article is part of a series if you found it useful, consider reading the others on keys, relationships, super nodes, and categorical variables.;Oct 22, 2020;[]
https://medium.com/neo4j/introducing-the-new-twitch-sandbox-bdda36a946bb;Tomaz BratanicFollowOct 29, 2021·3 min readIntroducing the New Twitch SandboxAnalyze Twitch streamers and their audiences with Neo4j APOC and Graph Data SciencePhoto by ELLA DON on UnsplashTwitch is one of the newer social network platforms. The idea behind the platform is that it allows anyone to stream or broadcast content. Other users can then support streamers through subscriptions and donations. A couple of months ago, I wrote a series of blogs covering constructing the Twitch social network and then performing network analysis on top of it.Twitchverse: A network analysis of Twitch universe using Neo4j Graph Data Sciencetowardsdatascience.comNeo4j Twitch SandboxToday, I am happy to announce that the Twitch dataset was introduced into the hall of Neo4j Sandbox fame.What does that mean for you?You can dive into the basics of network analysis without having to download, install, and configure a Neo4j environment. Not only that, but after you open the Twitch project in Neo4j Sandbox, you’ll have an interactive browser guide waiting for you that will help you get started with graph analysis and algorithms.Select the Twitch project on Neo4j Sandbox. Image by the author.A sandbox lasts for three days but can be extended to a maximum of 10 days. As mentioned, once you open the Neo4j Browser, you’ll have the browser guide waiting for you to take you through the network analysis.Browser guide in Neo4j Sandbox. Image by the author.By walking through the browser guide, you’ll learn how to use Cypher query language to evaluate overall network statistics, use PageRank to determine the most influential streamers. In the last part, you’ll use the Node Similarity algorithm to analyze which streamers have the highest overlap of viewer audiences.Viewer audience overlap of Twitch streamers. Image by the author.You can then also use Neo4j Bloom (as in the image above) in Sandbox to visualize your algorithmic results and color/style/size your nodes and relationships based on the computed metrics.If you have some ideas how to analyze the data that are not included in the guide, you can experiment on your own and come up with new insights.Final WordsIf you have some experience with Neo4j and graphs, or even if this is your first contact with graphs, but want to embark on the network analysis journey, the Twitch sandbox is the perfect first step.;Oct 29, 2021;[]
https://medium.com/neo4j/gaming-the-christmas-market-a70963bec154;David BartonFollowJan 23, 2019·10 min readGaming the Christmas MarketIntroductionThe Bath Christmas Market is an annual extravaganza, when the city of Bath is transformed into a veritable Winter Wonderland offering a selection of gift chalets for all your Christmas purchasing requirements. Or at least that’s one perspective. For me, long in the tooth and a little bit grumpy, it’s not quite so evocative. A log jam of people shuffling between chalets with their Christmas spirit disappearing faster than the mince pies and hot toddy. When it comes to Christmas shopping, a high focus on efficiency is what is required!This mini project uses the Neo4j graph database to determine the optimum route through the Christmas Market, given a set of mandatory chalets to purchase from. (a.k.a. The Travelling Salesman Problem). It gives a user friendly visual of the route using Neo4j Desktop, making use of APOC’s virtual nodes and relationships.The project is written using Neo4j 3.4.10 and licensed under Apache License 2.0. Full source code is available, and the original blog post is published at https://github.com/dbarton-uk/christmas-marketUse CasesThe project looks to address the following two use cases:Find Optimum Route: Given a set of chalets to visit, define an optimum travel route between the chalets such that each of the set is visited at least once.Visualize Route: Given the optimum route, provide a user friendly visual of the route.Setting up the DataOverviewThe Christmas Market is split into zones, each defined by a unique name. A zone hosts a number of chalets, each with a unique name and number. A chalet has a description and is categorized by the type of gift that it sells. Links between chalets have been manually defined, with a cost assigned to each link. These links and their associated costs are used to determine the optimal route to take when visiting a given set of chalets.60 chalets across 8 zones are defined, with the data sourced originally sourced from The Bath Christmas Market website The raw data is available in the spreadsheet.For reference, the original map of the market is here:Create constraints and indexesRun create_constraints.cql to setup constraints and indexes.CREATE CONSTRAINT ON (z:Zone) ASSERT (z.name) IS NODE KEY CREATE CONSTRAINT ON (c:Chalet) ASSERT (c.number) IS NODE KEY CREATE CONSTRAINT ON (c:Chalet) ASSERT c.name IS UNIQUECREATE CONSTRAINT ON (c:Chalet) ASSERT c.sequence IS UNIQUELoading the dataThe schema is shown below.SchemaFirst run load_chalets.cql.LOAD CSV WITH HEADERS FROM https://raw.githubusercontent.com/dbarton-uk/christmas-market/master/data/Chalets-Chalets.csv AS csv CREATE (c :Chalet {   sequence: toInteger(csv.Id),   number: toInteger(csv.Number),   name: csv.Name, description: csv.Description,   zone: csv.Zone,   category: csv.Category }) MERGE (z:Zone { name: csv.Zone}) WITH c, z CALL apoc.create.addLabels(   id(c),   apoc.text.capitalize(apoc.text.camelCase(z.name))] ) YIELD node MERGE (z) -[:HOSTS]-> (c)Added 68 labels, created 68 nodes, set 308 properties, created 60 relationships, completed after 540 ms.The load chalet script does the following:Creates chalet nodes based on the extracted chalet csv data.Create zone nodes based on zone data.Adds zone labels to chalet nodesLinks zones to chalets with a :HOSTS relationship.The chalets are split into 5 categories: Clothing and Accessories, Food and Drink, Gifts and Homeware, Health and Beauty and Home and Garden. Zone names are added as redundant labels to improve Neo4j Desktop visualization options.Next run load_links.cql.LOAD CSV WITH HEADERS FROM https://raw.githubusercontent.com/dbarton-uk/christmas-market/master/data/Links-Links.csv AS csv MATCH (c1:Chalet { sequence: toInteger(csv.from)}) MATCH (c2:Chalet {sequence: toInteger(csv.to)}) MERGE (c1) -[:LINKS_TO {cost: toInteger(csv.cost)}]-> (c2)Set 87 properties, created 87 relationships, completed after 347 ms.The script creates the links between chalets based on the extracted link csv data. A cost is defined for each link, which is used by the algorithm when calculating an optimal route.Checking the dataOk, so, let’s see what we have. First the chalets.MATCH (c :Chalet) RETURN   c.number as Number,   c.name as Name,   c.description,   c.category as Category,   c.zone as Zone ORDER BY c.zone, c.category, c.numberChalets, Categories and ZonesNext, the intra-zone links.MATCH p = (c1:Chalet) -[:LINKS_TO]-> (c2:Chalet) WHERE c1.zone = c2.zone RETURN pIntra-Zone LinksAnd finally, the inter-zone links.MATCH p = (c1:Chalet) -[:LINKS_TO]-> (c2:Chalet) WHERE c1.zone <> c2.zone RETURN pInter-Zone LinksUsing zone as a label, means that in Neo4j Desktop, we can colour each chalet by zone. Lovely Jubbly. 👍Optimizing the routeChoosing the giftsSo now that we are set up and ready to go, let’s choose the chalets that we are going to purchase gifts from.For Bro, something to share (no. 109)For Nan, something for the garden. (no. 24)For Grandpa, something tasty (no. 169)For Toby the dog, some doggy treats (no. 89)For the Kids, something that won’t get me in trouble (no. 32, no. 184)For the Trouble and Strife, something to keep her warm and sweet (no. 181, no. 19)WITH [  {number: 109, for:  Bro },   {number: 24, for:  Nan },   {number: 169, for:  Grandpa },   {number: 89, for:  Toby },   {number: 32, for:  The Girl },   {number: 184, for:  The Boy },   {number: 181, for:  The Missus },   {number: 19, for:  The Missus } ] as gifts UNWIND gifts as gift MATCH (c:Chalet { number: gift.number}) RETURN   gift.number as Number,   gift.for as For,   c.name as Name,   c.description as Description,   c.zone as Zone,   c.category as Category ORDER by c.zone, c.numberGift SelectionThe AlgorithmSo to the main event. Let’s find the optimal route around the market.Here is the bad boy cypher statement, augmented with steps. An explanation is given below.// Step 1 WITH [109, 24, 169, 89, 32, 184, 181, 19] as selection MATCH (c:Chalet) WHERE c.number in selection WITH collect(c) as chalets UNWIND chalets as c1 WITH   c1,   filter(c in chalets where c.number > c1.number) as c2s,   chalets UNWIND c2s as c2 CALL algo.shortestPath.stream(  c1,   c2,   cost,   {relationshipQuery: LINKS_TO}) YIELD nodeId, cost WITH   c1,   c2,   max(cost) as totalCost,   collect(nodeId) as shortestHopNodeIds,   chalets MERGE (c1) -[r:SHORTEST_ROUTE_TO]- (c2) SET r.cost = totalCost SET r.shortestHopNodeIds = shortestHopNodeIds // Step 2 WITH   c1,   c2,   (size(chalets) - 1) as level,   chalets CALL apoc.path.expandConfig(  c1,   { relationshipFilter: SHORTEST_ROUTE_TO,     minLevel: level,     maxLevel: level,     whitelistNodes: chalets,     terminatorNodes: [c2],     uniqueness: NODE_PATH } ) YIELD path WITH nodes(path) as orderedChalets, extract(n in nodes(path) | id(n)) as ids, reduce(cost = 0, x in relationships(path) | cost + x.cost) as totalCost, extract(r in relationships(path) | r.shortestHopNodeIds) as shortestRouteNodeIds ORDER BY totalCost LIMIT 1 // Step 3 UNWIND range(0, size(orderedChalets) - 1) as index UNWIND shortestRouteNodeIds[index] as shortestHopNodeId WITH   orderedChalets,   totalCost,   index,   CASE     WHEN shortestRouteNodeIds[index][0] = ids[index]     THEN tail(collect(shortestHopNodeId))     ELSE tail(reverse(collect(shortestHopNodeId)))     END as orderedHopNodeIds ORDER BY index UNWIND orderedHopNodeIds as orderedHopNodeId MATCH (c: Chalet) where id(c) = orderedHopNodeId RETURN   extract(c in orderedChalets | c.name) as names,   extract(c in orderedChalets | c.number) as chaletNumbers,     [orderedChalets[0].number] + collect(c.number) as chaletRoute,   totalCostThe algorithm should be considered in three steps. Each step is explained below.Step 1In Step 1, the shortest path between each of the chalets, is calculated using the graph algorithm algo.shortestPath.stream procedure.  SHORTEST_ROUTE_TO  relationships are merged between each distinct pair of selected chalets, with the total cost of the shortest path and the hops of the shortest path stored on the relationship. Some optimization is achieved by ensuring the shortest path between a pair of nodes is only calculated in one direction.To get a picture of the result of Step 1, running the following query give us the output of step 1.WITH [109, 24, 169, 89, 32, 184, 181, 19] AS selection MATCH p = (c1) -[r:SHORTEST_ROUTE_TO]- (c2)WHERE c1.number in selection AND c2.number in selection RETURN   c1.number as Chalet1,   c2.number as Chalet2,   r.cost as Cost,   r.shortestHopNodeIds as Hops ORDER BY Chalet1, Chalet2Shortest Routes between selected chaletsStep 2Step 2 of the algorithm uses these newly calculated costs to return the shortest path that includes all of the selected chalets. The step makes use of APOC’s apoc.path.expandConfig procedure, using SHORTEST_ROUTE_TO relationships to determine paths. Resultant paths are ordered by total cost.The path expander config constrains the path expanding algorithm to ensure that all of the selected chalets nodes are visited only once within the SHORTEST_ROUTE_TO mesh. It does this by ensuring that the number of levels traversed (minLevel and maxLevel) is equal to the number of chalets - 1 (7 in this case), and that all nodes traversed are unique. whitelistNodes limits the paths to the chalet selection, which provides some optimisation.The path returned with lowest cost is the route that we want to take.Step 3Step 3 of the algorithm prepares the data for return, and provides a list of the full route through the chalets. It uses the shortestHopNodeIds calculated in Step 1, it tidies up the route removing duplicates and it ensures a consistent direction.For our selected chalets the resulting of running the algorithm is:Optimal RouteNote: You may need to run the algorithm twice before getting consistent results! I have raised an issue here which describes the problem in more detail. If anyone has any further insight here, please let me know!So there we have it. An optimal route through the Christmas Market, stopping at all selected chalets. Now let’s see if we can get a good visual of the route, using the functionality of Neo4j Desktop.Visualizing the routeIn this second section, we will look at how we can display the route calculated by the algorithm in the first section. The idea is to produce a useable route planner, that guides the user from chalet to chalet.Here is the wishlist of useful things to display in the the route planner.The chalets in the route.The order in which to traverse the chalets.Entry and exit points for the route.Chalets in the same zone coloured the same.Zone nodes attached to chalets to indicate whenever there is a zone change.Chalet names and numbers displayed.Selected gift chalets should be indicated.Here is the visual of the route, satisfying the wishlist above.APOC’s virtual nodes and relationships help achieve this custom visual. The cypher code is shown and explained below.WITH   [169, 109, 19, 24, 32, 184, 89, 181] AS selection,   [169, 108, 109, 134, 127, 126, 123, 119, 13, 15, 16, 17, 18, 19,    24, 45, 88, 34, 32, 74, 78, 83, 184, 83, 78, 74, 89, 176, 181]   AS route MATCH (chalet :Chalet) WHERE chalet.number IN route WITH route, chalet,      CASE        WHEN chalet.number in selection        THEN Selected        ELSE NotSelected        END AS selected,      CASE       WHEN chalet.number in selection       THEN * +chalet.number+: +chalet.name       ELSE chalet.number+: +chalet.name       END AS title CALL apoc.create.vNode(  [selected] + labels(chalet),   {title: title}) YIELD node WITH   route,   collect(chalet) AS chalets,   collect(node) as vChalets CALL apoc.create.vNode(  [EntryExit],   { ee: Enter}) YIELD node as enter CALL apoc.create.vNode(  [EntryExit],   { ee: Exit}) YIELD node as exit MATCH (firstChalet :Chalet { number: head(route)}) MATCH (lastChalet :Chalet { number: last(route)}) CALL apoc.create.vRelationship(  enter,   VIA,   {},   vChalets[apoc.coll.indexOf(chalets, firstChalet)] ) YIELD rel as enteringVia CALL apoc.create.vRelationship(  vChalets[apoc.coll.indexOf(chalets, lastChalet)],   VIA,   {},   exit ) YIELD rel as exitingVia WITH   apoc.coll.pairs(route) as hops,   chalets,   vChalets,   enter,   exit,   enteringVia,   exitingVia UNWIND hops as hop MATCH (from :Chalet {number: hop[0]}) -[l:LINKS_TO]- (to :Chalet {number: hop[1]}) CALL apoc.create.vRelationship(   vChalets[apoc.coll.indexOf(chalets, from)],   NEXT,   properties(l),   vChalets[apoc.coll.indexOf(chalets, to)] ) YIELD rel as next CALL apoc.create.vNode(  [null],   { name: to.zone }) YIELD node as zoneCALL apoc.create.vRelationship(  zone,   HOSTS,   {},   vChalets[apoc.coll.indexOf(chalets, to)] ) YIELD rel as hosts WITH   chalets,   vChalets,   enter,   exit,   enteringVia,   exitingVia,   collect(next) as nexts,   hops MATCH (startZone :Zone) -[:HOSTS]-> (startChalet:Chalet { number: hops[0][0]}) CALL apoc.create.vNode(  [Zone],   properties(startZone)) YIELD node as vStartZone CALL apoc.create.vRelationship(   vStartZone,   HOSTS,   {},   vChalets[apoc.coll.indexOf(chalets, startChalet)] ) YIELD rel as startHost UNWIND hops as hop MATCH (zone1 :Zone) -[:HOSTS]-> (from:Chalet { number: hop[0]}) MATCH (zone2 :Zone) -[:HOSTS]-> (to:Chalet { number: hop[1]}) WHERE apoc.coll.different([zone1, zone2]) CALL apoc.create.vNode(  [Zone],   properties(zone2)) YIELD node as zone CALL apoc.create.vRelationship(  zone,   HOSTS,   {},   vChalets[apoc.coll.indexOf(chalets, to)]) YIELD rel as host RETURN   vChalets,   enter,   exit,   enteringVia,   exitingVia,   nexts,   vStartZone,   startHost,   collect(zone) as zones,   collect(host) as hostsFirst chalets are matched based on the route calculated by the algorithm in Section 1. Chalets are then marked as Selected” or NotSelected” based on whether or not they also exist in the original selection list. The text to display on each chaletnode is defined by title and virtual nodes are created for each chalet with labels and properties reflecting the matches.Virtual nodes for entry and exit points are created, and linked with a virtual VIA relationship to the first and last chalets in the route.Virtual NEXT relationships are created to show the hops between each chalet. The NEXT relationship provides directionality from chalet to chalet through the route.Virtual nodes are created for the zone, and attached to a virtual chalet nodes whenever there is a zone change.ConclusionAnd that’s it, typically it took a little longer than I expected and it’s a bit late now for the 2018 Christmas Market. Maybe next year!I hope you enjoyed it, and any questions or comments please feel free to drop me a line.Originally published at https://github.com/dbarton-uk/christmas-market;Jan 23, 2019;[]
https://medium.com/neo4j/building-a-similarity-graph-with-neo4js-approximate-nearest-neighbors-algorithm-1398583b280b;Mark NeedhamFollowOct 1, 2019·6 min readBuilding a similarity graph with Neo4j’s Approximate Nearest Neighbors AlgorithmIn version 3.5.11.0 of the Neo4j Graph Algorithms Library we added the Approximate Nearest Neighbors or ANN procedure.ANN leverages similarity algorithms to efficiently find more alike items. In this post, we’ll look at our motivation for creating this algorithm, where it can be used, and will show how to use it with the help of a worked example.You can download the Graph Algorithms Library from the Neo4j Download Center or install it into Neo4j Desktop via the Graph Algorithms Playground Graph AppNearest NeighborsIt’s now been almost a year since we added similarity algorithms to the Neo4j Graph Algorithms Library, and they’ve quickly become some of the most widely used ones in the library.A popular use case is creating a nearest neighbor graph or similarity graph. Such a graph contains relationships between two nodes a and b if the distance between aand bis among the k-th smallest distances from ato all other nodes.The distance between nodes would be computed by using one of the similarity algorithms to compare attributes such as schools attended, movies rated, or product purchased in common.This graph may then be used as part of a recommendation system. We might make recommendations in the following domains:e-commerce — find people similar to me and recommend products that they bought but that I haven’t yet boughtcontent — find articles similar to each other and recommend them to readersphotos — if you like this image you might also like these imagesOk that sounds useful, but why do we need this ANN thing?While this approach does work, it’s computationally expensive.We have to compare every item with every other item, which gives us O(n^2) complexity or (numberOfNodes*numberOfNodes-1)/2 computations. It’s very much a brute force approach.For example, if we wanted to compute the similarity of 10,000 items, we’d need to do:10,000*9,999 / 2 = 49,995,000computationsThat doesn’t sound too bad, but what if we increase it by a factor of 10, and now want to compute the similarity of 100,000 items? We’d now need to do:100,000 * 99,9999 2 = 4,999,950,000 computations, which would take a long time to complete.Introducing Approximate Nearest Neighbors (ANN)The Approximate Nearest Neighbors algorithm reduces the amount of computation needed to build a similarity graph because we are no longer comparing every node with every other node.A conceptual explanation of approximate nearest neighborsWe’ve implemented an algorithm called NN-Descent, which is described in the Efficient K-Nearest Neighbor Graph Construction for Generic Similarity Measures paper.The paper describes a simple but efficient algorithm for creating nearest neighbor similarity graphs.Pseudo-code for the algorithm can be seen in the diagram below:The NN Descent AlgorithmThe trade off with this algorithm is that we are computing the approximate nearest neighbors, rather than absolute nearest neighbors as with the brute force approach.Recall DefinitionHaving said that, we’ve found that we get a recall rate of 90% or above on the test data sets on which we’ve tried out the algorithm.When should we not use it?As with most things, ANN isn’t a silver bullet, so we don’t want to use it everywhere.If we’re trying to compute a nearest neighbors graph for a small data set, the brute force approach still makes sense. On very tiny data sets containing 10s of nodes, the ANN approach will require more computation and produce worse results.Equally, if we’re making for high-stakes decisions, e.g. if you’re trying to estimate a medical prediction, you might be very sensitive to false positives and the longer compute time of the brute force approach would be acceptable.Let’s see it in actionThe ANN Benchmarks GitHub repository contains data sets that we can use to test our algorithm.We’ll use with the Fashion-MNIST dataset, which contains 60,000 of Zalando’s article images.Each example has a 784 digit embedding that represents the pixels in the image.Importing Fashion MNISTThe data is in HDF5 format, so we’ll need to convert that into CSV format before we can more import into Neo4j. We’ll then use the apoc.load.csv procedure to create a node representing each image and store the embedding in the property embedding on the node.Importing the Fashion MNIST Data setFashion MNIST Data set in Neo4jBuilding a brute force nearest neighbors graphOnce we’ve got the data imported we’re ready to create our nearest neighbors similarity graph. We’ll start with the brute force approach, which we can do by executing the Euclidean Distance Similarity algorithm:MATCH (p:MNISTItem)WITH {item:id(p), weights: p.embedding} as userDataWITH collect(userData) as dataCALL algo.similarity.euclidean(data, {  topK:20, write:true,   showComputations: true,   writeRelationshipType:  SIMILAR20  })YIELD nodes, similarityPairs, computationsRETURN nodes,        apoc.number.format(similarityPairs) AS similarityPairs,        apoc.number.format(computations) AS computationsWe’ve used some config parameters in this procedure call. Let’s explore those:topKspecifies the upper bound for the number of similar nodes to findwriteRelationshipType specifies the relationship type to use for our nearest neighbor graphshowComputations keeps track of the number of computations done in generating the nearest neighbor graphMNIST Similarity GraphBuilding an approximate nearest neighbors graphAnd now let’s do the same thing using our new Approximate Nearest Neighbors procedure:MATCH (p:MNISTItem)WITH {item:id(p), weights: p.embedding} as userDataWITH collect(userData) as dataCALL algo.labs.ml.ann( euclidean , data, {  topK:20, write:true, showComputations: true, iterations: 50,   writeRelationshipType:  SIMILAR20_APPROX,   p: 0.5})YIELD nodes, similarityPairs, computations, iterations, scanRateRETURN nodes,        apoc.number.format(similarityPairs) AS similarityPairs,        apoc.number.format(computations) AS computations,        iterations,        scanRateThere are a few config options of interest in this procedure call:iterations specifies the upper bound for number of iterations that the algorithm will executetopKspecifies the upper bound for the number of similar nodes to findp specifies the sample rate i.e. how many relationships to consider when sampling the graph. For example, if topK is 20 and p is 0.5 we’d sample 20*0.5 = 10 relationships.writeRelationshipType specifies the relationship type to use for our nearest neighbor graphshowComputations keeps track of the number of computations done in generating the nearest neighbor graph. We’ll use this so that we can compare the number of computations against the brute force approach.Evaluating the approximate nearest neighbors graphNow we’re going to compare our approximate nearest neighbors graph with the bruce force variant. The following query:iterates over each example, collecting both the brute force and approximate neighbors into listsfor each example, computes the intersection of those listsfor each example, computes a recall value i.e. how many of the items in the approximate neighbors list were in the brute force neighbors listcomputes the average recall value across all examplesmatch (r:MNISTItem)WITH r,     [(r)-[:SIMILAR20]->(i) | id(i)] AS bruteForce,     [(r)-[:SIMILAR20_APPROX]->(i) | id(i)] AS approxWITH r, bruteForce, approx,      apoc.coll.intersection(approx, bruteForce) AS intersectionWHERE size(bruteForce) > 0WITH r,      size(intersection) * 1.0 / size(bruteForce) AS recall,      bruteForce, approxreturn avg(recall)We create approximate nearest neighbors graph using different sampling rates, and the following table shows number of computations, computation time, as well as recall for each of these configurations:Brute Force vs ANNSummaryIn this post we’ve learnt how to use the approximate nearest neighbor algorithm in the Neo4j Graph Algorithms Library.If you enjoyed learning how to apply graph algorithms to make sense of data, you might like the O’Reilly Graph Algorithms Book that Amy Hodler and I wrote.You can download a free copy from neo4j.com/graph-algorithms-book;Oct 1, 2019;[]
https://medium.com/neo4j/python-packages-licenses-analysis-with-neo4j-c4269ae0ef14;Tom NijhofFollowJun 27, 2022·3 min readPython Package Licenses Analysis With Neo4jDo you ever wonder what licenses you need for your Python project? Me too! So I made a tool to figure it out.In short, every package has its license, but so do it’s dependencies and those dependencies’ dependencies. Creating a graph of dependencies is where Neo4j comes into play.ExampleLink: wagenrace.github.io/python_dep_frontendAs an example, with a project using only Pandas, it shows you that you need three licenses for Pandas and its five dependencies. The dependencies are direct or indirect.Pandas does NOT require Six, but python-dateutil does.A screenshot of Python Project Insight with a project using only PandasDatabaseThe data needs to be scraped, stored, and accessed.ScrapingFirst the data needed to be scraped (code). Luckily every PyPi package has a special JSON page. For example, pypi.python.org/pypi/tomni/json holds the requires_dist, license, and package size.Two design choices were made: every requirement with extra ==” was ignored, and the biggest package size was always taken.As a starting point, I used the hugovk project with the top 5,000 packages. An extra 315 appeared because some dependencies were not on the list.The last step was manually combining different writings of the same license, given this lookup table.For example, 3-clause bsd license” was written in 10 different ways:3-clause bsd licensebsd 3-clausebsd-3-clause3-bsdbsd 3bsd (3-clause)bsd 3 clause3-clause bsd <http://www.opensource.org/licenses/bsd-license.php>bsd 3-clause license3-clause bsdStoring the dataThe data is connected and the connection is important to find all the dependencies. This means that graph databases form a very natural fit.Using the data importer, I created a very simple model: packages can depend on other packages.Licenses were not made into their own node because I did not want to find packages based on their license, and the free Aura Cloud had a limit (200k nodes/400k relationships) on the number of nodes that I wanted to use for packages!PS: I did not use all the nodes (yet). Not even close. 5.317/200.000 were used.Graph model in data importerAccessingIt will not surprise people to learn I like Python. So to set up a rest API, I used Fast-API (four licenses needed). And to access the Neo4j I use py2neo (four licenses needed).These two together (five licenses needed) are the whole back-end hosted by dot-asterisk.I used to fight with Cypher because it collected my packages per license. But this turned out to be a great feature!The query looks for all the packages the start packages depend on with any number of packages in between.This list is then DISTINCT (removing duplicates) and collected per license. The only downside is that the size in bytes is also collected per license. The front end can deal with that though.MATCH (n:Package)-[:DEPENDS_ON*0..]->(m:Package)                                         WHERE n.name in [ fastapi ,  py2neo ]                                        WITH DISTINCT m as p                                         RETURN DISTINCT p.license as licenses, collect(p.name) as packageNames, sum(p.package_size) as totalSizeBytesConclusionThe whole setup works, and now it’s very easy to find out what licenses are needed for your project (if your project only uses the 5,317 most downloaded packages).I don’t know whether thinking about all the licenses you need is helpful or just another useless thing to worry about. However, there is now a tool for it!Other findingsThe most popular license is MIT (42%), followed by Apache (Apache + Apache V2 = 26%), then BSD (16%), and then UNKNOWN (3%).Adding all licenses with GNU together (13 different names with V2, V3, GPL, LGPL, affero, ect.) is 6% of all licenses.Python, Mozilla, and ISC licenses are all below 1%, yet still above other/proprietary licenses.”;Jun 27, 2022;[]
https://medium.com/neo4j/neo4j-online-developer-summit-nodes-2019-coming-up-3b46285a8068;Elaine RosenbergFollowSep 19, 2019·2 min readNeo4j Online Developer Summit (NODES 2019) coming up!Join us Oct 10 for our world-wide free event from your homes and offices.Our first-ever online conference for Neo4j Developers, NODES takes place on Thursday, October 10, 2019. We hope that you will tune in and attend the sessions that interest you.The conference will begin with a Keynote Address by Emil Eifrem, CEO and co-founder of Neo4j, where he will announce some exciting news around Neo4j.Register today for the free eventThis conference has 37 30-minute sessions and 18 10-minute lightning talks that occur concurrently throughout the day in 4 tracks. The presenters are Neo4j Community members and Neo4j staff representing 10+ countries. You can watch a session live, or you will be able to watch any session as recorded session.In addition to attending this online conference by yourself, there are a number of Viewing Parties” that will watch the Keynote Address together and a selection of other sessions. You can see information about the Viewing Parties at the conference site.After you register for the online conference, you will receive emails with instructions for watching live and recorded sessions.Here is the schedule for the sessions and lightning talks of this online conference (times are EDT in the U.S.):8:00: Keynote Address (Emil Eifrem, Neo4j)9:30: GraphHack Winners Announcement (Karin Wolok, Neo4j)10:00–10:30 sessions:Intro to Neo4j for Developers (Jennifer Reif, Neo4j)Databases on Kubernetes Using a Custom Operator (Johannes Unterstein, Neo4j)Making Graph Algorithms ‘Clique’ (Joe Depeau, Neo4j)Ten to Dine: Building Possibility Spaces with Neo4j and ReactJS (Brandon Campbell, Northrop Grumman)10:00–10:10 lightning talk: It Depends: And Why It’s the Most Frequent Answer in Modeling (Luanne Misquitta, GraphAware)10:20–10:30 lightning talk: NeoSemantics, A Linked Data Toolkit for Neo4j (Jesus Barrisa, Neo4j)10:45–11:15 sessions:Visualizing Graph Data in JavaScript (Irfan Nuri Karaca, Neo4j)Neo4j Bolt Driver Architecture Now and in the Future (Nigel Small, Neo4j)Building Spatial Search Algorithms for Neo4j (Craig Taverner, Neo4j)Software Applications are Graphs: How Structr Works (Axel Morgner, Structr GmbH)10:45–10:55 lightning talk: We’re Not Just Relational Anymore: Teaching Intro to Neo4j as a Course (Risa Myers, Rice University)11:05–11:15 lightning talk: Predicting and Prosecuting Crime in Rio de Janeiro: An ML Story (Daniel Belchior, Public Prosecutor Office, Brazil)MORE HERE….We are looking forward to having your join us for NODES 2019!;Sep 19, 2019;[]
https://medium.com/neo4j/orai-project-planning-with-neo4j-eca26ecab56f;CristinaFollowJan 6, 2022·5 min readOrai: Project Planning with Neo4jA student project was developed though Florida Atlantic University’s Senior Design Program by student team Logic13.BackgroundSprint cycles are an extremely common way for development teams to plan and execute the development of projects. This agile method can be used by a variety of different development teams. Working in sprint cycles allows teams to get fast feedback, improve their product’s quality, reduce risk, and it makes it easier to stay on schedule.The ProblemProject Managers must work carefully to make sure the sprint cycles can flow as best as possible with minimal stagnation. To do so, Project Managers set aside time for sprint planning where they can create a road map of what tasks need to be done in what order so that the project can progress and be completed. This can be especially difficult to accomplish since some tasks can be more important or more complicated than others or rely on the completion of other tasks before they may be worked on.The SolutionThe Orai application is a recommendation engine built around project management data. It combines the querying power of the Neo4j graph database with the flexibility of the Django web framework using the django-neomodel plugin.GitHub - Group13-FAU/OraiThis web application is a recommendation engine built around project management data. It combines the querying power of…github.comThe Orai system monitors all tasks dependent on each other and builds sprints from them with the highest priority tasks getting scheduled first. The length of these sprints is based on each tasks’ complexity, allowing the user to gain more control of how much the development team can handle.Furthermore, the user can control the number of sprints, the complexity of sprints, the essential tasks, and the number of essential tasks that can be worked on per sprint.Implementation DetailsOrai takes in four user inputs:The max complexity value of each sprintMinimum number of sprints desired(Optional) The maximum length of the dependency treeA set of Storys to be prioritizedThe Story Model and Relationships are extremely lightweight — the application uses only Story nodes and a REQUIRES relationship between Storys:from neomodel import (    StringProperty,    StructuredNode,    RelationshipTo,    IntegerProperty,    FloatProperty,    BooleanProperty)class Story(StructuredNode):    # Properties    nodeID = IntegerProperty(index=True, db_property=id)    sentimentScore = StringProperty()    sentiment = StringProperty()    notes = StringProperty()    name = StringProperty()    value = StringProperty()    acceptance_criteria = StringProperty()    complexity = IntegerProperty()    priority = FloatProperty()    approved = BooleanProperty()    # Relationships    requires = RelationshipTo(.story.Story, REQUIRES)The project used a dataset from The SilverLogic’s internal project management system.MATCH p=(:Story)-[:REQUIRES]->(:Story)RETURN pLIMIT 10Selection of Story Nodes from the DatasetIn the example output above, if a team wanted to prioritize the implementation of the Invoice Picker story, they would have to first complete the Share text and No Stories Empty State stories, as they are prerequisites.Given enough team members and a long-enough sprint, it might be possible to work on Share text and No Stories Empty State in parallel, and immediately move to Invoice Picker once both tasks are completed. If the team knows roughly how many complexity points they can complete in one sprint — and have thoughtfully given their best estimates to the stories in the backlog — having a planning tool that can provide example roadmaps that take into account inter-story relationships can be a useful starting point for planning and scheduling.Example Input FormExample Input FormImagine selecting the following parameters:Max complexity value per sprint: 12Minimum number of sprints: 3Priority Story :Epic / Actor / Platform ArchivingHarvest Integration V2 Phase 4 Time Entry AugmentationInvoice Time Entry MovingExample OutputExample OutputThe above is an example of a sprint cycle. The required stories are organized such that the stories it depends on are scheduled before it. For example, Sub Epics requires that Invoice Time Entry Moving be completed first.Conclusion and RetrospectiveOver the course of two semesters, Logic13 was able to successfully design and implement a low-cost solution to the project sponsor’s (The SilverLogic) requested problem.Our project, Orai, is a user-friendly system that can generate optimal sprint cycles from user inputs and display them along with informative details about the sprints and the tasks.The team was able to learn new programming and computer skills, such as working with and collaborating through GitHub. The team also learned about software development and how to help customers with their requests by giving them alternatives and explanations.If we had the opportunity to start over and do it again, we’d make a few improvements. One is that on the suggestion page where the user inputs the information, we would add more information to the list of tasks to make it easier for the user to make a choice. This would make the system easier and more convenient to use for the user. Besides that, the team believes this solution is the most optimal and efficient way to schedule tasks for sprints. We are very proud of the work put into the development of the system.Team Logic13Team Logic13 (pictured below) consisted of very talented individuals that came together to develop a great system. Felix Medrano took on the role of project lead and handled project scheduling and synchronization, as well as communicating with the sponsor.Because of his experience with web development and algorithms he would develop the scheduling algorithm used for Orai as well as develop the front end. Valentina Diaz, also having experience with the algorithms we would need, oversaw the chart development and helped with documentation. Because of his experience in the field, Jacob Christensen oversaw the back end and the development of the foundation for the project, including database and system management.Team Logic13Further ResourcesFall 2021 Showcasesenior design showcase fall 2021www.fau.eduGitHub - neo4j-examples/paradise-papers-django: A simple Django web app for searching the Paradise…A simple Django web app for searching the Paradise Papers dataset backed by Neo4j Welcome to Paradise Paper Search…github.comFuture SponsorsFill out the form below to let us know how you would like to get involved in next semesters showcase. Please include a…www.fau.edu;Jan 6, 2022;[]
https://medium.com/neo4j/conversational-artificial-intelligence-with-neo4j-and-the-unreal-engine-part-3-3623b0212570;Antonio OrigliaFollowDec 16, 2022·6 min readConversational Artificial Intelligence with Neo4j and the Unreal Engine — Part 3In the previous part, we considered an example of describing how to handle logical conflicts on the basis of linguistic theories. We also sketched how we can introduce intentionality in AI by using Behavior Trees (BT). Let’s suppose, now that our graph is not incoherent and let’s talk about intentional moves, that is, behavior that the system exhibits while trying to reach a desirable graph configuration. This is an important difference with conversational systems based on machine learning only as they simply just react to input but without any real goal.Taking decisionsIf the graph is not incoherent, then the system can perform intentional dialog moves and attempt to direct the graph configuration towards the desired one. First of all, the system commits changes to the Belief Graph so that new decisions can be made on their basis. Then, it checks if open issues, like unanswered questions, are present. In this case, the graph is defined as unstable so closing these issues takes priority. When the graph is not unstable but it does not exhibit the goal pattern, it is undesirable. To solve undesirability, in ANGMAR we use Bayesian Networks (BNs) to manage the decision making process and select the strategy that will most likely take the graph to a configuration that is closer to the desired one. FANTASIA integrates in Unreal the functionalities provided by the aGRuM library and, together with Neo4j, allows it to dynamically assemble BNs on the basis of the information collected in the graph. Briefly, BNs model causal influence between random variables so that evidence collected about one variable propagates to the rest of the network to improve the estimate about the true value of each node in the network. In the case of the movies domain, a simple example consists in building networks that consider actors, movies and genres. The performance of the actors influences the rating of the movies they participate in and the rating of the movies influences the rating of the genres they belong to. We assume that the rating of movies and genres is estimated as the median of their parents. In the beginning, since we do not know anything, variables in the network have random distributions: the estimated appreciation of actors is represented by the uniform distribution over ratings, indicating that anything may be true. Depending distributions are computed accordingly. Next, we estimate the rating of movies as a gaussian distribution over the possible ratings fitted to the Movielens ratings using Kernel Density Estimation: this constitutes soft evidence about the movies and distributions can be updated consequently, as shown in the following Figure.A Bayesian network with and without soft evidence applied. The probability distribution over ratings in all nodes changes accordingly to the provided information.Asking questionsAsking questions, in this framework, corresponds to collecting hard evidence about the variables in the network. By computing the nodes’ entropy, which represents the uncertainty of the information contained in the node, and considering the general goals of the system, dialogue management strategies can evaluate the next move. It is not possible, however, to create BNs that are as large as the entire database as this would crash the application. BNs, however, are also graphs! This means that we can use Cypher queries to extract interesting subgraphs, graphs that are coherent with the collected beliefs about the user, from the general domain and dynamically assemble BNs, inside Unreal, on the basis of the Neo4j data structure. In the graph, we use ontological part_of relationships to guide this part of the process. Extracting sub-graphs on which to reason with BNs may be done with graph patterns or with node embeddings, simulating what may resemble something like Artificial Instinct. Also, Graph Data Science techniques like link prediction can support reasoning establishing how likely it is for a desirable connection to form given the available dialogue moves. The following Figure shows how this mechanism is implemented using BTs.The Behaviour Tree using Neo4j to extract a subgraph of interest for the user and dynamically assembling a Bayesian Network to take decisions.Argumenting recommendationsWhen producing dialog moves, it may be necessary to present inferential statements, obtained by extracting paths over nodes in the graph, to support the positions expressed by the system. Depending on the illocutionary force of the statements, the motivations behind their expression, they may represent argumentations, if they are meant to persuade the interlocutor into accepting an unsettled claim, or they may represent explanations, if they are meant to let the interlocutor understand a point. In terms of a BT, the illocutionary force of a statement is represented by the position, in the BT, of the task that generates the statement. In the current version of ANGMAR, inferential statements supporting the main one are explanations if they are produced to support an answer to a question. If they are generated during exploitation, the actual recommendation phase, or during the exploration phase, where more information about the user is collected, they are considered argumentations, as shown in the following Figure.The last part of the Behaviour Tree uses the assembled Bayesian Network to decide whether to recommend an item or improve the user model by asking more questions.This distinction is still under development as, in some cases, argumentation supports explanations and vice-versa. Through LORIEN, we are analysing dialogues to better understand argumentation from the point of view of linguistics theory. For the purposes of this article, it is sufficient to show that, in ANGMAR, the structure of the BT and the way decisions are made concerning dialog moves represent a computational interpretation of linguistic theories. As such, ANGMAR can be used to test such theories by deploying it in specific domains, like the movie recommendation task. Problems observed in ANGMAR, on the other hand, provide information about potential loopholes in the theoretical background and the corresponding fixes may suggest ways to better understand the linguistic and cognitive mechanisms underlying dialog management, an advantage that was also present in old-school approaches to AI. Consistently with the general framework, argumentation moves and their relationships are being modeled in graph form to introduce a further dimension to dialog management. For the case of recommender systems, being able to select and adequately present supporting arguments to ask for commitment from an interlocutor (accepting a movie) is a powerful ability to provide these systems with. This problem involves the capability to identify arguments that amplify the system’s claims and to present them in natural language using the correct forms. This goes beyond the well-known motivation for recommendations that sounds like People who got this also bought that” and supports both personalization and transparency, improving quality of service and of interaction. Studies concerning argumentation-based dialogues are conducted by Martina Di Bratto, one of the PhD students from our lab.Final remarksThere are many ways in which linguistics research conducted using graph structures can inform the development of explainable-by-design technological systems. Developing LORIEN and ANGMAR in parallel represents a formalization effort to build a two-way bridge between researchers in linguistics and computer science. The goal is to use theoretically motivated computational models to iteratively develop and test hypotheses about dialog management. This way, linguistic theories can be tested with modern instruments through simulation and interaction with real users while, on the other hand, explainable and computationally manageable AIs can be developed. Our work, therefore, is less focused on producing final applications and is, instead, devoted to the exploration of theoretical mechanisms that, if unraveled, can lower the technological cost of developing intelligent systems using natural language as an interface.;Dec 16, 2022;[]
https://medium.com/neo4j/cypher-sleuthing-dealing-with-dates-part-1-90eff82b113d;Jennifer ReifFollowApr 9, 2021·9 min readCypher Sleuthing: Dealing with Dates, Part 1No matter what database, programming language, or webpage you might be using, dates always seem to cause headaches. Different date formats require calculations between application date pickers in user-friendly formats and system dates in backend devices and data sources. Then, programming languages each have their own libraries and structures for dealing with dates, too.This concept in the Neo4j ecosystem isn’t any less complex with Cypher (a graph query language) date formats, the APOC library date functions/procedures, and countless possible integration tools/APIs for data import and export. I feel like I’m always looking at documentation and dealing with lots of trial and error in order to format the date just right. You may have heard about dependency whack-a-mole,” but dates are another aspect of programming that can feel like whack-a-mole, too.In this post, I will do my best to provide you with the tools for less random whacking and more accurate decision making when it comes to formatting dates with Cypher. You can follow along by launching a blank sandbox (free) and copying the Cypher into the browser or tweaking and running the queries for your own data set. Let’s dive in!Time ConundrumThe general concept of time is rather confusing, and one that I did not realize was quite so complex. There have been a number of humorous and eye-opening content pieces around time being the programmer’s nightmare. Why is that?First, standard measures of time aren’t always true. The number of hours in a day can vary depending on daylight savings time (and geographies changing at different points during the year), days in a month can vary by month and leap years, and weeks in a year can vary depending on the day of the week Jan 1st falls on and leap years. Time zones are another matter entirely. Countries change time zones somewhat frequently and different eras in the past had entirely different calendars and time zone structures.There is a humorous and sobering comprehensive list of one programmer’s experiences of time variance, as well as an entertaining video on time zones from a programmer’s point of view. It was very valuable and educational for me to see how much time can morph, making it exceptionally complicated to calculate and present a consistently accurate measure of time. Also, thank you to my colleagues @rotnroll666 and @mdavidallen for those links. :)Cypher DatesLet’s start at the base with Cypher date formats. For this, we can go to the official Cypher manual and take a look at the two different sections that cover dates. The first section is for the date and temporal data types themselves. The second section is for instant and duration calculations using functions. We’ll stick with just the instant today and worry about durations and other details in another post.The date and temporal data types in Cypher are based on the ISO 8601 date format. It supports three different categories of time: date, time, and timezone. Within those three categories are the instant types Date, Time, Datetime, LocalTime, and LocalDatetime. There are also three ways to specify timezone — 1) with the number of hours offset from UTC (e.g. -06:00), 2) with a named timezone (e.g. [America/Chicago]), 3) with the offset and name (e.g. -0600[America/Chicago]).For this blog post, we won’t explore the LocalTime and LocalDatetime types. These types are the exception to most rules and are very rarely required because they leave valuable timezone information out of the temporal value.Alright, let’s stop discussing concepts and see Cypher temporal types in action. We will create a few different dates using the instant types, then handle some timezone examples.Example 1: Setting a node property to current datetime.MERGE (b:BlogPost) SET b.publishedDatetime = datetime()RETURN b.publishedDatetimeNOTE: You might notice the literal T between the date and time values. This vital little connector is easily forgotten and something we’ll need to keep in mind when we start doing translations and conversions with other formats!Example 2: Setting a relationship property where date value equals a specific string.MERGE (e:Employee)-[rel:ASSIGNED]->(p:Project) SET rel.startDate = date(‘2021–02–15’)RETURN rel.startDateExample 3: Setting a node property to time with time zone.MERGE (s:Speaker {username: ‘jmhreif’})-[rel:PRESENTS]->(p:Presentation) SET p.time = time(‘09:30:00–06:00’)RETURN p.timeExample 4: Setting a node property to full date time (with time zone).MERGE (c:Conference) SET c.startDatetime = datetime(‘2021–03–01T08:00:00–05:00’)RETURN c.startDatetimeTo round out our instant types section, you can specify the date as parameters to the instant, and you can also access individual pieces of the instant. I haven’t run across cases where the parameter-like definition of the date is required, but I’m sure it was built in for a reason!Here are a couple of examples.Example 5: Setting date property using parameter-style format.MERGE (p:Project) SET p.expectedEndDate = date({year: 2021, month: 9, day: 30})RETURN p.expectedEndDateExample 6: Setting date using date component.MERGE (c:Conference) SET c.year = date().yearRETURN c.yearExample 7: Find blog posts published in March using date component.MATCH (b:BlogPost)WHERE b.publishedDatetime.month = 3RETURN b.publishedDatetimeExample 8: Return date component (dayOfWeek) of created node.MERGE (b:BlogPost) SET b.publishedDatetime = datetime()RETURN b.publishedDatetime.dayOfWeekNOTE: dayOfWeek has Monday as the start of the week. Since I’m writing this on Tuesday, these results are accurate. :)Getting to Neo4j-Supported Date FormatsNow, these are great if you have a date/time value that is already formatted for ISO 8601. But what happens when you don’t? How do you translate a date into something Cypher will understand and Neo4j will store?In this post, we will stick to what is probably the common temporal measurements — i.e. using year, month, day, hour, minute, second. For weeks, quarters, milliseconds, and so on, check out the docs. Also, recall that a literal T character is required between date and time in a combined value, so we’ll have to keep that in mind.We will look at the following scenarios to get the dates converted to values Neo4j and Cypher can read:Epoch time (formatted in seconds or milliseconds)Other date string formats (yyyy-MM-dd HH:mm:ss and similar)Multi-conversions (one conversion wrapped in another on one line)Epoch TimeThe website epochconverter.com defines epoch time as follows:… the Unix epoch (or Unix time or POSIX time or Unix timestamp) is the number of seconds that have elapsed since January 1, 1970 (midnight UTC/GMT), not counting leap seconds (in ISO 8601: 1970–01–01T00:00:00Z)”.This website is really easy to use, and I visit it quite frequently for ad hoc conversions or example dates to use. As an example of epoch time and other date formats, here is the same date in three formats:Human-readable: Monday, March 1, 2021 12:00:00 AMISO 8601: 2021–03–01T00:00:00ZEpoch time (seconds): 1614556800Cypher does have the capability to convert epoch values for certain cases, though the syntax is a bit different than the conventions we’ve seen thus far. For other types of formats, we will go to the APOC library, which is a very popular extension for Neo4j containing procedures and functions for many different utilities.Ok, let’s see some examples of how to programmatically convert epoch time. We will use our example epoch time from above (1614556800, which is March 1, 2021 12:00:00 AM), just to keep things simple and consistent. We will show the results of the converted value, as well as the final converted Neo4j temporal value next to it.Example 1: Epoch to datetime using CypherWITH 1614556800 as epochTimeRETURN datetime({epochSeconds: epochTime})Example 2: Epoch to date string using apoc.date.format()WITH apoc.date.format(1614556800, s”, yyyy-MM-dd”) as convertedRETURN converted, date(converted)Now, because epoch time is a date and time in a seconds format (time-based), we are unable to convert straight from epoch time to a simple date (without time). However, we could either store as a datetime and return date portions for queries….or we could use APOC to get our date!Example 3: Epoch to ISO 8601 format using apoc.date.toISO8601()WITH apoc.date.toISO8601(1614556800,’s’) as convertedRETURN converted, datetime(converted)Other Date String FormatsNow we know how to convert Unix-based epoch time, but what about strings in all different kinds of formats? How do we translate them to something Cypher will read? Cypher does accept strings and can convert strings in the ISO 8601 format to a temporal value, so we just need to convert a variety of string values to an ISO 8601 string format. We can do that using apoc.date.convertFormat().Note: all of the possible formats in the procedure’s third parameter below are listed here.Example 4: Similar date format to ISO 8601 stringWITH apoc.date.convertFormat(‘2021–03–01 00:00:00’, ‘yyyy-MM-dd HH:mm:ss’, ‘iso_date_time’) as convertedRETURN converted, datetime(converted)Example 5: American date format to ISO 8601 stringWITH apoc.date.convertFormat(‘03/01/2021’, ‘MM/dd/yyyy’, ‘iso_date’) as convertedRETURN converted, date(converted)Finally, there are a few APOC procedures that deal directly with temporal values. Only one goes to a Neo4j date format, though, and it transforms a string to a temporal.Example 6: Datetime string to Neo4j datetimeWITH apoc.temporal.toZonedTemporal(‘2021–03–01 00:00:00’, ‘yyyy-MM-dd HH:mm:ss’) as convertedRETURN converted, datetime(converted)Notice that both the results are the same, showing that the apoc.temporal.toZonedTemporal() function transforms directly to the Cypher datetime() value.Multi-ConversionsOkay, so we have done several conversions that translate strings or epoch times to strings, but that doesn’t always get us to the Neo4j date. In order to do that, we can wrap our converted value in another conversion function. This isn’t really different from what we’ve seen before, but they can get convoluted and you might think you can do that?” Yes… yes, you can. :)Let’s take a look!Example 7 (from Example 1 above): Convert epoch time to string and then to datetimeRETURN datetime(apoc.date.format(1614556800, s”, yyyy-MM-dd’T’HH:mm:ss”))Example 8: Convert date from Twitter API to ISO date time string, then to Neo4j datetimeRETURN datetime(apoc.date.convertFormat(‘Mon Mar 01 00:00:00 -0000 2021’, ‘EEE LLL dd HH:mm:ss Z yyyy’, ‘iso_date_time’))For a reference to the letters in that date format, the documentation is here (under Patterns for formatting and parsing).Wrapping UpIn this post, we covered most of the Neo4j-supported temporal instant types — date(), datetime(), time() — for creating the values either from a current instant or from an ISO8601-formatted string. We then saw how to use the utility functions in the APOC library to transform epoch Unix time values and strings in non-ISO8601 formats into strings or temporal values Cypher can work with.There is so much more to explore on the topic of Neo4j dates. Next time, we will discuss Cypher durations for calculating the time between two instants or for adding/subtracting dates and amounts from temporal values.Until then, happy coding!ResourcesCypher manual: Temporal instantsAPOC documentation: Datetime conversionsNeo4j sandbox: Test out date/time on a free instance;Apr 9, 2021;[]
https://medium.com/neo4j/optimizing-cypher-query-by-using-parameters-ec0caaf8b393;Tom NijhofFollowSep 19, 2022·3 min readOptimizing Cypher Query Using ParametersI got a helpful email from Ron van Weverwijk pointing out that constructing a Cypher query every time does not give the best performance. I didn’t know at the time that this would also solve another problem. The problem is the Neo4j desktop app crashing after approximately 1500 requests.In a previous blog, I used python to construct a query with a merge for every synonym I wanted to add.Python code creating the Cypher query (old method)Every time a new query is sent to the Neo4j database, it needs to be transformed into an execution plan. But Neo4j caches the last 1000 queries for reuse, meaning if you send a query that is cached, the optimization can be skipped. This is why the advice is to make 1 query with parameters and reuse it every time.For this, the Python for loop needs to be replaced with Cypher Unwind and the Python f-string with Cypher parameters. This gives us the following query.Cypher query with parameters, used within python (new method)MeasuringI know better than to believe a stranger on the internet with advice or documentation. So I constructed some experiments to find out what benefits I got from rewriting it.Effect of UnwindIn a completely empty database, I created 1 compound with 16 synonyms in the same way as I did before and also with the new method. This is repeated a thousand times without emptying the database or restarting it. This means only the first time the query needs to be optimized, after that the cached query could be used.Old methodmedian time: 0.001029smean time: 0.002305sNew methodmedian time: 0.001006smean time: 0.001609sIt seems the unwind and constructed perform the same in most cases but the constructed query has more extreme cases, resulting in similar medians but different means for the measured execution times.Effect of CachingTo test the effect of caching 999 unique compounds with synonyms are sent because 1 of the 1000 compounds did not work out the way I hoped…The rest of the experiment is the same.Old methodmedian time: 0.04199smean time: 0.19140sNew methodmedian time: 0.03316smean time: 0.03744sHere we see a way clearer benefit. The new method will improve performance and thus will be implemented.ResultThe new method is a clear improvement, constructing a new query per request was a bad idea.Using the backend function with the new method on the more than 5000 requests that crashed with the old method, now it works. It still takes more than 5 hours, including scraping and cleaning the data.;Sep 19, 2022;[]
https://medium.com/neo4j/scale-up-your-d3-graph-visualisation-webgl-canvas-with-pixi-js-63f119d96a28;Jan ZakFollowSep 19, 2019·4 min readScale up your D3 graph visualisationWebGL & Canvas with PIXI.jsDo you use D3 for data visualisation and either you are considering, or already using it also for graph visualisation? Keep in mind that D3 uses SVG for rendering. While it is the easiest to work with API for drawing 2D graphics on the Web, its downside is that the browser keeps the entire DOM tree of vector elements in memory, even for elements that are effectively invisible. You might hit a performance drop with complex graphics, specifically for graph visualisation when you try drawing graphs larger than ~1000 nodes, or even less with complex SVG effects.At this time you should reconsider why you run into performance issues in the first place. Why do you have such complex graph to draw, what is the purpose of the visualisation, and how is user going to interact with it? Sometimes too much information can result into meaningless graphics, if user doesn’t know what to do with it.Since you’re still reading, you probably have reasons? Ok, I warned you :-)In this article we are going to explore together how you can scale up your existing D3 graph visualisation. Another solution could be resorting to a commercial library, but we are not going to cover it here, because it could mean significant changes to your existing application. However if you are starting a new project, all of these libraries are great to work with and we suggest you evaluate them (listed in alphabetical order):Keylines by Cambridge Intelligence,Ogma by Linkurious oryFiles by yWorks.Canvas HTML element has a few available drawing APIs, with different performance and browser support. The most advanced, WebGL, uses GPU for hardware-accelerated drawing. However this means that for the best coverage of drawing size, browser and hardware compatibility you would need to implement drawing code multiple times, for each chosen API separately.Another complexity arises with mouse interaction, because with Canvas you can only detect the mouse position and color at the position. Detecting which element is at the position can be implemented for example by rendering a separate hidden canvas, where color designates the element.Visible canvas / Separate hidden canvas for detecting clicked elementPIXI.jsEnter PIXI.js, a 2D drawing library. It allows you to express a declarative render tree, similarly to SVG. However the render tree is processed in the JavaScript engine only, instead of in the DOM as in SVG. The browser only receives drawing instructions, as if you would write Canvas drawing code yourself.PIXI.js uses WebGL by default if available, and supports fallback to Canvas otherwise. Mouse interaction complexities are also abstracted away from the developer. No wonder that this library has a heavily active community related to development of browser-based games.ImplementationWe are going to start by forking the original D3 graph visualisation example, which uses d3-force for a force-directed layout.Replacing SVG rendering with PIXI.js involves creating an instance of PIXI.Application and adding children to it, according to the desired style and interaction. Follow API docs for details, their Performance Tips are also helpful. We can add richer features such as labels, font icons, hover effect, zoom & drag viewport and a simple toolbar. Anything is possible!A live demo of this exercise shows there is almost nothing left from the original code which was related to SVG rendering, it was replaced with code for a different rendering target. PIXI.js rendering runs on top of D3, which stays only for computing graph layout.Live demoSummaryWe have uncovered a hidden strength of D3. A few years ago, D3 was refactored with modularity in mind. A single monolithic library was split into many composable single-purpose libraries, which can be used also standalone. If you need to increase performance of your existing D3 data visualisation, you can replace rendering by another drawing library such as PIXI.js, while still using D3 for underlying layout computation.Further performance improvement can involve moving layout computation to a separate WebWorker thread, so that it doesn’t block UI from other actions. I can see Part 2 of this article coming soon -)Get in touch with GraphAware to see how we can help you with performant graph visualisations!This article was originally published on September 5, 2019 at GraphAware blog, and is republished here with permission.;Sep 19, 2019;[]
https://medium.com/neo4j/querying-neo4j-clusters-7d6fde75b5b4;David AllenFollowMar 1, 2019·8 min readQuerying Neo4j ClustersWith Neo4j as with most databases, if you want to just query the database it’s simple. You use a driver, create a connection, send a query and get back some results. That’s all there is to it!Behind the scenes though, if you’re working with any clustered database, there’s a whole lot more going on than that. To begin with, the database isn’t in a single place but is composed of multiple servers. In this article, we’ll explore how Neo4j Clusters work, and how Neo4j drivers get your query executed.How queries get run on Neo4jBefore describing the drivers, we need a brief overview of how Neo4j clusters work, and what the cluster roles are. This, in turn, will help you understand what a driver is doing.Questions or comment on any of this? Come ask on the Neo4j Community Site!Neo4j Causal ClusteringA cluster is composed of three or more Neo4j instances that communicate with one another to provide fault-tolerance and high-availability using a consensus protocol (RAFT). In Neo4j clustering, each database has a perfect, complete copy of the entire database (the graph is not sharded). Each machine in the cluster has a role”. It can either be the leader or a follower.Cluster architectureCluster RolesThe leader is responsible for coordinating the cluster and accepting all writes. Followers help scale the read workload ability of the cluster and provide for high-availability of data. Should one of the cluster machines fail, and you still have a majority, you can still process reads and writes. If your cluster loses the majority it can only serve (stale) reads and has no leader anymore.Optionally, you can have any number caches in the form of read replicas. They are read-only copies of your database for scaling out read-query load. They are not officially part of the cluster, but rather are tag along” copies that get replicated transactions from the main cluster.Topology ChangesIn the lifecycle of a cluster, cluster roles are temporary, not things you configure. Suppose you have machines A, B, and C. If A fails, then the remaining nodes (B and C) will elect a new leader amongst themselves. When A restarts, later on, it will rejoin the cluster, but probably as a follower. So roles can change through the lifecycle of the cluster. There are various other reasons where everything is working fine, where the cluster might elect a new leader — and so by themselves, role changes are not a cause for concern.Neo4j uses the RAFT consensus algorithm to coordinate the cluster. Quite a lot is published on that topic if you’d like to go deeper.Neo4j DriversThe Driver API consists of 4 key parts illustrated below. Whether talking to a Neo4j cluster or single instance, all of these concepts apply.Core parts of the Neo4j Driver APIIn the cluster world though, because we are talking to a group of machines, how the transactions get executed and where they go is the subject of how Routing Drivers” work.Routing DriversWhen you use one of the supported Neo4j drivers (Java, Javascript, Python, .Net and Go), there is an option to use the bolt+routing protocol. You’ll know you’re using it because it’s in the URI of the connection string. For example, you can connect to bolt+routing://neo4j.myhost.com.TipFor clusters, set up a single DNS record with multiple A entries, each pointing to the cluster members.This way, all clients can connect to the same DNS address but may have different machines as points of entry depending on cluster state and what is up. You don’t have to do it this way you can connect directly to any of the server member’s IP addresses, but this way is more flexible should query topology change.If you’re connecting to any single host or address, the driver handles the routing smarts for you. The driver will first check if the database has routing capabilities, and if so will fetch that holds a full or partial snapshot of the read and write services available. After that moment the initial host will not be used unless the driver loses contact with the cluster and has to re-initialize routing.Routing TablesThe routing table holds a list of servers that provide ROUTE, READ and WRITE capabilities. This routing information is considered volatile and is refreshed regularly.Neo4j Cluster Routing TablesDrivers refresh this regularly because the cluster topology could change according to runtime events (like a machine failing). Generally, the WRITE node is going to be the leader, and the READ nodes are going to be the followers in the cluster or read-replicas.Important TipRead replicas do not participate in cluster topology management, and as such they do not provide routing tables. Only core nodes provide routing tables. This may change in subsequent releases of Neo4j.If you’d like to try this out for yourself, just execute this statement on any clustered Neo4j instance:CALL dbms.cluster.routing.getRoutingTable({}) YIELD ttl, servers UNWIND servers as serverRETURN ttl, server.role, server.addressesand you can see your routing tableRouting table of a Neo4j clusterThis particular cypher query is only intended for internal use and may change in future versions of Neo4j so don’t write your code to it, but it gives you the idea of what the driver is actually doing.Advertised AddressesWhere did those addresses in the routing table example come from? These are set by the user in the neo4j.conf file, as dbms.connector.bolt.advertised_address . In this way, Neo4j knows what address to publish to the external world where it can be contacted. (Configuration Reference)Important TipThe advertised address setting is crucial for external clients to know how to contact your cluster. Set it explicitly in neo4j.confMake sure it is an address that can be resolved by external clients (and not an internal private address, such as 192.168.0.5). A very common error with Neo4j connectivity is to fail to set this, or set it to an internal private address, and have external clients on the Internet fail to connect because they cannot figure out how to reach your database!Connection ManagementOnce a routing table in place, the driver can manage a pool of connections to all of the different machines in the cluster. What the user sees, is usually just the creation of sessions, and running queries from those sessions. What’s actually inside of the driver looks more like a pool of connections to a set of machines (A, B, and so on). Sessions simply borrow and ride on top of connections as needed to execute queries.Neo4j driver connection managementThese connections are handled separately so that if (for example) Server A goes away and all of those connections end up dying, you at the session level don’t necessarily need to know about that. A session can still borrow a different connection and your application can keep chugging. An ongoing statement execution would fail and automatically retried by the driver.Users execute queries by using sessions. Sessions are cheap objects to create (unlike connections, which are expensive to set up). Sessions also provide a logical construct for chaining work together in a way that is causally consistent, meaning that you can do a series of transactions where subsequent transactions are always reading the writes made by earlier transactions in the same session.Query RoutingNow let’s say your driver sends a query to the database. I’ll use JavaScript as an example, but the same concepts apply in any language.The first thing we do is pull a session from the driver. We then use that session to execute the query.const session = driver.session()session.readTransaction(tx => tx.run(MATCH (n) RETURN count(n) as value))   .then(results => {       console.log(results.records[0].get(value))   })   .finally(() => session.close())How does the driver know where to send this query? In this code example, we have used an explicit transaction or transaction function, meaning that we told the driver we’re doing a readTransaction. We were given a tx Transaction object, and with that we ran our query. Because it has the routing table, and it knows you’re doing a read, it will probably end up sending this query to one of the nodes with the READ role. If we did a write transaction, it would be sent to any node in the routing table that has a role of WRITE. (Which is generally the cluster leader)If the routing driver has more than one choice of where to send the query, it uses a least connected” strategy, which helps avoid accidentally routing to a server which is presently too busy to answer in a timely way.Least Connected StrategyAuto-Commit TransactionsWhat if you don’t use explicit transactions? You can skip the transaction function and instead you could write:const session = driver.session()session.run(  tx => tx.run(CALL library.myProcedure()))   .then(results => {       console.log(results.records[0].get(value))   })   .finally(() => session.close())In this case, the driver cannot tell if you are doing a read or a write and will send your transaction to the leader. This way, the transaction should succeed, even if you haven’t told it whether it needs to do a read or a write.Important TipNeo4j drivers do not parse your Cypher to determine whether you’re reading or writing. Always use explicit transactions to tell it whether it’s a read or a write.This, in turn, helps it route the query effectively, and get the best utilization out of your cluster. If you never used explicit transactions, you might be sending all of your query load to the leader, beating it up while leaving the other machines in your cluster idle.ConclusionIn a clustered deployment, Neo4j utilizes smart bolt+routing clients to dynamically discover and monitor cluster topology, and route your queries to machines in the cluster using a least connected” strategy. All of this is done transparently for you, so at the user level all you’ll see is creating sessions, running queries, and consuming the results.Getting maximum benefit out of this setup requires some understanding of how this operates, and the most important parts to keep in mind are the following:Proper configuration of your advertised addressUse of explicit transactions in driver codeWhere possible, DNS setup to create a single DNS record that all clients can use to talk to any node in the clusterFor more about Neo4j routing drivers, consult the Neo4j Drivers Manual.;Mar 1, 2019;[]
https://medium.com/neo4j/neo4j-go-driver-is-out-fbb4ba5b3a30;Ali InceFollowNov 13, 2018·5 min readThe All New Neo4j Drivers Are Out — Now Including Go!We’re excited to announce the 1.7 series of our official Neo4j Drivers. As well as the regular improvements and new features, we’ve also introduced a new language to our driver family: Go.We’ve seen a big demand for Go over the past few years and began work on it a while back. You may have read our other blog post from earlier in the release cycle. The first release is version 1.7, which brings Go immediately in line with our other languages in terms of features (such as routing for causal clusters).The Go driver is the first driver built on top of our C Connector, Seabolt. This provides the building blocks for the Bolt protocol and the routing logic required to effectively communicate with causal clusters. Installing a C library naturally comes with cross platform challenges and we are aware that there are several different deployment scenarios out there. For that reason, we’ve put together a few easy-to-use binary packages and are also working on alternative installation mechanisms.In this blog post, we’ll be guiding you on how to build a simple Neo4j-backed Go application that executes a specified query against the database and prints the returned results out.What do you need to get started?First of all, you need to have a working cgo development environment. We also depend on pkg-config to discover Seabolt-related compiler and linker parameters.Having sorted out these, you’ll need to have Seabolt headers and libraries installed as well. Our experimental binary packages for popular platforms can be found here — or you can simply build from source.If you get stuck on installing any of these requirements, both neo4j-go-driver and seabolt repositories contain detailed instructions that can guide you to success.We’ll be using dep in the next sections since it makes our lives a bit easier to manage dependencies.SkeletonLet’s start with some skeleton code. We are expecting the user to provide us a Bolt URI, a username and a password along with a Cypher query to execute and output its results.The starter skeletonWhen you save the above contents on your main.go file, then execute dep init && dep ensure -add github.com/neo4j/neo4j-go-driver/neo4j in the same directory where this main.go file resides. This will pull in the driver from GitHub.Creating a driver instanceEvery application requires a Driver instance. This can be created using theneo4j.NewDriver function. The function expects a bolt URI, an authentication token and an optional list of configuration functions that can tweak possible configuration options including connection pool management, trust strategy, etc.Creating a driver instanceEach driver instance is thread-safe and holds a pool of connections that can be re-used over time. If you don’t have a good reason to do otherwise, a typical application should have a single driver instance throughout its lifetime.Acquiring a sessionIn order to execute a query against the database we need a neo4j.Session instance. A session is a container for a sequence of transactions, where it borrows connections from the underlying connection pool as required and returns them back when they are no more needed. Please bear in mind that session objects are not thread-safe and should not be accessed concurrently.Sessions can be acquired from a driver object through its Session() function. This expects an access mode, which specifies whether the intended operation is read or write, plus an optional list of bookmarks to establish the casual chain on the server.Acquiring a session instanceExecuting a query and processing resultsNow that you have acquired a session, you are ready to execute Cypher queries. There are several options here. The simplest is to issue queries in auto-commit transactions with the session.Run function. This is limited to allowing only one statement per transaction, but requires least code. You can also create an explicit transaction with session.BeginTransaction and gain more control over the transaction process. Lastly, we provide transaction functions through session.ReadTransaction and session.WriteTransaction which not only hide the transaction control logic but also bring in automatic retry capabilities to overcome transient failures related to network errors or casual cluster re-elections, etc.We’ll make use of transaction functions in this blog post, since this is our preferred means of query execution. This requires a function object that can execute the query and perform the actual record processing.Transaction function and result processingThe transaction function can be invoked with session.ReadTransaction. Here, a transaction will be created and the function itself will be called. If this completes successfully (i.e. doesn’t return any error) then the transaction will be committed, if not it will be rolled back.Executing the transaction functionIf you’re using a write query instead of a read query then you’ll need WriteTransaction instead. The choice of function sets the access mode which is used along with the driver’s routing table to select an appropriate server for execution.We haven’t used query parameters (passed as nil to tx.Run above) for this blog post, but parameters are expected to be of type map[string]interface{} and can be passed as follows:tx.Run( RETURN $x , map[string]interface{}{ x : 10})Completed sourceSo, here is our complete example…Complete source codeLet’s run itAssuming you’ve followed each step, you can simply execute the code using go run main.go. Without an argument, this will display a usage message. In order to properly execute it with a real Cypher query, other options can be provided:go run main.go -uri bolt://localhost:7687 -username neo4j -password password -query  UNWIND RANGE(1,5) AS N RETURN N AS SEQUENCE, Text  + N AS TEXT This should produce the following output:SEQUENCE   TEXT      ========== ==========1          Text1     2          Text2     3          Text3     4          Text4     5          Text5     5 records processedWhat about static linking, is this possible?Yes! As we mentioned in the introduction, we’ve been working on improving the installation options for Seabolt.Below is a Docker file to create a statically linked executable from the source code and run it :This file should hopefully be mostly self describing. To see it in action, save this file on your Docker-enabled computer as Dockerfile and issue the following command in the same directory:docker image build -t neo4j-go-driver-blog-post .This will build Seabolt and our sample executable and copy it to a Neo4j-based image, tagged as neo4j-go-driver-blog-post.Now, fire up a new container from this image:docker container run --name go-driver-blog-post neo4j-go-driver-blog-postWhen it says that Neo4j has started, we can run our sample in another terminal:docker container exec go-driver-blog-post /blog/sample -uri bolt://localhost:7687 -username neo4j -password neo4j -query  UNWIND RANGE(1,5) AS N RETURN N AS SEQUENCE, Text + N AS TEXT You should see the following output (from a single executable without any external dependencies):SEQUENCE   TEXT      ========== ==========1          Text1     2          Text2     3          Text3     4          Text4     5          Text5     5 records processedThat’s all for now, apart from to mention that as with our other official drivers, the API is designed to be topologically agnostic — which means that you can run the above code against a casual cluster only by pointing it to a URL with a bolt+routing scheme (like bolt+routing://server1:7687).We’ve covered most of the essential parts on how to use our new Go driver. We hope that you like it!If you run into any issues, please feel free to ask questions on our neo4j community site or open a GitHub issue.;Nov 13, 2018;[]
https://medium.com/neo4j/flights-search-application-with-neo4j-dockerizing-part-1-bcb861dc0c83;Vlad BatushkovFollowDec 3, 2019·13 min readFlights Search Application with Neo4j — Dockerizing (Part 1)How to build Neo4j Docker Image with Database import using neo4j-admin import toolTargetIn this series of articles, I will share my experience of building a simple web application that you can use to search for flights. The application was built for Neo4j Workshop in Bangkok, November 5, 2019.The general idea is to have a page with a search box where you can define a place of departure and destination, choose the date and voila — a list of available flights would appear below. Our needs limited by only a one-way flight, no class options, and no other fancy options. Brutal one-way flight for MVP. Solving this task includes doing one by one smaller sub-tasks.Flights Search Application is a perfect use-case for building using GRANDstack framework and the how to do it” will be covered in the third part of this series.Flights Search context is based on the graph-oriented problem: traverse through all possible routes to find all matched paths. We will cover Domain Model exploration, Cypher querying, and basic query performance improvements in future articles.In this article, I will share an example of how to build Neo4j Docker Image with a Flights Database. Data Import will be achieved by using the neo4j-admin import tool.If you are planning to build an amazing application using the Neo4j technology stack, I hope this series of articles is going to be very useful to you.Architecture for the Flights Search applicationPlanThe more complex and graceful the plan, the more likely it will fail. (Murphy’s Law)Docker ImageOne of the most critical features for effective development is easy setup and fast development loop: build, run, test. This is why Docker was chosen as an approach to build and run all system modules.Database SchemaI do not plan to reinvent the wheel in Flights area, this is why I decided to follow Max De Marzi’s article about the Flights Search data modeling and reuse his Database Schema.DataInitial data of Airlines, Airports, and Routes can be downloaded from openflights.org — a resource of worldwide flight information.Data ImportThe Database schema requires many more entities asides from what is provided by the openflights.org website. Additionally, to simulate a search the application will need additional data, for example, for future flights one month ahead. Running tons of LOAD CSV” does not seem to be the best strategy for dealing with such imports.When the size of the import data is very big, it is better to use the neo4j-admin import tool. The neo4j-admin import tool allows you to do an offline data import at great speed. Once the data is imported, you can start the database and set and generate indexes. Data Import can be part of Docker Image build.One More ApplicationUsing the neo4j-admin import tool requires us to prepare specially formatted .csv files. So, I will create a file generator application. It will prepare all nodes and relationships in a .csv file format that can be used by the neo4j-admin import tool. I will use dotnet core console app written in C#, but you can go with any other programming language.One More Docker ImageWith data in the database, I can now write a query to achieve our main goal — finding a list of flights. I know for sure that we’ll want to have APOC on board to facilitate the application.So I thought, why not prepare the all-plugins-installed” Docker Image first, and then build the Flights Docker Image on top of it. This Custom Neo4j Docker Image could also be reused in any other future development.DesignThe picture below illustrates how all the pieces of this puzzle are related to each other. So, in the end, we can build a Docker Image with Flights Database data inside.Building a Docker Image with Flights Database insideIt looked like a challenge from the beginning. But, maybe you already know — I love challenges. Let’s talk about the implementation step by step and see what we can learn from this lesson.Customized Neo4j Docker ImageFor the powerful version of Docker Image, it is better to build on top of the latest version of an Official Neo4j Docker Image (I decided to use last stable 3.5.x version). Now let’s add the first useful plugin: Awesome Procedures On Cypher (APOC). APOC contains tons of must-have procedures that are very useful for different kinds of queries.Plugin installation is actually nothing more than a downloading of a particular .jar file to the /plugins folder and minimal configurations. The available release versions of plugin you can find in GitHub.DockerfileFROM neo4j:3.5.12ENV APOC_VERSION=3.5.0.5ENV APOC_URI=https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/${APOC_VERSION}/apoc-${APOC_VERSION}-all.jarADD --chown=neo4j:neo4j ${APOC_URI} pluginsENV NEO4J_AUTH=neo4j/testCMD [  neo4j  ]The ENV NEO4J_AUTH=none statement will remove authorization. You should not normally do this, but it is ok for the development phase on the local machine. It can be handy.docker build . -t=vladbatushkov/neo4j-apoc:devReplace my name with your docker hub username (if you plan to push it to the public hub) and enjoy your first Custom Neo4j Image.docker run -it --rm -p 7474:7474 -p 7687:7687 vladbatushkov/neo4j-apoc:devFor some useful Neo4j Docker basics, you can read Developer Guide.Go to your http://localhost:7474/ and try the availability of APOC. For example, you can list all APOC functions or call any:CALL apoc.help( apoc )CALL apoc.coll.sum([1,2,3])There is another one way to achieve the same result but much more simply. APOC plugin is ready ready to use by NEO4JLABS_PLUGINS environment option.DockerfileFROM neo4j:3.5.12ENV NEO4JLABS_PLUGINS=’[ apoc ]’ENV NEO4J_AUTH=neo4j/testCMD [  neo4j  ]It seems like my idea to build a customized Neo4j Docker Image is not a big deal, we can install any plugin with one line. List of plugins ready to use:DockerfileFROM neo4j:3.5.12ENV NEO4JLABS_PLUGINS=[ apoc ,  graph-algorithms ,  graphql ]ENV NEO4J_dbms_unmanaged__extension__classes=org.neo4j.graphql=/graphqlENV NEO4J_AUTH=neo4j/testCMD [  neo4j  ]Magic, is not it? A super powerful and simple Docker Image with three plugins:docker build . -t=vladbatushkov/neo4j-apoc-algo-graphql:devIn case something goes wrong and plugins from the box do not work as expected, you always can do all the necessary things in the old-fashion” way. Just know, that there is no magic.DockerfileFROM neo4j:3.5.12ENV APOC_VERSION=3.5.0.5ENV APOC_URI=https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/${APOC_VERSION}/apoc-${APOC_VERSION}-all.jarENV ALGO_VERSION=3.5.4.0ENV GRAPH_ALGORITHMS_URI=https://github.com/neo4j-contrib/neo4j-graph-algorithms/releases/download/${ALGO_VERSION}/graph-algorithms-algo-${ALGO_VERSION}.jarENV GRAPHQL_VERSION=3.5.0.4ENV GRAPHQL_URI=https://github.com/neo4j-graphql/neo4j-graphql/releases/download/${GRAPHQL_VERSION}/neo4j-graphql-${GRAPHQL_VERSION}.jarADD --chown=neo4j:neo4j ${APOC_URI} pluginsADD --chown=neo4j:neo4j ${GRAPH_ALGORITHMS_URI} pluginsADD --chown=neo4j:neo4j ${GRAPHQL_URI} pluginsENV NEO4J_dbms_unmanaged__extension__classes=org.neo4j.graphql=/graphqlENV NEO4J_AUTH=neo4j/testCMD [  neo4j  ]Let’s do a small test-drive to confirm that GraphQL works properly. Register one dummy type and fetch the data directly from Neo4j via GraphQL API endpoint using any fancy GraphQL client.CALL graphql.idl( type Person { name: String! } )CREATE (:Person { name:  me  })If you use neo4j/test credentials, then do not forget to add Auth Http Header:{ Authorization” : Basic bmVvNGo6dGVzdA==” }Data ImportI got daily graph building experience during my One Month Graph Challenge. 30 times I successfully answered the question Where do I get the data?” and now I can tell you the truth: data is the most critical part of indie projects. And we as creators have several options to get the data:Manual generation. Do not expect anything serious with this approach.Example: Map of CitiesRandom generation. It can fit for some dummy and simple use-cases.Example: Random generated GalaxyParsing / Scraping. The amount of work can vary from small APOC function to big Python script. You might face some weird issues and heroically solve them.Example: Bands and GenresUse public resources like API or file storage. Very promising option, but usually required structure changes, merge or some other modifications before import.Example: Chess.com APIIn this project, we will break through the complexity of option number four. No pain, no gain.Database SchemaDatabase Schema of this project is a copy of Max De Marzi Database Schema from Flight Search article. I highly recommend you to read his blogs from time to time, it is a great example of high-level expertise, full of creative ideas and interesting topics. Let’s look at the nodes and relationships.Max De Marzi original content. Flight Search POC Database Schema.For the answer to the question Why it’s like that?” you’d better read in his blog post. For now, we will not discuss Data Modeling at depth (it is a topic for the second part of my series). Rather, I want to focus your attention on the technical solution of importing massive data, that should fit Database Schema.Initial DataFlight domain is a popular area and I found wonderful web resource, that provides enough amount of initial data: openflights.org.Useful data from openflights includes these three .csv files:~6162 airlines.csv{ AirlineId, Name, Alias, IATA, ICAO, Callsign, Country, Active }~7699 airports.csv{ AirportId, Name, City, Country, IATA, ICAO, Latitude, Longitude, Altitude, TZ_UTCHoursOffset, DST, DST, TZ_Olson, Type, Source }~67664 routes.csv{ Airline, AirlineId, Source_Airport, Source_AirportId, Destination_Airport, Destination_AirportId, Codeshare, Stops, Equipment }Data DiscoveryWith this data, we can already build Airports nodes and relationships between them. Airports nodes almost never change, the number of Airport nodes does not depend on time, they are rarely added or removed from the Database. Routes between them also extremely stable. If we have at least one flight between 2 Airports, then we have FLIES_TO relationship between them.The file routes.csv contains information about the Airline and 2 connected Airports, this means we can build a Flight node from it. How to utilize Airline nodes? Well, we can add OPERATED_BY relationship between node Flight and node Airline.Airports and Airlines is stable information, it change rarely. There is also very active data in the Database, and representation for it that should be created for every date: AirportDate nodes and Flight nodes. Every Airport is connected with AirportDay node for each date. AirportDay is connected with all outgoing or incoming Flight nodes. By the direction of the *_FLIGHT relationship, we know where the Flight is going to.For example, if we have 2 connected Airports and 1 Flight operated in 2 days, we need to insert 2 AirportDay nodes for each Airport and 1 Flight for each pair of AirportDay nodes.By having an understanding of the nature of the data, you can plan how to build all necessary amount of nodes and relationships.We need to have some operating days in the Database, so lets make an assumption that all of existing flights is operated every day. For example, 1 way directed route between 2 Airports for 30 days gives us the next nodes and relationships:1 Airline => 1 Airline node2 Airports => 2 Airport nodes (BKK, SVO)1 Route => 1 FLIES_TO relationship (BKK --> SVO)2 Airports * 30 days => 60 AirportDay nodes60 AirportDays => 60 HAS_DAY relationships1 Route * 30 days => 30 Flight nodes30 Flights => 60 BKK_FLIGHT relationships (in and out directed)1 airline 30 Flights => 30 OPERATED_BY relationshipsNow we can say, that the business logic” for Flights Database generation is defined. We don’t really need all this math, but we need to know how the data-generation console application will work. But before coding a generator application, we need to understand how output files should be formatted. Files produced by generator should be ready to use with the neo4j-admin import tool.How to use the neo4j-admin import toolHere a few words about the tool, so you don’t need to click on the documentation link just yet. The import tool consumes .csv files with a specific structure, and imports thousands of nodes and relationships into the graph in just seconds.Command-line interfaceneo4j-admin import /--database=flights.db / #database to import--mode=csv / #file format--nodes=... / #files to import nodes--relationships=... / #files to import relationships--ignore-missing-nodes #some settingsYou can choose between the single-file mode and multi-file mode to import your data.Single-file modeNodesairlines.csvcode:ID,name:STRING,country:STRINGSU,Aeroflot Russian Airlines,RussiaCommand-line interface--nodes:Airline= airlines.csv All nodes are marked with the Airline label. :ID is a unique ID and is used across the whole import process. It is not the ID for your future nodes or relationships. It is important to ensure that no two entities of nodes or relationships have the same ID during import.RelationshipsfliesTo.csv:START_ID,distance:INT,:END_IDBKK,7111,SVOCommand-line interface--relationships:FLIES_TO= fliesTo.csv All relationships have FLIES_TO type. :START_ID is node ID of an outgoing node, while :END_ID is node ID of an incoming node. This snippet shows an example of BKK → SVO. All relationship properties are in the middle, here we have only one — distance.Multi-file modeNodesflights_header.csvflightNumber:ID,departs:DATETIME,duration:STRING,distance:INT,price:INTflights_data_20191201.csvflights_data_20191202.csvflights_data_20191203.csv and moreSU_BKK_SVO_20191201,175200.000+0700,P09H09M,7111,16554Command-line interface--nodes:Flight= flights_header.csv,flights_data_.* The header file contains only a declaration of columns, while all the data placed in separate files or one data-file. This approach very useful when you have a lot of data to import and it makes sense to keep these files in reasonable size limits.RelationshipshasDay_header.csv:START_ID,:END_IDhasDay_data_20191201.csvhasDay_data_20191202.csvhasDay_data_20191203.csv and moreBKK,BKK_20191201Command-line interface--relationships:HAS_DAY= hasDay_header.csv,hasDay_data_.* Example with Relationship Type declaration inside Data fileIf you want to have different Labels for your nodes or Types for your relationships, you can try another technique. You can define a node Label or relationship Type along with other properties in each row of your data to be imported.inFlight_header.csv:START_ID,:END_ID,:TYPEinFlight_data_20191201.csvinFlight_data_20191202.csvinFlight_data_20191203.csv and moreBKK_20191201,SU_BKK_SVO_20191201,BKK_FLIGHTBLQ_20191201,SU_BLQ_SVO_20191201,BLQ_FLIGHTCommand-line interface--relationships= inFlight_header.csv,inFlight_data_.* The same is valid for nodes with Labels, using :LABEL statement instead of :TYPE.If you want to know more about other settings and commands then check out the documentation.Import OverviewGraph Schema to importThe Flights Database files generation is not a trivial task. We will need to generate 9 separate chunks of import data: Airport, AirportDay, Flight, Airline, FLIES_TO, HAS_DAY, OPERATED_BY and *_FLIGHT in both directions. And to remind you, some of that data is dependent on dates, so it is also required to prepare separate headers and data files for each date. Flight price and duration of flight are both heuristic guesses based on distances between airports. You can imagine how strongly I exhaled when I finally wrote a generator application!I think the generator application code and the application itself are optional topics and are not discussed in this article. If you feel you will be faced with a problem to write an import file generator, choose any programming language you know and simply write a generator.Full dockerized example of Flights Database Import scriptFlights Docker ImageCongrats! You are at the final base: how to build a Docker Image with imported Database inside.The idea behind this task is a super simple story:Copy all .csv files into docker imageCopy the import script into docker imageSet up the execution of the import script at the containers launchSelect the new database as an active oneDockerfileFROM vladbatushkov/neo4j-apoc-algo-graphql:latestCOPY import/*.csv import/COPY import.sh import.shENV EXTENSION_SCRIPT=import.shENV NEO4J_dbms_active__database=flights.dbCMD [  neo4j  ]import.shThe EXTENSION_SCRIPT allowed us to define an import script that will be executed before Neo4j starts.docker run -p 7474:7474 -p 7687:7687 -v c:/neo4j/data:/data -v c:/neo4j/logs:/logs vladbatushkov/neo4j-flights:devVolume params are optional. It just helps you to build several containers using the same database data, avoiding the execution of import scripts.Both Docker Images from this article at available from my Docker Hub page.I bet you are interested to see how the flow of the import works! How fast is the import? How many nodes and relationships will there be in the Flights Database for one year ahead? What is the approximate size of the database? Will all of these things actually work?! Well, let’s try a small benchmark test and look at the numbers.Import BenchmarkingMy docker environment is going to be the same for every import.Available resources:  Total machine memory: 9.73 GB  Free machine memory: 8.83 GB  Max heap memory : 2.16 GB  Processors: 4  Configured max memory: 6.81 GB  High-IO: falseI want to compare One-day import (1 of January 2020) VS One-month import (January 2020) VS One-year import (2020).For example, here you can see the contents of a folder with one-day data ready for import. All files with a name pattern like *_data_YYYYMMDD are going to be created for every import day.One-Day Import ResultsIMPORT DONE in 3s 943ms.Imported:  74 057 nodes  226 113 relationships  386 217 propertiesSize: 19.53 MiBQuery result based on One-Day ImportMATCH (a:Airport)-->(ad1:AirportDay)--(f:Flight)--(ad2:AirportDay)<--(b:Airport)MATCH (f)-->(al:Airline)WHERE a.city =  Bangkok  AND b.city =  Moscow RETURN *One-Month Import ResultsIMPORT DONE in 30s 167ms.Imported:  2 101 007 nodes  5 977 023 relationships  9 861 087 propertiesSize: 479.99 MiBQuery result based on One-Month ImportOne-Year Import ResultsIMPORT DONE in 5m 39s 912ms.Imported:  24 735 282 nodes  70 195 518 relationships  115 663 802 propertiesSize: 5.49 GiBQuery result based on One-Year ImportEnd of Part 1Thanks for reading!In the next article of this series, I plan a dedication to querying the Neo4j Database, and going deep into the details on how to write a Cypher query searching for flights. Stay in touch and clap-clap-clap.Resourcesvladbatushkov/flightsCypher queries Original .csv files from openflights.org Dotnet core C# console application to generate .csv files of…github.comFlight Search with Neo4jI think I am going to take the opportunity to explain why I love graphs in this blog post. Im going to try to explain…maxdemarzi.comB.2. Use the Import tool - Appendix B. TutorialsThis tutorial provides detailed examples of using the Neo4j import tool. This tutorial walks us through a series of…neo4j.com;Dec 3, 2019;[]
https://medium.com/neo4j/graph-databases-for-journalists-5ac116fe0f54;Dagoberto José Herrera MurilloFollowApr 29, 2019·16 min readPart I: Graph databases for journalistsUsing Neo4j to explore public contracting dataHigh Risk (Bertani, 1994)Investigative reporting such as the stories behind the Panama Papers and Swiss Leaks are complex, involving multiple persons, roles, organizations, countries, and intricate relationships between them.Understanding knotty connections between elements scattered across thousands or millions of documents can really be like looking for a needle in a haystack. Excel spreadsheets or relational databases are powerful tools, but they were not designed to carry out this type of explorations focused on relationships. Instead, relationships are first-class citizens of graph databases.The goal of this tutorial is to provide the foundations needed to analyze and visualize open contracting data using a real graph database (Neo4j). The primary audience is journalists, because they are particularly hungry for new tools to explore relationships.Neo4jNeo4j is a popular graph database management system with an open-source community edition. Even though this is not the only graph database, Neo4j is well known for the fact that the International Consortium of Investigative Journalists (ICIJ) has made intensive use of this technology to aid investigations like the Panama Papers. Graphs have proven to be very effective to represent these networks in an intuitive way.Open Contracting DataEvery year, governments devote gigantic amounts of money to public contracts, from papers and pencils to large infrastructure projects such as airports, roads, schools, and hospitals.The size of public procurement represents a considerable segment of the global economy. In the case of the OECD, the volume of these transactions represents approximately 12% of GDP.A large amount of resources along with the complex interaction between public and private interests exposes public contracting processes to multiple risks, including collusion and corruption.The Open Contracting Data Standard (OCDS) is a global, non-proprietary data standard designed to reflect the entire contracting life-cycle. The stages of the procurement process are depicted in figure 1. The OCDS was created to increase contracting transparency and facilitate deeper analysis of contracting data by a wide range of users. This standard is now being implemented by several governments and cities around the world. Check the complete list here.Figure 1. Stages of the procurement process (Source: standard.open-contracting.org)An OCDS document is made up of a number of sections. These are:release metadata — contextual information about each release of dataparties — information about the organizations and other participants involved in the contracting processplanning — information about the goals, budgets and projects a contracting process relates totender — information about how a tender will take place, or has taken placeawards — information on awards made as part of a contracting processcontract — information on contracts signed as part of a contracting processimplementation — information on the progress of each contract towards completion.Source: Open Contracting Data StandardThese sections are represented in a JSON document as follows:Source: standard.open-contracting.orgMexicoThe Mexican economy is the 15th largest in the world, and the second in Latin America. At the same time, this developing country is the 138 most corrupt nation out of 180 countries, according to the 2018 Corruption Perceptions Index reported by Transparency International.The irregularities in public procurement have definitely contributed to this poor rating. Recent cases, such as La Estafa Maestra” (Master Scam), reveal that the magnitude of fraud schemes in public procurement in Mexico is counted by hundreds of millions of dollars.Mexico was one of the pioneering countries in the adoption of the OCDS. Currently, approximately 300,000 contracting processes of the federal public administration under this standard can be accessed. A portion of these contracts will be analyzed through our graph.Data ModelNeo4j follows the property graph model to manage the stored data.The graph is made of nodes for entities, their relationships, and properties.Nodes are represented using circles. A node may have zero to many labels. Labels are used to group nodes into sets.Relationships are represented using arrows connecting nodes. They are directed, which means that each relationship has a start node and an end node. A relationship has only one relationship type (name).Properties are used to add qualities to nodes and relationships.In any scenario, the analyst is responsible for carefully formulating the best possible model to capture the characteristics of the phenomenon to be studied. The quality of the insights that can be extracted from the graph through queries and visualizations depends on it. The initial data model proposed to analyze open contracting data is depicted in figure 2.Figure 2. Data ModelThe graph model includes nodes with the following labels and their respective properties mapped to the corresponding OCDS fields:A contracting process (Contract). Take into account that a contracting process may involve one or more contracts, and some are declared void.ocid: It is a globally unique identifier for a contracting process. This property is matched to the ocid field in the OCDS.title: This field usually provides some idea about the object of the contract. It is matched to tender/title.procurementMethod: There are three types of procurement methods: open, selective and direct. It is matched to tender/procurementMethod.cycle: It refers to the year in which the contracting process took place, and is matched to cycle.region: Although this is a feature initially associated with the procuring entity in the OCDS, we consider it convenient to place it directly in the contracting process to have an idea of where the money is spent without resorting to other nodes. It is matched to tender/procuringEntity/address/region.amount: This property is very important to obtain conclusions. However, its interpretation must be cautious because most contracts do not report this value in the tender phase in our dataset. It is matched to tender/value/amount.2. A company or person which offers goods or services (Tenderer)rfc: It is a unique tax number in Mexico and is matched to parties/id when the role is tenderer.name: It is matched to parties/name when the role is tenderer.person: A boolean property to distinguish people (yes ) from companies (no). It is inferred from the RFC.3. A public entity managing the contracting process (ProcuringEntity). It may be different from the buyer who pays for products or services.name: It is matched to tender/procuringEntity/name.4. An address (Address)address: It is matched to parties/address/streetAddress when the role is tenderer.5. A telephone (Telephone)telephone: It is matched to parties/contactPoint/telephone when the role is tenderer.6. A contact person (ContactPerson)name: It is matched to parties/contactPoint/name when the role is tenderer.7. An email (Email)email: It is matched to parties/contactPoint/email when the role is tenderer.There are two possible relationship types to capture the roles companies and persons play in a contracting process. (TENDER) identifies all entities competing in the tender phase, and (AWARD) only those winning contracts in the process.Other relationships capture the connection between companies and their respective contact information: (ADDRESS), (TELEPHONE), (CONTACTPERSON), and (EMAIL). Finally, (MANAGE) connect procuring entities with their respective contracts.Installing Neo4j and creating our first graphDownload a copy of Neo4j Desktop from the Neo4j download page. Execute the downloaded .exe file and accept the default configuration. You will see a detailed explanation of the installation of Neo4j after the download.When opening Neo4j, you will find a screen similar to the following. Click on Add Graph .2. Click on Create a Local Graph .3. Choose a graph name and password. Then click on Create .Populating the graph with dataThe first step consists of acquiring the desired open contracting records and parsing them to nodes and relationships according to the corresponding OCDS fields mentioned early.For doing that, some programming tools that are not of interest in this tutorial are required, and therefore they are not described here. The CSV files with nodes and relationships required for this exercise can be downloaded directly from this link.This sample only contains 21,428 contracting processes whose procurement method is open (all interested suppliers may submit a tender) or selective (only qualified suppliers are invited to submit a tender) in years 2017 and 2018. Direct contracts ( the procuring entity contacts a number of suppliers of its choice) were excluded this time to simplify the analysis.1. In the graph, you just created, click on Start and then on Manage.2. Click on theOpen Folder dropdown.3. Visit the folder called Import and copy the CSV files there.4. Go back to Neo4j and click on Open Browser .The following lines contain the queries required to populate graph. Copy and paste each of them in the Neo4j browser and click on thePlay button. In a later section, we will explain the structure of Cypher queries. For now, the important aspect is to know that the LOAD CSV clause loads the CSV file with the nodes or relationships that you want to create, as well as their respective properties. The MATCH clause identifies graph patterns. And the CREATE clause creates nodes and relationships.If you have a Neo4j version that has multi-line-editor enabled, you can also paste the whole script in one go into the command line.Otherwise and if you want to follow more closely, do it one by one.Create nodes for (Contract)LOAD CSV WITH HEADERS FROM file:///contracts.csv AS row CREATE (contract:Contract{ocid:row.ocid, title:row.title, cycle:row.cycle, region:row.region, procurementMethod:row.procurementMethod, amount:row.amount})Create nodes for (Tenderer)LOAD CSV WITH HEADERS FROM file:///tenderers.csv AS row CREATE (p:Tenderer {rfc:row.rfc, name:row.name, person:row.person})Create nodes for (ProcuringEntity)LOAD CSV WITH HEADERS FROM file:///procuringEntities.csv AS row CREATE (procuringEntity:ProcuringEntity{name:(row.name)})Create nodes for (Address)LOAD CSV WITH HEADERS FROM file:///addresses.csv AS row CREATE (address:Address{address:(row.address)})Create nodes for (Telephone)LOAD CSV WITH HEADERS FROM file:///telephones.csv AS row CREATE (telephone:Telephone{telephone:(row.telephone)})Create nodes for (ContactPerson)LOAD CSV WITH HEADERS FROM file:///contactPersons.csv AS row CREATE (contactPerson:ContactPerson{name:(row.name)})Create nodes for (Email)LOAD CSV WITH HEADERS FROM file:///emails.csv AS row CREATE (email:Email{email:(row.email)})Now, let’s create some indexes. These are data structures that improve the speed of lookups of nodes by property.Create an index for (Contract:ocid)CREATE INDEX ON :Contract(ocid)Create an index for (Tenderer:rfc)CREATE INDEX ON :Tenderer(rfc)Create an index for (ProcuringEntity:name)CREATE INDEX ON :ProcuringEntity(name)Create an index for (Address:address)CREATE INDEX ON :Address(address)Create an index for (Telephone:telephone)CREATE INDEX ON :Telephone(telephone)Create an index for (ContactPerson:name)CREATE INDEX ON :ContactPerson(name)Create an index for (Email:email)CREATE INDEX ON :Email(email)Finally, let’s create relationships.Create a relationship (TENDER) between (Tenderer) and (Contract)LOAD CSV WITH HEADERS FROM file:///rcc.csv AS row MATCH (tenderer:Tenderer{rfc:(row.rfc)}),(contract:Contract{ocid:(row.ocid)}) CREATE (tenderer)-[:TENDER]->(contract)Create a relationship (AWARD) between (Tenderer) and (Contract)LOAD CSV WITH HEADERS FROM file:///raw.csv AS row MATCH (tenderer:Tenderer{rfc:(row.rfc)}),(contract:Contract{ocid:(row.ocid)}) CREATE (tenderer)-[:AWARD]->(contract)Create a relationship (MANAGE) between (ProcuringEntity) and (Contract)LOAD CSV WITH HEADERS FROM file:///rp.csv AS row MATCH (procuringEntity:ProcuringEntity{name:(row.name)}),(contract:Contract{ocid:(row.ocid)}) CREATE (procuringEntity)-[:MANAGES]->(contract)Create a relationship (ADDRESS) between (Tenderer) and (Address)LOAD CSV WITH HEADERS FROM file:///ra.csv AS row MATCH (tenderer:Tenderer{rfc:(row.rfc)}),(address:Address{address:(row.address)}) CREATE (tenderer)-[:ADDRESS]->(address)Create a relationship (TELEPHONE) between (Tenderer) and (Telephone)LOAD CSV WITH HEADERS FROM file:///rt.csv AS row MATCH (tenderer:Tenderer { rfc:(row.rfc)}),(telephone:Telephone {telephone:(row.telephone)}) CREATE (tenderer)-[:TELEPHONE]->(telephone)Create a relationship (CONTACTPERSON) between (Tenderer) and (ContactPerson)LOAD CSV WITH HEADERS FROM file:///rc.csv AS row MATCH (tenderer:Tenderer{rfc:(row.rfc)}),(contactPerson:ContactPerson{name:(row.name)}) CREATE (tenderer)-[:CONTACTPERSON]->(contactPerson)Create a relationship (EMAIL) between (Tenderer) and (Email)LOAD CSV WITH HEADERS FROM file:///re.csv AS row MATCH (tenderer:Tenderer{rfc:(row.rfc)}),(email:Email{email:(row.email)}) CREATE (tenderer)-[:EMAIL]->(email)Click on Database to see the nodes, relationships, and properties that have been created. Now, your graph is ready to be analyzed.An Introduction to CypherCypher is Neo4j’s graph query language. The objective of this section is explaining the basic concepts of Cypher through a series of basic queries. You can find a detailed explanation of this language in the following link. There is also a very useful Cypher Refcard to give an overview over all options.The simplest queries consist of a MATCH and a RETURN clause. The MATCH clause searches for the pattern specified in it. The RETURN clause defines what would be included in the result set. Patterns are expressed using the following notation for (nodes) and -[relationships]->.Again, copy and paste each query in the Neo4j browser and click on Play.a. Retrieve all the tenderers in the graph.MATCH (a:Tenderer)RETURN aIf you select a specific node, you will see its properties at the bottom of the query page.Double-click on a specific node and select Expand/Collapse child relationships to see connected nodes.Note that the browser is actually displaying just a part of the entire set of nodes, with a default count of 300. You can change the default settings to modify that number. It is also possible to see and download the entire set in text format clicking on Text.b. Retrieve all tenderers whose name is acondicionamiento en potencia y comunicaciones sa de cv” (power conditioning and communications).MATCH (a:Tenderer   {name:acondicionamiento en potencia y comunicaciones sa de cv})RETURN aOnly one tenderer node satisfies the condition of having a name property with the specified value. It is a company and its RFC is APC0206109QA.c. Retrieve all tenderers whose name is acondicionamiento en potencia y comunicaciones sa de cv” and the contracts in which it has participated as a tenderer.MATCH (a:Tenderer   {name:acondicionamiento en potencia y comunicaciones sa de cv})        -[:TENDER]->(b:Contract)RETURN a, bThe result shows that two selective contracts , ocds-07smqs-1405253 and ocds-07smqs-1422505, are associated with the tenderer acondicionamiento en potencia y comunicaciones sa de cv”.d. Retrieve all tenderers whose name is acondicionamiento en potencia y comunicaciones sa de cv”, the contracts in which it has participated as a tenderer, and other tenderers competing in these contracts.MATCH (a:Tenderer {name: acondicionamiento en potencia y comunicaciones sa de cv})-[:TENDER]->(b:Contract)<-[:TENDER]-(c:Tenderer)RETURN a,b,cThe picture shows that the company named acondicionamiento en potencia y comunicaciones sa de cv” has tendered and won two selective contracts in the year 2017 (ocds-07smqs-1405253 and ocds-07smqs-1422505) against the same competitor, faragauss del golfo sa de cv”.Red flags and more complex queriesRed flags of suspicious behavior are abnormalities which require additional investigation to confirm or rule out the presence of corruption or collusion in a contract.In 2016, Open Contracting Partnership in close collaboration with Development Gateway published the first guide to identify red flags throughout the entire procurement process: Red flags for integrity: Giving the green light to open data solutions.It is very important to remember that the presence of a red flag is not an indisputable proof that there is an irregularity in the contract, but suggests that it is necessary to continue investigating the case.Some of these flags involve exploring connections between the entities that participate in a contracting process. For example, business similarities between bidders (common addresses, personnel, phone numbers, etc) may suggest collusion between competitors.Collusion involves a horizontal relationship between bidders procurement, who conspire to remove the element of competition from the process. In the normal course, independent bidders in a procurement process compete against each other to win the contract, and it is via this mechanism that best value for money for the purchaser is achieved (OCDE, 2011)The following query retrieves all tenderers who shared contact information while competing for the same contract. The return statement includes all types of nodes. For that, an asterisk is used in the RETURN clause. Note that this query may take a few minutes to run.MATCH (p1:Tenderer)-[:TENDER]->(p4:Contract)                  <-[:TENDER]-(p3:Tenderer)WHERE p1 <> p3AND exists ((p1)-[:CONTACTPERSON|TELEPHONE|ADDRESS|EMAIL]->            ()<-[:CONTACTPERSON|TELEPHONE|ADDRESS|EMAIL]-(p3))RETURN *As you can see, this query retrieves an intricate network of tenderers, contracts, and contacts. Take into account that in many of these contracting processes an official of the procuring entity appears by default as a common contact of all tenderers who participate. This does not imply any irregularity, but it makes it difficult to detect true cases of collusion.Let’s focus our attention on one of the suspicious contracts detected in the previous query. In the selective contracting process ocds-07smqs-1520579, 3 companies tendered, two of them share the same address. The following query explores the first and second-degree relationships [*1..2] of that specific contract.MATCH (c:Contract{ocid:ocds-07smqs-1520579})-[*1..2]-(p) RETURN *When exploring the resulting network, we discovered that:Three tenderers (internet movil s de rl de cv”, tecnologia de diseno integral sa de cv”, and maysoft global s de rl de cv”) have competed in two other contracts.internet movil s de rl de cv” received two awards in these common contracts.The query detected that tecnologia de diseno integral” and maysoft global” share the same address.But this common address is not the only coincidence among these tenderers. There are other matches that were not detected by the query, but can be identified by visually inspecting the graph.Removing a prefix, internet movil s de rl de cv” and maysoft global s de rl de cv” share the same telephone.The domain of the email address of maysoft global s de rl de cv” is @internetmovil.com.mx, which seems more related to its competitor internet movil s de rl de cv”.Again, the presence of a red flag is not proof of an integrity problem, but it is a reason for future investigations.Using external data sourcesOpen contracting data is a powerful tool that can be used to tackle corruption at different levels. And its potential can be even greater when it is combined with other critical related datasets to respond to increasingly complex corruption schemas. See some examples in the following link.In Mexico, the Tax Administration Service (SAT) regularly updates a list of taxpayers which has been formally declared as shell companies.When the tax authority detects that a company has been issuing receipts without having the assets, personnel, infrastructure or material capacity to provide the services or produce the goods that support such receipts, or that such taxpayers are not located, it will be presumed the inexistence of its operations.7,889 taxpayers have been mentioned in the definitive list to April 2019. These companies are identified by RFC, the same property included in the Tenderer node.By crossing the list of shell companies with tenderers, we realize that 30 of these companies participated in contracts during 2017 and 2018. The next query looks for those tenderers and their respective contracts.MATCH (c:Tenderer)-[r:TENDER]-(p) WHERE c.rfc IN[IBI120525IV9,SSC161011DS3,QAL101119IG2,PAG1207103F3,CUR1405141F4,DCE0908051U8,GVI100428IM8,CPY110413I37,BAS090909A56,ADS101126FL0,GCA110429V28,SAM1401219X5,MCC091121EI3,GCC120614SA8,CIP050715F83,EPJ1011206R1,CEU150403968,CBX150529BJ5,HIN131218B16,FAC130318H39,SMB071126NX9,SAM1411187K9,KGI151019BG4,AGI1512015H0,CAC160129NA3,TDI120709PH7,LBL1505117S1,COC101213AK9,GPS980316TI6,GRE111110I43]RETURN *The result of the query allows us to appreciate that some of these shell companies participated in dozens of contracts and even won some bids. There are also a couple of contracting processes (ocds-07smqs-1654909 and ocds-07smqs-1496466) in which several shell companies participated together, although they were not winners.Another example of an external source of information that can be used is the Public Registry of Commerce. Here you can review for example who are the shareholders of the companies. The only drawback is that entering the system requires registering a username and password.In the previous section, we saw the case of internet movil s de rl de cv” and maysoft global s de rl de cv”. Two companies that seem to have business similarities based on their contact information.When looking for both companies in the Public Registry of Commerce, we realize that they have common shareholders. Again, these types of findings are not conclusive proofs of collusion, especially if the many exceptions stipulated in Mexican legislation for the assignment of selective contracts are taken into account.But they are a good starting point to inquire about the criteria used to invite and select the tenderers that participate in the suspicious contract.LimitationsSo far the queries presented suppose exact equalities between the textual terms, which supposes a very limited approach. The use of regular expressions and techniques such as edit distance can expand possible search patterns.Conclusion & InvitationThis tutorial did not pretend to be an exhaustive review of graph databases and its applications. Instead, we encourage our readers to explore and learn more about this amazing technology.Through the examples presented here, we can appreciate the enormous potential and flexibility that graph databases have to offer to the world of journalistic research. In the specific case of open contacting data, note that with small adjustments the proposed model can be applied to any of the countries or cities that are publishing data under the Open Contracting Data Standard.Now it is time to use what we learned to replicate and improve this exercise with similar datasets.Additional resourcesAnalyzing the Panama Papers with Neo4j: Data Models, Queries & MoreAs the world has seen, the International Consortium of Investigative Journalists (ICIJ) has exposed highly connected…neo4j.comHow the ICIJ Used Neo4j to Unravel the Panama PapersEditor’s note: The recently revealed Paradise Papers investigation used Neo4j as a key technology to find connections…neo4j.comNeo4j Graph Database Sandbox — Get Started with GraphsThe Neo4j Sandbox enables you to get started with Neo4j, with built-in guides and sample datasets for popular use…neo4j.comParadise Papers: an in-depth graph analysisToday, the ICIJ has publicly released data from its most recent year-long investigation into the offshore industry…neo4j.comAnalyzing the Paradise Papers with Neo4j: A Closer Look at Queries, Data Models & MoreOur friends from the ICIJ (International Consortium of Investigative Journalists) just announced the Paradise Papers…neo4j.comAnticorruption Open Data Package — International Open Data CharterWhilst recognizing the importance of transparency to inhibit corruption, it is key to go beyond the idea that…opendatacharter.netWritten by Dagoberto José Herrera Murillo (@djherreram, e-mail: d.j.herrera.murillo @ student.tue.nl).;Apr 29, 2019;[]
https://medium.com/neo4j/getting-started-with-play-movies-251228c12f2c;Ljubica LazarevicFollowAug 19, 2021·4 min readGetting Started with :PLAY MoviesHow to get up and running on one of my favorite graphy examples in minutes on either the Neo4j Sandbox or Neo4j Aura Free tier.Markus Spiske on UnsplashThe :PLAY movies built-in Browser guide is one of my most favourite examples of getting started with a graph database. Why? I hear you ask! Here’s why:It provides an example that we call all relate to, we all know what movies are, what are the roles and responsibilities actors, directors, producers and writers, and we know how they’re all connected!It uses a pretty small data set— this means that if you want to examine every single data element in the database, you can.The browser guide walks you through the example, with descriptions of the data, and what’s going on in general.You get a challenging, but not impossible, introduction to Cypher, the Neo4j graph query language.You can have a play with the queries. Nobody is making you stick to the script, and experimentation is encouraged!You may be fooled by the small data set, but don’t be! Despite its size, you get hands-on experience with fun things such as Kevin Bacon numbers, shortest path, and even building your very own recommendations query!:PLAY movies is available absolutely everywhere. Wherever there is a Neo4j Browser, you’ll be able to access this example, whether you’re on Neo4j Aura, the Sandbox, Neo4j Desktop, or running your own version as a service or console.Excellent, so now that you know what the deal is, let’s get going! Below we’ll show you how to get set up on either Sandbox or Aura.Getting Up and Running on Neo4j SandboxNeo4j Sandbox is a Neo4j database with guided examples, hosted in the cloud. It allows you to work with sample data sets (or a blank sandbox!). By default, a Sandbox will last for 3 days, and can be extended to a maximum of 10 days total. You are free to try out as many sandboxes as you like.First of all, navigate to the sandbox page, login (or register), and then select a blank sandbox:New project landing pageOnce you’ve selected ‘Blank Sandbox’, click on Launch Project. A new blank Neo4j instance will be created in the background. Once it’s ready, click ‘Open’ and select ‘Open with Browser’Select the ‘Open’ Button and Select ‘Open with Browser’Getting Up and Running on Neo4j Aura Free TierNeo4j Aura is Neo4j’s Database as a Service offering. Everybody can get a free instance Neo4j Aura, as Neo4j Aura Free tier. Available forever, Neo4j Aura Free tier allows you to create databases of up to 50k nodes and 175k relationships.First of all, navigate to the Neo4j Aura landing page, logging in (or registering) as required:Click on Create a databaseClick on ‘Create a database’ to get you to the next screen:Select ‘Aura Free’, give your database a name, and then click ‘Create Free DB’. Make sure, when prompted, that you make a copy of your database password, and store it in a safe place! The database will then take a few minutes to be created.Select ‘Open with’ and click ‘Neo4j Browser’As with Sandbox, select ‘Open with’ and choose ‘Neo4j Browser’.Starting the :PLAY Movies Browser GuideWe will be using Neo4j Browser for exploring the Movies example dataset. Neo4j Browser is a developer-focused tool that allows you to execute Cypher queries and visualize the results. It is the default developer interface for all versions, variants and editions of Neo4j. It comes out-of-the-box with all of Neo4j’s graph database offerings.Having opened Neo4j Browser in either Neo4j Sandbox or Neo4j Aura, you will be presented with something looking similar to this:Just type in :PLAY movies and press the ► to run the query. The Movie Graph browser guide will appear. Press the pin (red circle) to stop the frame from moving down the screen as you run queries, and the > arrow (blue circle) to advance to the next slide of the browser guide.You may find the following links helpful:Neo4j Browser developer guideCypher query language developer guideHave fun!;Aug 19, 2021;[]
https://medium.com/neo4j/youre-invited-to-the-most-graph-y-virtual-developer-conference-ever-15cc4b214262;Karin WolokFollowMay 28, 2020·3 min readYou’re Invited to the Most Graph-y Virtual Developer Conference EverDid you miss the Neo4j Online Developer Expo & Summit (NODES) with sessions on Graph Visualization, GraphQL, Advanced Cypher, Tuning, App Development, and Data Science last year?That’s ok! We collected all our lessons learned from last year to bring you an even better experience for NODES 2020! (I know what you’re thinking…how is that even possible?) DMore details to come, but to give you a little glimpse of what you can expect:Imagine a virtual place where all the technical people (who love graphs as much as you!) gather together for one full day to talk about all things graphs.Yeah … it’s kind of like that —Packed into one full day, you’ll experience 8+ hours loaded with technical presentations by the graph experts of the world.The topics will be highly technical (you know those, the gettin’ deep in the code” kind of talks). They will be spread across multiple tracks throughout the day to ensure you’ll always have a topic that’s interesting to YOU.Save your Spot!Register now to make sure you don’t miss a thing!Interested in speaking??Whether it’s your first time or you’re a seasoned graph enthusiast, we’re looking for talks of all levels, so we encourage you to submit! Call-for-Papers is open NOW!If you’re not sure what to submit, check out last year’s sessions for inspiration. Also don’t be afraid to share things you’ve learned or problems you’ve solved, those are super interesting talks.Neo4j Certified Professionals Training SessionsWe have an extra little bonus this year!All Neo4j Certified Professionals will have access to hands-on virtual training sessions in the two weeks prior to the summit.More info about how to access the training sessions can be found HERE.If you’re not certified yet? Go ahead and get certified! The exam is free and takes less than 1-hour!We look forward to seeing you at NODES 2020 on October 20 2020 (aka 20–10–2020) !!!Can’t wait until October to get your ‘graph-fix’?Check out the talks from NODES 2019 and sign up for our weekly developer newsletter This Week in Neo4j”, which is basically the written version of a NODES track every week :);May 28, 2020;[]
https://medium.com/neo4j/into-the-multiverse-exploring-multiple-scenarios-for-structural-balance-48c460462510;Nathan SmithFollowOct 26, 2019·8 min readInto the multiverse: Exploring multiple scenarios for structural balanceIn a previous post, I explored the concept of structural balance in a graph. We saw how Antal, Krapivsky, and Redner used structural balance theory to explain geopolitics in the run-up to World War I. We built a simulation of social dynamics with Neo4j based on that premise, in which we selected an edge of an unbalanced triangle at random and flipped it.If we think about structural balance in the real world, it seems not all relationships are equally likely to flip. If there are two powerful nodes that dislike each other so strongly that they will not reconcile, then the balance theorem says that all other nodes will divide into two camps, aligned as a friend of one of the powerful nodes and an enemy of the other.You can picture this on a personal scale when two people get in a fight, and their mutual friends feel pressured to take sides. In politics, it reminds me of the US and the USSR during the cold war.I invited readers to visit the National World War I museum in Kansas City in my last post, and I would be remiss if I didn’t also encourage you to visit the Harry S. Truman Presidential Library in Independence, Missouri after it reopens from a major renovation in 2020. Items in their collection make you feel like you are right there observing crucial decisions that changed the course of the twentieth century.Harry S. Truman Presidential Library. Photograph by Michael Barera CC BY-SA 4.0, via Wikimedia CommonsIf we leave the case of super-power nodes and assume all nodes have equal influence, some edges still might be more inclined to flip than others. Each edge is part of multiple triangles. If an edge feels pressure from participation in unbalanced triangles to flip, there is a competing pressure from participating in other balanced triangles to remain the same. This idea is similar to the idea of constrained triad dynamics that T. Antal, P. L. Krapivsky, and S. Redner discuss in their paper Dynamics of social balance on networks.”We’ll explore this idea with Neo4j in this blog post. We’ll make a rule that the edge that participates in the most unbalanced triangles will flip at each step in our simulation. If there are multiple edges that tie for the most unbalanced triangles, we will branch our simulation into multiple child scenarios, one for each edge in the tie. Instead of a linked list, we’ll end up with a tree of scenarios that looks something like this.Here are the steps we want to execute.For each active scenario, identify all edges participating in unbalanced triangles.Select the edges that participate in highest number of unbalanced triangles for that scenario. These are our frequently unbalanced” edges.For each frequently unbalanced edge, create a copy of the country graph. Update the scenario name and increment the age on the nodes in the copies.Flip the type of the frequently unbalanced edges from :LIKES to :DISLIKES or from :DISLIKES to :LIKES. This will balance multiple triangles in each scenario.Take a snapshot of the current state of each scenario.Delete the country nodes from prior iterations of the algorithm.Repeat these steps until all scenarios have converged.We start with the same setup as in the previous post with six country nodes, but we add a scenario property to the nodes. Again we have about half of the countries like each other with the remaining relationships set to DISLIKE. If you check Enable multi statement query editor” in your Neo4j Browser preferences, you can run all three of these statements at once.UNWIND [Italy, Germany, Great Britain, Russia,         Austria-Hungary, France] AS countryNameCREATE (c:Country {name:countryName, age:0, scenario: 0 })RETURN cMATCH (c1:Country), (c2:Country)WITH c1, c2, Rand() AS randWHERE c1.name < c2.name and rand < .5MERGE (c1)-[:LIKES {likeCount:1}]->(c2)RETURN c1, c2MATCH (c1:Country), (c2:Country)WHERE c1.name < c2.nameAND NOT (c1)-[:LIKES]->(c2)MERGE (c1)-[:DISLIKES {likeCount:0}]->(c2)RETURN c1, c2Check your scenario starting point.MATCH (c:Country) RETURN cThe makeSnapshot procedure is like the one we used in the last blog post, but it includes the scenario property.CALL apoc.custom.asProcedure( makeSnapshot , match (c1:Country)-[rel]->(c2:Country)WITH c1.scenario AS scenario, c1.age as age, collect(c1.name +  |  + c2.name) AS keys, collect(type(rel)) AS valuesMERGE (s:Snapshot {scenario:scenario, age:age})WITH s, keys, valuesCALL apoc.create.setProperties(s, keys, values)yield nodeRETURN node,  write , [[ node ,  NODE ]], [],  Create a snapshot of the current state of the scenarios. )If you are running this code in the same environment as the code from the last blog post, you might have to clear query cache to get our updated makeSnapshot function to overwrite the old one.CALL dbms.clearQueryCaches()CALL custom.makeSnapshot() YIELD node RETURN nodeThe findUnbalanced function returns all of the paths that participate in unbalanced triangles for a scenario.CALL apoc.custom.asFunction( findUnbalanced ,  MATCH p1 = (c1:Country {scenario:$scenario})-[rel1]->(c2:Country),p2 = (c1:Country)-[rel2]->(c3:Country),p3 = (c2:Country)-[rel3]->(c3:Country)WHERE(rel1.likeCount + rel2.likeCount + rel3.likeCount) % 2 = 0RETURN apoc.coll.flatten(collect([p1, p2, p3])) AS unbalancingPaths ,   LIST OF PATH , [[ scenario ,  STRING ]], true, Get the edges that participate in unbalanced triangles for a scenario. )The getFrequentUnbalanced function will figure out which unbalancing paths appear in the most triangles. If there is a tie, we will keep all paths that tied and we will branch the scenario for each path.CALL apoc.custom.asFunction( getFrequentUnbalanced , UNWIND $unbalancedPaths as pWITH p, count(*) as unbalancedCountWITH unbalancedCount, collect(p) as freqPathsORDER BY unbalancedCount DESCLIMIT 1RETURN freqPaths , LIST OF PATH ,[[ unbalancedPaths ,  LIST OF PATH ]],true, Find the paths that participate in the largest number of unbalanced triangles. )Next, we create a cloneScenario procedure that clones the countries and relationships in a scenario and updates the scenario and age properties on each node.CALL apoc.custom.asProcedure( cloneScenario , MATCH (country:Country {scenario:$parentScenarioID})WITH collect(country) as countriesCALL apoc.refactor.cloneSubgraph(countries) YIELD output AS scenarioSubgraphWITH scenarioSubgraphSET scenarioSubgraph.scenario = scenarioSubgraph.scenario + . + $childScenarioID,scenarioSubgraph.age = scenarioSubgraph.age + 1return scenarioSubgraph , write ,[[ scenarioSubgraph ,  NODE ]],[[ parentScenarioID ,  STRING ],[ childScenarioID ,  STRING ]], Duplicate the subgraph for a scenario and update properties. )Our last building block is an updateEdge procedure that will flip the type of the edge between to nodes.call apoc.custom.asProcedure( updateEdge , WITH $endpoints[0] AS e1, $endpoints[1] AS e2MATCH (e1)-[rel]-(e2)SET rel.likeCount = (rel.likeCount + 1) % 2WITH rel, e1.age AS agecall apoc.refactor.setType(rel, CASE type(rel) WHEN LIKES THEN DISLIKES else LIKES END)YIELD outputreturn age , write ,[[ age ,  INTEGER ]],[[ endpoints ,  LIST OF NODE ]], Flip the type of the edge between two nodes. )Now we’re ready to put it all together. The apoc.periodic.commit statement will execute the code block repeatedly until it returns 0.CALL apoc.periodic.commit( MATCH (c:Country)WITH DISTINCT c.scenario AS scenarioWITH scenario, custom.findUnbalanced(scenario) AS unbalancedPathsWHERE size(unbalancedPaths) > 0WITH scenario, custom.getFrequentUnbalanced(unbalancedPaths) AS freqPathsWITH scenario AS parentScenarioID, apoc.coll.zip(freqPaths, range(0, size(freqPaths)-1)) AS childScenariosUNWIND childScenarios AS childScenarioWITH parentScenarioID, childScenario[0] AS unbalancingPath, toString(childScenario[1]) AS childScenarioIDCALL custom.cloneScenario(parentScenarioID, childScenarioID) YIELD scenarioSubgraphWITH scenarioSubgraph.scenario AS scenario, scenarioSubgraphWHERE scenarioSubgraph.name in [n in NODES(unbalancingPath)|n.name]WITH scenario, collect(scenarioSubgraph) AS endpointsCALL custom.updateEdge(endpoints)YIELD ageWITH MAX(age) AS maxAgeMATCH (d:Country) WHERE d.age < maxAgeDETACH DELETE dWITH count(*) as delCountCALL custom.makeSnapshot()YIELD nodeRETURN delCountLIMIT 1 )Each snapshot has been created as an unconnected node. Connect them into a tree.MATCH (s1:Snapshot), (s2:Snapshot) WHERE left(s2.scenario, size(s1.scenario) + 1) = s1.scenario + .AND s2.age = s1.age + 1with s1, s2MERGE (s1)-[:LEADS_TO]->(s2)Take a look at the results. You will have to do a little dragging around in Neo4j Browser to get your graph to look like a tree.Each branch in the snapshot tree represents a unique sequence of edge flips, but often the various paths end up at the same structurally balanced state when they reach the leaf nodes. Add a label to the leaf nodes identifying the unique outcomes of the simulation.MATCH (s:Snapshot) WHERE not (s)-[:LEADS_TO]->()WITH s, apoc.any.properties(s) AS propMapWITH s, apoc.map.removeKeys(propMap, [ age ,  scenario ]) AS countryMapWITH countryMap, collect(s) as snapshotListWITH collect(snapshotList) as listOfListsWITH apoc.coll.zip(listOfLists, range(0, size(listOfLists)-1)) AS numberedListsUNWIND numberedLists AS olWITH olunwind ol[0] as snapshotcall apoc.create.addLabels(snapshot, [ Outcome   + ol[1]]) YIELD nodeRETURN nodeWhen I look at my snapshots after adding the :Outcome labels, I can see that all four of my unique paths end up at the same state despite flipping edges in different orders to get there.It can also be interesting to see how an individual relationship looks at the same age across multiple scenarios.MATCH (s:Snapshot) RETURN s.age, sum(case s.`France|Russia` when  LIKES  then 1 else 0 end) AS likeCount, sum(case s.`France|Russia` when  DISLIKES  then 1 else 0 end) AS dislikeCount, sum(case s.`France|Russia` when  LIKES  then 1.0 else 0.0 end)/count(*)  AS likePercentORDER BY s.ageFlipping the edges that participate in the highest number of unbalanced triangles causes our simulations to converge in a small number of steps, often with few branches. If your snapshot graph has many branches, that means there were many ties for the designation of most unbalancing edge.” That’s a sign of symmetry in your graph. Notice the symmetry in this unbalanced set up.The symmetrical starting layout leads to this graph of snapshots with 48 unique scenarios leading to three unique outcomes.It would be interesting to explore more formally the initial conditions that lead to various outcomes with this algorithm. If you have done that, or know someone who has, I’d be happy to hear about it in your comments!;Oct 26, 2019;[]
https://medium.com/neo4j/using-neogma-to-build-a-type-safe-node-js-app-with-a-neo4j-graph-database-f289d79dbc52;Jason AthanasoglouFollowJan 11, 2021·15 min readUsing Neogma to build a type-safe Node.js app with a Neo4j graph databaseThis is an introduction to Neogma, an open source Object-Graph-Mapper for Node.js which utilizes TypeScript. It achieves a fast, easy and safe way to interact with a Neo4j database.A simple example of Neogma usage.OverviewThis article will demonstrate how to Create, Read, Update, and Delete nodes and relationships of a Neo4j graph database by using the Neogma framework for the Node.js runtime environment for JavaScript. We’ll also build a simple app to find common movie interests between the users.Traditionally, you may use the Neo4j JavaScript driver, with the Cypher query language, by way of string statements to interact with the database. For example, in order to create a node with the label Movie and the name Inception, you need to run the statement CREATE (:Movie { name: $name }), with a parameter of { name:  Inception  }However, with Neogma, you just need to call a simple method:Movies.create({ name: Inception })and Neogma takes care of generating the intended Cypher, by using the label you’ve defined and automatically generates and runs the query by setting the correct parameters!By defining Models, you can benefit from the automated methods of Creating, Reading, Updating, and Deleting nodes and relationships. By using the Query Builder, you easily create and run flexible queries to suit your needs.Using TypeScript is recommended as Neogma has built-in types which are used in every part of it. However, it can always be used with plain JavaScript.You can find the final source code of this article here.In case you want to build something on your own, you can always refer to the detailed Neogma Documentation.Getting StartedYou need to have the following installed:Neo4j. After you install it, set it up so a graph database is running. Take note of the connection information (url, username, password).Alternatively, a Neo4j sandbox can be used out of the box without having to install anything.Node.js(optional) Git, for version control.(optional) Yarn, to be used instead of npm. However, every yarn command can be replaced by the equivalent npm command.Next up we need to create and configure the project. If you are already familiar with TypeScript, you can use your own configuration.Otherwise, you should use this template I’ve created where everything is pre-configured. You can either click on the Use this template button to create a new repository, or you can just download the code.Then, navigate to the project root and run yarn. To make sure it is set-up correctly, edit the src/app.ts file to add a simpleconsole.log(hello, neogma!)You can run the app by running yarn build followed by yarn start at the root project directory.We’re now ready to start building our app, starting by creating the Models, populating data, and querying our graph!Creating the ModelsThe first thing we need to do is add Neogma to our project.While on the root project directory, run yarn add neogma. This installs neogma from the npm registry and adds it as a project dependency.Next up, we start using Neogma by creating an instance of it.Add the following to src/app.ts.You need to replace the url, username, password fields with your own connection. This will also enables logging the queries to the console, so we can see them in action. If you cannot connect like this because your connection is encrypted, you need to change the corresponding field.Now, let’s think about how our models will be like.In our application, there are Users and Movies. A User can indicate that they like a Movie. In our graph, we need User nodes, Movie nodes, and a relationship about a User liking a Movie. Our simple database will consist of the following nodes and relationships:Each node is labelled, and is represented by circle. A relationship has a name and a direction, and is represented by an arrow between the two nodes it relates. All of them belong in the same graph.Now let’s define our first Model!Create a directory models in src, and a file Movies.ts inside it. Let’s start editing the src/models/Movies.ts file:1. First of all we need to import the Neogma instance we have created:import { neogma } from ../app2. Next up, we need to create some interfaces so we can have type-safety with TypeScript:import { ModelFactory, NeogmaInstance } from neogmaexport type MoviesPropertiesI = {    name: string    year: number}export interface MoviesRelatedNodesI {}export type MoviesInstance = NeogmaInstance<    MoviesPropertiesI,    MoviesRelatedNodesI,>MoviesPropertiesI defines what properties each Movie node has. Here we define a string property called name and a number property called year.MoviesRelatedNodesI defines what relationships this Model has. We can leave it empty as we’ll define the relationship between Users and Movies at the Users Model.Finally, we define the interface of the Movie instance. This corresponds to a node in the database and provides access to its properties, useful methods for editing its data etc.3. Now, we can create our Movies class, which is the last part of our src/models/Movies.ts file as a whole:As we can see, the Model class is created by calling ModelFactory and using the interfaces we declared above. Also, we provide:The label that the nodes of this Model have.Optionally a field which will be considered a (unique) primary key. This is needed by Neogma only for calling some methods of the Instances of Movies. An index can be created on this field for performance benefits, in case many Movie nodes exist in the database.Our schema, consisting of the properties of each node and a validation for them.Now let’s define the Users Model (src/models/Users.ts).It’s very similar to the Movies Model, but we additionally include the information for the User-Likes-Movie relationship.We need to import ModelRelatedNodesI, a helper type we’ll use. We also need to import the Movies Model and the MoviesInstance interface.import { ModelFactory, NeogmaInstance, ModelRelatedNodesI } from neogmaimport { Movies, MoviesInstance } from ./Movies2. We need to define the type for the relationships, for type-safety benefits.export interface UsersRelatedNodesI {    LikesMovie: ModelRelatedNodesI<typeof Movies, MoviesInstance>}We define a relationship configuration with the LikesMovie alias, and the related Model which is Movies.3. When creating the Users class, we also need to provide some information about the relationship. In the first parameter of the ModelFactory call, we also include a relationships field:relationships: {    LikesMovie: {        model: Movies,        direction: out,        name: LIKES,    },},The relationships field has the relationship alias as its key. As its value, the relationship information is provided, including the related Model (Movies), the name of the relationship (LIKES) and the direction of it (from Users to Movies, so outwards).The complete code for src/models/Users.ts is:Will all this information, our Model classes are ready to be used for Creating, Reading, Updating, and Deleting nodes and relationships, with proper types for safety and ease of use. Let’s see them in action!Populating DataNow that our models are ready, we can start using them by creating some nodes and relationships.Let’s go back to our src/app.ts file. We need to import our models and some of their interfaces. This needs to happen after the neogma variable declaration. We additionally need a function for populating data and a main function which calls it. Finally, we need to call our main function:import { Users, UsersPropertiesI } from ./models/Usersimport { Movies, MoviesPropertiesI } from ./models/Moviesconst populateData = async () => {}const main = async () => {    // populate the data    await populateData()}main()It’s time to create our first node in the database! Inside the populateData function, add the following:await Users.createOne({    name: Barry,    age: 36,})The above function creates a node with the given name and age properties. They are validated, and a runtime error is thrown if they are invalid. Additionally, a TypeScript error is thrown on compilation time if a field is missing or is of the wrong type.The created node has the label that’s given at the Users Model definition (in this case, User).Let’s now create some Movies. Add:await Movies.createMany([    {        name: The Dark Knight,        year: 2008,    },    {        name: Inception,        year: 2010,    },])This acts in a similar fashion with createOne, while creating multiple nodes at the same time.An alternative way to save a node into a database is to build an Instance of the Model. To demonstrate this, let’s build an Instance of the Users Model:const cynthia = Users.build({    name: Cynthia,    age: 21,})This Instance is of the UsersInstance type we defined earlier. It has a lot of useful methods we can use.At this stage, this Instance isn’t saved in the database yet. For it to persist, we need to call its save method:await cynthia.save()And just like that, our Instance is now a node in our graph!Now let’s relate this Instance to a Movie. We can use the Instance’s relateTo method:await cynthia.relateTo({    alias: LikesMovie,    where: {        name: Inception,    },})As we can see, this needs:1. The alias of the relationship. We provided it in our Users Model definition. With this, the relationship name, direction, and the related Model will be used automatically.2. A where statement, to match the nodes of the related Model.The above method relates the cynthia node with a node which has the name Inception. The target node has the information provided by the LikesMovie alias, which means: Its Model is Movies and its label is Movie. The relationship name is LIKES, with a direction from the User towards the Movie.Now let’s try something a bit more complex.We will create a User node and at the same time relate it with an existing Movie node. We’ll also create a Movie node on the fly and relate our User with it.We’ll use the createOne method, and the relationship alias key to indicate that this node will be related to a Movie.A where field indicates that the User we’re creating will be related to the existing Movie with the name The Dark Knight.A properties field indicates that a new Movie will be created with the given properties and will be related to the User.await Users.createOne({    name: Jason,    age: 26,    LikesMovie: {        where: {            params: {                name: The Dark Knight,            },        },        properties: [            {                name: Interstellar,                year: 2014,            },        ],    },})Let us now find a User from the database. The simplest way is by using the findOne static of the Users class:const barry = await Users.findOne({   where: {      name: Barry,   },})The found user, who has the name Barry, is represented by an Instance of the Users model. This means that we can easily access its properties and use its methods.By adding the following:console.log(`Barrys age is ${barry.age}`)We see that Barry’s age is 36 .We can easily edit Barry’s age by setting the age field and saving it to the database.barry.age = 37await barry.save()Since Barry already exists in the database, the last line will just update it, instead of create it like in Cynthia’s example. That way, by just saving an Instance, its data will always be correctly reflected in the database!Now, let’s have Barry like every movie by leaving the where statement empty:await barry.relateTo({    alias: LikesMovie,    where: {},})As a last step, let’s have Cynthia like Interstellar, which was created during Jason’s createOne call.await cynthia.relateTo({    alias: LikesMovie,    where: {        name: Interstellar,    },})Tip: If you’ve ran this populator more than once, you’ll see that nodes with the same names are created again. To solve this, we could have used a MERGE clause instead of a CREATE one, by using a merge: true parameter. However, for simplicity, in this example we can just delete all existing data (nodes and relationships) when the populator starts. Add the following line to the beginning of the populateData function to run it as a raw query:await neogma.queryRunner.run(MATCH (n) DETACH DELETE n)The final code for our populateData function inside src/app.ts is:By running the function, you can see in the console every query that was generated and ran by Neogma. Also, our graph has the following data:Users, Movies, and relationships that a User likes a MovieAnd we are ready to start querying it!Querying our GraphNow that our nodes and relationships are in place, it’s time to perform some queries on them. As we’ve already seen, we can find nodes by using the findOne or findMany static of a Model. However, for more flexible and complex queries, we can use the QueryBuilder class so we can construct a custom one directly from JavaScript objects.Let’s create a queryData function below our populateData function.const queryData = async () => {}Now let’s call it. Replace the main function with:const main = async () => {    await populateData()    await queryData()}Additionally, we need to import the QueryBuilder class from Neogma. Replace the existing ‘neogma’ import with:import { Neogma, QueryBuilder, QueryRunner } from neogmaWe’ll now see 3 different examples of querying our data. In each, we add the given code to our queryData function.Finding a specific User and a specific MovieA great introductory example is just finding a specific User and a specific Movie, regardless of their relationship.First we need to create an instance of QueryBuilder. On this instance we can call the methods which correspond to the clauses we want to use.Since each of them returns the instance, we can chain them to construct our query.The code which runs a query for matching a User and a Movie, and returning both of them is the following:const specificUserAndMovieResult = await new QueryBuilder()    .match({        model: Users,        where: {            name: Jason,        },        identifier: user,    })    .match({        model: Movies,        where: {            name: Inception,        },        identifier: movie,    })    .return([user, movie])    .run(neogma.queryRunner)In the first match parameter, we provide the Model we want to use. So, the nodes label will be User. We also provide a where parameter, so the matched node’s name needs to equal Jason. Lastly, we specify an identifier to be used in the query (user).The second match parameter is very similar to the first one, with the difference that we now match the Movie Inception and we use the movie identifier for it.The final part of the query construction is the return clause. We just return the identifiers we used earlier, user and movie.To run this QueryBuilder instance, we just need to call its run method by passing a QueryRunner instance. We can use the one that our Neogma instance has (neogma.queryRunner). The return value of this is the QueryResult of the Neo4j driver.The generated Cypher is:MATCH (user:`User` { name: $name }) MATCH (movie:`Movie` { name: $name__aaaa }) RETURN user, moviewith the parameters:{ name: Jason, name__aaaa: Inception }The return value is not typed since it’s arbitrary and not a result of a Model operation. We can use the getResultProperties static of the QueryRunner to get the properties from our result:const jason = QueryRunner.getResultProperties<UsersPropertiesI>(    specificUserAndMovieResult,    user,)[0]const inception = QueryRunner.getResultProperties<MoviesPropertiesI>(    specificUserAndMovieResult,    movie,)[0]console.log(`${jason.name} is ${jason.age} years old`)console.log(`${inception.name} came out in ${inception.year}`)For example, for jason: We use the UsersPropertiesI interface because we know that a User node has the properties that we defined. We pass the query result (specificUserAndMovieResult) and the identifier (user) as parameters. The results could consist of many records but we only care about the first (array element [0]). Then, we can access the node properties directly from the variables.When we run it, we get the following result:Jason is 26 years oldInception came out in 2010Finding the common liked Movies between two UsersLet’s find the common liked movies between Jason and Barry. We need to match a node for Jason, a node for Barry, and all the Movie nodes that both of them are related to.Thus, we’ll use the related field of a match parameter, which is an array of objects. Its first and third elements correspond to information about which nodes to match. Its second element represents information about the relationship between them (direction, and optionally a name and an identifier). This can continue indefinitely to relate more than 2 nodes. Each even entry describes a node, and each odd entry describes a relationship.In this case: User Jason(entry 0) is related (entry 1) to any Movie (entry 2) which is related (entry 3) to User Barry (entry 4).The query to find the common liked Movies between Jason and Barry is:const commonMoviesBetweenJasonAndBarryResult = await new QueryBuilder()    .match({        related: [            {                model: Users,                where: {                    name: Jason,                },            },            Users.getRelationshipByAlias(LikesMovie),            {                model: Movies,                identifier: movie,            },            {                direction: in,            },            {                 model: Users,                 where: {                     name: Barry,                 },            },        ],    })    .return(movie)    .run(neogma.queryRunner)In our related field in match we first match the User Jason.Then, we specify that the relationship configuration (name, label, direction) is the one we have defined in our Model and has the alias LikesMovie. To extract this information, we use the getRelationshipByAlias static of our Model Users.This relationship is between the node above this object (Jason) and the one below it (movie).We match a Movie and we give it the identifier movie.A relationship is matched between the node above (movie) and the one below (Barry). For its configuration, we just specify that its direction will be inwards (from Barry to movie).We match the User Barry.Finally, we return the nodes associated with the identifier movie and we run the query.The generated query is:MATCH (:`User` { name: $name })-[:LIKES]->(movie:`Movie`)<-[]-(:`User` { name: $name__aaaa }) RETURN moviewith the parameters:{ name: Jason, name__aaaa: Barry }To parse the result, we’ll use QueryRunner.getResultProperties again. This time we want all the records, not just the first one. We’ll map each record to just its name, and then join all those names with a comma before logging them.const commonMovieNames = QueryRunner.getResultProperties<MoviesPropertiesI>(    commonMoviesBetweenJasonAndBarryResult,    movie,)    .map((result) => result.name)    .join(, )console.log(    `Common liked movies between Jason and Barry: ${commonMovieNames}`,)The above code logs:Common liked movies between Jason and Barry: Interstellar, The Dark KnightFinding the most liked movieWe’ll find which Movie is liked by most Users, how many of them like it, and their average age.Again, we’ll use a match parameter with a related field.We’ll match any User, any Movie, and the relationship between them.We’ll return the Movie, the count of the relationships between any User and that Movie, and the average User age.Finally, we’ll order by the total likes, descending.const mostLikedMovieResult = await new QueryBuilder()    .match({        related: [            {                model: Users,                identifier: user,            },            {                ...Users.getRelationshipByAlias(LikesMovie),                identifier: likesMovie,            },            {                model: Movies,                identifier: movie,            },        ],    })    .return([        movie,        count (likesMovie) as totalLikes,        avg(user.age) as averageAge,    ])    .orderBy([[totalLikes, DESC]])    .run(neogma.queryRunner)In our related array, we first match a User while giving it an identifier. Then we use an object which spreads the return value of getRelationshipByAlias of the Users Model. We additionally give an identifier to the relationship match (likesMovie). We also match a Movie and give it an identifier.We return:1. The movie identifier, which represents the Movie node we’re getting info about.2. A count of the relationships between any user and that Movie. We assign the totalLikes identifier to it.3. The average age of the Users who like that Movie. We assign the averageAge identifier to it.We order by the totalLikes identifier, descending. Then, we run the query.The following Cypher gets generated, without any parameters:MATCH (user:`User`)-[likesMovie:LIKES]->(movie:`Movie`) RETURN movie, count (likesMovie) as totalLikes, avg(user.age) as averageAge ORDER BY totalLikes DESCWe can get the properties of the movie identifier with the QueryRunner.getResultProperties function. The totalLikes and averageAge identifiers are number, so we’ll get their values directly from the result.const movie = QueryRunner.getResultProperties<MoviesPropertiesI>(    mostLikedMovieResult,    movie,)[0]const totalLikes = mostLikedMovieResult.records[0].get(totalLikes)const averageAge = mostLikedMovieResult.records[0].get(averageAge)console.log(    `The most liked movie is ${movie.name}, as ${totalLikes} users like it! Their average age is ${averageAge}`,)By running it, we get the following output:The most liked movie is Interstellar, as 3 users like it! Their average age is 28Altogether, the entire queryData function is the following:And that’s all! As a reminder, you can find the complete code of this application here.ConclusionIn this article we’ve seen Neogma in action and how it gets rid of the need of manually creating Cypher statements and assigning parameters. It uses the Model definitions for validations and to automatically use the needed data (labels, relationship information, etc.) in queries.It also includes a powerful Query Builder, in case more custom queries are needed.All these are performed with type-safety, making development easier thanks to the built-in Neogma types.The simple application we created can extended to a complete application.With adequate data, we can offer each User personalized recommendations about which movies might be of interest (i.e. with Collaborative Filtering).Feel free to check out Neogma at GitHub, or get in contact with me for any question, recommendation etc.ImprovementsThis example application can be improved in order to better reflect a real-world application. The improvements branch implements the following:Remove the logger, so the generated queries don’t flood the console.Use a .env file for the connection configuration, instead of being directly in the code.Move the query functions to Model methods and statics. The app.ts file shouldn’t create and run queries, but just run methods or statics of our Models.Add an id property to each Model, which will also be the primary key field in its definition.We additionally define the User-Movie relationship at the Movies Model (using the reverseRelationshipConfiguration static of the Users Model). That way, the relationship information can also be obtained from the Movie’s perspective. It’s used in a query.The data populator should be a script which runs regardless of the main application. To achieve this more easily, moving the creation of the neogma instance to a new file is useful (init/neogma.ts).Reastically, the data will arrive externally, i.e. from a front-end or another back-end service.;Jan 11, 2021;[]
https://medium.com/neo4j/conversational-artificial-intelligence-with-neo4j-and-the-unreal-engine-part-1-6e3d4e56931;Antonio OrigliaFollowDec 10, 2022·9 min readConversational Artificial Intelligence With Neo4j and the Unreal Engine — Part 1The dream of Artificial Intelligence (AI) has accompanied our society since the pervasive advent of the digital age.Existing ApproachesDifferent approaches have been adopted throughout the years to model different aspects of intelligence. And other models have, instead, been developed to integrate intelligent capabilities simulations to build technological artifacts that exhibit a certain degree of flexibility.While the ability to communicate has always been associated with intelligence, it is only in recent years that new technologies have made possible a vast diffusion of voice-activated devices.As in many other fields, it has been the exponential growth in the performance of neural networks, powered by the availability of higher computational power, that has changed the perspective on the field of Natural Human-Machine Interaction (HMI).This, however, comes with a price: first of all, neural models need a humongous amount of data to reach their performance, thus becoming more and more exclusive to build. They are also hard to interpret, thus posing an explainability problem, which is currently investigated, for example, with probing methods.The same technological advances that power the race towards machine learning models, however, can actually be used to improve approaches coming from old-school AI, which were undoubtedly rigid, often being rule-based. But this also had the advantage of always providing clear explanations for their behavior.Combined ModelAn attempt to recover positive aspects of older approaches to AI can be made by leveraging modern computational power and relaxing rigid rule-based systems by mixing some statistics in them. This hybrid method, which has been gaining attention recently, avoids a one-wins-all approach to computational model selection for complex behavior simulation and intends to take the best out of multiple models, harmonizing them in such a way that the right tool is used for the right task in an explainable way.Framework for Natural Tools and Applications With Social Interactive Agents (FANTASIA)The Framework for Natural Tools and Applications with Social Interactive Agents (FANTASIA) was built upon this concept, and we will talk about it in Part 2 of this article. Here, I will concentrate on how Neo4j supports cross-disciplinary research between technology and humanities researchers.In this article, I will describe the research conducted in the field of Embodied Conversational Agents (ECAs) at the URBAN/ECO Research Centre of Federico II University in Naples.Interactive Graph of Knowledge With Neo4jIn particular, I will describe how Neo4j helps bring the massive amount of information possessed by humanities researchers inside technological frameworks for Real-Time Interactive 3D applications (RTI3D) powered by the Unreal Engine.Being a very natural way to represent data, researchers from fields like linguistics, art history, or architecture easily acquire the basic concepts needed to interpret their data, research procedures, and make conclusions as graphs. Moreover, Neo4j makes graph database technology accessible to create, modify, and query graph data, so that humanities knowledge can easily be represented and investigated inside a framework that lets, at the same time, computer scientists enrich those representations with algorithmic approaches.For the specific case of linguistics research, graph structures offer the possibility to cross-reference encyclopedic knowledge sources with dialogue corpora, thus jointly representing domain knowledge and the way people communicate about it.Interdisciplinary CollaborationInterdisciplinary research can then be conducted with a mutual exchange, as computer scientists add new perspectives to the data using graph data science for linguists to exploit, and at the same time, use analyses conducted by linguists for explainable AI design. Collaboration among groups with different needs and expertise is a classic application case of Neo4j: in our case, graph-based approaches to cross-disciplinary research help computer scientists design dialogue systems, and they bring new tools to humanities researchers, who are directly involved in the interaction design process, rather than just being content providers.It is worth noting that the presence of humanities researchers is particularly precious to building explainable-by-design systems. These are of interest to the academic community, as they allow testing theoretical models to check if they adequately describe dialogue dynamics, updating an investigation strategy that was popular with logical engines. This also has long-term implications for the industry though, as explainable-by-design systems typically need less data and computational power to work, as only some parts are built by relying on machine learning.Studying dialogues with graphsFrom a methodological point of view, analyzing dialogues concerning the domain of interest is the foundational part of the building process of ECAs we adopt.The goal is to represent how people talk about the domain of interest, preferably in the same context where the ECA should operate. Knowledge graphs are a popular choice to represent domain information supporting technological applications and dialogue-based applications are often based on knowledge graphs, so it is useful to use them to represent and study the relationship between how people make use of the domain during human-human dialogues.This supports modeling how a machine should mimic their behavior. Explainable-by-design systems do not rely only on machine learning to make these strategies emerge so it is important to cross-reference recorded dialogues with the background knowledge concepts they use. One of the case studies we adopt to explore how linguistics research methods can be directly linked to ECAs development is the movie recommendation task.This has been deeply explored, in the past, and there are a lot of available resources on which to test data collection, annotation and analysis. In our case, our ongoing work aims at formalizing a methodology to organize both data and linguistic knowledge as graphs connecting multiple resources, to extract more information from the combination of the resources. We refer to this methodology as Linguistically Oriented Resources and Insights as Expressive Graphs” (LORIEN).Data SourcesTypically, we start by importing common knowledge from Linked Open Data (LOD) sources, using Wikidata as a starting point. Alternative sources are, then, cross-referenced with the Wikidata graph to support bridging and analyses. In the movie recommendation domain, we extract movies, people that worked in those movies, the genres they belong to, the awards they won and the people they were awarded to.This subgraph represents what, in linguistics, is referred to as the Communal Common Ground (CCG): the set of background knowledge that is possessed by people belonging to a community. Briefly, it represents encyclopedic knowledge about the domain that can be considered objective and generally available. Next, we import human-human movie recommendation dialogues coming from the Inspired corpus¹, representing utterances, in each dialogue, as nodes and linking them through relationships chains.Linking Utterances to the Common GroundAlso, we link the utterances to the elements of the CCG they refer to. Since dialogues collected from a corpus of actual Human-Machine interactions cannot be considered common domain knowledge, they represent the Personal Experience (PE) the system has about dialogue management in the domain of interest. The dialogues sub-graph, once again, corresponds to a linguistic concept to support theoretical explainability of the model and, later, of the application behavior. A simplified extract of the graph is shown in the Figure.Merging the Knowledge Domain graph with a movie recommendation dialogue corpus (Inspired)With such a structure, it is possible to jump between the collected dialogues and the CCG to explore more deeply the dialogue strategies followed by the parties involved in the Inspired corpus. First of all, it is possible to analyze the use of CCG items throughout the dialogues to extract some interesting patterns. From the following Figure, for example, it can be observed that people, in the first phases of the dialogue, tend to talk more about genres than they do in the latest parts.Distribution of named entities throughout dialogues in the Inspired corpusAnother interesting pattern that emerges from the resources cross-referencing is that people tend to talk about recent movies, as shown in the following Figure.Histogram of the release years for movies people talk about in the Inspired corpusThese, in general, are intuitive strategies as restricting the search field by focusing on categories first and talking about recent movies are natural choices but having a machine automatically adopt these strategies by itself is not trivial.You should consider, in fact, that we are not interested in the movie recommendation task per se, but rather on the general dialogue management strategies that hold independently of the domain.We do not seek to solve the movie recommendation task alone: we rather investigate the underlying dialogue management mechanisms that let domain-specific strategies emerge. Our goal is to find the general principles that would let a conversational agent move through the domains with minimal reconfiguration effort.Contextual InformationIn linguistics, many concepts are context-dependent, meaning that the same utterance may have different meaning or even be understandable depending on how the dialogue has been evolving. By connecting dialogues and the CCG, it is possible to adopt disambiguation strategies by considering the sub-graphs representing interactions up to the point of interest.For example, in the case of movies, the CCG built from Wikidata contains more than a hundred people named Chris Evans so, when a person refers to a Chris Evans, performing entity linking becomes easier if we consider that, a few turns before, people were talking about Knives Out, in which Chris Evans appeared.This kind of ring-like pattern, shown in the following Figure, can be used for disambiguation or to investigate more in depth dialogue dynamics and understand how people are using the domain to negotiate a common set of shared beliefs, which is called Personal Common Ground (PCG). You can find out more about how this database was built and analyzed in the dedicated paper².A ring-like pattern can be found in the graph to disambiguate which Chris Evans people are talking about. This pattern is also interesting to investigate why people talk about actors or movies at specific points in the dialogueGraph Analysis ProcessingLORIEN is meant to extend research methods based on corpus linguistics by introducing a set of analysis tools that draw from graph theory to support better understanding of dialogue dynamics.After the data representation step, graph analysis tools allow to find regularities that can be exploited both to form new background theories and to support technological approaches built upon them.For the case of the movies, for example, after detecting that people talk about genres in the first phases of the dialogue, we perform network analysis with the HITS algorithm, implemented in the Neo4j Graph Data Science library, to discover which network characteristics genre nodes have that can explain why they are preferred in that phase.Since nodes representing genres emerge from the network as having a very high authority score, this suggests the general dialogue management principle that, while collecting information about the interlocutor, their position concerning authoritative nodes should be discovered first. Other information comes from sources like Movielens, to estimate how popular movies are for the general public or IMDB for catalographic information and plot texts.Results obtained through LORIEN are meant to be used in dialogue systems for ECAs so, in the next part of this article, I will describe the technological framework we develop on the basis of LORIEN structures.This includes FANTASIA, an Unreal Engine plugin to support development, and a computational model to reinterpret linguistic knowledge in a general framework for argumentation-based dialogue we call Artificial Neural and Graphical Models for Argumentation Research (ANGMAR).References¹Hayati, S. A., Kang, D., Zhu, Q., Shi, W., & Yu, Z. (2020). INSPIRED: Toward sociable recommendation dialog systems. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 8142–8152²Origlia, A., Di Bratto, M., Di Maro, M., & Mennella, S. (2022). A Multi-source Graph Representation of the Movie Domain for Recommendation Dialogues Analysis. In Proceedings of the Thirteenth Language Resources and Evaluation Conference (pp. 1297–1306).;Dec 10, 2022;[]
https://medium.com/neo4j/introducing-myself-using-a-graph-16029b68b8ab;Frederico BragaFollowJan 6, 2019·3 min readIntroducing myself using a GraphI was looking for an easy way to explain what a Graph Database is, using simple language and an example data-set that fits the way I present. Perhaps this will show the inner geek in me a bit too much but I thought, why not? :)Normally my meetings start with introductions and I said to myself: ‘Why not create a graph database with my introduction?’ This would show how easy it is to ‘read’ a graph-based data model.I was inspired by this article which shows how a Graph DB can be helpful in gaining insights from the diverse skills and projects of the workforce at a large company:Daimler drives new HR insights with graph database technologyLast week, Daimler CEO Dieter Zetsche was at the White House for trade talks with President Trump, along with his peers…diginomica.comSo I started by using the Arrows tool to create my Model:After that, I just copied the code automatically generated by Arrows:CREATE   (`0` :Person {name:Frederico Braga}) ,  (`3` :Country {name:Switzerland}) ,  (`4` :Company {name:HighPoint}) ,  (`5` :Company {name:Medidata}) ,  (`6` :Skill {name:graph databases}) ,  (`7` :GraphDB {name:My CV}) ,  (`8` :Country {name:Portugal}) ,  (`0`)-[:`lives_in` {from:01.05.2011,to:today}]->(`3`),  (`0`)-[:`works_for` {from:01.02.2018,to:today}]->(`4`),  (`0`)-[:`worked_for` {from:01.04.2017,to:31.01.2018}]->(`5`),  (`0`)-[:`has` ]->(`6`),  (`7`)-[:`requires` ]->(`6`),  (`0`)-[:`developed` ]->(`7`),  (`0`)-[:`born` {year:1977}]->(`8`),  (`7`)-[:`used` ]->(`6`)Next, I created my Neo4j database using Neo4j desktop and pasted it into the Neo4j browser. That’s it.After creating the model I decided I wanted to add another Label and a node for an Application I built. Now I can see my DB schema running the commandcall db.schema()Nice, I have my intro ready, now back to the slides and to write some queries.Who do I work for?match (p:Person)-[w:works_for]-(c:Company) where p.name=Frederico Braga return  p, w, cWho have I worked for?match (p:Person)-[:works_for]->(c:Company),       (p:Person)-[:worked_for]->(oc:Company) where p.name=Frederico Braga return p, c, ocWhich skills have I used?match (p:Person)--(s:Skill)--(a) where p.name=Frederico Braga AND (a:Application OR a:GraphDB)return p, s, aThe more traditional way…Not sure anyone had this idea before but I think its an easy, intuitive and time-saving way to explain some of the characteristics of graph property models while introducing yourself. It will likely be much more interesting once I start combining it with other colleagues data which is why Daimler decided to use this technology.One of the great benefits of graph technology is that it can help discover ‘hidden’ relationships in data, certainly useful when it comes to understanding people’s skills, right? What do you think?;Jan 6, 2019;[]
https://medium.com/neo4j/summer-of-nodes-final-week-exploring-the-area-ac4b24735612;Ljubica LazarevicFollowAug 24, 2020·9 min read*Update! Now with hints and solution!*Summer of Nodes: Final Week — Exploring the AreaMissed the live stream? Catch up here!Missed the hints stream? Catch up here!Missed the solutions stream? Catch up here! (sorry about the echo!)Series playlistHello everybody!Summer of Nodes 2020 is now over. If you’ve not had a chance to look at the challenges, you can always have a go at your leisure:The socially distanced barbecueThe online day outThe whodunitExploring the area (this post)This week’s theme — exploring the areaCentral Park in New York is one of the most popular tourist attractions in the city. Last year, Central Park saw 42 million visitors alone. This week we are going to be using this amazing location to do some exploration, with graphs!We have imported some routes and tagged Points of Interest for Central Park, based on OpenStreetMap, and we will be asking you to take on the role of the virtual tourist. We will be using the sandbox exclusively for the challenge this week.For those of you who are interested, the data was extracted with this plugin, and there was some Cypher data wrangling applied after import. As a result, we don’t have all the sights, but we do have a pretty good sample to work with.Before we get stuck in…We’d like to draw your attention to the excellent graph app, NeoMap map visualiser created by Estelle Scifo.Points of Interest from this week’s challenge data setThis fantastic app allows you to plot spatial data onto an OpenStreetMap view. You can see an example here where we’ve plotted points from this week’s data set to view. To find out how to get going, you can read about it here.You will probably find it helpful and fun to try plotting out the various Points of Interest we ask you to look for during the challenge. You can even plot routes!To install NeoMap, you just need to paste the following link into the graph apps tab in Neo4j Desktop:https://registry.npmjs.org/neomapTo run NeoMap, you will need to create a remote database in Neo4j Desktop using your sandbox credentials, and then open NeoMaps there.If you’re not sure how to follow the above steps, check back here when we’ll add a link on how to do it soon.Estelle also gives you an example of how to get up and running with your own locations if you want to check out your local area too! It’s also useful for seeing how to work with shortest path.Overview of the dataWe will be using the sandbox database this week. It’s probably worth mentioning a little bit about the data. Should you go and CALL db.schema.visualization() you will be greeted with this…Don’t panic! Many of these labels we are not going to use. There are only two node label types we’re going to be using:OSMNode — Think of this as a junction node — it hooks together routes from specific pointsPointOfInterest:OSMNode — In addition to the above, this also has our points of interest, e.g. types of statues, restaurants, tennis courts, etc. and their namesAnd we’ll only be using the ROUTE relationship type. Here’s a simplified data model that you’ll need for the challenges:Note that you might use either the latitude/longitude or location (point). Check out the spatial documentation on how to use either.Beginner’s challenge: WalkaboutsThis week we’ve got some questions for you to navigate the park with. Some of them may push you a bit out of your comfort zone, but you’ve got this! All of the questions are achievable with Cypher, and the reference card will be very helpful. Don’t forget, if you’re really stuck, ask on the forum. We’ve also have some extra hints to get you on the road (no pun intended, or maybe it was?!).We’d like you to answer the following:You start your adventure at a clock (type:’clock’), What’s its name?From this position, what other Points of Interest is within 100m from a straight line?(Hint, you will need to use the spatial functions. Do not assume anything about the relationships just yet)How far apart are they as a straight line? (Hint, check out the spatial functions)How far is the actual distance? (Hint, check out shortestpath())You’re thinking of going for a cycle ride, but first a coffee! Locate which cafe is closest to a bicycle rental place. What’s the name of the cafe? (Hint, you may need to list out all of the Point of Interest types to determine the correct syntax)If you want to (not required), compare your outputs of shortestPath() against weighted shortest path with the Dijkstra APOC functionDon’t forget about NeoMap! Why not have a go at visualising the different Points of Interest you discover?Some hintsThis is a bit harder than normal, so we’ll give you some early hints to get you started:Do read up on the documentation on how to use the spatial functions. When you are comparing Points of Interest to see how far apart they are as a straight line, you will not be using relationships, e.g. MATCH (p1:PointOfInterest {some properties}), (p2:PointOfInterest {some other properties})When you use shortestPath() you will have a path that contains relationships and nodes. You will need to extract the relationships out to get distance. This will look something like this snippet:WITH relationships(path) AS relCollection //extract out the rels as an arrayUNWIND relCollection AS rel //unwind the array into individual rels//now you can do stuff with relYou will also be doing some aggregation, the sum() function will be helpfulYou may notice sometimes that you might get a long distance when using shortestPath() between Points of Interest. Don’t panic! This is due this function looking for the minimum distance based on number of hops, not based on properties on the relationship (e.g. distance!). This function will find a valid shortest path (not all of them). If you want to do weighted shortest path (optional), have a look at apoc.algo.dijkstra()If you only want a list of distinct items, e.g. unique sports pitch names, use the DISTINCT keyword, e.g. RETURN DISTINCT poi.typeThe dataWe will be using this specially created sandbox for the challenge.Hints please!Of course! Catch up on the hints stream here.SolutionsThe beginners solution video stream can be viewed here.You start your adventure at a clock (type:’clock’), What’s its name?For this, we’ll bring back all Points of Interest that have the type of clock, and see the name:MATCH (p:PointOfInterest {type:clock})RETURN p.nameWhich will return: Delacorte Musical ClockFrom this position, what other Points of Interest is within 100m from a straight line?We are now going to use the spatial functions to what other points of interests are within 100 m of the clock:MATCH (p1:PointOfInterest {type:clock}), (p2:PointOfInterest)WHERE p1<>p2 AND distance(p1.location,p2.location) < 100RETURN p2.nameThere’s only one place nearby: Zoo SchoolHow far apart are they as a straight line? (Hint, check out the spatial functions)For this, we just make a slight change to the query above (and there’s more than one way of doing it!):MATCH (p1:PointOfInterest {type:clock}), (p2:PointOfInterest {name:Zoo School})RETURN distance(p1.location,p2.location)Which returns 21.5 m (to 1 decimal place)How far is the actual distance? (Hint, check out shortestpath())We’re going to use shortestpath() for this. Bear in mind that:This is based on the shortest number of hops between nodes, not absolute distance, andThere could be more than one valid shortest path between two points, this function will only return one of them (at random).As a result, there could be a number of different answers:MATCH path=shortestpath((p1:PointOfInterest {type:clock})-[:ROUTE*]-(p2:PointOfInterest {name:Zoo School}))WITH relationships(path) AS relsUNWIND rels AS relRETURN sum(rel.distance)In this scenario, there is only one route, so the answer is: 212.2 m (to 1 decimal place).You’re thinking of going for a cycle ride, but first a coffee! Locate which cafe is closest to a bicycle rental place. What’s the name of the cafe?Firstly, we’ll want to check the correct type property value to use, we could use something like this:MATCH (p:PointOfInterest)WHERE p.type contains cafe OR p.type contains cycleRETURN DISTINCT p.typeThis confirms we want to use the types of cafe and bicycle rental.Now, let’s find the closest (bearing in mind the warning around shortestpath() from above):MATCH path = shortestpath((p1:PointOfInterest {type:cafe})-[:ROUTE*]- (p2:PointOfInterest {type:bicycle rental}))WITH p1, p2, relationships(path) AS relsUNWIND rels AS relRETURN p1.name, p2.name, sum(rel.distance) AS dist ORDER BY distThere may be different answers, one of them may be: Petrie Court Cafe.If you were feeling adventurous, you could have tried this challenge with the Dijkstra APOC function:MATCH path = (p1:PointOfInterest {type:cafe}),(p2:PointOfInterest {type:bicycle rental})CALL apoc.algo.dijkstra(p1, p2, ROUTE, distance) YIELD weightRETURN p1.name, p2.name, weight ORDER BY weightYou will get: Le Pain Quotidien.Experienced challenge: Fountain flightSo, it turns out you’re on bit of a mission. You’re very keen to get through the park, but it’s a very hot day, and you want to make sure you pass every single fountain through the park, in the most efficient way! We’d like you to tell us, what is a likely minimum distance to visit all of the fountains (type:’fountain’) in the park?What are we looking for?Minimum distance to visit all of the fountains Points of Interest (type:’fountain’)Whilst you can ‘brute force’ the answer, why not have a look at what’s available in the APOC and GDS libraries. Do any of the shortest path algorithms help? NeoMap might be helpful to help visualise the task at hand.The dataWe will be using this specially-created sandbox for the challenge.Hints please!Of course! Catch up on the hints stream here.SolutionYou can catch up on the experienced solution stream here.Let’s have a look at a potential way to solve this particular challenge of the minimum distance to visit all the fountains:Find all the shortest paths between all the fountainsAs the points conveniently form a ‘line’ of sorts, we could use Minimum Spanning Tree from the Graph Data Science library to tell us the shortest path to connect all the points. We just need to determine a sensible start pointTo link all the fountains to each other by shortest distance, let’s use apoc.algo.dijkstra(), to make it more performant, we’ll wrap it in apoc.periodic.commit(). We will create a new relationship between fountain pairs and note the distance between them:CALL apoc.periodic.commit( MATCH (p1:PointOfInterest {type:fountain}), (p2:PointOfInterest {type:fountain})WHERE p1 <> p2 AND NOT (p1)-[:SHORTEST_PATH]-(p2)WITH p1, p2 limit $limitCALL apoc.algo.dijkstra(p1, p2, ROUTE, distance) YIELD weightCREATE (p1)-[:SHORTEST_PATH {distance:weight}]->(p2)RETURN count(*) , {limit:1})As the points line up, we could use Minimum Spanning Tree to link all of the points together, and it will give us ‘one’ path with no branching off. Firstly, let’s figure out a sensible start point. We’ll do that by querying all the points with SHORTEST_PATH and pick the one that appears most frequently. Were using internal ids as there are lots of fountains named fountain:MATCH (a:PointOfInterest)-[r:SHORTEST_PATH]->(b:PointOfInterest) RETURN id(a), id(b), r.distance ORDER BY r.distance descNow, let’s use this as a start point for minimum spanning tree. This will write new relationships between the nodes. We’ll again write the distance on this new relationship type:MATCH (p:PointOfInterest) WHERE id(p) = 3490CALL gds.alpha.spanningTree.minimum.write({  nodeProjection: PointOfInterest,  relationshipProjection: {  	SHORTEST_PATH: {    	type: SHORTEST_PATH,        properties:distance,        orientation:UNDIRECTED     }   },   startNodeId:id(p),   relationshipWeightProperty:distance,   writeProperty: MST,   weightWriteProperty:distance }) YIELD effectiveNodeCount AS n RETURN count(n)Last but not least, let’s get the distance:MATCH p = ()-[r:MST]->()RETURN sum(r.distance)Which will give us 5023.7 m (to 1 decimal place). A rather nice, hearty walk through Central Park :).;Aug 24, 2020;[]
https://medium.com/neo4j/testing-your-neo4j-nest-js-application-49959313a32c;Adam CowleyFollowSep 22, 2020·9 min readFrank Wang on UnsplashTesting your Neo4j & Nest.js ApplicationIn this post, I will outline how you can test your Neo4j-based Nest.js application using unit tests and End-to-End testingThis article is one of a series of blog posts that accompany the Livestream on the Neo4j Twitch Channel where I build an application on top of Neo4j with Nest.js.The recordings are also all available in a YouTube Playlist.This article assumes some prior knowledge of Neo4j and Nest.js. If you haven’t already done so, you can read the previous articles at:Building a Web Application with Neo4j and Nest.jsAuthentication in a Nest.js Application with Neo4jAuthorising Requests in Nest.js with Neo4jHandling Neo4j Constraint Errors with Nest InterceptorsWhich tests should I use?So far, I’ve only really touched End-to-End testing. End-to-end (or E2E) tests are functional tests that test the application as a whole. In the streams so far I’ve used E2E tests to test the entire application stack and represent a user’s entire journey through the website, including:Sign UpAuthenticationAuthorisationInput ValidationDatabase Constraint ValidationFor example, the tests that cover the POST /articles endpoint provide tests to ensure that the following elements are working correctly:Validation Pipe are working correctlyThe user is correctly authenticatingThe database is up and the data is being correctly added to the databaseThese tests are all run through Jest using Supertest to mimic HTTP requests to the API. With a front end, it would also make sense to use a tool like Selenium or Cypress to automate the clicking of buttons and filling in of forms.These are great and can be quick to run for small applications, but if you are following Test Driven Development on a large project, running hundreds of tests on each save could become time consuming. At the start of the project, a lot of those elements may not be in place, so it doesn’t make much sense to write a set of failing tests for middleware that aren’t on the roadmap for several months.Test PyramidFor this reason, most people start with unit tests. The goal of unit testing is to write more precise tests that verify that each single element that makes up the code base work under specific conditions. For example, what happens when a third-party API returns a particular response? What happens if the API goes down?These can be hard to simulate in real-life, besides it could cost a lot of money to keep sending requests to some Google API to constantly test these conditions.This is where mocking comes in handy. Instead of sending a request with each test, mocking the API class will mean that we can spy” on a call to a method and return a specific result on a test-by-test basis.In our case, we don’t want to rely on a Neo4j instance to unit test our code. We can instead mock the read and write methods on the Neo4jService and return certain results based on the test case.Testing Nest.js with JestFor this, we will use the tests that have already been auto-generated with the nest generate commands. These use the @nestjs/testing package and a testing framework called Jest.As an example, we’ll take a look at testing the create() method on the ArticleService. The generated unit test file is in the same folder, called article.service.spec.ts.At the top of the test file, you’ll see a beforeEach function that creates a new testing module. The idea of the testing module is that rather than registering the entire application, we only register the elements that we require to make the test pass — in the long run this will make the tests a lot quicker.// article.service.spec.tsbeforeEach(async () => {  const module: TestingModule = await Test.createTestingModule({    providers: [ArticleService],  }).compile()  service = module.get<ArticleService>(ArticleService)})Running the test as it stands will cause a couple of knock-on problems:ArticleService is marked as a scoped providerFirstly, the following line ensures that the ArticleService is scoped to the request:// article.service.ts@Injectable({ scope: Scope.REQUEST })By default Injectable classes are singletons — meaning that a single instance of the class is created for the entire application. Because this services is scoped to the request, a new instance will be instantiated with each request, allowing us to inject the Request.Calling module.get to retrieve an instance of the module will return the following error:ArticleService is marked as a scoped provider. Request and transient-scoped providers cant be used in combination with  get()  method. Please, use  resolve()  instead.Because this is a scoped provider that is instantiated with the request, Nest can’t instantiate it outside of a request. It will instead need to resolve a new instance. Changing the call from .get to .resolve will solve this error:service = await module.resolve<ArticleService>(ArticleService)Nest can’t resolve dependencies of the ArticleServiceNext, because we pass through an instance of the Neo4jService into the ArticleService constructor, we need to make sure that the Neo4jModule has been registered within the testing module.As it stands, the testing module is not aware of the Neo4jService and will throw the following error:Nest cant resolve dependencies of the ArticleService (REQUEST, ?). Please make sure that the argument Neo4jService at index [1] is available in the RootTestModule context.To fix this, we’ll need to register the Neo4jModule with the application. In the main application, we use the ConfigService to pull the applicable config values from the .env file in the root. But as we’ll be mocking the interactions between the application and Neo4j we can put in any old information.Just as we do in the AppModule, we can add the Neo4jModule to the imports key on instantiating the testing module, but instead using forRoot rather than forRootAsync:// article.service.spec.tsimport { Neo4jModule} from nest-neo4j// ...const module: TestingModule = await Test.createTestingModule({  imports: [    Neo4jModule.forRoot({      scheme: neo4j,       host: localhost,       port: 7687,      username: neo4j,       password: neox    })  ],  providers: [ArticleService],}).compile()The client is unauthorized due to authentication failureRe-running the tests, you’ll an error message saying that authentication to Neo4j has failed:// article.service.spec.tsNeo4jError: The client is unauthorized due to authentication failure.At the moment Neo4j is still trying to authenticate — to stop this from happening, we can instruct jest to mock the entire Neo4j Driver class:jest.mock(neo4j-driver/lib/driver)Now for each test in this suite, the Neo4j driver module will be mocked, so none of the code from that file will actually be executed, and instead we can check that the functions have been called and if necessary return our own responses.More information on mocking entire modulesThe Neo4j driver itself is down-stream of any of the code we will be testing.Testing the create() methodIn order to test the create method, we’ll first need to create a group to hold the test.describe(::create(), () => {  it(should create a new article, async () => {    // Test will go here...  })})Because this is a service scoped to each request, many methods including the create method expect the request to be injected into the service and for it also to contain a User.The request property on the method is a private method, so we’ll have to use Object.defineProperty to an object that contains an instance of the User entity class. In order to fake this, we can import Node class from the Neo4j driverneo4j-driver/lib/graph-types.js and create a new instance.// article.service.spec.ts import { User } from ../user/entity/user.entityimport { Node } from neo4j-driver/lib/graph-typesimport { int } from neo4j-driver// Create User const userNode = new Node(int(9999), [User], { id: test-user })const user = new User(userNode)In order to ensure that this is found in our class, we can use Object.defineProperty to set the value of service.request to an object that contains the User.Object.defineProperty(service, request, { value: { user } })Next, we’ll want to mock a response from Neo4j. We can assume from the E2E tests that the actual Cypher query is fine, but we should check that the values returned by Neo4j are correctly processed by the service.Taking a look at the RETURN portion of the query you can see that the service expects the driver to return a user node, article node, array of nodes to represent tags, a boolean to indicate whether the user has favorited the node and a number to represent the total number of :FAVORITED relationships.RETURN u, // User Node  a, // Article Node  [ (a)-[:HAS_TAG]->(t) | t ] AS tagList, // Array of tag nodes  exists((a)<-[:FAVORITED]-(u)) AS favorited, // Boolean  size((a)<-[:FAVORITED]-()) AS favoritesCount // NumberWe can listen for calls to the write method on the Neo4jService using the jest.spyOn method. We can retrieve the instance of the Neo4jService that will be injected into the service by calling module.get.// Get the type of result to returned by the mocked methodimport { Result } from neo4j-driver/lib/result// Get the instance of the Neo4jService created by the TestModuleconst neo4jService: Neo4jService = await module.get(Neo4jService)// Add some test data to pass to the callconst data = {  title: Title,  description: Description,  body: Body,  tagList: [tag1, tag2],}// Listen to calls on neo4jService.write()const write = jest.spyOn(neo4jService, write)   // Each time the   .mockResolvedValue(    // Return a mocked value to mimic a <Result>  )The resolved value from the write method should be a QueryResult. In the ArticleService we only use the records array which contains an array of Record objects. Each record has a get method that is used to pull the individual items from the return.We can mock what the driver would return by adding a case statement..mockResolvedValue(<Result> {  records: [    {      get: key => {        switch (key) {          case a:            // If requesting a, return a `Node` with the data            // passed to the `create` method            const { tagList, ...properties } = data           return new Node( int(100), [Article], { ...properties, id: test-article-1 })          case tagList:             // If tagList return an array of Nodes with a             // property to represent the name            return data.tagList.map((name, index) => new Node ( int(200 + index), Tag, { name }))          case favoritesCount:             // If favouritesCount then return a random number            return 100          case favorited:             // If favorited, return a boolean            return false        }        // Otherwise return null        return null      }    }  ]})As we only expect a single node, that is as complicated as we need to get.Then in order to rest the hydration we can run the method and then call toJson on the article that has been returned.const article = await service.create(data.title, data.description, data.body, data.tagList)const json = article.toJson()If all has gone well, we should get a JSON object with the original information passed to the create method with some additional information including the article ID. The author object should all properties passed to the mock User entity, and the values returned from the Driver” should match.expect(json).toEqual({  ...data,  author: user.toJson(),  id: test-article-1,  favorited: false,  favoritesCount: 100,})TLDR: I don’t want boilerplateI’ve added a set of methods to the nest-neo4j package so you don’t need to scour the neo4j-driver repository in order to find the methods.mockNode(labels: string | string[], properties: object) — Return a node with the label(s) and properties supplied.mockRelationship(type: string, properties: object, start?: Node, end?: Node) — Return a relationship object with the type and properties defined. You can either pass node instances to represent the start and end nodes, or a random one will be generatedmockResult(rows: object[]) — this method will return a mocked version of the Result class with a records key. Each record has a keys array and a get method for retrieving a single value.For example, the code above could be replaced with the following:// Import methodsimport { mockNode, mockResult } from nest-neo4j/dist/test// Mock Resultconst write = jest.spyOn(neo4jService, write)  .mockResolvedValue(    mockResult([      {        u: user,        a: mockNode(Article, { ...data, id: test-article-1 }),        tagList: data.tagList.map(name => mockNode(Tag, { name })),        favoritesCount,        favorited,      },    ])  )As the weeks progress, I will be adding more test cases and examples — for example what happens when Neo4j throws a Constraint Error? Or what happens if the service is unavailable?Star or Watch the nest-neo4j repository to be notified of any commits.adam-cowley/nest-neo4jNeo4j integration for Nest This repository provides Neo4j integration for Nest. $ npm i --save nest-neo4j Register the…github.comUntil next week!~ Adam;Sep 22, 2020;[]
https://medium.com/neo4j/machine-learning-on-graphs-fca6eeb8f1d1;joydeep bhattacharjeeFollowNov 3, 2018·6 min readMachine Learning on Graphslight through the branches: sourceGraphs are an excellent way of encoding domain knowledge for your business data. One of the popular databases for graphs is Neo4j and I have written multiple blog posts and videos on the topic.But it was always a challenge how we can utilize the knowledge encoded in the knowledge graph to use in machine learning models. This can be achieved using the node2vec algorithm. Node2vec takes the graph and its edges and encodes this graph information in node embeddings. I know what you are thinking and you are correct, node embeddings are inspired by the word embeddings and is created using the skipgram model which is one of the core algorithms in word2vec and fasttext. Skipgram models are simple, for each target word, training is done considering the output to be the surrounding words. The previous sentence ofcourse does not give justice to the power of the algorithm so please take a look at the amazing link below.Word2Vec Tutorial - The Skip-Gram ModelThis tutorial covers the skip gram neural network architecture for Word2Vec. My intention with this tutorial was to…mccormickml.comOn that note in case you are interested in NLP and fasttext, I have worked on a book on this topic which you can buy from here.fastText Quick Start Guide: Get started with Facebooks library for text representation and…Perform efficient fast text representation and classification with Facebooks fastText libraryKey FeaturesIntroduction…amzn.toIn this post we will build a simple graph from the movie lens dataset and then use the graph to build a node2vec model. Then we will understand how to use the model to answer simple similarity based questions.Create Nodes and EdgesFirst we need to read the datasets and create the nodes and relationships. Below is the code where we read through the movies.csv and then create a BELONGSTO relationship between the Movie and its Genres. You can get the rest of the node building code in this notebook.Lets take a look at how the graph looks. Once the import runs to completion we can open the Neo4j Browser, then click on the MovieIdlabel and then expand any one of the nodes (or directly click on the BELONGS_TO relationship).The red nodes are the genres and the blue nodes are the movies.Node2vecOnce the nodes and edges have been created, we will create the node embeddings using the node2vec program. For that we will create the edgelist file using the below code.You should be able to run node2vec on the files.$ ./node2vec -i:graph/movies.edgelist -o:emb/movies.emb -l:80 -d:100 -p:0.3 -dr -vAn understanding of the above command can be seen through the help command.$ ./node2vec -hAn algorithmic framework for representational learning on graphs. [Oct 27 2018]================================================================================Input graph path (-i:)=graph/karate.edgelistOutput graph path (-o:)=emb/karate.embNumber of dimensions. Default is 128 (-d:)=128Length of walk per source. Default is 80 (-l:)=80Number of walks per source. Default is 10 (-r:)=10Context size for optimization. Default is 10 (-k:)=10Number of epochs in SGD. Default is 1 (-e:)=1Return hyperparameter. Default is 1 (-p:)=1Inout hyperparameter. Default is 1 (-q:)=1Verbose output. (-v)=NOGraph is directed. (-dr)=NOGraph is weighted. (-w)=NOOutput random walks instead of embeddings. (-ow)=NOYou can download and compile the high performance node2vec package from this link. There is also a node2vec python implementation for reference but that is blindingly slow, so dont use it.aditya-grover/node2vecContribute to aditya-grover/node2vec development by creating an account on GitHub.github.comThe format of the embeddings file is similar to what word2vec outputs.The first line will have the number of entries” and the dimension of each vectors and the remaining lines would have the entry with the vectors/embeddings. In this case the entries are the node-ids.➜  head emb/movies.emb27299 100260169 -0.0833466 ... -0.00598206278634 -0.0338385 ... -0.07563880 -1.29378 ... -0.093047270464 -0.161561 ... -0.0193972Once you are done creating the embeddings, you should store the embeddings back in the graph for future reference and updating.Use the embeddingsNow since the embeddings are in the word2vec format, we can load them using the gensim topic modeling tool. Gensim has a lot of functions that we can utilise for different tasks. Some of the methods that we can use are:closer_than,distance,doesnt_match,evaluate_word_analogies,evaluate_word_pairs,get_keras_embedding,get_vector,most_similar,most_similar_to_given,n_similarityand so on. You can get the documentation of the methods from the gensim link.gensim: topic modelling for humansEfficient topic modelling in Pythonradimrehurek.comIn this case we will see if we can get the most similar nodes to a given node.Here in this code we are passing the node id as a string to the model. The output of the above code is below.[(252573, 0.9870771169662476),  (276773, 0.984420895576477),  (260679, 0.9837837219238281),  (263498, 0.983630895614624),  (254262, 0.9832301735877991),  (253096, 0.9815400242805481),  (274147, 0.981526255607605),  (263074, 0.9814952611923218),  (270921, 0.9811148047447205),  (270035, 0.9809507131576538)]Thus we can see that we are getting a list of similar nodes. That is awesome news. Thus we were able to create embeddings from a graph using node2vec and then able to make some predictions from the graph.You can now go ahead and implement this on your graph. Of course if you didnt already build a graph in your domain, go ahead and build one already.Now before we close this post, one last thing is remaining. Passing node ids is not very friendly in terms of understanding if the graph that we have is actually a good graph. So below is a function that takes in an actual movie name and computes similar movies based on the graph.The above function will take a movie as the key, get the nodeid of the specific movie based on the key. We can then find other similar nodeids and then it will list down the movies based on the nodeid’s.We are passing the Money Train” movie and below are the results from the function.Waco: The Rules of Engagement (1997) 0.9953966736793518Five Wives, Three Secretaries and Me (1998) 0.9936010837554932Dark Matter (2007) 0.9935312271118164Old Man and the Sea, The (1958) 0.9933647513389587Pop Redemption (2013) 0.9931492805480957Woman Next Door, The (Femme dà côté, La) (1981) 0.9930013418197632Brainstorm (1965) 0.9929700493812561Suddenly (Tan de Repente) (2002) 0.9929301142692566Springsteen & I (2013) 0.9922378659248352Fly Away (Bis zum Horizont, dann links!) (2012) 0.9920598864555359If we take a look at wikipedia this is the description of the target movie and one of the results.Money Train: A revengeful New York transit cop resolves to rob a high-tech train laden with an immense amount of money. His foster brother, a fellow policeman, endeavours to safeguard him.Waco: The Rules of Engagement: This documentary about the 1993 showdown between the FBI and the Branch Davidians in Waco, Texas, presents an alternate theory about the tragedy. The government has long contended that this fringe Christian group was a danger, and that the siege on its compound, which resulted in the death of 70 people.source: wikipediaI will leave it upto you if you think the results are good or not.You can get the full code that is explained in this post in the following jupyter notebook linked to below.infinite-Joy/kernelsContribute to infinite-Joy/kernels development by creating an account on GitHub.github.comThanks for reading this post. If you found this useful, please click on the claps button and share the post with your friends and colleagues.References:https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fefNode2vec paper: https://arxiv.org/abs/1607.00653node2vec website: https://snap.stanford.edu/node2vec/;Nov 3, 2018;[]
https://medium.com/neo4j/exploring-fraud-detection-with-neo4j-graph-data-science-summary-74b921ac4e18;Zach BlumenfeldFollowMar 1, 2022·3 min readExploring Fraud Detection With Neo4j & Graph Data Science — SummaryA multi-part series with hands-on examples using peer-to-peer (P2P) dataFraud Detection is one of today’s most challenging data science problems. Thankfully, Neo4j Graph Data Science (GDS) offers practical solutions that empower data scientists to make rapid progress in fraud detection analytics and machine learning.SummaryWhether you are responsible for combating financial crimes, online identity theft, or smuggling or trafficking operations, you are aware of the significant costs incurred when fraud goes unnoticed for too long. There are massive benefits to be gained if only you could leverage data science and machine learning to detect more fraud earlier.Despite this, viable fraud detection remains one of today’s most challenging data science problems. This is because fraud detection has a unique quality — the entity you want to predict is trying assiduously to prevent you from doing so. It’s not just a needle-in-the-haystack” problem. In this case, the proverbial needle is actively trying to hide from your detection.Fortunately, graph-based approaches explicitly model relationships between entities in the data. This, coupled with the intuitive analytics capabilities of Neo4j Graph Data Science (GDS), empower practitioners to rapidly explore, analyze, resolve, and predict fraud entities and patterns. Patterns which would otherwise remain obfuscated and challenging to infer in other data models.In this blog series, we will explore how Neo4j and GDS can be practically applied to a fraud detection workflow. We will explore a real anonymized data sample from a peer-to-peer (P2P) payment platform, identify fraud patterns, resolve high risk fraud communities, and apply recommendation methods and machine learning.P2P Graph SchemaUsing this real P2P data, and with little knowledge beforehand, we will identify new fraud risks that went undetected with non-graph methods, increasing the number of flagged users by 87.5 percent. We will also demonstrate methods to further recommend similar suspicious user accounts and train a machine learning model to predict similar fraud risks.While we focused on this specific P2P sample dataset here, the methodologies introduced are highly scalable and transferable to a broad range of fraud detection use cases. It is designed to be representative of a more general graph workflow where the goal is to identify sparse instances of people and organizations who attempt to hide their identities while performing nefarious activities.Want to learn more?Part 1: Exploring Connected Fraud DataPart 2: Resolving Fraud Communities Using Entity Resolution & Community DetectionPart 3: Recommending Suspicious Accounts With Centrality & Node SimilarityPart 4: Predicting Fraud Risk Accounts with Machine Learning;Mar 1, 2022;[]
https://medium.com/neo4j/digging-into-the-icij-pandora-papers-dataset-with-neo4j-d96f0cddb898;Michael HungerFollowDec 9, 2021·5 min readDigging Into the ICIJ Pandora Papers Dataset with Neo4jYesterday, the Pulitzer Award winning International Consortium of Investigative Journalists (ICIJ) published the first data-release of the recent Pandora Papers investigation.This time the data publication was not split across the different investigations but contains the full offshoreleaks database in one dataset, so you can explore the data of shell companies, law firms, banks, and ultimate owners across all leaks and investigations.The data model is consistent with previous publications. Officers are related in several ways (directors, shareholders, beneficiaries) with Entities (shell companies). Intermediaries (banks, law firms) manage the creation and operation of those shell companies. And all of them have addresses that can be used for investigations as well.Offshoreleaks data modelEach node has fields for countries and country codes to associate them with specific geographies and many other pieces of information.The first data release for the Pandora Papers consists of 26k Officers, 18k Entities, and 1,000 Intermediaries.Installing the Dataset in Neo4j DesktopThe easiest way to get started with the dataset is to Download & Install Neo4j Desktop.1. Download the dump” file from the public GitHub repository.2. Either in the example project or a newly created project you can use Add File” to add the dump file to your project.3. Then choose Create new DBMS from Dump.”4. Provide a password.5. Wait a few seconds until the db is created then hit Start.”6. After it’s started, open Neo4j Browser” on the running database.7. Within Neo4j Browser use :play icij-offshoreleaks to launch the interactive guide. (Pin it on top with the pin icon).Exploring the Data with Neo4j BloomBesides exploring the provided guides, you can also pick any of the published stories and look behind the scenes by searching for the people, organizations, and jurisdictions mentioned in the story.If you don’t feel comfortable with a query language, you can also use the Neo4j Bloom Visualization Software to explore the data with a more natural language interface and visually.You can start Neo4j Bloom from the Graph Apps” sidebar in Neo4j Desktop. It will open for your currently running database.Neo4j Bloom with Search PhraseNeo4j Bloom VisualiyationFollow a Story from the The Landlords” InvestigationsAs an example of how you can investigate published stories yourself, here is an example of the South Africa’s smart city” from the The Landlords” investigations on property ownership by shell companies with unbeknownst owners.The Land Lords - ICIJExplore nine case studies that show how investors are not only putting money into luxury real estate, a traditional…www.icij.orgIn South Africa’s smart city” Ruslan Goryukhin — a key aide to some of Putin’s closet friends — is reported to be connected to companies involved in the development of the first post apartheid smart city” — Cradle City in South Africa.Let’s see what we find in our data, first querying for Goryukhin.MATCH path = (o:Officer)-[r]->(:Entity)WHERE o.name CONTAINS GORYUKHIN AND o.sourceID STARTS WITH  Pandora Papers RETURN path LIMIT 100Another person mentioned in the story was Preston Hampton Haskell IV, the son of a Texas construction billionaire.” Let’s see if we can find him too, and his connections to Goryukhin.MATCH (o:Officer), (o2:Officer)WHERE o.name CONTAINS GORYUKHIN AND o.sourceID STARTS WITH  Pandora Papers  AND o2.name contains PRESTON HAMPTON HASKELLMATCH path=allShortestPaths((o)-[*]-(o2))RETURN path LIMIT 25The report speaks of a shell company named Kelburn One and Amari Land International Ltd — later renamed to Forum Properties Africa — which unfortunately are not in the published dataset.Programmatic AccessThe example repository also comes with code-examples in Python, Java, Javascript, .Net, and Go. There is also a full GraphQL project for the dataset with a schema for the neo4j/graphql integration library that you can use to run and deploy a GraphQL API.These examples show how to connect to the database and run a query against the data. So if you’re inclined to build a dashboard or app, feel free to use those.Here is the JavaScript example:// npm install --save neo4j-driver// node example.jsconst neo4j = require(neo4j-driver)const driver = neo4j.driver(bolt://<HOST>:<BOLTPORT>,                  neo4j.auth.basic(<USERNAME>, <PASSWORD>),                   {/* encrypted: ENCRYPTION_OFF */})const query =  `  MATCH (a:Officer {name:$name})-[r:officer_of|intermediary_of|registered_address*..10]-(b)  RETURN b.name as name LIMIT 20  `const params = { name :  Ross, Jr. - Wilbur Louis }const session = driver.session({database: neo4j })session.run(query, params)  .then((result) => {    result.records.forEach((record) => {        console.log(record.get(name))    })    session.close()    driver.close()  })  .catch((error) => {    console.error(error)  })Load the Database into Neo4j AuraDBIf you want to dive deeper into the dataset you can also use our cloud service Neo4j AuraDB to load the dataset into an AuraDB Pro instance.Register or Log in at console.neo4j.ioCreate an AuraDB Professional instance (a size of 2G should be enough to upload the file).Save the password!Go to the Import Database” and upload the dump file.Open” the database, provide your password.Then run :play icij-offshoreleaks for the interactive browser guide.From here you can continue to do anything that you want.Available on the Neo4j Labs Demo Server (read-only)A read-only database is also available on the neo4j-labs demo server. Just use offshoreleaks” as username/password/database.With :play icij-offshoreleaks you can run the interactive guides there.Happy investigations, please share in the comments or tag #neo4j on Twitter when you find something interesting.;Dec 9, 2021;[]
https://medium.com/neo4j/querying-neo4j-from-microsoft-excel-for-macos-via-odbc-using-cypher-b9308c03aad5;Daniel Heward-MillsFollowNov 25, 2020·4 min readQuerying Neo4j from Microsoft Excel for macOS — via ODBC using CypherThis post describes how to interact with a Neo4j Database Instance, using its Cypher Query Language, from Microsoft Excel running on macOS.The Open Source JDBC driver for Neo4j enables Java applications to interact with Neo4j instances using the Cypher query language. This driver is different from the Neo4j BI Connector, which uses the SQL-92 query language.OpenLink Software’s ODBC-JDBC Bridge Driver extends interactivity from JDBC Drivers such as Neo4j’s, to ODBC-compliant applications such as Excel without needing to create additional scripts or add-ins.PrerequisitesNeo4j JDBC Driver downloaded, and added manually to /Library/Java/Extensions. If you are using Java 11, or later, you might need to manually add the JAR location to your CLASSPATH environment variable.A working OpenLink Software ODBC-JDBC Bridge Driver Installation (Installation Guide, Free Trial License)Basic Cypher Test QueryI used the movies sample database for this demonstration. The query that will be used in Excel is:MATCH (n:Person)-[p:ACTED_IN]->(f:Movie)RETURN n.name as name,       n.born as born,       f.name as movie,       f.released as releasedA quick test of the query was run from Neo4j Desktop to ensure that the correct data will be returned in Excel.Success. We’re now ready to configure OpenLink’s ODBC-JDBC Bridge Driver to work with the Neo4j JDBC Driver.ODBC Setup and ConfigurationOpen the iODBC Administrator application included with your ODBC-JDBC Bridge application, click Add, and type in a Data Source Name (DSN).Click Continue and add the org.neo4j.jdbc.Driver Class Name for the Neo4j JDBC Driver, and the Connection URL for accessing your Neo4j instance.My JDBC connection URL uses the HTTP protocol. HTTPS should work if enabled server-side and supported by the Open Source JDBC driver:{jdbc:neo4j:http://localhost:7474?username=demo&password=demo}The ODBC-JDBC Bridge doesn’t currently support Bolt connections from the Open Source JDBC driver, but can do so with Neo4j’s JDBC BI Connector (Guide) .Click on the Test tab, and then click on Finish.Confirm that your data source has been added. Once confirmed, it’s time to open Excel .Querying Data from ExcelOpen or create a new workbook and click on New Database Query -> From DatabaseSelect your ODBC DSN and click OKEnter the username and password used in your JDBC connection URL, and click Connect.Click Connect, add your Cypher query, and click Run. Query results will be populated in tabular form, and can be returned to your Excel workbook.Click Return Data, and select a location for your data to be added into.Data from your Neo4j instance has been successfully queried from and returned to Excel 🎉 .Related ContentFree 30 Day ODBC-JDBC Bridge Driver EvaluationMaking ODBC Connections to Neo4j from macOS;Nov 25, 2020;[]
https://medium.com/neo4j/importing-neo4j-desktop-databases-to-neo4j-aura-f0024fe7b07f;Mark NeedhamFollowNov 27, 2019·5 min readImporting Neo4j Desktop Databases to Neo4j AuraImport databases created in Neo4j Desktop to Neo4j Aura, the graph database as a service.So we’ve been using Neo4j via Neo4j Desktop for a while now, building up a formidable collection of databases, and then a few weeks ago we learned about the launch of this thing called Neo4j Aura.Neo4j AuraNeo4j Aura is the simplest way to run Neo4j in the cloud. Completely automated and fully-managed, Neo4j Aura delivers the world’s most flexible, reliable and developer-friendly graph database as a service. With Neo4j Aura, you leave the day-to-day management of your database to the same engineers who built Neo4j, freeing you to focus on building rich graph-powered applications.We’d like to give it a spin, but need to figure out how to get those Neo4j Desktop databases imported. There is a tool called push-to-cloud, that can be used via Neo4j Admin. But that means we have to use the command line.If this sounds like you, you’re reading the right blog post.We wanted to make that experience more friendly, so we built the Neo4j Cloud Tool Graph App to help out.How do I install it?There are two ways to install the Neo4j Cloud Tool:One click installThe Neo4j Desktop one click install is available via the Graph Apps Gallery at install.graphapp.io. Just look for the Neo4j Cloud Tool card, which is on the bottom row:Installing Neo4j Cloud ToolManual installIf the one click approach doesn’t work, you can also install the Neo4j Cloud Tool manually by pasting https://neo.jfrog.io/neo/api/npm/npm/neo4j-cloud-ui into the ‘Install Graph Application’ form.Installing the Neo4j Cloud ToolWith either of those approaches, you then press the Install button. Once you do that, you will see the following screen:Installing the Neo4j Cloud Tool Graph AppThe Neo4j Cloud Tool will be available to add to any of your databases after you click the (+) Add Application button in the project:Adding a Graph AppYou will then see a screen similar to the following. Click on the Add button of the Neo4j Cloud Tool to install the graph app:You should now see Neo4j Cloud Tool on the list of Graph Apps, just under the name of your project.You launch the Neo4j Cloud Tool by clicking on that box.Setting up a remote Aura connectionIf you launch the Neo4j Cloud Tool without having an active connection to a Neo4j Aura database, you will see the following screen, which contains instructions explaining how to create such a connection:No active Neo4j connectionAlso see the Connect to Neo4j Aura with Neo4j Desktop Developer Guide, which walks through the steps in more detail. Below is an example of a project that has a remote connection to Aura activated:Importing a databaseOnce that connection is activated, the Graph App will automatically refresh, and you will see a screen similar to the following one:Neo4j Cloud Tool Launch ScreenIf you click the Check for existing database button, it will check whether your Aura database contains any data. In the example below it does, so we’re asked to confirm that we’d like to replace the database:Aura database contains dataNext, you click on the Next button, which takes you to the following screen, from where you can select the database you want to import:Select a database to importThe Database value of graph.db is the default, and refers to the directory where the database lives on your machine. Most likely you won’t need to change this value.Once you have selected the database, you click the Import Database button. It will take at least a few minutes to import the database, but the UI will indicate which step is currently being executed.Import in progressWhen you reach the Started importing database dump step, you can go to the Neo4j Aura console to view further progress.If everything is successful, you will see the Aura UI move from the Loading state back to Running again. The Neo4j Cloud Tool UI will also update with a success message:A successful importIf you have a problem importing your database please post a message in the Aura & Cloud topic on the Neo4j Community site and tag @mark.needham so that I’ll definitely see your message.Happy Aura’ing!;Nov 27, 2019;[]
https://medium.com/neo4j/a-new-steer-for-the-neo4j-drivers-ed3de051209e;Richard MacaskillFollowFeb 24·5 min readA New Steer for the (Neo4j) Drivers?Who among us remembers how hard it was to learn to drive? If you haven’t learned to drive yet, or don’t intend to learn, how hard do you think it would be?Pretty daunting, as I remember (confession: it took me three attempts to pass my test — gulp).Then there’s the question of what you learn to drive. In the United States, most vehicles have automatic transmissions, so it makes sense to learn that mode. In Europe and many other places, however, manual transmission is the norm, so many decide they might as well take on the greater burden of learning clutch control and gear changing.Many vehicles at the high end nowadays have both, so even the seasoned manual driver can choose the simpler option when it suits them.If we apply the analogy to the application drivers for Neo4j, we have to concede they have been more like a manual transmission — offering the maximum amount of capability (with reactive sessions, flow control, and much more) — but at the cost of being more difficult to learn.That’s about to change, as the drivers team at Neo4j is adding the equivalent of an automatic transmission mode to the APIs.SummaryThe drivers team has been working on simplifying the experience of new users getting started with Neo4j. The existing interfaces have not changed (you won’t have to change existing application code), but a new experience that builds on top of them is being introduced.There’s a new path through the driver APIs using an Execute method (or ExecuteAsync to be idiomatic to .net) on the Driver object. The simplest call to the Neo4j database now looks like this (code samples here are in C#)await _driver.ExecutableQuery( RETURN 1 ).ExecuteAsync()The Bigger PictureThe Neo4j drivers offer a great deal of control for use cases which can become pretty complex. Optimizing performance across highly variable architectures, while ensuring consistency and routing correctness can be a non-trivial exercise.All this control can come at a cost though, in terms of complexity for the developer.For some time, running a command against Neo4j from your application has involved understanding several (for some) novel concepts.Sessions, their configuration, and lifetimeTransaction FunctionsRouting ControlTransactionsBookmarks and Causal ChainingResult Cursors, Streaming Behavior, Lifetime of ResultsThere have been valiant efforts to capture the essentials in an accessible form, such as https://neo4j.com/developer-blog/neo4j-driver-best-practices/Still, apart from being a lot to take on board, it also involved quite a lot of… well… scaffolding. Even an apparently simple query execution is recommended to be written using an ‘explicit transaction function’.It got worse, as some users were drawn toward a section of the API that seemed simpler, but didn’t provide retries for transient failures (as can occur for cloud databases, like Aura, where Neo4j’s fastest growth is happening).The engineering team — who look after the Bolt protocol and server, as well as the 5 official language drivers — following research, embraced a challenge to reduce the number of concepts that users had to learn, along with the number of ‘inflection points’, in order to foster a user experience of ‘progressive discovery’ (i.e. you learn the complicated bits only when you need to do something complicated).(with apologies to Randall Munroe)So they have built a path through the API that shields many of the more complex aspects from the user (while retaining them for the most sophisticated use cases, and for backward compatibility).The new API allows the developer to acquire results from the database using some configuration defaults which we expect most users will choose in most cases anyway.Changes includeResults are delivered to memory without requiring an additional stepSession and Transaction objects are called by executeQuery automatically, so not exposed to the user. Retry behavior is the same, but without requiring lambdas or callbacksBookmarks are passed automatically, so causal consistency does not require manual stepsConfiguration options are set by default but accessible for change using a QueryConfig objectVersion 5.5 of the official drivers see the initial public release of these changes, for now under an ‘experimental’ tag.To highlight the improvements that the new API introduces we will again use some C# code. Here is an example of two causally chained queries using the existing session-level interface, in the classic read-your-own-write scenario.The two queries:const string writeQuery = @     MERGE (p1:Person { name: $person1Name })    MERGE (p2:Person { name: $person2Name })    MERGE (p1)-[:KNOWS]->(p2) const string readQuery = @     MATCH (p:Person)    RETURN p The session API usage:var _driver = GraphDatabase.Driver( localhost ,                                    AuthTokens.Basic( user ,  password ))      var savedBookmarks = new List<Bookmark>()await using var session = driver.AsyncSession()                 await session.ExecuteWriteAsync(async tx =>{     var resultCursor = await tx.RunAsync(writeQuery,                                           new { personName1, personName2 })      await resultCursor.ConsumeAsync()                          })savedBookmarks.Add(session.LastBookmark)                       await using var session2 = driver.AsyncSession(                                  o => o.WithBookmarks(savedBookmarks.ToArray()))                var records = await session2.ExecuteReadAsync(async tx =>         {                                                                         var resultCursor = await tx.RunAsync(readQuery)            return await resultCursor.ToListAsync()                          })Now, to highlight the differences, this is the same functionality implemented with the new driver-level API:var _driver = GraphDatabase.Driver( localhost ,                                    AuthTokens.Basic( user ,  password ))await _driver.ExecutableQuery(writeQuery)             .WithParameters(new { person1Name, person2Name })             .ExecuteAsync()var readResults = await _driver.ExecutableQuery(readQuery)                               .ExecuteAsync()Note the reduction in code achieved by the driver handling a lot of tasks for you, such as bookmark management.The default behaviors can be configured and overridden as needed, giving control to the developer as their requirements change. Should you need to have fine-grained control then you can move on or continue to use the session interface.It is worth noting that this interface is idiomatic depending on the language. The Python code for example does not use a fluent API and slightly differing naming. Take a look at the examples in the linked GitHub discussions below for the language(s) of your choice.Which one works best for you? Let us know at the links below.So, What’s Next for the Simplified API?After consultation and perhaps some tweaks, we’ll remove the experimental tags and promote the use of the API for new users.Going forward, we think we can automatically detect whether your query might contain a write operation, so you don’t need to indicate AccessMode to optimize the distribution of reads and writes across clusters.The team is also investigating object mapping from results into suitable application classes, and other ideas to reduce the need for boilerplate code and complexity.We’d love to hear from you — check out the code samples and add to the discussion for your preferred languageJava: https://github.com/neo4j/neo4j-java-driver/discussions/1374Python: https://github.com/neo4j/neo4j-python-driver/discussions/896.net: https://github.com/neo4j/neo4j-dotnet-driver/discussions/677JavaScript: https://github.com/neo4j/neo4j-javascript-driver/discussions/1052Go: https://github.com/neo4j/neo4j-go-driver/discussions/441;Feb 24, 2023;[]
https://medium.com/neo4j/keeping-track-of-graph-changes-using-temporal-versioning-3b0f854536fa;Ljubica LazarevicFollowDec 16, 2019·8 min readKeeping track of graph changes using temporal versioningIn this post we’re going to cover why versioning is important and how to do time-based versioning in Neo4j including managing retrospective (bi-temporal) changes.Photo by Fabrizio Verrecchia on UnsplashIntroductionBoth Ian Robinson and Tom Geudens wrote excellent posts on the topic of time-based versioning in Neo4j. Unfortunately those post are no longer available. This post is a write-up of the NODES2019 talk I did on the subject.Why versioning?First and foremost, versioning allows us to keep track of change. We can see what has been altered and retrace our steps if necessary.Another reason for versioning would be for what-if analysis. Whereas keeping track of change shows us historical changes, suggesting a change that hasn’t already occurred allows us to start examining what might happen under certain scenarios and project into the future.There are many use-cases where you might see versioning in use, for example:Identity and Access management — We can keep track of who is accessing what and when, and start to do analysis on any interesting behaviours we should be investigatingMonitoring changes in networks — Much like the above example, not only can we keep track of what’s been accessed to watch out for unusual behaviour, but we can also do predictive behaviours based on what happens when we make changes and what might the impacts beCollaborative working — Think GitHub and other collaborative working tools. We can track changes being made, see when they occurred, and reduce the likelihood of losing contributions.Versioning in Neo4jAs with many other database systems, there is no out of the box solution with Neo4j. You will need to build in versioning as part of your data modelling process. We’ll show you some approaches later on in the post.One thing to bear in mind, the community has created a versioning plugin, called Versioner-core. It does provide for some degree of automated versioning. You can check it out from the author’s GitHub repository.Introduction to time-based versioningTime-based versioning is very useful for a number of situations:You can use it to track changes, reversing any errorsYou can make updates to your data without deleting anythingYou too can become a time-traveller and move through time to understand state change.The principles behind time-based versioning are pretty straightforward:Separate the object from state, these will be linked by a relationshipCapture change times within the relationship property linking these two entities.And so, it is time to introduce our example (no pun intended!):Our original data item, Product with some associated propertiesScenario 1There is a company that produces a product called Widget. On the 4th May 2016, a couple of decisions were made:Rename Widget to MiniWidget, due to a new product similar to Widget coming outReduce the price of this product down to 3.99As you can see, we’ve split out our Product node which just contains the property to uniquely identify it (id), and we then capture other information such as name and price within ProductState nodes. We then connect Product to ProductSate using the HAS_PRODUCT_STATE relationship. Lastly, we capture information about when this state was valid with from and to properties on the relationship itself.With our newly refactored graph, we can start to ask some questions of it.Query 1: What is the current name of product with id:123?MATCH (:Product {id:123})-[r:HAS_PRODUCT_STATE]->(ps:ProductState)WHERE NOT EXISTS(r.to)RETURN ps.nameWe use WHERE NOT EXISTS to bring back the most recent state, as that will be the node with no to property.Query 2: How much did the product with id:123 cost 3 years ago?MATCH (:Product {id:123})-[r:HAS_PRODUCT_STATE]->(ps:ProductState)WHERE r.from <= 20161010 AND (r.to>=20161010 OR NOT EXISTS(r.to))WITH ps, r ORDER BY r.from LIMIT 1RETURN ps.priceHere we try to gather all the ProductState nodes from 3 years ago, bearing in mind the current node may still be current.Query 3: What is the SKU (Stock Keeping Unit) for MiniWidget?MATCH (:ProductState {name:”MiniWidget”})<-[:HAS_PRODUCT_STATE]-(p:Product)RETURN p.idAs you can see, just by separating out object from state, we are able to capture a lot of information about changes, and pull back information depending on the time filter.Versioning relationshipsMore than likely, as well as versioning objects, we’re quite likely to want to version the relationships between objects. We’ll want to do this because we want to understand how entities are or were connected to other entities and how that changed over time.The principles behind versioning relationships is pretty much the same as how we version object states:Connect the two nodes involved in the relationshipProvide a time range for when that relationship became live, if relevantTime to have a look at our next scenario!BUYS would be our versioned relationship in this imageIn the next iteration of our data model, you can see how we’ve extended versioning to relationships by joining Customer and Product with the BUYS relationship. By adding a date as a property, we show when that relationship occurred, hence versioning it. Some of you may have spotted that Customer is not versioned, more on that in the next model….Scenario 2A customer buys a product on 18th September 2016. Sometime after the purchase has been made, the customer has moved home and updates their address.Here Jane Doe purchases a product, and then later on updates her address, as reflected in CustomerDetailsAs previously, let’s ask some questions!Query 4: Which customer last purchased a product with id:123?MATCH (:Product {id:123})<-[r1:BUYS]-(c:Customer)WITH c, r1 ORDER BY r1.date DESC LIMIT 1MATCH (c)-[r2:HAS_CUSTOMER_DETAILS]->(cd:CustomerDetails)WHERE NOT EXISTS r2.toRETURN cd.nameWe use the ORDER BY r1.date DESC LIMIT 1 to get the newest BUYS relationship.Query 5: Where has Jane lived and when did she move?MATCH (:Customer {id:456})-[r:HAS_CUSTOMER_DETAILS]->(cd:CustomerDetails)RETURN DISTINCT cd.address AS Address, r.from AS From, r.to AS ToORDER BY FromManaging retrospective changesSometimes just capturing one date or timestamp is not enough. There are scenarios where we capture (or don’t capture!) something happening. Later on we discover we need to apply a correction for that event. We can’t just go and correct it, because we lose information about something going wrong. We need to be able to manage retrospective changes. For example, you may need to manage retrospective changes in situations such as:Dealing with new information that is discovered that needs to be reconciled — for example, you deposited some money into your savings account, but the bank missed itProvide an audit trail for regulatory purposes — a company needs to demonstrate to a governing body what went wrong and how it was correctedUsed in what-if and other analysis based on events happening at different potential points in time — applying when a process should execute in the future, but the current date when it was authorised, e.g. price increase in a monthly subscription or changing how much power is flowed down a network.This type of versioning is also commonly known as bi-temporal versioning.The principles are fairly simple, we now use two date/timestamps instead of one:one to represent when something should have happened, in our following scenario we shall refer to this as business date, or bizDate for shortone to represent when something actually happened, for example when a correction has taken place. In the scenario we shall refer to this as process date, or procDate for shortSo, on to our final scenario!Scenario 3A customer buys a product from the company. Unfortunately something has happened during the process and the transaction to capture the sale fails. During the bi-annual stock-take, it is identified that there is one item of type product less compared to the records. After some investigation, the missing transaction is identified and rectified.Customer with id:836 bought a product, but the transaction didn’t register. Later on in the year the situation is rectifiedIn our latest iteration we added bi-temporal versioning elements to the BUYS relationships:bizDate captures the business date of when a transaction took place (or should have taken place). E.g. this is the date we’d show the sale went throughprocDate captures the date of when the transaction or any corrections actually took place. If everything is working as expected, bizDate and procDate will be identical. If, per our scenario, we miss and then later identify a transaction, we would set procDate as the current date when the correction is applied, and bizDate would be the retrospective date of when the transaction should have taken place.Query 6: How many transactions were missed that we retrospectively captured?MATCH (:Customer)-[r:BUYS]->(:Product)WHERE r.bizDate < r.procDateRETURN count(*)Query 7: What were the captured transactions for the past year?MATCH (:Customer)-[r:BUYS]->(p:Product)WHERE r.bizDate = r.procDate AND r.bizDate >= 20181010RETURN p.id AS SKU, r.bizDate AS `Sale Date`ORDER BY `Sale Date`Query 8: What are all of the actual transactions for the past year?MATCH (:Customer)-[r:BUYS]->(p:Product)WHERE r.bizDate >= 20181010RETURN p.id AS SKU, r.bizDate AS `Sale Date`, r.procDate AS `Transaction Date` ORDER BY `Sale Date`Advantages and Disadvantages of these approaches for capturing changeAs with all modelling decisions, there will be advantages and disadvantages, and time-based versioning is no different.Advantages:All changes to the data are captured, including relationship changesAble to step backwards and forwards in time according to the questions we are looking to answer.Disadvantages:Need to do additional work to model changes in relationshipsNo indexing on relationship properties — further model iteration is required if there are many state changesQuerying is a bit more complex.What has been shown here is the formalised view for time-based versioning. Of course, the data you are working with and the questions you want to ask will provide you with opportunities to exercise pragmatism and only use the components that are useful to capture up to the level of versioning you require.SummaryIn this post, we’ve looked at why you may want to have versioning in your graph database and some use-cases where it is useful. We discussed that, as versioning is not available out of the box, we need to incorporate it into our data model.We looked at time-based versioning, and how this can be extended for versioning relationships, as well as capturing retrospective changes.Last but not least, we discussed some of the advantages and disadvantages with this method, and the importance of pragmatism in your versioning approaches.;Dec 16, 2019;[]
https://medium.com/neo4j/all-those-beautiful-records-15baece33876;Michael SimonsFollowNov 21, 2022·2 min readAll Those Beautiful Records…Java 17 Records and the Neo4j-Java-DriverBack in March 2020 Java 14 has been released with the first preview of java.lang.Record respectively the record keyword: https://docs.oracle.com/en/java/javase/17/language/records.html. Record classes, which are a special kind of class, help to model plain data aggregates with less ceremony than normal classes and can be thought of as nominal tuples. Records did receive a lot functionality since then and will continue to do so.Photo by Mick Haupt on UnsplashHowever, Record” is such a nice synonym for Tuple that it has been used extensively elsewhere, especially in database connectivity. Abstractions for relational databases like jOOQ deal in records and so do the Neo4j-Drivers, especially the Java-Driver. For a long time now we have org.neo4j.driver.Record which represent a tuple or a row if you so will in the greater result set of a query.This has not been a problem until Java 14. With Java 14 however, java.lang.Record appeared and as it is in the java.lang package, it is always imported into your Java program, even though the chances that you will use the class that backs the actual records, are close to zero. Most IDEs, prominently IntelliJ, won’t suggest any import for you on JDK 17 these days for a type of Record because it’s there already. Also, star-imports such as org.neo4j.driver.* , will lead to an error during compilation stating java: reference to Record is ambiguous, both interface org.neo4j.driver.Record in org.neo4j.driver and class java.lang.Record in java.lang match.As a user of the Neo4j-Java-Driver that can lead to awkward situations in which you will want to process a result set but can’t access its items, as the wrong” record type is imported into your scope or you just end up with above compilation error.That didn’t happen to often in the past, but with the brand new Neo4j-Java-Driver 5.0 and higher we bumped our requirements to the latest Java LTS release which is version 17 and here this will happen. This is not a bug in the Java driver or in your code, it’s just something we as a provider of that library need to communicate, and we are not alone in this regard and others also chose to communicate things instead of changing their record implementation.Here’s a working example that should explain things:Java program demonstrating how Java and Neo4j records coexistsAlso, please don’t use star- or wildcard-imports, really.Happy coding!;Nov 21, 2022;[]
https://medium.com/neo4j/twin4j-linear-regression-on-neo4j-graphql-for-database-admin-bloom-on-world-cup-graph-c9987ff527b1;Mark NeedhamFollowJul 2, 2018·3 min readTWIN4j: Linear Regression on Neo4j, GraphQL for database admin, Bloom on World Cup GraphA procedures library for running linear regression on Neo4j, the Yen’s k-shortest path algorithm, using GraphQL for database administration, exploring the World Cup with Neo4j BloomThis week we’ve got a procedures library for running linear regression on Neo4j, the Yen’s k-shortest path algorithm, using GraphQL for database administration, exploring the World Cup with Neo4j Bloom, and more!If you have suggestions of things you’d like to see in future editions or any questions let me know.Cheers, MarkFeatured Community Member: Alicia PowersThis week’s featured community member is Alicia Powers, Founder at Heart Metrics, Inc.Alicia has been part of the Neo4j community for several years and presented a popular talk at GraphConnect EU 2016 titled Who Cares What Beyonce Ate for Lunch?.Alicia was also interviewed about her work and opinions on the future of graph databases at GraphConnect 2017 in New York.On behalf of the Neo4j community, thanks for all your work Alicia!Graphs and ML: Linear Regression?In Graphs and ML: Linear Regression? my colleague Lauren Shin describes a set of user-defined procedures that she implemented to create a linear regression model in Neo4j.Lauren goes on to demonstrate the use of linear regression from the Neo4j browser to suggest prices for short term rentals in Austin, Texas.Read the blog postAnalysing the World Cup with Cypher, GraphQL, and Neo4j BloomIn this week’s online meetup Jesus Barrasa and I showed how to query the World Cup 2018 Graph with 3 different tools: Cypher, GraphQL, and Neo4j Bloom.It was quite a fun experience trying to work out which tool worked best for which type of queries.YouTube: Neo4j APOC SeriesMichael started the Neo4j APOC HowTo Series of YouTube videos designed to help get you quickly up to speed with the indispensable procedures library.So far 2 videos have been released — User Defined Procedures & Functions and APOC Introduction and Installation Desktop & Sandbox Neo4j — and there are many more to come!Graph Algorithms: Yen’s k-shortest pathIn Tomaz Bratanic’s latest post he shows how to find alternative routes in the California road network using Neo4j Graph Algorithms.He starts by showing how to find the shortest path between two places using Dijkstra algorithm’s and then goes on to demonstrate Yen’s k-shortest paths algorithm which allows us to get the 2nd shortest path, 3rd shortest path, and so on.This algorithm is very handy in the transport domain where we might want to find alternative routes if there’s a problem with the shortest one.Read the blog postPreserving the Neo4j pagecache across database restartsWhile browsing the Neo4j Knowledge base I came across an entry explaining a 3.4 feature that I didn’t even know about! 3.4 has a new active page cache warmup feature in the Enterprise Edition.The memory contents of the page cache are periodically profiled, and these profiles are used to quickly reheat the cache on startup. This decreases the time-to-performance for restarts or crashes.Tune your page cache20 minutes: How long it takes to write a Neo4j Procedure in KotlinEmad Panah explains how it took only 20 minutes to create a Neo4j Procedure that converts a string to SHA256 Hash using the Kotlin programming language.Read the blog post;Jul 2, 2018;[]
https://medium.com/neo4j/sustaingraph-a-knowledge-graph-for-the-un-sustainability-goals-a03a2fb2ff92;Alexander ErdlFollowMar 13·4 min readSustainGraph — A Knowledge Graph for the UN Sustainability GoalsThe team from ARSINOE joined me to present their Knowledge Graph, which tracks information related to the achievement of targets defined in the United Nations Sustainable Development Goals (SDGs) at national and regional levels.To watch the full episode scroll to the end of this summary blog post!Source: https://sdgs.un.org/goalsUnited Nations Sustainable Development GoalsThe United Nations Sustainable Development Goals (SDGs) are a set of 17 interconnected goals that were adopted by all UN Member States in 2015 as a universal call to action to end poverty, protect the planet, and ensure that all people enjoy peace and prosperity by 2030.They cover a range of social, economic, and environmental issues, including no poverty, zero hunger, good health and well-being, quality education, gender equality, clean water and sanitation, affordable and clean energy, decent work, economic growth, and more.The UN SDGs homepage covers information about all goals as well as yearly implementation reports and related events and publications.Qualitative, interoperable, and well-interlinked data is required to support these objectives, hence creating SustainGraph.SustainGraphFrom Data Silo to Knowledge GraphThe SustainGraph project is led by researchers from several universities and research institutions and is supported by the European Union’s Horizon 2020 research and innovation program. Eleni Fotopoulou, Ioanna Mandilara, Anastasios Zafeiropoulos, and Christina-Maria Androna from the ARSINOE Project joined this livestream to present their knowledge graph.SustainGraph is an open-source project that aims to develop a software platform for measuring and visualizing the sustainability of various systems, such as organizations, supply chains, and cities. The project utilizes various data sources and sustainability metrics to ultimately provide insights and recommendations for improving sustainability performance.Eleni Fotopoulou explains that even though there is a lot of data around these sustainability goals, the data is stored in various data silos in different formats and semantics with little to no interoperability.The SustainGraph knowledge graph aims to act as a unified source of truth for information related to the SDGs that is available for everyone working with it.The team is achieving this by taking advantage of the power of graph databases and the exploitation of Natural Language Processing (NLP) and Machine Learning (ML) techniques for data population, knowledge production, and analysis.High level view of the SustainGraphSustainGraph includes not only the UN Sustainable Development Goals, but also the EU Green Deal and their related documents, which cover a variety of topics and recommended actions for the coming 12 months for each member state. In addition to that, local governmental policies based on the Paris Agreement were also included.DemoSustainGraph DemoDuring the stream, Ioanna Mandilara continued to give a demo of SustainGraph by using Neo4j Bloom as well as NeoDash.By combining graph visualizations with a more traditional dashboard the team achieved an extensive view on their data. Now, it is possible to show how different policies impact the Sustainable Development Goals differently and how they are connected, as well as inventions and hazards influencing a specific goal. With a single view, we can see the progress toward the Sustainable Development Goals for each country.As of today, the knowledge graph consists of 5M nodes and 10M relationships. The team is hoping to include more and more resources to SustainGraph in the future as they work on making the knowledge graph accessible for scientists and other interested parties from around the world.As part of their work, the team published SustainGraph on their GitLab page.They also wrote an article titled SustainGraph: A knowledge graph for tracking the progress and interlinking the Sustainable Development Goal targets” which discusses the challenges of developing solutions to manage or mitigate the impacts of climate change, given the complexity and dynamics of socio-environmental and socio-ecological systems.Watch the Full episodeInteresting LinksEleni Fotopoulou Ioanna MandilaraAnastasios ZafeiropoulosGlobal assessment of the environmental sustainability of seaweed aquacultureARSINOE ProjectSustainGraph GitLab;Mar 13, 2023;[]
https://medium.com/neo4j/neo4j-graph-algorithms-release-memory-requirements-concurrency-settings-bug-fixes-37c501df105d;Mark NeedhamFollowJul 18, 2019·5 min readNeo4j Graph Algorithms Release — Memory Requirements, Concurrency Settings, Bug FixesOver the last few weeks we’ve released new functionality for the Neo4j Graph Algorithms Library, in versions 3.5.6.1 and 3.5.7.0.tldrThese releases see the introduction of a procedure to compute the memory requirements of the PageRank, Louvain, and Union Find algorithms, as well as the in memory graph. We’ve also added support for different concurrency values for read, write, and compute tasks, and there are a bunch of bug fixes as well.You can install the latest release directly from the Neo4j Desktop in the ‘Plugins’ section of your project. Jennifer Reif also has a detailed post explaining how to install plugins if you haven’t used any yet.Installing the Graph Algorithms LibraryIf you’re installing the library on a server installation of Neo4j you can download the JAR from the Neo4j Download Center.Neo4j Download CenterMemory RequirementsThe graph algorithms library operates completely on the heap, which means we’ll need to configure our Neo4j Server with a much larger heap size than we would for transactional workloads.When we execute one of the algorithm procedures, an in memory graph representation of the graph is created before the algorithm is executed against that graph. The diagram below shows the steps involved:Steps involved when executing graph algorithmsThe in memory projected graph contains the following pieces of data:In Memory Graph ModelThe library supports two types of in memory graph:heavy, which can process up to 2³¹ (2 billion) nodeshuge, which can process up to 2⁴⁵ (35 trillion) nodesThe Memory Requirements Procedure returns an estimate of the amount of memory required to run graph algorithms, without actually running them.This procedure has the following signature:CALL algo.memrec(label, relationship, algorithm, config)If we want to find out the amount of memory required to build an in memory graph containing all the nodes and all the relationships using the heavy graph, we could compute this by running the following query:CALL algo.memrec(null, null,  graph.load , {graph:  heavy })If we wanted to compute the memory requirements for the huge graph we can do that as well:CALL algo.memrec(null, null,  graph.load , {graph:  huge })Below we can see the amount of memory needed for Dbpedia and Twitter 2010, two well known benchmark datasets:Dbpedia Memory RequirementsTwitter 2010 Memory RequirementsWe can also compute the memory requirements for three individual algorithms: PageRank, Louvain, and Union Find.The following query computes the amount of memory needed by the PageRank algorithm:CALL algo.memrec(null, null,  pageRank , {   graph:  heavy , concurrency: 8})YIELD mapViewUNWIND mapView.components AS componentRETURN component.name AS component,        component.memoryUsage AS memoryUsageBelow we see the memory requirements of our benchmark datasets:Dbpedia PageRank Memory RequirementsTwitter 2010 PageRank Memory RequirementsAs mentioned at the beginning of this section, the library stores all its data structures on the heap, so we need to configure our memory settings accordingly.This can be done via the following configuration parameters in the Neo4j Settings (neo4j.conf) file:dbms.memory.heap.initial_size=512mdbms.memory.heap.max_size=1GSo if we wanted to run PageRank on the Twitter 2010 dataset using the heavy graph, we’d need to make sure that we had a heap size of at least 10GB.Non existent node labels or relationship typesWe’ve also made a usability improvement to all algorithms so that they won’t execute if provided non existent node labels or relationship types.Running PageRank with a non existent node labelIn previous versions the algorithm would have run against the full graph.Concurrency SettingsBefore this release the number of threads to be used by graph algorithms could be controlled by the concurrency config parameter.This parameter was used for reading data from Neo4j into the in memory projected graph, executing the algorithm itself, and writing results back to Neo4j.This release adds two new config parameters:readConcurrency — the number of threads used to read data into the in memory projected graphwriteConcurrency — the number of threads used to write data into Neo4jCALL algo.pageRank(null, null, {  writeConcurrency: int,    readConcurrency: int,  concurrency: int})If these parameters aren’t specified they will default to the value for concurrency, which itself defaults to the number of cores on the machine on which the algorithm is executed.Bug Fixes and ImprovementsBug Fixed and ImprovementsThis release also contains bug fixes for relationship loading for the huge graph, and a null-pointer exception for PageRank when running on a graph loaded with incoming relationships.We’ve also made performance improvements for node loading on huge graphs, increased histogram accuracy in community detection algorithms, and reduce memory allocation for the Louvain algorithm.We hope these changes improve your experience with the library, and if you have any questions or suggestions please send us an email to devrel@neo4j.comFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Jul 18, 2019;[]
https://medium.com/neo4j/building-interactive-neo4j-dashboards-with-neodash-1-1-2431489a864e;Niels de JongFollowJun 21, 2021·4 min readBuilding Interactive Neo4j Dashboards with NeoDash 1.1In November 2020, I released the first version of NeoDash — a graph app to build Neo4j dashboards in minutes. Since then I’ve received a lot of great feedback from users who built their own dashboards (check out this one!), and I got to work on extending the app with your suggestions.Now, 7 months later, I’m very excited to announce the next big release — NeoDash 1.1. The latest version of NeoDash comes with a ton of new features to make your dashboards more dynamic, structured, and exciting.What’s New?This post will walk you through the main new features of NeoDash:Interactive multi-page dashboardsMap visualizationsEmbeddable web apps (Neo4j Bloom) & extended markdown supportThere’s also been many minor stability improvements, which you’ll find on the project’s GitHub repository.nielsdejong/neodashNeoDash is a lightweight web app to create Neo4j dashboards. Writing Cypher is all thats required to build your first…github.comTo demonstrate these new features, I made a dashboard to plan my next brewery tour. I’ll be using the data from the Beer & Breweries GraphGist, which tells me all about beers and breweries in different parts of the world. You can download the dataset here. I’ve added a screenshot of the schema below:The schema of the beer & breweries dataset.1. Interactive & Multi-Page DashboardsFirst, I’d like to do a bit of exploration of the beers and breweries in different countries. I want to see, for each country, which breweries serve beers in a style I enjoy. For this, I use selection boxes, which let me easily specify Cypher parameters:An example dashboard that uses selection reports to specify query parameters.On this page, I can select two parameters, a brewery country and a beer style, which are stored as $neodash_brewery_country and $neodash_style_name. The selected parameters are then used by the other reports to get results from the database:MATCH (s:Style)<-[:BEER_STYLE]-(b:Beer)-[:BREWED_AT]->(br:Brewery) WHERE      br.country = $neodash_brewery_country AND      s.name = $neodash_style_name RETURN b.name as Beer, br.name as BreweryYou can use the selected parameters across all queries in your dashboard. Reports get automatically updated when new values are selected.After some exploration of the data, and a gentle push from my Belgian colleagues, I choose Belgium as the destination of my tour.Next, I want to create multiple pages that visualize my trip. In NeoDash 1.1, you can add as many pages as you’d like to create a more cohesive structure, as well as prevent long pages with an overload of data.2. Map VisualizationsNow that I know which country to visit, I’d like to see the brewery tour on a map. In NeoDash, geographical visualizations can be created automatically from nodes with latitude/longitude properties, or when you use Spatial data types. As an example, consider the next two queries:// All breweries that arent top rated MATCH (b:Brewery) WHERE NOT b:`Top Rated` RETURN b// A brewery tour in Belgium MATCH (b:Brewery)-[route:ROUTE_TO]->(b2:Brewery) WHERE b.country =  Belgium  RETURN b, route, b2NeoDash will pick up the latitude and longitude properties from the Brewery nodes, and create a map view:Two map visualizations showing brewery locations, and a tour between them.Nodes are assigned a color automatically based on their labels. Just like in the graph visualization view, these colors can be overridden by using a Cypher parameter. Additionally, both nodes and relationships can be clicked to inspect their properties.3. Embedded Applications & Better MarkdownFinally, I’d like to see a nice graph view of the breweries and the beers I’ll be trying out.In NeoDash 1.1, you can embed web apps directly into reports using iFrames. In my case, I embed Neo4j Bloom directly into my dashboard. The image below shows how I use a Bloom visualization next to a table with distances between the breweries:A dashboard with an embedded Bloom view.Important: you must run Bloom from a web browser to be able to get this working. Here’s what to do to set this up yourself:Create a Neo4j Aura instance (I’m using the Aura Free Tier).Open the Aura Console, find your database, choose Open With, and select Neo4j Bloom (Beta).Copy the URL from the tab your browser just opened.In NeoDash, create a new iFrame” report. Paste the Aura URL in there, and you’re ready to go.You’ll also see that the screenshot above contains a report with a code block. This is part of the extended markdown support, which also includes a better display of ordered and unordered lists.Wrapping UpYou can try NeoDash from the Graph App Gallery, or by running the online demo.I recommend you check out some of the other great work that’s been done on dashboarding Neo4j:Jennifer Reif’s post on using Retool with Neo4j GraphQL.Adam Cowley’s work on charts, a graph app with a ton of visualization options.David Allen’s blog post on the Neo4j BI Connector for Tableau.Graphileon, a low-code graph application builder for the enterprise.As always, please reach out with any questions or recommendations. If you’re interested in building user-friendly dashboards, you should also check out this cool article on dashboard psychology.;Jun 21, 2021;[]
https://medium.com/neo4j/graph-data-modeling-keys-a5a5334a1297;David AllenFollowJun 29, 2020·7 min readGraph Data Modeling: All About KeysIn two previous articles, we covered aspects of graph data modeling such as categorical variables, and how relationships work. In this article, let’s address how to identify things in your graph with keys.What’s a Key?A graph key is a property or set of properties which help you to identify a node or relationship in a graph.They are frequently used as a starting point for a graph traversal, or used as a condition to constrain.What do we want out of a key?Before we get into different options for keys in Neo4j, let’s list the attributes of what makes for a really great database key.Authority: We have to know who is responsible for the key. Someone has to issue it, and someone has to say which is which. That could be the database itself, you the application developer, or maybe some external authority (for example, your local DMV if you use driver’s license numbers as a key).Stability: Data is constantly changing around us. We want stable identifiers so that older systems & code can refer to newer data.Uniqueness Context: Of course keys have to be unique, but they’re unique within a certain context. A driver’s license ID is unique only within a certain state in the USA (it could be reused elsewhere).Opacity: This means, does the key have an internal meaning? Could a person guess the next key by looking at a previous one? If you saw a key like 2020–06–19-abc”, then you might infer that the key value is a date with a suffix, which would not be opaque.Do TheseHave an authority for your IDs that is either you, or a 3rd party you can live with, that publishes rules about how they do things that make sense. Whatever the authority is, whether it is you or a 3rd party, know who it is.Use a single property, and put a unique property constraint on it, which will ensure your keys cannot be duplicated and add an index for fast lookups.Have as wide of a uniqueness context as possible to prevent you from tripping on duplicates.Use opaque identifiers. It’s enough for a key to be unique, don’t try to overload it with extra meaning.Don’t Do TheseDon’t Use Smart KeysSmart keys are usually compound values which encode information into a key. Imagine we had an ordering system and we identified a customer order as 2020-06–19-VA-9912. It might seem convenient that we’ve encoded the order date (2020–06–19), the state of the order (Virginia) and the order number (9912) into a single key. In practice though, smart keys usually end up a disaster, for several reasons:If the date of the order was entered incorrectly, the ID for the record needs to change (?!?!).It encourages other developers to try to parse the ID” to extract information, which is a pain, and could give the wrong result (if the order number changed!).The key thing to notice about smart keys is that they always have low opacity that’s the point of them.Don’t Use Compound Keys in GraphsIn relational databases, it’s typical to define compound keys of two or more attributes, but in my view that never makes sense in a graph. A usual reason why someone would use a compound key is because of a dependency between columns. For example, maybe your customer code + state code together is what uniquely identifies a record. But since graphs let you have as many nodes as you want, this Cypher code:MATCH (r:Record { ccode:  X , scode:  Y  })Will usually be worse than this:MATCH (:A { scode:  Y  })-[:LINKED_TO]->(r:Record { ccode:  X  })The point is that in most cases, a good data model can eliminate the need for a compound key.What are your options in Neo4j?The Neo4j Internal IDEvery node and relationship gets its own internal” identifier which you can access with the id() function.internal ID of a nodeThe advantage of these IDs is that they’re always guaranteed to be there for you. And lookup by ID is very fast in Neo4j because of the way the graph storage in memory works. But internal node IDs (in my view) make for very bad application identifiers, for a number of reasons:They get reused. They’re guaranteed to be unique in a graph, but if you delete node 25 and keep creating data, you may have a different node 25 later on.They don’t track between databases. If you dump all of your data from system A and load the same data into a different system B, you won’t necessarily have the same IDs. So they’re not useful for connecting data in different databases, for example with Neo4j Fabric.Basically, the only guarantee you get is that they are globally (to that graph, not to the DBMS) unique. While this is opaque, the authority for the identifier is the database (not your application) and the uniqueness context is scoped to a single graph on a single system only.Globally Unique IDsUsing APOC’s built in UUIDs, you can create them on the fly like this:CREATE (m:Thing { id: apoc.create.uuid() })An APOC-generated UUIDThese are quite good, because you are the authority and manage them yourself. They are stable and never need to change. They are extremely unique across all contexts, and they’re very opaque. They are 128-bit numbers that are pseudo-randomly generated. Practically speaking, you don’t have to worry about collisions, since the space is so large that if you generate 103 trillion identifiers this way (and we’re pretty sure you’re going to be under that) your chances of a collision are still one in a billion. Good enough.They come with downsides though.They’re big and clunky, and contain more data than is needed for an identifier, which means they take up space when you have billions of them.They’re hostile to human readability, which can matter if your IDs end up in URLs, which is pretty common.Somebody Else’s IDsLet’s face it, usually our source data is coming from somewhere else. If we’re importing tweets from twitter into a graph, all of those tweets have existing IDs. And so often, a good approach will be to adopt someone else’e ID scheme that came with your data import.It’s tough to say what the pros of this approach are, because it will depend on what the identifier is. The best we can do is go back to those principles we’re looking for (opacity, uniqueness, etc) and evaluate an ID against those.We can talk about specific negatives of adopting someone else’s identifiers though:Authority: You aren’t it. Which means you’re trusting some element of your data’s durability to that outside authority’s IDs. Is this a problem? Depends on your situation. Maybe, maybe not.Mix-ability: Your graph might have one feeder source right now (Twitter, for example). What happens when you start importing other sources? If you bring in Facebook posts, will you have two identifiers, or will the ID depend on the source? This gets ugly quickly.As a general recommendation — always store any upstream identifier that you can get your hands on. But don’t use it to be your identifier. Use it for correlation with your upstream system. There’s nothing wrong with choosing your own ID in addition to storing a remote identifier.Auto-Incrementing NumbersA common approach is to use an auto-incrementing number. Neo4j doesn’t support this straight out of the box, but it’s common to find it in other libraries, and it’s a common technique in the relational world. It’s usually not the best approach though, because:If each node label gets is own incrementer”, the ID isn’t really unique to the graph, only to the label. This makes your key implicitly id + label, not just the ID. This is the weakest scope of uniqueness” you can choose.It has the same potential reuse weaknesses as the neo4j internal ID.That being said, this approach is still opaque (good) controlled by you (good), and compact/storage efficient.Relationship IdentifiersAs of Neo4j 4.1.0, the database does not have regular b-tree relationship property indexes (it does support full-text indexes on relationship properties though) This has important consequences, and means that it’s not possible to look up individual relationships quickly by an ID, because the database simply doesn’t store things that way. The way you find relationships is by looking up one (or both) of the incident nodes, like so:MATCH (a:Person { id: 1 })-[r:KNOWS]->(b:Person { id: 2 })RETURN rIn this scenario, effectively we’re using the from” and to” nodes as the relationship key. The id() function still exists for relationships, and they all have internal Neo4j IDs, but typically we don’t need to ever assign property IDs to relationships. Not only are they locatable in this other way, but lacking property indexes, lookup by key wouldn’t be the efficient way to go anyway.This article is part of a series if you found it useful, consider reading the others on labels, relationships, super nodes, and categorical variables.;Jun 29, 2020;[]
https://medium.com/neo4j/neo4j-ogm-3-2-released-cdbaf1be1400;Michael SimonsFollowOct 7, 2019·7 min readNeo4j-OGM 3.2 releasedCurrent state and future plans of Neo4j-OGMThere is happening so much right now in the world of graphs. We have just released our first public milestone release of Neo4j 4.0, the upcoming major version of Neo4j, next week the NODES2019 online conference is happening and the team behind Spring Data Neo4j and Neo4j-OGM wasn’t sleeping as well:SDN/RX was released as a public beta with a fantastic introduction video by Gerrit Meier.I personally was super happy to find a blog post written by someone else about SDN/RX title Reactive programming with Neo4j”.Last but not least, even Jürgen Höller mentioned our work on reactive transactions during the announcement of Spring Framework 5.2.Photo by delfi de la Rua on UnsplashBut aside from all those amazing new development, we are determined to maintain the SDN+OGM legacy.Last week, we released Neo4j-OGM 3.2, which is the baseline of Spring Data Neo4j 5.2, released as part of the Spring Data Moore release train.Neo4j-OGM 3.2 contains all bug fixes and improvements contained in 3.1 since the original release of 3.1, but also the following:Allow configuration of packages to scan in ogm.properties through base-packages.Introduced exception translator to unify exceptions of different transports into an OGM hierarchy.Deprecated OgmPluginInitializerSessionFactory.getDriver() has been replaced with SessionFactory.unwrap(Class<T> clazz) which provides a consistent way to get the underlying Neo4j-OGM driver or the native driver.Add support for containing filter in combination with ignore case.Improved support for enums in Map-propertiesImproved Kotlin supportSupports all native Neo4j 3.5 types in embedded an Bolt modesWe also removed some things:Removed neo4j.ha.properties.file” property from OGM configuration. Use neo4j.conf.location” instead.Removed org.neo4j.ogm.autoindex.AutoIndexManager#build. Use org.neo4j.ogm.autoindex.AutoIndexManager#run instead.Removed deprecated and unsupported method org.neo4j.ogm.session.Neo4jSession#setDriver.Removed deprecated @GraphId. Please use a Long field annotated with @Id @GeneratedValue instead.Removed deprecated org.neo4j.ogm.session.Session.doInTransaction(GraphCallback<T>).Removed deprecated and unused ServiceNotFoundException.Removed deprecated org.neo4j.ogm.session.Neo4jException.Removed deprecated org.neo4j.ogm.exception.core.NotFoundException.Removed deprecated org.neo4j.ogm.exception.core.ResultErrorsExceptionFrom 3.2.0 onwards, we don’t distribute neo4j-ogm-test anymore. That package contains some test utilities that we are using internally and which we don’t maintain in a stable way to be used from the outside.Thanks a lot to everyone reporting issues (https://github.com/neo4j/neo4j-ogm/issues), providing pull request or just gave us feedback, on external as well as internal channels.Neo4j-OGM 3.2 is build and tested against Neo4j 3.4 and 3.5. You can use it already to connect against Neo4j 4.0 over Bolt (using Neo4j in server mode), but this is untested as of now. We aim to ensure compatibility with Neo4j 4.0 with the GA release of Neo4j 4.0I like to highlight two features of this release, in addition to the overall improvements and stability work:Improved Kotlin supportNeo4j-OGM is a database to object and vice versa mapper. Especially coming from the database side of things, many aspects are to be considered while building the object graph:Which properties have been loaded?Which relationships have been traversed and how are they returned to the client? Are the returned as a cartesian product along with the main-node, as collections or as projections?Are required properties missing and do we need to execute additional queries?Those three aspects are somewhat against the concepts of immutable data classes, which are the preferred way in the Kotlin universe to pass around blocks of data.While the Spring Data infrastructure provides superb support for constructor based creation of entities after all properties and associations have been loaded, we cannot use those infrastructure from Neo4j-OGM as we still want that library to be usable without Spring.We could however make life easier for all Kotlin fans: final attributes are no longer excluded from mapping. Neo4j-OGM uses field access internally to populate them, but that doesn’t leak into your domain.How to use Kotlin Data classes then?Neo4j-OGM 3.2 still needs the no-args constructor on your @NodeEntity- and @RelationshipEntity annotated classes. Luckily, there’s a Maven build plugin for that, the org.jetbrains.kotlin:kotlin-maven-plugin.Add it to your build like this (kotlin.version being 1.3.50 in our case):<build>    <plugins>        <plugin>            <groupId>org.jetbrains.kotlin</groupId>            <artifactId>kotlin-maven-plugin</artifactId>            <version>${kotlin.version}</version>            <configuration>                <args>                    <arg>-Xjsr305=strict</arg>                </args>                <pluginOptions>                    <option>no-arg:annotation=org.neo4j.ogm.annotation.NodeEntity</option>                </pluginOptions>                <compilerPlugins>                    <plugin>no-arg</plugin>                </compilerPlugins>                <jvmTarget>1.8</jvmTarget>            </configuration>            <executions>                <execution>                    <id>compile</id>                    <phase>compile</phase>                    <goals>                        <goal>compile</goal>                    </goals>                </execution>                <execution>                    <id>test-compile</id>                    <phase>test-compile</phase>                    <goals>                        <goal>test-compile</goal>                    </goals>                    <configuration>                        <sourceDirs>                            <source>src/test/java</source>                            <source>src/test/kotlin</source>                        </sourceDirs>                    </configuration>                </execution>            </executions>            <dependencies>                <dependency>                    <groupId>org.jetbrains.kotlin</groupId>                    <artifactId>kotlin-maven-allopen</artifactId>                    <version>${kotlin.version}</version>                </dependency>                <dependency>                    <groupId>org.jetbrains.kotlin</groupId>                    <artifactId>kotlin-maven-noarg</artifactId>                    <version>${kotlin.version}</version>                </dependency>            </dependencies>        </plugin>    </plugins></build>This configures the plugin to generate a no-args constructor for all classes annotated with @NodeEntity. Similar can be done for @RelationshipEntity. The constructor will not be visible or usable in your application, so you don’t need to worry about invalid data inside your application. Gradle is of course covered as well, but please refer to the Kotlin documentation.Having that in a build, you could use classes like the following in your domain:import org.neo4j.ogm.annotation.*@NodeEntitydata class MyNode (    @Id @GeneratedValue var dbId: Long? = null,    @Index(unique = true) val name: String,    val description: String,    @Relationship( IS_LINKED_TO , direction = Relationship.OUTGOING)    val otherNodes: List<OtherNode> = emptyList())@NodeEntitydata class OtherNode (        @Id @GeneratedValue var dbId: Long? = null,        @Index(unique = true) val name: String)The usage would look like this, where sessionFactory points to an OGM SessionFactory, containing the class definitions above@Testfun basicMappingShouldWork() {    var myNode = MyNode(        name =  Node1 , description =  A node ,        otherNodes = listOf(            OtherNode(name =  o1 ), OtherNode(name =  o2 )        )    )    val session = sessionFactory.openSession()    session.save(myNode)    myNode = session.load(myNode.dbId!!)    assertThat(myNode.name).isEqualTo( Node1 )    assertThat(myNode.description).isEqualTo( A node )    assertThat(myNode.otherNodes)        .hasSize(2)        .extracting( name ).containsExactlyInAnyOrder( o1 ,  o2 )}This little example shows something else we added to Neo4j-OGM: We added support for reified types on all the methods of Neo4j-OGM Session and SessionFactory that deal with class type parameters. That means, you can either assign the result to a known type or use type parameters (but don’t need to fallback to extracting the Java class). Here are some examples:val session = sessionFactory.openSession()val farin : MyNode = session.queryForObject(         MATCH (n:MyNode {name: \$name}) RETURN n ,        mapOf(Pair( name ,  Farin )))assertThat(farin.name).isEqualTo( Farin )val nodes = session.loadAll<MyNode>(Filter( name , ComparisonOperator.EQUALS,  John ))val returnedNames : Iterable<String> = session.query( MATCH (n:MyNode) RETURN n.name ORDER BY n.name DESC )session.deleteAll<MyNode>()Working on this actually originates directly from our R&D work with SDN/RX: Things we find usable and desirable there, we try to port over to Neo4j-OGM.Native types supportSince Neo4j 3.4, the database has seen many additional interesting and very useful types. Among them are Cypher dates and times, which all map neatly to Java’s JDK 8 java.time types, but also spatial types. Spatial types describes coordinates, either two- or three dimensional in either cartesian or geographic coordinate systems.Neo4j-OGM has a complicated background and has seen several installments, leading to supportHTTPBoltand embedded connections to a Neo4j databaseYou’ll find structures and conversions from the HTTP transport all over the place and it has given us a hard time to cater for native types. In the end we decided to support native types (all Cypher dates and times as well as spatial types) only for connections over Bolt and embedded usage.Photo by Eric Rothermel on UnsplashTo enable this, you’ll need to one of org.neo4j:neo4j-ogm-bolt-native-types or org.neo4j:neo4j-ogm-embedded-native-types to your project. If you’re on Spring Boot, you don’t need to add a version number for the dependencies from Spring Boot 2.2.0.RC1 onwards. Those are part of the managed dependencies.If you are on Bolt, running Neo4j in server mode, than you have to add<dependency>    <groupId>org.neo4j</groupId>    <artifactId>neo4j-ogm-bolt-driver</artifactId>    <version>${neo4j-ogm.version}</version></dependency><dependency>    <groupId>org.neo4j</groupId>    <artifactId>neo4j-ogm-bolt-native-types</artifactId>    <version>${neo4j-ogm.version}</version></dependency>to enjoy native types, on embedded you need<dependency>    <groupId>org.neo4j</groupId>    <artifactId>neo4j-ogm-embedded-driver</artifactId>    <version>${neo4j-ogm.version}</version></dependency><dependency>    <groupId>org.neo4j</groupId>    <artifactId>neo4j-ogm-embedded-native-types</artifactId>    <version>${neo4j-ogm.version}</version></dependency>Using native types for temporal and spatial property types is a behaviour changing feature, as it will turn the default type conversion off and dates are neither written to nor read from strings anymore. Therefore it is an opt-in feature.To opt-in, please first add the corresponding module for your driver and than use the new configuration property use-native-types, either through ogm.properties:URI=bolt://neo4j:password@localhostuse-native-types=trueor programmatically configuration:Configuration configuration = new Configuration.Builder()        .uri( bolt://neo4j:password@localhost )        .useNativeTypes()        .build()In a Spring Boot 2.2.0 application, you will be able to configure this via a Spring Boot property:spring.data.neo4j.use-native-types=trueYou still need to declare the native types for the transport you want to use as stated above, though. Spring Boot 2.2 will be released this year and is now in release candidate phase.There’s nothing more to do after that to use one of the following types in your domain:java.time.LocalDatejava.time.OffsetTimejava.time.LocalTimejava.time.ZonedDateTimejava.time.LocalDateTimejava.time.Durationjava.time.Periodorg.neo4j.ogm.types.spatial.CartesianPoint2dorg.neo4j.ogm.types.spatial.CartesianPoint3dorg.neo4j.ogm.types.spatial.GeographicPoint2dorg.neo4j.ogm.types.spatial.GeographicPoint3dThe time types are mapped the same way as the Java driver for bolt does it (see our manual here https://neo4j.com/docs/driver-manual/1.7/cypher-values/#driver-neo4j-type-system), the additional spatial types are mapped to aTo a cartesian point exhibiting x, y (and z)Or to a geographic point with the WGS 84 coordinate reference system exhibiting longitude, latitude (and height). The SRID used for 2D will be 4326, for 3D 4979.One possible domain class in a Spring Boot / Spring Data application making use of this would be this:import lombok.Getterimport lombok.Setterimport java.time.LocalDateTimeimport org.neo4j.ogm.annotation.GeneratedValueimport org.neo4j.ogm.annotation.Idimport org.neo4j.ogm.annotation.Indeximport org.neo4j.ogm.annotation.NodeEntityimport org.neo4j.ogm.annotation.Relationshipimport org.neo4j.ogm.types.spatial.GeographicPoint2dimport org.springframework.data.annotation.CreatedDateimport org.springframework.data.annotation.LastModifiedDate@NodeEntity( MusicVenue )@Getterpublic class MusicVenueEntity {	@Id	@GeneratedValue	private Long id	@Index	private String name	@CreatedDate	private LocalDateTime createdAt	@LastModifiedDate	private LocalDateTime updatedAt	private GeographicPoint2d location	@Relationship( IS_LOCATED_IN )	@Setter	private CountryEntity foundedIn	public MusicVenueEntity(            String name, GeographicPoint2d location        ) {		this.name = name		this.location = location	}}What’s next?We will be working on making the embedded version 4.0 compatible. After that, we have ideas in the pipeline to make better use of https://github.com/classgraph/classgraph to improve the class analysis.;Oct 7, 2019;[]
https://medium.com/neo4j/finding-the-best-tennis-players-of-all-time-using-weighted-pagerank-6950ed5fc98e;Mark NeedhamFollowOct 11, 2018·7 min readFinding the best tennis players of all time using Weighted PageRankIn the latest release of the Neo4j Graph Algorithms library we added support for the weighted variant to the PageRank algorithm.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comI’m a big tennis fan and my colleague Ryan shared a paper titled Who Is the Best Player Ever? A Complex Network Analysis of the History of Professional Tennis which uses a variant of PageRank, so I was keen to take the algorithm for a spin.I got myself ready to do some screen scraping to get the data, but it turns out that Kevin Lin has done all the hard work and put all the results for matches up until the end of 2017 as CSV files in the atp-world-tour-tennis-data Github repository.Thanks Kevin!Import the dataBefore we import anything, we’ll create some constraints so that we don’t end accidentally import duplicate players:CREATE constraint on (p:Player)ASSERT p.id is uniqueCREATE constraint on (m:Match)ASSERT m.id is uniqueNow we’re ready to import the data into Neo4j. We need to copy the CSV files that Kevin created into the import folder of our Neo4j installation.Once we’ve done that we can import the data into Neo4j using Cypher’s LOAD CSV command:LOAD CSV FROM  file:///match_scores_1968-1990_UNINDEXED.csv  AS rowMERGE (winner:Player {id: row[8]}) ON CREATE SET winner.name = row[7]MERGE (loser:Player {id: row[11]}) ON CREATE SET loser.name = row[10]MERGE (m:Match {id: row[22]})SET m.score = row[15], m.year = toInteger(split(row[0],  - )[0])MERGE (m)-[w:WINNER]->(winner) SET w.seed = toInteger(row[13])MERGE (m)-[l:LOSER]->(loser) SET l.seed = toInteger(row[14])LOAD CSV FROM  file:///match_scores_1991-2016_UNINDEXED.csv  AS rowMERGE (winner:Player {id: row[8]})ON CREATE SET winner.name = row[7]MERGE (loser:Player {id: row[11]})ON CREATE SET loser.name = row[10]MERGE (m:Match {id: row[22]})SET m.score = row[15], m.year = toInteger(split(row[0],  - )[0])MERGE (m)-[w:WINNER]->(winner) SET w.seed = toInteger(row[13])MERGE (m)-[l:LOSER]->(loser) SET l.seed = toInteger(row[14])LOAD CSV FROM  file:///match_scores_2017_UNINDEXED.csv  AS rowMERGE (winner:Player {id: row[8]}) ON CREATE SET winner.name = row[7]MERGE (loser:Player {id: row[11]}) ON CREATE SET loser.name = row[10]MERGE (m:Match {id: row[22]})SET m.score = row[15], m.year = toInteger(split(row[0],  - )[0])MERGE (m)-[w:WINNER]->(winner) SET w.seed = toInteger(row[13])MERGE (m)-[l:LOSER]->(loser) SET l.seed = toInteger(row[14])Our schema is very simple. We can run the following query to see a visual representation:CALL db.schema()And here we go:Cool, looks good. Before we continue let’s write a query so that we can quickly see what the data looks like in graph form:MATCH p=()<-[:LOSER]-()-[r:WINNER]->() RETURN p LIMIT 25Biggest winnersNow let’s write a query to find the players that have the most wins:MATCH (p:Player)WITH p,      size((p)<-[:WINNER]-()) AS wins,      size((p)<-[:LOSER]-()) as defeatsRETURN p.name, wins, defeats,        CASE WHEN wins+defeats = 0 THEN 0         ELSE (wins * 100.0) / (wins + defeats) END       AS percentageWinsORDER BY wins DESCLIMIT 10If we run that query we’ll see this output:If you follow tennis you’ll probably recognise most of the names on that list. Most of them are considered amongst the best players of all time, but just counting the number of wins doesn’t seem very satisfying.It’s time to try something a bit fancier! Enter the PageRank algorithm…Building a projected credibility graphThe way that the PageRank algorithm works is that nodes receive credibility from their incoming relationship. For example, in graph of web , one web page gives credibility to another by linking to it. The amount of credibility it gives can be determined by a weight property on that relationship.In our tennis graph credibility between players can be modelled based on the number of wins that players have in matches between each other. For example the following query finds out how many times Federer and Nadal have beaten each other:MATCH (p1:Player {name:  Roger Federer }),      (p2:Player {name:  Rafael Nadal })RETURN p1.name, p2.name,        size((p1)<-[:WINNER]-()-[:LOSER]->(p2)) AS p1Wins,        size((p1)<-[:LOSER]-()-[:WINNER]->(p2)) AS p2WinsIf we run that query we’ll see this output:Our projected graph should have relationships between Federer and Nadal with a weight equal to the number of times they beat each other. The weight of the relationship from Federer to Nadal will be 23 as Federer is giving Nadal that much credibility for those 23 wins. The weight of the relationship from Nadal to Federer will be 15.We can write the following query to project this graph:MATCH (p1)<-[:WINNER]-(match)-[:LOSER]->(p2) WHERE p1.name IN [ Roger Federer ,  Rafael Nadal ]AND p2.name IN [ Roger Federer ,  Rafael Nadal ]RETURN p2.name AS source, p1.name AS target, count(*) as weightLIMIT 10If we run that query we’ll see this output:Now all we need to do is remove the WHERE clause from the query and we have a generic query that we can use across our whole graph.Finding the best tennis player ever using weighted PageRankNow we’re ready to call the PageRank algorithm. We want to use the weighted variant of the algorithm, which means that we’ll need to pass the weightProperty parameter. By default the PageRank algorithm assumes it’s running the non-weighted variant.The following query runs weighted PageRank over the whole graph:CALL algo.pageRank.stream(  MATCH (p:Player) RETURN id(p) AS id ,  MATCH (p1)<-[:WINNER]-(match)-[:LOSER]->(p2)   RETURN id(p2) AS source, id(p1) AS target, count(*) as weight  , {graph: cypher , weightProperty:  weight })YIELD nodeId, scoreRETURN algo.getNodeById(nodeId).name AS player, scoreORDER BY score DESCLIMIT 10If we run that query we’ll see this output:We see a different order at the top of the rankings than in Filippo Radicchi’s paper. The main difference is that Federer, Nadal, and Djokovic have pushed up into the top 5 of all time. Radicchi’s analysis only covers up to 2010 and these 3 players have been extremely dominant in the 8 years since then, so perhaps it’s not all that surprising.We can simulate that by only including matches from 2010 and earlier. The following query does this:CALL algo.pageRank.stream(  MATCH (p:Player) RETURN id(p) AS id ,  MATCH (p1)<-[:WINNER]-(match)-[:LOSER]->(p2)   WHERE match.year <= $year  RETURN id(p2) AS source, id(p1) AS target, count(*) as weight  , {graph: cypher , weightProperty:  weight , params: {year: 2010}})YIELD nodeId, scoreRETURN algo.getNodeById(nodeId).name AS player, scoreORDER BY score DESCLIMIT 10And we’ll get this output:Notice in this query that we’re passing in the year as a parameter to the Cypher projection query via the params key in our config map.Our top 2 are now the same as Radicchi although he has Federer down in 7th position rather than in the top 3. Nadal and Djokovic have dropped out of the top 10 in our version.We can also find the PageRank for matches within one calendar year. The following query calculates the PageRank for 2017:CALL algo.pageRank.stream(  MATCH (p:Player) RETURN id(p) AS id ,  MATCH (p1)<-[:WINNER]-(match)-[:LOSER]->(p2)  WHERE match.year = $year   RETURN id(p2) AS source, id(p1) AS target, count(*) as weight  , {graph: cypher , weightProperty:  weight , params: {year: 2017}})YIELD nodeId, scoreRETURN algo.getNodeById(nodeId).name AS player, scoreORDER BY score DESCLIMIT 10If we run that query we’ll see the following output:Below are the ATP World Tour end of year rankings for 2017:The order of the players isn’t exactly the same, but the PageRank rankings aren’t that different to the official ones. One difference is that the official rankings give different weighting for the performance in different tournaments, whereas with our PageRank approach each win has the same weight.I look forward to seeing what graph analytics problems you’re now able to solve with the addition of weights for this algorithm. Let me know what you come up with — devrel@neo4j.comEnjoy!Free download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Oct 11, 2018;[]
https://medium.com/neo4j/exploring-the-u-s-national-bridge-inventory-with-neo4j-part-1-background-3e222e1d2d99;Michael McKenzieFollowSep 1, 2020·2 min readOlga Subach on UnsplashExploring the U.S. National Bridge Inventory with Neo4j— Part 1: BackgroundThe United States infrastructure is in a state of significant deterioration. As of the 2017 ASCE Infrastructure Report the overall bridge rating is a C+ with 9.1% of the bridges rated as structurally deficient and approximately 4 out of every 10 bridges being of 50 years of age or older.Bridges are only one of many components that makeup the overall United States infrastructure, but are easily one of the most recognizable due to how we interact with them on a daily basis. Bridges allow us great access across rivers, valleys, and other roadways permitting us to travel more easily. Every year there are multiple reports of bridges around the U.S. that are closed due to threat of collapse or that collapse unexpectedly.One of the most recent infamous cases was the catastrophic failure of the I-35W Mississippi River bridge that collapsed on August 1, 2007. This series will not elaborate on the cause and analysis as there are plenty of reports and commentary about this fateful event. What matters is that in less than 10 seconds a major city lifeline crumpled into the mighty river below.This series will focus on the exploration the United States National Bridge Inventory (NBI), a publicly available database that represents an annual snapshot of all bridges in the 50 states, Washington DC, and Puerto Rico. The tool of choice is Neo4j, a labeled property graph database.Part 2: Importing and Storing Raw Data in Neo4j →;Sep 1, 2020;[]
https://medium.com/neo4j/building-a-web-app-with-neo4j-auradb-and-php-990deca0d213;Ghlen NagelsFollowDec 16, 2021·4 min readBuilding a Web App with Neo4j AuraDB and PHPIf you have decided to leverage the power of the worlds’ leading graph database as a PHP developer or team, getting started can look incredibly difficult. Do you use Docker, bare metal solutions, do you host multiple instances at once for horizontal scaling? How will you make sure the data is secure? Does it need to become part of your CI? Will your team be able to develop locally with their own private instance? These questions exponentially increase the complexity, putting a massive drain on your teams’ resources without even getting started.Luckily, there is an easy way to quickly build an excellent future-proof web app with Neo4j — and it’s called Neo4j AuraDB.Neo4j AuraDB is a cloud solution that takes away the headaches when building web apps. All you need to get started is your trusty old PHP installation and an email to sign up for the free Neo4j Aura instance no credit card required.Neo4j AuraDB is a great fit if you care about:an effortless installationbuilt-in scalability, reliability and securityproductivityfocusing on developing your business solutionQuickly get started by following these two steps!Step 1: Get Your Free Neo4j AuraDB instanceAll you need to get started is your trusty old PHP installation and an email to sign up for the free Neo4j AuraDB instance no credit card required.Signing up for a free database instance is super convenient simply navigate to this page with your browser.Easy and free registration with Neo4j AuraDBYou can quickly register with Google, but any email address will do.If you want to follow along with a built-in example, make sure to select the starting dataset: learn about graphs with a movie dataset.” This is not required of course.Make sure to select the starting movie dataset if you want to follow the example.Once you have created your database. You will receive your credentials. Make sure to store them for later as you will need them to connect from PHP!A username and password for the Neo4j credentials. Don’t worry, the password has been changed :)You will be redirected to your dashboard.The Neo4j AuraDB dashboard. Remember the connection URIYou can then open it with the Neo4j browser and start querying to your hearts contempt!Querying all nodes at once found in the movies datasetNow we need to get ready with our PHP setup!Step 2: Install the Neo4j PHP lientThe PHP client can be quickly installed with this command of composer:Installing is easy as one two threeCongratulations! You are almost done creating your connections with Neo4j AuraDB 🎉Remember me warning you to store your Neo4j AuraDB credentials? I hope you listened as we need them right now. Build your client with your credentials like this:Building a client to drive Neo4j AuraDBWARNING: make sure to use systems like environment variables to detach your login variables from your code.The ClientBuilder::addDriver uses three parameters:an alias for the driver (‘aura’)the URI to point to the database (you can find this one in your dashboard as described in step 1)the authentication logicThis client can now be used to freely query your database to build an application.This configuration is straightforward to set up but is incredibly versatile! You can use it to connect to cluster deployments, Neo4j AuraDB or single instances without any trouble. This truly makes your app future-proof!You can now start querying any way you like:Print out the first 10 movies in alphabetical orderYou can also refer to the Neo4j Movies example project for more in depth use cases:https://github.com/neo4j-examples/movies-neo4j-php-clientFurther ReadingI can sit here as the author of the driver and dive deep into the technical details of the driver. But I will spare you these details for another day. You can already read about some of them here:A blog post about the initial releaseThe PHP developer guide for Neo4jThe repository readme.mdWe are putting together some live streams and videos to tutor the basics of the driver, Neo4j and PHP. Stay tuned!The Neo4j PHP ecosystem is currently experiencing a true renaissance. We are working hard on reviving the integrations with Laravel and Symfony, while other people are building a stand-alone Cypher query builder! Feel free to follow along or help us along the way.;Dec 16, 2021;[]
https://medium.com/neo4j/authorising-requests-in-nest-js-with-neo4j-28e285e0edcd;Adam CowleyFollowJul 24, 2020·16 min readJustine Camacho on UnsplashAuthorising Requests in Nest.js with Neo4jThis post is the third in a series of blog posts that accompany the Twitch stream on the Neo4j Twitch channel where I build an application on top of Neo4j with NestJS.This article assumes some prior knowledge of Neo4j and Nest.js. If you haven’t already done so, you can read the first two articles at:Building a Web Application with Neo4j and Nest.jsAuthentication in a Nest.js Application with Neo4jAuthorisationPreviously, we looked at authentication how we could verify that the user is who they say they are. Now, we’ll look at ensuring that a user is actually able to do what they want — in other words authorisation. To clarify the difference, let’s take a look at a scenario for Neoflix.When a user first registers for Neoflix they should automatically be given a 30 day free trial, which will give them access to all content on the site. After this free trial ends, they will be required to buy a subscription, which automatically renews every 30 days, unless the user cancels the subscription.The fact that the user has valid user credentials (in our case email and password) means that the API can correctly authenticate them. Although they have valid credentials, the absence of a current subscription means that we can’t authorise their request. Authorisation may also extend to users who are under 18 years old, but are trying to access adult content.Adding Subscriptions to the DatabaseTo build authorisation into the API, we’ll first need to add nodes to represent packages and subscriptions to the database.Our :Package nodes will be uniquely identified by an ID, so the first thing to do is to create the constraint in the database. Subscriptions will also be created with a unique ID so we can also create the constraint on :Subscription nodes.CREATE CONSTRAINT ON (p:Package) ASSERT p.id IS UNIQUECREATE CONSTRAINT ON (s:Subscription) ASSERT s.id IS UNIQUEI have created a CSV file with six packages, each of which has a unique ID (integer), name, price (float) and the number of days that a package is valid for for that price. Similar to Sky Movies subscriptions, packages will provide access to one or more genres - these are represented in the CSV file as a pipe delimited list of the genres.head -n 3 data/packages.csvid,name,price,days,genres1,Childrens,4.99,30,Animation|Comedy|Family|Adventure2,Bronze,7.99,30,Animation|Comedy|Family|Adventure|Fantasy|Romance|DramaThis CSV file can be imported using LOAD CSV. As mentioned in the session on Modelling, all data loaded by LOAD CSV is cast as a string by default, so well have to convert the package to an integer using toInteger and the price into a float using toFloat(). We can convert the days column into a native Neo4j duration using a string in a format recognised by a Java Duration. In this case were interested in the number of days, for example P{X}D where {X} is the number of days.LOAD CSV WITH HEADERS FROM file:///packages.csv AS rowMERGE (p:Package {id: toInteger(row.id)})SET p.name = row.name,  p.duration = duration(P+ row.days +D),  p.price = toFloat(row.price)FOREACH (name IN split(row.genres, |) |	MERGE (g:Genre {name: name})  MERGE (p)-[:PROVIDES_ACCESS_TO]->(g))The FOREACH statement at the end of the query splits the genres column by pipe (|), finds or creates the :Genre node on its name, and then creates a relationship to the package signifying that a valid subscription for this Package provides access to :Movie within the :Genre.Checking Subscriptions using CypherTo show the power of querying this data as a graph, we can also provide access to movies produced by a certain production company. For example, the bronze package provides access to films in the genres of Animation|Comedy|Family|Adventure|Fantasy|Romance|Drama. We can traverse from the :User node, through the :Subscription and :Package, to the :Genre, all in real-time. A user will be able to access any :Movie node with a relationship.(:User)-[:HAS_SUBSCRIPTION]->(:Subscription {expiresAt: datetime})    -[:FOR_PACKAGE]->(:Package)    -[:PROVIDES_ACCESS_TO]->(:Genre)<-[:IN_GENRE]-(:Movie)Cypher allows us to be flexible. Say we also want to grant access to videos produced by certain :ProductionCompany nodes. We can create a relationship type with the same name, and remove the label check on the node between the :Package and :Movie, and add another relationship type.(:Package)-[:PROVIDES_ACCESS_TO]->( ??? )<-[:IN_GENRE|PRODUCED_BY]-(:Movie)So, let’s find some production companies to provide additional access too:MATCH (p:ProductionCompany)<-[:PRODUCED_BY]-(m)-[:IN_GENRE]->(g)WITH p, collect(distinct g.name) AS genres, count(distinct m) AS moviesWHERE none(g in genres WHERE g in split( Animation|Comedy|Family|Adventure|Fantasy|Romance|Drama ,  | ))RETURN * LIMIT 10The companies ‘Bluehorse Films’ (id: 93174), ‘InPictures’ (id: 12912) and ‘Ciak Filmproduktion’ (id: 83201) seem like good options they have all produced a single film listed as Documentary. We can use the IN predicate to find the production companies by their IDs, and create a new PROVIDES_ACCESS_TO relationship between the bronze package and the production company.MATCH (p:Package {id: 2})MATCH (c:ProductionCompany)WHERE m.id IN [93174, 12912, 83201]CREATE (p)-[:PROVIDES_ACCESS_TO]->(c)If we create a test user, we can demonstrate how to check the user is allowed access to a :Movie:MATCH (p:Package {id: 2})CREATE (u:User {    id: test,    email: bronze.user@neoflix.com,    firstName: Test,    lastName: User})CREATE (u)-[:HAS_SUBSCRIPTION]->(s:Subscription {    expiresAt: datetime() + duration(P2D)})-[:FOR_PACKAGE]->(p)The :Subscription node has an expiresAt property which contains a Neo4j datetime - for the subscription to be active, that date should be greater than the current date and time (s.expiresAt >= datetime()).MATCH (u:User {id: test})-[:HAS_SUBSCRIPTION]->(s)-[:FOR_PACKAGE]->(p)-[:PROVIDES_ACCESS_TO]->(g)<-[:IN_GENRE]-(m)WHERE s.expiresAt > datetime()RETURN * LIMIT 10Given a few films selected at random, we can check the path between the user and the movie, to check whether the user has access from a valid subscription:MATCH (u:User {id: test})-[:HAS_SUBSCRIPTION]->(s)-[:FOR_PACKAGE]->(p)WHERE s.expiresAt >= datetime()MATCH (m:Movie)WITH u, s, p, m ORDER BY rand() LIMIT 10RETURN    m.id,    m.title,    exists((m)-[:IN_GENRE|PRODUCED_BY]->()<-[:PROVIDES_ACCESS_TO]-(p)) AS canAccess╒══════╤════════════════════════════╤═══════════╕│ m.id │ m.title                    │ canAccess │╞══════╪════════════════════════════╪═══════════╡│31527 │ The Scarlet Empress        │true       │├──────┼────────────────────────────┼───────────┤│4988  │ Semi-Tough                 │true       │├──────┼────────────────────────────┼───────────┤│12623 │ Suspect                    │true       │├──────┼────────────────────────────┼───────────┤│11896 │ Throw Momma from the Train │true       │├──────┼────────────────────────────┼───────────┤│25538 │ Yi Yi                      │true       │├──────┼────────────────────────────┼───────────┤│17971 │ Midnight Madness           │true       │├──────┼────────────────────────────┼───────────┤│6498  │ Nightwatch                 │false      │├──────┼────────────────────────────┼───────────┤│5923  │ The Sand Pebbles           │true       │├──────┼────────────────────────────┼───────────┤│15497 │ Twelve OClock High        │true       │├──────┼────────────────────────────┼───────────┤│21876 │ Von Ryans Express         │true       │└──────┴────────────────────────────┴───────────┘Adding Subscriptions to the APITo add subscription functionality to the API, we’ll need to create a subscription module and service. We’ll do this using the Nest CLI:nest g mo subscription # or nest generate module subscriptionnest g s subscription  # or nest generate service subscriptionInside the SubscriptionService, we should create a method to make a new subscription for the user called createSubscription. As the user will always exist and the node already assigned to the request by the JwtAuthGuard, we can pass this through as the first parameter. To save the extra database query, well pass through the package ID as a number, as this will most likely be passed as part of the request body. Optionally, we can add a parameter to override the number of days until the subscription expires. If that number isnt provided, well fall back to the duration property on the :Package node.// subscription.service.tsexport type Subscription = Node// ...async createSubscription(user: User, packageId: number, days: number = null): Promise<Subscription> {    // ...}For the Cypher query itself, we want to find the user and package by their ID’s, then create a :Subscription node. The subscription node should have its own ID (generated with cyphers randomUUID function), a createdAt datetime, and also the expiration date and time.const userId = (<Record<string, any>> user.properties).idconst res = await this.neo4jService.write(`    MATCH (u:User {id: $userId})    MATCH (p:Package {id: $packageId})    CREATE (u)-[:PURCHASED]->(s:Subscription {        id: randomUUID(),        createdAt: datetime(),        expiresAt: datetime() +            CASE WHEN $days IS NOT NULL            THEN duration(P+ $days +D)            ELSE p.duration END    })-[:FOR_PACKAGE]->(p)    RETURN s`, { userId, packageId: this.neo4jService.int(packageId), days })return res.records[0].get(s)Note: Working with IntegersSo far we’ve not worked with integers in Neo4j, so the this.neo4jService.int function will need some explaining. The Neo4j type system uses 64bit integers with a max value of 922337203685477600 - considerably higher than the maximum value that JavaScript can safely represent as an integer (Number.MIN_SAFE_INTEGER or 9007199254740991). For this reason, the Neo4j driver comes with its own Integer value (exported under neo4j.types.Integer). Any number that isnt explicitly converted to this integer type will be passed to the driver as a Float.For this reason, we’ll either need to explicitly convert our integers to this Integer type using the int function provided by the driver, or use the toInteger function in Cypher to convert the number back from a float.For this reason, I’ve added a method to the Neo4j service which will uses the int function exported from the Neo4j Driver.// neo4j.service.tsimport { int } from neo4j-driver// ...toInteger(value: number) {    return int(value)}More on integers with the Neo4j DriverUsing the SubscriptionService within the AuthModuleTo use the new subscription service, we need to add the SubscriptionModule to the imports for the AuthModule:// subscription.module.tsimport { SubscriptionModule } from ../subscription/subscription.module// ...@Module({  imports: [    JwtModule.registerAsync({      imports: [ ConfigModule, ],      inject: [ ConfigService ],      useFactory: (configService: ConfigService) => ({        secret: configService.get<string>(JWT_SECRET),        signOptions: {          expiresIn: configService.get<string>(JWT_EXPIRES_IN),        },      })    }),    UserModule,    EncryptionModule,    SubscriptionModule, // <-- new module  ],  providers: [AuthService, LocalStrategy, JwtStrategy],  controllers: [AuthController]})export class AuthModule {}Then we can inject the SubscriptionService into the AuthController.// auth.controller.tsimport { SubscriptionService } from ../subscription/subscription.service// ...export class AuthController {    constructor(        private readonly userService: UserService,        private readonly authService: AuthService,        private readonly subscriptionService: SubscriptionService    ) { }    // ...}Creating a new Free Trial” PackageWe’ll also need to create a Free Trial” package to automatically subscribe new customers. Let’s go ahead and create a new Package with an ID of 0, so that it can be easily found. We’ll give it a price of 0.00, and a default duration of 30 days.To give the user the best experience, well also create :PROVIDES_ACCESS_TO relationships to each of the genre nodes.CREATE (p:Package {  id: 0,   name:  Free Trial ,   price: 0.00,   duration: duration(P30D)})WITH pMATCH (g:Genre)CREATE (p)-[:PROVIDES_ACCESS_TO]->(g)Then, in the postRegister route handler, we can add the call to the new createSubscription method on the subscription service.// auth.controller.ts@Post(register)async postRegister(@Body() createUserDto: CreateUserDto) {    const user = await this.userService.create(        createUserDto.email,        createUserDto.password,        new Date(createUserDto.dateOfBirth),        createUserDto.firstName,        createUserDto.lastName    )    // Create a free subscription    await this.subscriptionService.createSubscription(user, 0)    return await this.authService.createToken(user)}Note: Currently this executes two separate database transactions. That’s fine for small workloads, but these two operations should really take place within the same database transaction. We’ll sort that out at a later date.Testing the new Subscription ModuleWe’ve not introduced any breaking changes so running the end-to-end tests should still pass:npm run test:e2e> api@0.0.1 test:e2e /Users/adam/projects/twitch/api> jest --detectOpenHandles --config ./test/jest-e2e.json PASS  test/app.e2e-spec.ts  AppController (e2e)    Auth      POST /auth/register        ✓ should validate the request (238 ms)        ✓ should return HTTP 200 successful on successful registration (165 ms)      POST /auth/login        ✓ should return 401 if username does not exist (41 ms)        ✓ should return 401 if password is incorrect (96 ms)        ✓ should return 201 if username and password are correct  (89 ms)      GET /auth/user        ✓ should return unauthorised if no token is provided (30 ms)        ✓ should return unauthorised on incorrect token (25 ms)        ✓ should authenticate a user with the JWT token (30 ms)Test Suites: 1 passed, 1 totalTests:       8 passed, 8 totalSnapshots:   0 totalTime:        1.834 s, estimated 2 sIf we check in the database, there should now also be a :Subscription node connected to the newly created :Package node.MATCH (p:Package {id: 0})RETURN size((p)<-[:PURCHASED]-()) // 1Getting Genres from a User’s SubscriptionsNow that we have a subscription node related to a user, we can use the graph to provide a list of genres that they have access to. Let’s create a genre module, service and controller to take care of this.nest g mo genrenest g s genrenest g co genreIn the GenreController, well want to create a route handler to listen for GET requests to the genres/ endpoint and return a list of genres. The route will be guarded by the JwtAuthGuard to ensure that the user is logged in. We can then use the @Request decorator to inject the request object into the method, where we can get the user node.// genre.controller.tsimport { Controller, Get, UseGuards } from @nestjs/commonimport { GenreService } from ./genre.serviceimport { JwtAuthGuard } from ../auth/jwt-auth.guard@Controller(genres)export class GenreController {    constructor(private readonly genreService: GenreService) {}    @UseGuards(JwtAuthGuard)    @Get(/)    async getIndex() {        // ...    }}The GenreService will need a getGenres method. This should take the User as its only argument, which we will use to find the starting point for the cypher query. From there, we will traverse the graph through to the genres that the user has permission to access.So far we’ve been working with Neo4j Driver’s Node data type. However, as this is public facing data, we should instead define a typescript interface. This will represent properties in the genre response.// genre.service.tsexport interface Genre {    id: string    name: string}In the GenreService class, we can do some processing inside the service, returning the information outlined in the interface.// genre.service.tsasync getGenresForUser(user: User): Promise<Genre[]> {    const res = await this.neo4jService.read(`        MATCH (u:User {id: $id})-[:PURCHASED]->(s:Subscription)-[:FOR_PACKAGE]->(p)-[:PROVIDES_ACCESS_TO]->(g)        WHERE s.expiresAt >= datetime()        RETURN g ORDER BY g.name ASC    `, {id: (user.properties as Record<string, any>).id})    return res.records.map(row => ({        ...row.get(g).properties,        id: row.get(g).properties.id.toNumber(),    }))}The read method on the Neo4jService returns a Result - which in turn exposes an array of records. The return statement runs the map function on that array, transforming the list of nodes into an array of plain objects. For each g value returned from Neo4j, Ive used the spread operator to mass-assign the properties to the node to the object, and then the final line converts the ID property from a Neo4j integer into a JavaScript integer.return res.records.map(row => ({    ...row.get(g).properties,    id: row.get(g).properties.id.toNumber(),}))Because we’re using a Neo4j integer type, we’ll need to call to toNumber method to convert it back into a Neo4j integer. If the number is within a safe range (Number.MIN_SAFE_INTEGER and Number.MAX_SAFE_INTEGER) it will be returned as a number, otherwise it will be returned as a string.All of the hard work is done within the service, so all that is left is to add the call to the new method into the GenreController:// genre.controller.ts@UseGuards(JwtAuthGuard)@Get(/)async getIndex(@Request() request) {    return this.genreService.getGenresForUser(request.user)}We can add another group to the end-to-end tests, and another test to ensure that the functionality works correctly. The first two tests to check the the JwtAuthGuard is working correctly can be copied and pasted from the GET /auth/user, changing the URL inside .get(...).To test the response, we can check that all 20 genre rows have been returned, and for each of those rows there should be keys for the id and name properties for the genre.describe(GET /genres, () => {    it(should return unauthorised if no token is provided, () => {        return request(app.getHttpServer())            .get(/genres)            .expect(401)    })    it(should return unauthorised on incorrect token, () => {        return request(app.getHttpServer())            .get(/genres)            .set(Authorization, `Bearer incorrect`)            .expect(401)    })    it(should return a list of genres in exchange for a valid token, () => {        return request(app.getHttpServer())            .get(/genres)            .set(Authorization, `Bearer ${token}`)            .expect(200)            .expect(res => {                // User should have access to all 20 genres                expect(res.body.length).toEqual(20)                // Each one of those genres should have id and name keys                res.body.forEach(row => {                    expect(Object.keys(row)).toEqual(expect.arrayContaining([id, name]))                })            })    })})Listing Videos in a GenreThe next feature that we should add to the API is an endpoint for the user to list movies within a genre that their subscription grants them access to. We’ll add a route handler in the GenreController for GET requests to /genre/:id. As with the previous route, this route should be guarded by the JwtAuthGuard, requiring a valid token to be sent with each request.We can use decorators in the method while defining the method to inject the variables that we are interested in, and also use some pre-built Pipes to coerce the values into the correct format. In order to process this request we will need:The :User node as added to the Auth the JwtAuthGuard — @Request() requestThe ID parameter included in the URL — we can inject this by using the @Param decorator, supplying the value that has been prefixed with : in the path as defined in the @Get decorator. In this case, we need a number so we can use the ParseIntPipe decorator as the second argument to ensure that Nest validates the input and converts it to a number.Optionally, the user may also provide some query parameters — for example if they click the Previous or Next buttons in the UI. These can all be extracted from request.query object using the @Query decorator. We can also use the DefaultValuePipe to supply a default value if none has been supplied.The user may want to order the results, so we can expose an orderBy property - by default this should be the movie title - @Query(orderBy, new DefaultValuePipe(title))Rather than the UI deciding the skip parameter to pass through to the Cypher, we’ll allow the UI to send a page parameter which it can easily increment/decrement. If no value has been supplied this should default to the first page. We also want this to be parsed into a number — @Query(page, new DefaultValuePipe(1), ParseIntPipe)The user may want to change the limit to view more results in the UI: @Query(limit, new DefaultValuePipe(10), ParseIntPipe)Piecing these values together, we’ll get a route handler that looks something like this:// genre.controller.ts@UseGuards(JwtAuthGuard)@Get(/:id)async getGenre(    // Request object to get the User    @Request() request,     // Extract the ID    @Param(id, ParseIntPipe) id: number,  // Extract the ID    // Which property to order by    @Query(orderBy, new DefaultValuePipe(title)) orderBy: string,     // Page number defaulting to the first page    @Query(page, new DefaultValuePipe(1), ParseIntPipe) page: number,      // Total number of results to return, default 10    @Query(limit, new DefaultValuePipe(10), ParseIntPipe) limit: number    ) {      // ...  }To process this request, we’ll pass on the responsibility to the GenreService by creating agetMoviesForGenre method. Like the getGenresForUser method, well take the :User node as the first parameter and add the rest of the parameters.// genre.service.tsasync getMoviesForGenre(user: User, genreId: number, orderBy: string = title, limit: number = 10, page: number = 1) {    // ...}In this method, we’ll traverse the graph from the :User, through their :Subscriptions, to a :Package which provides access to the requested :Genre. The subscriptions should be filtered to only include nodes that expire on or after the current date and time.MATCH (u:User {id: $userId})-[:PURCHASED]->(s)-[:FOR_PACKAGE]->(p)-[:PROVIDES_ACCESS_TO]->(g:Genre {id: $genreId})WHERE s.expiresAt >= datetime()If the user doesn’t have a subscription to this package then no rows will be returned. For now that is fine because the request is for available videos in the genre, but in future we may want to return a HTTP 403 Forbidden response so that the UI can encourage the user to buy a new subscription.We should also add in a check on the User’s age — if they are under 18 years they shouldn’t be able to access any movie with the :Adult label applied to it.u.dateOfBirth <= datetime() - duration(P18Y) OR NOT m:AdultThen we can traverse the IN_GENRE relationship to the movie, then apply the ordering and pagination. Piecing this all together in the service will look like this:/// genre.service.tsasync getMoviesForGenre(user: User, genreId: number, orderBy: string = title, limit: number = 10, page: number = 1) {    const res = await this.neo4jService.read(`        MATCH (u:User {id: $userId})-[:PURCHASED]->(s)-[:FOR_PACKAGE]->(p)-[:PROVIDES_ACCESS_TO]->(g:Genre {id: $genreId})<-[:IN_GENRE]-(m)        WHERE s.expiresAt >= datetime() AND (u.dateOfBirth <= datetime() - duration(P18Y) OR NOT m:Adult)        RETURN m,            [ (m)-[:IN_GENRE]->(g) | g ] as genres,            [ (m)<-[:CAST_FOR]-(p) | p ][0..5] as cast        ORDER BY m.title ASC        SKIP $skip        LIMIT $limit    `, {        userId: (user.properties as Record<string, any>).id,        genreId: int(genreId),        skip: int( (page - 1) * limit),        limit: int(limit),    })    return res.records.map(row => ({        ...row.get(m).properties,        id: row.get(m).properties.id.toNumber(),        genres: row.get(genres),        cast: row.get(cast),    }))}In the RETURN portion of the statement I have added a couple of Pattern Comprehensions to retrieve some extra information about the movie.The response will look something like this:{   popularity : 6.183889,   original_language :  en ,   vote_count : 23,   average_vote : 6.7,   id : 31527,   release_date :  1934-05-09 ,   status :  Released ,   revenue : 0,   overview :  Young German princess Sophia is married off to Russias half-mad Grand Duke Peter in the hope of improving the royal blood line. ,   budget : 900000,   title :  The Scarlet Empress ,   imdb_id :  tt0025746 ,   original_title :  The Scarlet Empress ,   poster_path :  /xa4t3CU168cVaNYL2g2dRMtSMDH.jpg ,   runtime : 104,   cast : [    {       profile_path :  /dKLUrgJSkcq7iPGliG81xL8Fdrw.jpg ,       id : 133470,       gender : 0,       name :  John Lodge     },    {       profile_path :  /geGrMcqxcFtMKKJO36OFpVwZFFW.jpg ,       id : 8515,       gender : 1,       name :  Jane Darwell     },    {       name :  Erville Alderson ,       id : 589728,       gender : 2,       profile_path :  /yi3crBgbU5m4jfsH3H15YNNPrpg.jpg     },    {       profile_path :  /iBqiyZ7nXnnLfFgXtMfwwazBx8X.jpg ,       id : 1070625,       gender : 0,       name :  Olive Tell     },    {       name :  Ruthelma Stevens ,       id : 1082774,       gender : 0,       profile_path :  /nOKxAwLIxUNrU2DFxGeLaV2nyLd.jpg     }  ],   genres : [    {       id : 18,       name :  Drama     },    {       id : 10749,       name :  Romance     },    {       id : 36,       name :  History     }  ],}Again, to test this endpoint we can create another group in the end-to-end tests. When provided with a valid token, the API should return 10 movies:describe(GET /genres/:id, () => {    it(should return a list of genres in exchange for a valid token, () => {        return request(app.getHttpServer())            .get(`/genres/1`)            .set(Authorization, `Bearer ${token}`)            .expect(200)            .expect(res => {                expect(res.body.length).toEqual(10)            })    })})Then, if a limit is supplied (in this case 20), the API should return that number of results:it(should return a paginated list of genres in exchange for a valid token, () => {    const limit = 20    return request(app.getHttpServer())        .get(`/genres/$1?limit=${limit}`)        .set(Authorization, `Bearer ${token}`)        .expect(200)        .expect(res => {            expect(res.body.length).toEqual(limit)        })})You can make the tests more sophisticated than this, but for now it demonstrates the functionality works as intended.RecapIn this session we’ve covered how to run some basic authorisation using the structure of the graph. When a user registers, they will have access to everything as part of a 30 day trial. Once that trial expires, they will be required to purchase a subscription.We already have the methods available to create the new subscription in the SubscriptionService, all we need to do is create another route handler to process the request. The tests are also pretty basic at the moment, so it is worth spending some time testing for different scenarios, for example if a user tries to access a genre that their subscription doesnt provide access to.The API now also return data in Neo4j specific formats — for example datetime or duration which currently take a lot of repetitive code to convert. Well look at this in more detail in the next session.If you have any questions, comments or if you would like to see a feature added to the API, feel free to open a Github Issue.Join me Tuesdays at 12:00 UTC / 13:00BST / 14:00CEST / 15:30 IST on the Neo4j Twitch Channel for the next session.;Jul 24, 2020;[]
https://medium.com/neo4j/recapping-global-graph-celebration-day-2021-807c35303973;William LyonFollowApr 21, 2021·6 min readRecapping Global Graph Celebration Day 2021The History of Graphs, Interview with Community Members, & GraphQL Community Update — Plus Hackathon Announcement!Leonhard Euler portrait by Jakob Emanuel Handmann (1753)On April 15 every year, the global graph community gets together to celebrate the evolution of the graph ecosystem by marking the birthday of the creator of graph theory, Leonhard Euler. This year the community gathered virtually during a special extended live stream to mark the occasion.In this post, we’ll review what was covered during GGCD 2021 including a bit about the history of graphs, special interviews with community members from around the globe, an update on the Neo4j GraphQL Library, plus a special announcement (spoiler alert: a Neo4j GraphQL hackathon with $10k in prizes!).You can watch the recording of the GGCD 2021 live stream on the Neo4j YouTube channel.Leonhard Euler & the History of GraphsWe started off by reviewing the history of graphs and how the property graph model and graph databases like Neo4j have evolved from the original graph problem: the Seven Bridges of Königsberg.But first, who is Leonhard Euler? Euler was an extremely influential Swiss mathematician, physicist, astronomer, geographer, logician, and engineer (!) during the 18th century. He made many contributions to human knowledge and understanding including discovering graph theory.From a Graph of Bridges to the Property Graph ModelWhile walking around Königsberg in Prussia (now Kaliningrad, Russia) Euler wondered if it was possible to walk through the city, crossing each of seven bridges over the Pregel River throughout the city only once. Euler realized this problem required a new method of mathematical analysis, which became graph theory: solving problems using a graph data structure composed of nodes and relationships.Wikipeda Seven Bridges of Königsberg”From this foundation the property graph model and graph databases evolved when it became apparent that the graph data structure offered advantages for modeling, storing, and querying data.The property graph model: nodes, relationships, & properties. What Is A Graph Database?Lju and Alex shared a host of resources for getting started with graphs and Neo4j, including:The GGCD 2021 Quiz! Answers must be received by April 30th for a chance to win a special graph t-shirt.New to Neo4j? Learn the graph database basics in 2 minutes.Join the Neo4j Community on Discord and the Neo4j Community Forum.Spin up a Neo4j Sandbox for hands-on guided experience with Cypher and your choice of data sets.Watch videos on the Neo4j YouTube channel.Read Neo4j developer blog posts from the community.Getting involved: https://neo4j.com/developer/contribute/​APOC information: https://neo4j.com/labs/apoc/What is a Graph Database? http://r.neo4j.com/what-is-a-graphdbCommunity Member Interviews from Around the WorldNext, we were joined by three graph community members from throughout the world to share their stories of how they got started working with graphs and where they see the graph ecosystem going.Shilpa Karkeraa, Luanne Misquitta, & Mike Morley joined GGCD 2021 to share their graph stories with the world.Shilpa Karkeraa, CEO of Myraa TechnologiesShilpa joined us to share her experience with graphs and talk about two interesting projects that she’s been working on in very different domains: medical research and human resources. Shilpa has been translating resumes and job histories into a graph with the goal of helping everyone find interesting job opportunities. She gave a presentation about this project at NODES 2020 Extended. Another project she’s been working on is in the area of medical research, connecting research foundation, studies, and disease symptoms in a graph with the goal of making the outcomes of these studies more discoverable.Shilpa also talked about the impact she sees the global graph community making on the world and how the visual UX of graphs and how they are presented make them more exciting and engaging for users.Luanne Misquitta, VP of Engineering at GraphAwareLuanne has been a Neo4j user since before the 1.0 release, first using Neo4j to build a graph of user profiles. Luanne talked about the impact that the introduction of the Cypher query language has had and how the graph ecosystem has evolved. Luanne has been an active part of the graph community for over 10 years, building applications, training other developers, publishing content, and sharing her graph expertise through presentations at conferences, community events, and the Neo4j community online forums.More recently, at GraphAware, Luanne has been working on Hume to enable users to discover insights and analyze data using graphs and Neo4j. With tools like Hume that make the power of graphs accessible to broader audiences, Luanne sees even more opportunities for uncovering hidden connections in data.Mike Morley, President and Co-Founder of Menome Technologies Inc.Mike is a geological engineer and does a lot of work with environmental data. He first came to graphs when he realized that relational models are not a good fit for representing the physical world. The power of the property graph model, Cypher, and the spatial extensions for Neo4j enable Mike and his team to work with both structured and unstructured data, extracting knowledge graphs from the text of environmental assessment reports and geo-sciences data for numerical analysis, for example.As the co-founder of Menome Technologies and now at Arcurve, Mike is working to build a multi-agent system to decompose documents (such as environmental assessments and regulations) and build graphs from them for further analysis. You can find some interesting examples of these concepts and tools on the Menome GitHub page.GraphQL Community UpdateThe final section of Global Graph Celebration Day 2021 was a Neo4j GraphQL community update from the Neo4j GraphQL team. GraphQL is an API query language that models application data as a graph. At Neo4j, we think using a graph database is the perfect fit for the backend data layer of your GraphQL API, and want to make it as easy as possible to build GraphQL APIs backed by Neo4j.Learn more about GraphQL at GraphQL.orgThe Neo4j GraphQL team recently released a beta version of the Neo4j GraphQL Library. The goals of the Neo4j GraphQL Library are to enable developers to build Node.js GraphQL APIs backed by Neo4j with a focus on reducing boilerplate code and boosting developer productivity.Read more about the Neo4j GraphQL Library in this blog post.The Neo4j GraphQL Library allows developers to use GraphQL type definitions to define a Neo4j property graph model. These type definitions are then used to auto-generate a GraphQL API and essentially drive the Neo4j database model.Defining a property graph model using GraphQL type definitions.This means with just a few lines of code you can create a fully functional GraphQL API backed by a native graph database.Getting started with the Neo4j GraphQL Library.Read more about the Neo4j GraphQL Library in this blog post and in the documentation. The Neo4j GraphQL Library is available in beta currently but stayed tuned for the 1.0 GA release coming soon.Announcing the Leonhard Euler Idea ContestJoin the Neo4j GraphQL Leonhard Euler Idea Contest” hackathon with $10k in prizes!The exciting announcement at the end of Global Graph Celebration Day was announcing the Leonhard Euler Idea Contest” hackathon! This hackathon has total prizes worth $10,000 and will inspire developers to build applications using Neo4j & GraphQL. You can learn more and register for the hackathon on the hackathon’s Devpost page.Build with Neo4j and GraphQL at the Leonhard Euler Idea ContestSubmit Breakthrough Solutions to Win $10k Worth of Prizesmedium.comThanks to everyone who joined us live for Global Graph Celebration Day 2021! You don’t have to wait until next year’s Global Graph Celebration Day to be a part of the global graph community.;Apr 21, 2021;[]
https://medium.com/neo4j/week-11-importing-and-querying-kickstarter-projects-583b30f92e84;Michael HungerFollowOct 19, 2021·10 min readDiscover Aura Free Week 11 — Importing and Querying Kickstarter ProjectsThis week for our Discover Aura Free” series, we want to look at a Kickstarter dataset from Maven Analytics recommended by our colleague Jennifer Reif.If you want to code along, sign up — or in — to https://dev.neo4j.com/aura, create your free database, and join us.Neo4j Aura - Fully Managed Cloud SolutionFor small development projects, learning, experimentation and prototyping. Start Free Aura Professional For medium…neo4j.comBeing avid Kickstarter backers ourselves (especially for board games and gadgets), that dataset was very compelling to us.If you’d rather watch than read, here is the video of our live-stream:We looked at the Kickstarter site overall — and at a particular project — to relate the data in the dataset with the actual site.The dataset contains 375k Kickstarter projects from 2009 to 2017, including category, country, and pledge data.Some questions we can ask after importing the datasets:What does the (sub)-category tree look like visually in terms of success?What are the most successful categories or countries?What are highest pledges of successful projects?DatasetThe data is available as a zip download with 2 CSVs — one with the data dictionary, the other with 375k rows of Kickstarter projects.Columns:ID — Internal kickstarter idName — Name of the projectCategory — Project categorySubcategory — Project subcategoryCountry — Country the project is fromLaunched — Date the project was launchedDeadline — Deadline date for crowdfundingGoal — Amount of money the creator needs to complete the project (USD)Pledged — Amount of money pledged by the crowd (USD)Backers — Number of backers (integer)State — Current condition the project is in (as of 2018–01–02) (Successful, Canceled, Failed, Live, Suspended)To make it easier to import, I would usually upload the CSV file somewhere publicly to access it easily, like GitHub, Pastebin, or S3.This time, we wanted to demonstrate how you can use Google Spreadsheets for that.After uploading the 100k chunk of the file as a new sheet, you can choose File → Publish to the Web” to publish a single sheet, e.g. as a CSV publicly.Other tools and people can access just that data read-only, including our Neo4j instance.Google Spreadsheets didn’t like importing the full 400k rows, so we split the file using xsv split -c 10000 split kickstarter.csv into smaller chunks (csvkit works too, or just head -10000 in any unix shell).As the long URL is a bit unwieldy, we created a bit.ly shortlink for our needs: https://dev.neo4j.com/kickstarterData ModelThe data model has some interesting tidbits, especially around modeling the state.We could model the state just as a property like in the CSV, but as there are only a few relevant states and it’s an important attribute of a project to distinguish them, we can also add labels to a project for:SuccessfulFailedCanceledThat helps us to visualize the state of a project quickly and also sub-select the relevant projects easily.We extract the subcategory and category and the country as separate nodes.We could extract the pledge information into a separate node — e.g. for finance and security reasons — but we kept it in the project for simplicity.ImportAfter creating our blank AuraDB Free database, and opening Neo4j Browser we can get going.Exploring the CSVWe can look at the first few rows of the data:load csv with headers from  https://dev.neo4j.com/kickstarter  as rowreturn row limit 5Which gives us object/hash/dict/map representations of each row with the headers as key and the row data as values.All values are strings, so if we want to convert them we need to do that manually.{   Goal :  3854 ,   Category :  Film & Video ,   Subcategory :  Webseries ,   State :  Failed ,   Pledged :  426 ,   Deadline :  2016-08-12 ,   Country :  Canada ,   Backers :  13 ,   ID :  689678626 ,   Launched :  2016-07-22 22:53:49 ,   Name :  The Vanishing Garden }We also check how many rows our dataset contains.load csv with headers from  https://dev.neo4j.com/kickstarter  as rowreturn count(*)That returns 75k rows, which is a bit much for our AuraDB free instance (50k nodes, 175k rels).So we need to cut it down, but because we want to have projects of all kinds of status, we can sub-select a single year to use, and that gives us roughly 22k rows to work with.load csv with headers from  https://dev.neo4j.com/kickstarter  as rowwith row where row.Launched starts with 2016return count(*)SetupFirst, we create a bunch of constraints to ensure data uniqueness and speed up the import when looking up existing data:create constraint on (p:Project) assert p.id is uniquecreate constraint on (c:Category) assert c.name is uniquecreate constraint on (c:SubCategory) assert c.name is uniquecreate constraint on (c:Country) assert c.name is uniqueImporting ProjectsWe import the data incrementally, starting with the projects. We use MERGE here so that we can re-run the import without causing duplication.load csv with headers from  https://dev.neo4j.com/kickstarter  as rowwith row where row.Launched starts with 2016MERGE (p:Project {id:row.ID})ON CREATE SETp.name = row.Name,p.launched = date(substring(row.Launched,0,10))p.deadline = date(row.Deadline)p.state = row.StateWe’ll add the pledge information later.We see that it imported some 22k projects. By clicking on the (Project) pill in the sidebar we can quickly pull up a few projects in the visualization.Now we want to turn the state information into labels. For just the three labels, we can run a single update statement each that adds a label of the right kind to the project node.MATCH (p:Project) WHERE p.state = Successful SET p:SuccessfulMATCH (p:Project) WHERE p.state = Failed SET p:FailedMATCH (p:Project) WHERE p.state = Canceled SET p:CanceledSo if we now query projects from the sidebar we see their success even visually. In the stream, we discuss styling the colors by clicking on the pills on top of the visualization and then choosing a different color.Importing CategoriesImporting the categories involves creating nodes for them and then connecting the subcategory to the project and the subcategory to the category. As we use MERGE here, it also ensures each relationship is created only once.load csv with headers from  https://dev.neo4j.com/kickstarter  as rowwith row where row.Launched starts with 2016match (p:Project {id:row.ID})merge (sc:SubCategory {name:row.Subcategory})merge (p)-[:IN_CATEGORY]->(sc)merge (c:Category {name:row.Category})merge (sc)-[:IN_CATEGORY]->(c)return count(*)Please note that shared names of subcategories in this approach will be merged together into a single one.If you don’t want this, you have to create the subcategories in the context of a category, as shown below.load csv with headers from  https://dev.neo4j.com/kickstarter  as rowwith row where row.Launched starts with 2016match (p:Project {id:row.ID})merge (c:Category {name:row.Category})merge (sc:SubCategory {name:row.Subcategory})-[:IN_CATEGORY]->(c)merge (p)-[:IN_CATEGORY]->(sc)return count(*)We can now run a bunch of queries to see which categories exist, how many there are, and to get a visual.Querying CategoriesMATCH (n:Category) RETURN n LIMIT 25MATCH (n:Category) RETURN count(*)MATCH (n:SubCategory) RETURN count(*)MATCH path = (c:Category)<-[:IN_CATEGORY]-(n:SubCategory) RETURN pathImporting Countries:Importing countries is pretty straightforward. Just create the node for a country if it doesn’t exist and connect it to the project.load csv with headers from  https://dev.neo4j.com/kickstarter  as rowwith row where row.Launched starts with 2016match (p:Project {id:row.ID})merge (c:Country {name:row.Country})merge (p)-[:IN_COUNTRY]->(c)return count(*)Importing Pledges and BackersOriginally we had discussed extracting the backing information into separate nodes, but given the questions and the time we had we left that for a future exercise.So we just find our projects again based on their id and set the few extra properties — remember to convert them to numeric values as needed.load csv with headers from  https://dev.neo4j.com/kickstarter  as rowwith row where row.Launched starts with 2016match (p:Project {id:row.ID})set p.pledge = toInteger(row.Pledged)set p.goal = toInteger(row.Goal)set p.backers = toInteger(row.Backers)return count(*)If we want to explore our imported data model visually, we can use apoc.meta.graph procedure.Which shows both the base data model but also our extra labels.call apoc.meta.graph()QueryingNow with the data in our graph we can start visualizing and querying it and answer our question.Most Backers of a Failed ProjectMATCH (p:Project:Failed)RETURN p.name, p.backersORDER BY p.backers DESC LIMIT 20Sad for the wolves :(╒═════════════════════════════════════════════════╤═══════════╕│ p.name                                          │ p.backers │╞═════════════════════════════════════════════════╪═══════════╡│ Save The Wolves!                                │3867       │├─────────────────────────────────────────────────┼───────────┤│ Voyage of Fortunes Star — a 7th Sea cRPG       │1287       │├─────────────────────────────────────────────────┼───────────┤│ K11 Bumper | iPhone 7 and 7 Plus                │1032       │├─────────────────────────────────────────────────┼───────────┤│ {THE AND} Global Relationship Project           │830        │├─────────────────────────────────────────────────┼───────────┤│ New55 COLOR 4x5 Peelapart Film                  │805        │├─────────────────────────────────────────────────┼───────────┤│ Russian Subway Dogs                             │778        │├─────────────────────────────────────────────────┼───────────┤│ Legrand Legacy – An Homage to the JRPG Classics │769        ││(PC)                                             │           │├─────────────────────────────────────────────────┼───────────┤│ Float The Boat - Save Mayflower II!             │760        │├─────────────────────────────────────────────────┼───────────┤│ Dallas Sucks: A Completely Biased Documentary   │737        │     │...Highest Average Pledge for Successful ProjectsMATCH (n:Project:Successful) where n.backers > 0RETURN n.name, toFloat(n.pledge) / n.backers as pledgeValueORDER BY pledgeValue desc limit 10Professional chefs cooking at your home, e-bikes, art-guitars, 3d-printers… all expected but still pretty high pledges.I think my highest was $750 for a laser cutter :)╒══════════════════════════════════════════╤═════════════╕│ n.name                                   │ pledgeValue │╞══════════════════════════════════════════╪═════════════╡│ Cooks at your home                       │6862         │├──────────────────────────────────────────┼─────────────┤│ Project ArtGuitar® features Palehorse    │3900         │├──────────────────────────────────────────┼─────────────┤│ Gocycle - the BEST folding electric bike │2509         ││in the world!                             │             │├──────────────────────────────────────────┼─────────────┤│ Futuristic Mechanical Tourbillon Watch Ma│2300         ││de Into Reality                           │             │├──────────────────────────────────────────┼─────────────┤│ Tantrum Cycles, the Missing Link in full │2079         ││suspension bikes                          │             │├──────────────────────────────────────────┼─────────────┤│ New Carbon SUV e-bike                    │2023         │├──────────────────────────────────────────┼─────────────┤│ RoVa4D Full Color Blender 3D Printer     │1998         │CategoriesVisual Exploration of Category Tree and ProjectsMATCH path = (c:Category)<-[:IN_CATEGORY]-(n:SubCategory)RETURN pathWe can show the category tree and expand a few subcategories to see their projects (and their state) visually in one picture.Top Categories and their StateMATCH (c:Category)<-[:IN_CATEGORY]-(n:SubCategory)                  <-[:IN_CATEGORY]-(p:Project)RETURN c.name, p.state, count(*)ORDER BY count(*) desc LIMIT 50Top Categories in Numbers of ProjectsMATCH (c:Category)<-[:IN_CATEGORY]-(n:SubCategory)                  <-[:IN_CATEGORY]-(p:Project)RETURN c.name, count(*)ORDER BY count(*) desc LIMIT 10Kinda expected. Let’s look at the successful ones.╒══════════════╤══════════╕│ c.name       │ count(*) │╞══════════════╪══════════╡│ Technology   │2959      │├──────────────┼──────────┤│ Games        │2889      │├──────────────┼──────────┤│ Film & Video │2673      │├──────────────┼──────────┤│ Design       │2592      │├──────────────┼──────────┤│ Publishing   │2353      │├──────────────┼──────────┤│ Music        │2175      │├──────────────┼──────────┤│ Fashion      │1630      │├──────────────┼──────────┤│ Food         │1416      │├──────────────┼──────────┤│ Art          │1355      │├──────────────┼──────────┤│ Comics       │802       │└──────────────┴──────────┘Top Categories in Numbers of Successful ProjectsMATCH (c:Category)<-[:IN_CATEGORY]-(n:SubCategory)<-[:IN_CATEGORY]-(p:Project:Successful)RETURN c.name, count(*)ORDER BY count(*) desc LIMIT 10Not that many in tech or art, but many comics and design projects, and music also has an almost 50% success rate.╒══════════════╤══════════╕│ c.name       │ count(*) │╞══════════════╪══════════╡│ Games        │1108      │├──────────────┼──────────┤│ Design       │1002      │├──────────────┼──────────┤│ Music        │969       │├──────────────┼──────────┤│ Film & Video │939       │├──────────────┼──────────┤│ Publishing   │859       │├──────────────┼──────────┤│ Technology   │599       │├──────────────┼──────────┤│ Art          │509       │├──────────────┼──────────┤│ Comics       │465       │├──────────────┼──────────┤│ Fashion      │426       │├──────────────┼──────────┤│ Food         │346       │└──────────────┴──────────┘Success Rate of CategoriesMATCH (c:Category)<-[:IN_CATEGORY]-(n:SubCategory)<-[:IN_CATEGORY]-(p:Project)WITH  c.name as category, count(*) as total,sum(case when p:Failed then 1 else 0 end) as failed,sum(case when p:Successful then 1 else 0 end) as successRETURN category, total, toFloat(failed)/total as fP, toFloat(success)/total as sPorder by sP descWe can calculate the totals and partials of failed and successful projects across the category tree and then sort by success rate.Of course, this is skewed towards categories with fewer projects, as there is less competition and a few more successful projects have oversized impact.╒══════════════╤═══════╤═══════════════════╤═══════════════════╕│ category     │ total │ fP                │ sP                │╞══════════════╪═══════╪═══════════════════╪═══════════════════╡│ Comics       │802    │0.3329177057356609 │0.5798004987531172 │├──────────────┼───────┼───────────────────┼───────────────────┤│ Dance        │195    │0.40512820512820513│0.5282051282051282 │├──────────────┼───────┼───────────────────┼───────────────────┤│ Theater      │462    │0.42207792207792205│0.5021645021645021 │├──────────────┼───────┼───────────────────┼───────────────────┤│ Music        │2175   │0.4606896551724138 │0.44551724137931037│├──────────────┼───────┼───────────────────┼───────────────────┤│ Design       │2592   │0.44753086419753085│0.38657407407407407│├──────────────┼───────┼───────────────────┼───────────────────┤│ Games        │2889   │0.42990654205607476│0.38352371062651436│├──────────────┼───────┼───────────────────┼───────────────────┤│ Art          │1355   │0.5328413284132841 │0.3756457564575646 │├──────────────┼───────┼───────────────────┼───────────────────┤│ Publishing   │2353   │0.5316617084572886 │0.36506587335316615│├──────────────┼───────┼───────────────────┼───────────────────┤│ Photography  │529    │0.5085066162570888 │0.3648393194706994 │...Querying CountriesCountries by ProjectsMATCH (n:Country)RETURN n.name,size((n)<-[:IN_COUNTRY]-()) AS projectsORDER BY projects DESCSurprised to see Canada and Australia so high up there, compared to their population, but the U.S. is leading by an order of magnitude.╒════════════════╤══════════╕│ n.name         │ projects │╞════════════════╪══════════╡│ United States  │14836     │├────────────────┼──────────┤│ United Kingdom │2181      │├────────────────┼──────────┤│ Canada         │1217      │├────────────────┼──────────┤│ Germany        │652       │├────────────────┼──────────┤│ Australia      │647       │├────────────────┼──────────┤│ Italy          │454       │├────────────────┼──────────┤│ France         │436       │├────────────────┼──────────┤│ Spain          │355       │├────────────────┼──────────┤│ Mexico         │290       │├────────────────┼──────────┤│ Netherlands    │240       │├────────────────┼──────────┤│ Sweden         │199       │Country Success RateMATCH (c:Country)<-[:IN_COUNTRY]-(p:Project)WITH  c.name as category, count(*) as total,sum(case when p:Failed then 1 else 0 end) as failed,sum(case when p:Successful then 1 else 0 end) as successRETURN category, total, toFloat(failed)/total as fP,       toFloat(success)/total as sPORDER BY sP DESCBut if you actually want to be successful, being in New Zealand, Singapore, Ireland, or Sweden helps :)╒════════════════╤═══════╤═══════════════════╤═══════════════════╕│ category       │ total │ fP                │ sP                │╞════════════════╪═══════╪═══════════════════╪═══════════════════╡│ United Kingdom │2181   │0.4667583677212288 │0.41127922971114167│├────────────────┼───────┼───────────────────┼───────────────────┤│ New Zealand    │128    │0.453125           │0.3984375          │├────────────────┼───────┼───────────────────┼───────────────────┤│ Singapore      │140    │0.5214285714285715 │0.37857142857142856│├────────────────┼───────┼───────────────────┼───────────────────┤│ Ireland        │95     │0.5263157894736842 │0.3684210526315789 │├────────────────┼───────┼───────────────────┼───────────────────┤│ Sweden         │199    │0.46733668341708545│0.36180904522613067│├────────────────┼───────┼───────────────────┼───────────────────┤│ United States  │14836  │0.5184685899164195 │0.35541925047182527│ConclusionWith the data in the AuraDB Free database you can now do a lot of things.Continue to explore and analyze the full datasetAdd more data from other sources, e.g. the Kickstarter API to create a fuller knowledge graphBuilt a Kickstarter-like app, or an analytics dashboard, e.g. using GraphQL.Have fun and happy coding! You can find the data and details from the past few weeks at the github repository for the stream.GitHub - neo4j-examples/discoveraurafree: A repo with cypher and examples from the Discover Neo4j…More drama than a cat video! (© Mike on Twitch chat) A repo with cypher and examples from the Discover Neo4j Aura Free…github.com;Oct 19, 2021;[]
https://medium.com/neo4j/getting-started-with-neo4j-making-a-follow-system-6530ee435392;Pedro MendonçaFollowDec 3, 2018·7 min readGetting started with Neo4j — Building a follow system.I often find myself trying telling people about how wonderful it is to use Graph databases such as Neo4j for projects. However, it is hard to show a new concept without a good example. So here is my attempt to convince you to try it out.A follow systemFor this article, I will run through how to make a follow system for a social network in Neo4j. If you are a Medium user it will work something like that. Any user can follow any other user. All users will then get recommended posts based on who he or she follows.Photo by NASA on UnsplashDatabase designHere is the first place where Neo4j really shines. If you ever worked with traditional relational databases you are probably thinking of normalisation, primary and foreign keys. Neo4j takes all that away and focuses on what they like to call the Whiteboard Model. I like to call it no database design. But for the sake of completeness here is what our database should look like by the end of this.Whiteboard ModelUsers and posts will be Nodes, and they will be related to each other with Created by, Liked or Follow Relationships.Creating UsersFor users, we are going to have a very simple structure. Let’s say every user has a name and an email, that should be enough for now at least.We also want to make sure that no two users have the same email. Almost like a primary key. Neo4j provides us with the concept of constraints so we can do the following.Cypher’s syntax for constraints is verbose, but if you are not familiar this basically states that Neo4j will not allow two entities of the type USER to have the same email property. In other words that all emails are unique.Now, all we have left to do is to create our first user.If you are using the Neo4j web browser to run this query you should have gotten your first node. And should look something like this.Our first nodeAs we can see from our previous query we created a node and assign it to a variable john That variable serves as a variable for the query scope. This is only useful because we want to be able to return it later so we can actually see what we have created.If you are implementing this in an API you can skip the return step but it may be useful depending on what your end goal is so I will be including it for the rest of this article but feel free to remove it.Because we set our constraints previously you can run the query one more time and you will be able to see that it will fail as Neo4j will not allow us to have two users with the same email.To finish this let’s just create one more user and move on.Creating a postA post in our little social network will only have two properties and they will look like this:Although posts don’t have the same constraints that users do we are going to be creating the posts in a slightly different manner. Every post is created by a user so to avoid problems our best bet is to create all posts in association to the user from the start.If your social network allows for anonymous posting then you can create a post just like we created our users in the previous step.Ok, this is a slightly more complex one so let me explain. This query has two parts:MATCH — We find the user which is creating this post in our case John”. We do this because we don’t want a post to be created if there isn’t a corresponding user in the database.CREATE — We create r a relationship of type CREATED_BY and also a post with the appropriate properties, very similarly to how we created our initial user.If you did this right we should now have something like so:Following a userNow to create a follow we can do something similar to what we did to create a post but this time we only need to create the relationship, as both users already exist.Again we are doing a match and a create, this way we can make sure that the users exist before creating the follow relationship.Liking a postFor a user to like a post, we can reuse the majority of the code from the follow query. However, there is one key difference. Because posts don’t have a unique identifier such as email we will be using Neo4j’s automatically created id.If you have just started using Neo4j you may have missed the fact that it actually assigns an Id to all its nodes, one easy way to check this is with the following query which will return all post titles and its respective IDs.This will return a table like this:In Neo4j’s query language(cypher) id(n) works somewhat like a function which may not be very intuitive but I will eventually get on how we can make this more straightforward when querying for posts.Now knowing that our post has ID = 20 we can do the following.With that, we finish our initial database design without the need of relationship tables, normalisation or foreign keys. I think for that reason alone you have to consider Neo4j instead of a traditional relational database for any kind of related data.To check out our progress so far we can perform the following query to return all our nodes and relationships:This will return all the nodes and relationships in the database. Consider using the LIMIT keyword if you have a database with more than just a few nodes.One last stepThis whole process would have been a bit useless if it wasn’t for the fact that we can query this data and get relevant information. I won’t go in much detail of how these queries are working but I will try to explain what they do.I will also be using a feature called mapping in Neo4j to convert all the output into JSON to make it more friendly to APIs.Get all usersNote that I am specifying which parameters to return (name and email) this may not sound useful now but is very good practice in the case that your user node contains private properties such as password hashes.Get a user with their postsHere we are using the collect function in order to create an array of related content. This is often done in SQL with a JOIN, which results in fairly complex and lengthy queries as well as redundant results. Neo4js mapping allows us to define this in a much more readable syntax.Get recommended postsThis is probably one of my favourite queries in this post. It will find recommendations given a user’s email. To explain the query on its own it returns all the posts(including Neo4j’s ID) related in any way to followed users. This means that not only will you get posts written by the people you follow but also posts liked by the people you follow. And because we don’t have to pre-define our relationship types in Neo4j that means if we eventually implement more relationships such as the concept of a Commented on or Repost the recommendations will take those in consideration as well.Note that we are also adding the id to the return object, allowing us to use that ID to perform things like adding likes to the post later on.ConclusionI hope that this example gives you a taste of how Neo4j works and how straight forwards it is to create very common data structure patterns. Neo4j’s graph nature works really well for a follow system, and most social network type structures. So if you have the opportunity do give this a try and see what kinds of stuff you can come up with.;Dec 3, 2018;[]
https://medium.com/neo4j/so-long-summer-of-nodes-2020-832f259baac6;Ljubica LazarevicFollowSep 29, 2020·3 min readEthan Robertson on UnsplashSo long, Summer of Nodes 2020See you soon, Summer of Nodes 2021!Hello everybody!(for those of you who are wondering what Summer of Nodes is — keep reading, you can find out more below!)We hope you had as much fun over these past weeks of Summer of Nodes as we had. We really appreciate you joining us this summer, and hope you have picked up a new trick or two along the way :) Thank you for your brilliant answers we really enjoyed seeing what you came up with, and were humbled by the positive feedback received. We were impressed with how many of you stayed with us from start to finish, having a go at all of the challenges.Reliving the spirit of Summer of Nodes 2020The goal of Summer of Nodes was to provide a month of fun challenges, accessible to the complete beginner through to the experienced Neo4j practitioner. Regardless of your level, the goal was for you to walk away with some new skills, or try your hand at something you’ve not looked at before.Perhaps you missed the event? Or maybe you just want to revisit the event, at your own pace, at leisure. Either way, here’s a reminder of those challenges.The barbecueWeek one was all about learning about modelling, focussing on barbecues around the world. For the more experienced graphistas, we challenged you to use your Neo4j toolkit to come up with queries that would help you plan for a socially-distanced seating plan.The online day outWeek two used the superb collections dataset from The Metropolitan Museum of Art in New York. We invited you to explore paintings by setting up and using Neo4j Bloom. For the more experienced, your mission was to plan for a virtual museum visit, by identifying which paintings met the criteria of your friends requirements.Whodunit?Week three was based on the excellent Knight Labs ‘Murder mystery in SQL City’. Your mission was to identify the murder and accomplice using your strengthening Cypher skills. The experienced were set the challenge to use Graph Data Science to identify other potential witnesses with connections to these culprits.Exploring the areaWeek four found us exploring Central Park in New York City. With your continuing growing Cypher knowledge, we introduced you to the spatial functionality, where you explored different points of interest within the park, as well as how far apart they were. The experienced graphistas were set the ‘fountain flight’ challenge — what was the distance of the most efficient route through to park to visit every fountain, utilising APOC and Graph Data Science.All the relevant links to data, the live streams and other resources are in the blog posts above. For those of you who’d prefer a stream binge, check out the Summer of Nodes playlist.We need YOU!We did it by Natalie Pedigo on UnsplashDid you hear about the event? Perhaps you took part? Were you an sofa supervisor? Or maybe you were chipping in with the chat? Regardless of whether you submitted an answer or not, we’d love to hear your feedback:What went well?What didn’t go so well?What would you like to see next year?Please let us know your thoughts, we really appreciate it!Thank you for taking part, and see you in next year’s Summer of Nodes!;Sep 29, 2020;[]
https://medium.com/neo4j/announcing-nodes-2022-submit-your-talk-bd54dc00503a;Yolande PoirierFollowAug 9, 2022·2 min readAnnouncing NODES 2022!NODES 2022 is a free, two-day virtual conference of highly technical presentations by developers and data scientists solving problems with graphs.When: November 16-17, 2022 - Save your seat!Last year, the NODES 2021 was buzzing with thousands of graph users who attended 85 sessions by 75 speakers from 30 different countries.This year’s fourth edition is bigger than ever:We expanded the conference with additional community speakers presenting fascinating use cases over two days.The conference experience will span time zones across the world to accommodate our global community. We encourage you to submit a talk regardless of your time zone.You’ll take a deep dive into graphy solutions with two-hour-long workshops.Speakers will present their sessions live and interact with you directly during the Q&A.Submitting Your TalksThe call for papers (CFP) is open now through August 31, 2022.Your presentations should be educational, demonstrating graph and Neo4j-related technologies, and focus on how” to provide a solution with code, data models, Cypher, or best practices.Topics can range from graph-powered machine learning or knowledge graphs to algorithms, application development, visualizations, or performance.There are three types of sessions:30-minute talks, with additional time for Q&A10-minute lightning talks2-hour hands-on workshopsDiscover Talks From Previous NODESYou can get an idea of what to expect at the conference from our collection of talks from NODES 2019 to NODES 2021. Check out the schedules and playlists below:NODES 2019 Schedule and PlaylistNODES 2020 Schedule and PlaylistNODES 2020 Extended PlaylistNODES 2021 Schedule and PlaylistRefine Your PresentationsExplore the postings from users and community members on the forums. Can you answer a question or contribute to a discussion on a community forum? Can you share best practices for something you are currently working on?You could also ask the community for feedback and suggestions on this forum thread. You might find someone who wants to collaborate on a topic.See you there!Submit your talks and share your stories at NODES 2022.;Aug 9, 2022;[]
https://medium.com/neo4j/how-to-automate-neo4j-deploys-on-azure-d1eaeb15b70a;David AllenFollowFeb 25, 2019·7 min readHow to Automate Neo4j Deploys on AzureNeo4j already provides some documentation on its site for how to do deployments of Neo4j to common clouds, including on Azure. But in this article, I’ll provide sample shell scripts that can do this automatically for you.These are useful when you want to integrate Neo4j into your CI/CD pipeline and be able to create/destroy instances temporarily, and also just to spin up a sample instance. But really, if you can automate Neo4j deployment, then any other piece of software can make a Neo4j instance whenever it needs, which is extremely handy.If you have any feedback or questions on these approaches, drop by the Neo4j Community site on this thread and share!Neo4j on Azure using Azure Resource Manager (ARM) templatesRequirementsBefore we begin, you’ll need the az command line interface program, which you can download and install with directions here. The azCLI is the main way you can automate all things with Azure. You’ll also need the jq tool for working with JSON responses, and other common linux utilities.It will also be necessary to authenticate your az CLI, to make sure it can interact with your resource groups, and that it is configured to use the right subscription by default. You can change the subscription the tool uses by consulting the documentation.Azure Resource ManagerNeo4j provides Azure Resource Manager (ARM) templates for Neo4j Causal Cluster (highly available clusters), and VM images for Neo4j Enterprise stand-alone. So first thing’s first, pick which one you would like to deploy. We’ll cover both in this article.ARM templates are really just a recipe that tells Azure how to deploy a whole set of interrelated resources. By deploying all of this as a stack we can keep all of our resources together, and delete just one thing when we’re done. In this method, we will be creating a new resource group for everything that we need for our Neo4j setup, so that we can manage the entire instance by just the resource group.ApproachHow to deploy a cluster is very simple: we just submit a new ARM Deployment job, pointing to the right template URL and providing some parameters in a JSON file. To keep things simple and contained, we’ll create an Azure resource group for our deploy to group everything we need.We’ll need to specify several common parameters, which you’ll see in the scripts below. Here’s an explanation of what they are.VM Size: This is the Azure VM type you want to launch, which controls how much hardware you’ll be using.Disk Size/Type: you can use these parameters to control whether Neo4j uses standard spinning magnetic platters or SSD disks as well as how many GB of storage you want to allocate.Location: Where in the world you want to deploy Neo4j. The template supports any Azure location, but in our example we’ll use East US.Authentication details: specifically, the administrative username and password for access to the VMs. It’s possible to set up the administrative login via either SSH key or simple username/password. We’ll do it with a password because there are no extra pre-requisites to set up.You can also choose other considerations like static/dynamic IP addresses, and naming in the script.Let’s get started. Simply run any of these scripts, and it will result in a running Neo4j instance you can use immediately!Deploying Neo4j Enterprise Causal ClusterBecause this is a clustered deploy, take note that we’re also using a parameter for Cores” and Read Replicas” to control how many nodes are in our cluster. This is in addition to the parameters we described above.The way ARM works, we’ll create a simple JSON file with all of the parameters we want submitted to the deployment, then create the deployment pointing to that JSON file. The ARM template is like a small function: it takes some inputs we’ll provide here, and produces a set of infrastructure as an output.#!/bin/bashexport CORE_NODES=3export READ_REPLICAS=0export NEO4J_PASSWORD=s00pers3cR3T:export ADMIN_AUTH_TYPE=passwordexport USERNAME=graph-hackerexport ADMIN_PASSWORD=s00pers3cR3T:export VM_SIZE=Standard_B2msexport DISK_TYPE=StandardSSD_LRSexport DISK_SIZE=256export IP_ALLOCATION=Dynamicexport SEED=$(head -c 3 /dev/urandom | base64 | sed s/[^a-zA-Z0-9]/X/g)export RESOURCE_GROUP= neo4j-RG-${SEED} export CLUSTERNAME= neo4j-${SEED} export DEPLOYMENT=neo4j-bmdeployexport LOCATION= East US # The ARM template to deployexport TEMPLATE_BASE=http://neo4j-arm.s3.amazonaws.com/3.5.5/causal-cluster/export TEMPLATE_URL=${TEMPLATE_BASE}mainTemplate.jsonecho $(cat <<JSON{ ClusterName : {  value :  ${CLUSTERNAME}  }, CoreNodes : {  value : ${CORE_NODES} }, ReadReplicas : {  value : ${READ_REPLICAS} }, VmSize : {  value :  ${VM_SIZE}  }, DataDiskType : {  value :  ${DISK_TYPE}  }, DataDiskSizeGB : {  value : ${DISK_SIZE} }, AdminUserName : {  value :  ${USERNAME}  }, AdminAuthType : {  value :  ${ADMIN_AUTH_TYPE}  }, AdminCredential : {  value :  ${ADMIN_PASSWORD}  }, PublicIPAllocationMethod : {  value :  ${IP_ALLOCATION}  }, Neo4jPassword : {  value :  ${NEO4J_PASSWORD}  }, _artifactsLocation : {  value :  ${TEMPLATE_BASE}  }}JSON) >  ${RESOURCE_GROUP}.json echo  Creating resource group named ${RESOURCE_GROUP} if ! az group create --name  ${RESOURCE_GROUP}  --location  ${LOCATION}  then   echo STACK_NAME=$RESOURCE_GROUP   echo  Failed to create necessary resource group ${RESOURCE_GROUP}    exit 1fiecho  Creating deployment az group deployment create \  --template-uri  $TEMPLATE_URL  \  --parameters @./${RESOURCE_GROUP}.json \  --resource-group  ${RESOURCE_GROUP}  \  --name  ${DEPLOYMENT} if [ $? -ne 0 ]  then  echo STACK_NAME=$RESOURCE_GROUP  echo  Stack deploy failed   exit 1fi# JSON Path to server response where IP address is.ADDR_FIELD= .[].virtualMachine.network.publicIpAddresses[0].ipAddress IP_ADDRESS=$(az vm list-ip-addresses --resource-group  ${RESOURCE_GROUP}  | jq -r  $ADDR_FIELD  | head -n 1)echo STACK_NAME=$RESOURCE_GROUPecho NEO4J_URI=bolt+routing://$IP_ADDRESS:7687At the end of this, a new resource group is created with all of the assets inside of it, and you get a URI of a bolt endpoint you can use. Alternatively, you could go to https://$IP_ADDRESS:7473/ to access Neo4j Browser for your new clustered instance.Example of a deployed resource groupDeploying Neo4j Enterprise Stand AloneThis will create a single instance of Neo4j without high-availability failover capabilities, but it’s a very fast way to get started. For this deploy, we don’t use ARM but just create a simple VM and configure its firewall/security rules.Neo4j provides the VM through an Azure marketplace offer. To refer to the right VM image, you need to know the publisher (that’s Neo4j), the offer” (Neo4j 3.5 series) and the SKU (which is the particular version of Neo4j you’ll use).Because we’re not using ARM for this one, this also provides an example of polling and waiting until the VM service comes up, and then changing the Neo4j default password when it does. At the top, you can choose a different password for the neo4j user as for the system administrator.Make sure to customize the SUBSCRIPTION variable to make this work.#!/bin/bashexport LOCATION=eastusexport SUBSCRIPTION=My-Subscription-Nameexport RG=neo4j-standalone-RGexport NAME=neo4j-standaloneexport ADMIN_USERNAME=graph-hackerexport ADMIN_PASSWORD=ch00se:A@PASSw0rdexport NEO4J_PASSWORD=ch00se:A@PASSw0rdexport NETWORK_SECURITY_GROUP=neo4j-nsg# Options: https://azure.microsoft.com/en-us/pricing/details/virtual-machines/export VM_SIZE=Standard_D2_v3# Can change this to static if desiredexport ADDRESS_ALLOCATION=dynamic# Configuration bits of what youre launching# Publisher:Offer:Sku:Versionexport PUBLISHER=neo4jexport OFFER=neo4j-enterprise-3_5export SKU=neo4j_3_5_5_apocexport VERSION=latestexport IMAGE=$PUBLISHER:$OFFER:$SKU:$VERSIONecho  Creating resource group named $RG az group create --location $LOCATION \   --name $RG \   --subscription $SUBSCRIPTIONecho  Creating Network Security Group named $NETWORK_SECURITY_GROUP az network nsg create \   --resource-group $RG \   --location $LOCATION \   --name $NETWORK_SECURITY_GROUPecho  Assigning NSG rules to allow inbound traffic on Neo4j ports... prio=1000for port in 7473 7474 7687 do  az network nsg rule create \    --resource-group $RG \    --nsg-name  $NETWORK_SECURITY_GROUP  \    --name neo4j-allow-$port \    --protocol tcp \    --priority $prio \    --destination-port-range $port  prio=$(($prio+1))doneecho  Creating Neo4j VM named $NAME az vm create --name $NAME \  --resource-group $RG \  --image $IMAGE \  --vnet-name $NAME-vnet \  --subnet $NAME-subnet \  --admin-username  $ADMIN_USERNAME  \  --admin-password  $ADMIN_PASSWORD  \  --public-ip-address-allocation $ADDRESS_ALLOCATION \  --size $VM_SIZEif [ $? -ne 0 ]  then  echo  VM creation failed   exit 1fiecho  Updating NIC to have our NSG # Uses default assigned NIC nameaz network nic update \  --resource-group  $RG  \  --name  ${NAME}VMNic  \  --network-security-group  $NETWORK_SECURITY_GROUP # Get the IP address of our instanceIP_ADDRESS=$(az vm list-ip-addresses -g  $RG  -n  $NAME  | jq -r .[0].virtualMachine.network.publicIpAddresses[0].ipAddress)export NEO4J_URI=bolt://$IP_ADDRESS# Change passwordecho  Checking if Neo4j is up and changing password.... while true do   if curl -s -I http://$IP_ADDRESS:7474 | grep  200 OK  then     echo  Neo4j is up changing default password  2>&1     curl -v -H  Content-Type: application/json  \       -XPOST -d { password : $NEO4J_PASSWORD } \       -u neo4j:neo4j \       http://$IP_ADDRESS:7474/user/neo4j/password 2>&1     echo  Password reset, signaling success  2>&1     break   fi   echo  Waiting for neo4j to come up  2>&1   sleep 1doneecho NEO4J_URI=$NEO4J_URIexit 0CleanupTo clean up and delete these deployment when you’re done, simply delete the entire resource group. Here’s a simple shell script which will get the job done.In both cases (cluster and standalone) this works nicely because we put all resources together into a single resource group. So by dropping the group, there’s no individual resource cleanup, the entire setup goes away.#!/bin/bashif [ -z $1 ]  then  echo  Usage: call me with deployment name   exit 1fiSTACK_NAME=$1if [ -f  $STACK_NAME.json  ]  then   rm -f  $STACK_NAME.json fiaz group delete -n  $STACK_NAME  --no-wait --yesexit $?This will schedule the entire group to be deleted and the script will exit immediately. Actually deleting all the resources may take some minutes, and if you log into your Azure console you’ll see the progress in the notifications area.;Feb 25, 2019;[]
https://medium.com/neo4j/neo4j-etl-tool-1-3-1-release-white-winter-2fc3c794d6a5;Michael HungerFollowJan 21, 2019·4 min readNeo4j ETL Tool 1.3.1 Release: White WinterThere is good news for all you folks importing data from relational databases into Neo4j. We’ve just released a new version of the Neo4j ETL Tool UI and command-line tool.The new version 1.3.1 should auto-update in Neo4j Desktop and also show a change log.From this version on, you no longer need an activation key to use the Neo4j ETL Tool in Neo4j Desktop, just install it by specifying: https://r.neo4j.com/neo4j-etl-app in the Neo4j Desktop Graph Application install box.The Neo4j ETL Tool UI was originally built very quickly for the launch as a Neo4j Desktop Graph App at GraphConnect 2017 based on parts of the 2017 Neo4j Browser code base.Based on our user feedback, the existing UI was sometimes confusing or not intuitive to use.In order to address this issue, we have implemented a complete revamp of the UI. It should now be very straightforward to use and much cleaner and more responsive. If you have any further suggestions for UI improvements, please let us know, they are now much easier to implement.Our good friends at The SilverLogic helped us to upgrade the code to React and design a new style and flow.Here’s a quick overview of the changes.After you install an open the ETL app and select your desired Neo4j Desktop project, you will see a list of your existing graphs and their status, along with any relational databases you may have added in the past.The first screen now displays relational and graph databases side by side so you see both the source and destination for your imports at the same time. You can now also edit the relational database connection in this screen. After selecting both, you can start the extracting the domain-mapping. Log files are readily available for your viewing.First step: Select source and target and then start mappingExample: Select, add and edit source and target databaseExample: Edit relational database connection detailsSecond step: Visualize and edit metadataAfter selecting your source and destination databases, you will transition to exploring and configuring your metadata. This second step has been updated to display a color-coded, editable selection of node labels and relationship types side by side. You can now also search for entities which is helpful for larger schemas. It is visualized as a graph on the right which reflects your changes immediately and whose elements you can double-click to bring up the property editor.The editing pop-up for properties is clearer and much more useable.After editing and saving the schema metadata here, you can advance to the last step, the import, or go back to the beginning and change your source or target.Example: Show graph view of meta modelExample: Edit node and relationship attributesThird step: Import the dataIn the third step, you start the data import either offline or online, depending on the state of your graph database. The tool will export the necessary CSV from the relational database and import it into the selected Neo4j database.Example: Import the dataBug fixesA number of issues were fixed.Support for new auth approach in MySQL 8.0.4+, upgraded packaged MySQL driverFix for Azure SQL databaseFix for CSV quotingComposite key handling for MSSQLSomething we learned is that extracting metadata from Postgres is quite slow, which is an issue of the Postgres JDBC driver. This issue becomes exacerbated with remote database, so please be aware.Try it nowWith the activation key-free installation everyone should be able to use this tool. Please let us know how well it works for you. The command-line version is available from the GitHub releases page.If you run into any issues or have suggestions for improvements please raise a GitHub issue. If you have questions you can ask on the Neo4j Community Site.Happy importing!;Jan 21, 2019;[]
https://medium.com/neo4j/efficient-neo4j-data-import-using-cypher-scripts-7d1268b0747;Andrea SanturbanoFollowApr 26, 2019·8 min readEfficient Neo4j Data Import Using Cypher-ScriptsFast Export/Import Operations with apoc.export.cypher and cypher-shellThe idea is to allow a user to export all data, an subgraph, the result of a query or a collections of paths into to an importable cypher-script, which you could import again by piping it to cypher-shell on the command-line. Very similar to using SQL based import scripts.cat script.cypher | bin/cypher-shell -u username -p passwordPlease note that for maximum performance you need to use cypher-shell version 1.1.9 or later, which got client-side parameter parsing, instead of sending the parameter to the server to be evaluated. You can install that as a standalone binary if your Neo4j server installation comes with an older version.On another note: If you are experimenting with imports that are failing you can add the --debug command line parameter, to see which statement was executed last and cause the failure. Also check the memory configuration of your Neo4j instance, you might want to up the HEAP to 2–4GB with the dbms.memory.heap.max_size=2G setting. And provide more memory to cypher-shell itself by prefixing the command with: JAVA_OPTS=-Xmx4G bin/cypher-shell …Neo4j’s APOC procedures contain a variety of procedures that allow to easily export/import data with Neo4j. In this article, we’ll focus on exporting cypher-script using the apoc.export.cypher.* procedures which changed significantly in the last APOC release in order to improve the import performance using cypher-shell.Before the improvementsThe procedure, before the improvements, behaved in this way.Starting from the following command:CALL apoc.export.cypher.all({fileName},{config})The cypher-script output of the operation was the following:beginCREATE (:Foo:`UNIQUE IMPORT LABEL` {name: foo , `UNIQUE IMPORT ID`:0})CREATE (:Bar {name: bar , age:42})CREATE (:Bar:`UNIQUE IMPORT LABEL` {age:12, `UNIQUE IMPORT ID`:2})commitbeginCREATE INDEX ON :Foo(name)CREATE CONSTRAINT ON (node:Bar) ASSERT node.name IS UNIQUECREATE CONSTRAINT ON (node:`UNIQUE IMPORT LABEL`) ASSERT node.`UNIQUE IMPORT ID` IS UNIQUEcommitschema awaitbeginMATCH (n1:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`:0}),  (n2:Bar{name: bar })CREATE (n1)-[:KNOWS]->(n2)commitbeginMATCH (n:`UNIQUE IMPORT LABEL`)  WITH n LIMIT 20000 REMOVE n:`UNIQUE IMPORT LABEL` REMOVE n.`UNIQUE IMPORT ID`commitbeginDROP CONSTRAINT ON (node:`UNIQUE IMPORT LABEL`) ASSERT node.`UNIQUE IMPORT ID` IS UNIQUEcommitAs you can see there is one `CREATE` statement for each node:CREATE (:Foo:`UNIQUE IMPORT LABEL` {name: foo , `UNIQUE IMPORT ID`:0})and relationships:MATCH (n1:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`:0}),  (n2:Bar{name: bar })CREATE (n1)-[:KNOWS]->(n2)Those UNIQUE IMPORT LABEL and UNIQUE IMPORT ID are used when the node in question has no unique constraint defined on any of its labels. Then we introduce that artificial label and id to be able to look them up later when creating relationships. The import script creates a constraint upfront and removes the constraint and additional information at the end.The statements are wrapped in batch-transactions of configurable size. It was the best we could do as a generic” approach which works everywhere.But it is not the most efficient way of creating such scripts:hard coding literal values instead of using parameters (despite auto-parameterization in Cypher)using a single statement per individual updateWe only realized when testing this that the limiting factor for running these queries is the performance of the Cypher parser, which gets strained a lot by millions of statements and the database spent more than 99% of its time parsing those statements.Other mistakes one could make in such an update scriptgenerating large, complex statements (hundreds of lines)sending in HUGE (millions) of updates in a single transaction will cause out-of-memory issuesBetter ApproachYou want small enough queries, that are constant in their shape (for caching of the execution plan) and are using parameters, so that even the string representation is exactly the same.Each statement should be able to update anything from a single property to a whole set of nodes, but has to stay the same in overall structure for caching.UNWIND to the RescueTo achieve a more efficient statement structure, you can prefix your regular single-entity-update-statement” with an UNWIND that turns a batch-list of data-entries (up to 10k or 50k elements) into individual rows, each of which contains the information for each of the (more or less complex) updates.Those large update lists only work well when you use a Neo4j driver and you provide the parameters from your programming language. In the cypher-shell also those update-list-parameters have to be parsed that’s why we try to keep them smaller (20 to 100 elements but configurable).Usually you send in a $batch parameter of data as a list of maps, which are then applied in a compact statement, which is also properly compiled and cached, as it has a fixed structure.General Idea{batch: [{row1},{row2},{row3},...]}UNWIND $batch as row// now perform updates with the data in each  row  variableFrom that perspective it’s very similar to a LOAD CSV statement.If you want to go deep into this kind of optimization please read this article from Michael Hunger.The New BehaviourStarting from the last APOC release we introduced the new optimization as the default behaviour for apoc.export.cypher so if you call the following procedure:CALL apoc.export.cypher.all({fileName},{config})The export script will look something like this::BEGINCREATE INDEX ON :Bar(first_name,last_name)CREATE INDEX ON :Foo(name)CREATE CONSTRAINT ON (node:Bar) ASSERT node.name IS UNIQUECREATE CONSTRAINT ON (node:`UNIQUE IMPORT LABEL`)   ASSERT node.`UNIQUE IMPORT ID` IS UNIQUE:COMMIT:SCHEMA AWAIT:BEGINUNWIND [{_id:3, properties:{age:12}}] as rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:BarUNWIND [{_id:2, properties:{age:12}}] as rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:Bar:PersonUNWIND [{_id:0, properties:{born:date(2018-10-31), name: foo }}, {_id:4, properties:{born:date(2017-09-29), name: foo2 }}] as rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:FooUNWIND [{name: bar , properties:{age:42}}, {name: bar2 , properties:{age:44}}] as rowCREATE (n:Bar{name: row.name}) SET n += row.propertiesUNWIND [{_id:6, properties:{age:99}}] as rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties:COMMIT:BEGINUNWIND [{start: {_id:0}, end: {name: bar }, properties:{since:2016}}, {start: {_id:4}, end: {name: bar2 }, properties:{since:2015}}] as rowMATCH (start:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.start._id})MATCH (end:Bar{name: row.end.name})CREATE (start)-[r:KNOWS]->(end) SET r += row.properties:COMMIT:BEGINMATCH (n:`UNIQUE IMPORT LABEL`)  WITH n LIMIT 20000 REMOVE n:`UNIQUE IMPORT LABEL` REMOVE n.`UNIQUE IMPORT ID`:COMMIT:BEGINDROP CONSTRAINT ON (node:`UNIQUE IMPORT LABEL`) ASSERT (node.`UNIQUE IMPORT ID`) IS UNIQUE:COMMITAs you can see, the script has significantly changed. For nodes it is turned into the discussed UNWIND structure:UNWIND [{_id:0, properties:{born:date(2018-10-31), name: foo }}, {_id:4, properties:{born:date(2017-09-29), name: foo2 }}] as rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id})   SET n += row.properties SET n:FooAs well as for relationships:UNWIND [{start: {_id:0}, end: {name: bar }, properties:{since:2016}}, {start: {_id:4}, end: {name: bar2 }, properties:{since:2015}}] as rowMATCH (start:`UNIQUE IMPORT LABEL`               {`UNIQUE IMPORT ID`: row.start._id})MATCH (end:Bar {name: row.end.name})CREATE (start)-[r:KNOWS]->(end) SET r += row.propertiesbut this is not enough yet because we are still hard-coding literal values instead of using parameters.Cypher-Shell’s parameter support to the rescue!In order to leverage the query parametrization and speeding-up (again) the import process we can use the cypher-shell-mode that allows using query parameters.In order to export the data in a cypher-shell compatible format we can execute the following procedure call:CALL apoc.export.cypher.all({file},{format:cypher-shell, useOptimizations:{type:unwind_batch_params,unwindBatchSize: 2}})That produces the following output script::beginCREATE INDEX ON :Bar(first_name,last_name)CREATE INDEX ON :Foo(name)CREATE CONSTRAINT ON (node:Bar) ASSERT (node.name) IS UNIQUECREATE CONSTRAINT ON (node:`UNIQUE IMPORT LABEL`) ASSERT (node.`UNIQUE IMPORT ID`) IS UNIQUE:commitCALL db.awaitIndex(:Foo(name))CALL db.awaitIndex(:Bar(first_name,last_name))CALL db.awaitIndex(:Bar(name)):param rows => [{_id:4, properties:{age:12}}, {_id:5, properties:{age:4}}]:beginUNWIND $rows AS rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:Bar:commit:param rows => [{_id:0, properties:{born:date(2018-10-31), name: foo }}, {_id:1, properties:{born:date(2017-09-29), name: foo2 }}]:beginUNWIND $rows AS rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:Foo:commit:param rows => [{_id:2, properties:{born:date(2016-03-12), name: foo3 }}]:beginUNWIND $rows AS rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:Foo:commit:param rows => [{name: bar , properties:{age:42}}, {name: bar2 , properties:{age:44}}]:beginUNWIND $rows AS rowCREATE (n:Bar{name: row.name}) SET n += row.properties:commit:param rows => [{start: {_id:0}, end: {name: bar }, properties:{since:2016}}]:beginUNWIND $rows AS rowMATCH (start:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row.start._id})MATCH (end:Bar{name: row.end.name})CREATE (start)-[r:KNOWS]->(end) SET r += row.properties:commit:beginMATCH (n:`UNIQUE IMPORT LABEL`)  WITH n LIMIT 2 REMOVE n:`UNIQUE IMPORT LABEL` REMOVE n.`UNIQUE IMPORT ID`:commit:beginMATCH (n:`UNIQUE IMPORT LABEL`)  WITH n LIMIT 2 REMOVE n:`UNIQUE IMPORT LABEL` REMOVE n.`UNIQUE IMPORT ID`:commit:beginMATCH (n:`UNIQUE IMPORT LABEL`)  WITH n LIMIT 2 REMOVE n:`UNIQUE IMPORT LABEL` REMOVE n.`UNIQUE IMPORT ID`:commit:beginDROP CONSTRAINT ON (node:`UNIQUE IMPORT LABEL`) ASSERT (node.`UNIQUE IMPORT ID`) IS UNIQUE:commitAs you can see, in cypher-shell it’s possible to define a parameter in this way::param name => expressionso in our case::param rows => [{_id:4, properties:{age:12}}, {_id:5, properties:{age:4}}]So we defined a parameter rows that can be used in a Cypher query with the $ prefix, in the following way:UNWIND $rows AS rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:BarIn previous versions, the cypher-shell much like Neo4j Browser evaluated those expressions by sending them with a prefixed RETURN over the wire to the server to be evaluated by the server-side cypher-engine. That introduced both a network roundtrip per parameter, prohibited their use inside of a client-side transaction and strained the cypher parser even more. That’s why our first experiments with parameters were actually a bit slower than the ones without.Thanks to the Cypher team, cypher-shell gained a client-side expression parser in version 1.1.9 so most of those drawbacks are now gone, and the import speed improved a lot.The BenchmarksWe used the following dataset (thanks to Alberto de Lazzari for providing that) for our benchmark.The dataset is composed by:4713605 nodes4549134 relationshipsThe Benchmarks: Exporting the cypher-script$ time ./bin/cypher-shell -u neo4j -p andrea  call apoc.export.cypher.all(import/exportDataCypherShellOld.cypher,{format:cypher-shell, useOptimizations: {type: none}, batchSize:100}) real 1m46.790suser 0m1.384ssys 0m0.229s$ time ./bin/cypher-shell -u neo4j -p andrea  call apoc.export.cypher.all(import/exportDataCypherShell.cypher,{format:cypher-shell, useOptimizations: {type: unwind_batch, unwindBatchSize: 20}, batchSize:100}) real 1m41.065suser 0m1.349ssys 0m0.214s$ time ./bin/cypher-shell -u neo4j -p andrea  call apoc.export.cypher.all(import/exportDataCypherShellParams.cypher,{format:cypher-shell, useOptimizations: {type: unwind_batch_params, unwindBatchSize:100}}) real 1m33.585suser 0m1.387ssys 0m0.222sAs you can see the performances in export are quite comparable with the export with params that is slightly more efficient. But all of them complete in around 100 seconds.The Benchmarks: importing the generated cypher-script$ time ./bin/cypher-shell -u neo4j -p andrea < import/exportDataCypherShellOld.cypher > import/output.exportDataCypherShellOld.logreal 252m33.279suser 13m53.566ssys     6m3.110s$ time ./cypher-shell -u neo4j -p andrea < import/exportDataCypherShell.cypher > import/output.exportDataCypherShell.logreal 110m27.697suser 2m48.452ssys 0m55.657s$ time ./bin/cypher-shell -u neo4j -p andrea < import/exportDataCypherShellParams.cypher > import/output.exportDataCypherShellParams.logreal 33m14.835suser 26m12.681ssys 0m46.270sAs you can see the new UNWIND powered scripts are more efficient in import, in particular:the simple UNWIND script is ~2.3x faster than the older export.the UNWIND with parametrization is ~7.6x faster than the older export and ~3.3x faster the simple unwind scriptSo you can import 4M nodes and 4M relationships using plain cypher-scripts in 33 minutes instead of 4 hours 12 minutes. There is still room for improvement in terms of parser performance but it is a great outcome of the work of the Cypher team on cypher-shell and our optimizations.APOC procedures provide many options for exporting/importing data with Neo4j. Today we showed how we significantly improved the import performance by using the combination of UNWIND with parametrization in cypher-shell.If you run into any issues or have thoughts about improving our work, please raise a GitHub issue.;Apr 26, 2019;[]
https://medium.com/neo4j/neuler-refresh-perspectives-light-gds-sandbox-integration-community-detection-layout-170f2e53f3fd;Mark NeedhamFollowSep 23, 2020·7 min readNEuler Refresh — Perspectives-light, GDS Sandbox integration, Community Detection layoutLearn about the latest updates to NEuler, the Graph Data Science Playground, including a new community detection layout and GDS Sandbox integration.NEuler (Neo4j Euler) is a UI that simplifies the onboarding process for users of Neo4j’s Graph Data Science Library (GDSL). It was first released in early 2019 when it was used to onboard users of GDSL’s predecessor, the Graph Algorithms Library.Leonhard EulerOver the last few months, we’ve made changes, which will (hopefully!) make it easier to use and get you up to speed quicker with GDSL.The NEuler Developer Guide contains instructions for installing the app or you can watch the video below.Installing NEulerLet’s have a look at what changes have been made.Perspectives-lightOne usability problem that many users had was working out how to reduce the node properties that were shown in each row of the table view. The screenshot below shows the results from running the Degree Centrality algorithm on the sample Twitter dataset:The old Degree Centrality table viewBy default, all the properties of a node (except the default ones used to store the results of algorithms) were shown. We had a feature that let you remove a property by hovering over it and pressing the red minus symbol.Removing a property keyWe thought this was quite intuitive, but after watching colleagues use the app, we realised that this wasn’t the case. So we’ve revamped that part of the UI with an idea shamelessly stolen from Neo4j Bloom — perspectives!The version of this feature in NEuler is much simpler — you can probably better think of it as perspectives-light. So how does it work?Both the home screen and the table results view have a list of node labels that you can click on to choose a background color and caption:Choosing the color and caption for the User labelIn this screenshot, we’re choosing a pink background color and the properties ‘id’ and ‘name’. If we run the Degree Centrality algorithm again, we’ll see the following output:The new Degree Centrality table viewIf you select multiple property keys, the values in the caption will be comma-separated, the same as they are in Bloom. We’re thinking about giving users the option to include property keys in the caption, so if you have any opinions let us know.And while we’re talking about feedback…In-app feedbackIf you’re using NEuler and something isn’t working or you have an idea on something that we can do better, we’ve added a feedback form in the bottom right-hand corner.Give us your NEuler feedbackFeedback is anonymous, so we don’t have a way of telling you that we’ve addressed your concerns, but we do review the comments regularly.Redesigned Community Detection algorithm outputAnother thing that has irritated me for ages is the way that we displayed the results of Community Detection algorithms. The screenshot below shows the output of running the Louvain algorithm:The old Louvain table viewIt has one node per row along with an id representing its community. This doesn’t make it particularly easy to understand the communities that exist in the graph. It would be better if we could see the results grouped by the community with a selection of nodes that belong in that community.With perspectives-light reducing the amount of screen space that a node consumes, this is now much easier. Below is a screenshot of running the Louvain algorithm now:The new Louvain table viewAlong with a configurable number of nodes per community, we can also see the size of each community. At the moment those nodes are chosen randomly, but a future enhancement would be to order nodes based on a configurable property.Algorithm config and results go togetherAnother thing users found confusing was that when they selected a new algorithm, the form for that algorithm would be displayed next to the results of the last algorithm run. In the screenshot below I’ve just selected the Triangles algorithm, but the results from Louvain are still there:The old way of selecting a new algorithmIn this version of the app, the algorithm form and algorithm results are separate components, but we’ve sorted that out now. In the latest version, we’ve added a ‘Configure’ tab that has the form to configure an algorithm run, as shown in the screenshot below:The new way of selecting a new algorithmWe’ve also made it easier to navigate to old algorithm results by listing algorithm runs in a drop-down menu instead of the previous/next navigation bar that we had before. If you select a new algorithm it will appear on the top of this list, but it will only be saved in the list if you run it. For example, if we navigate to the Local Clustering Coefficient algorithm without running Triangles, the drop-down will look like this:Previous algorithm runsWe’re also now keeping track of the form parameters used for an algorithm run. So we can now navigate back to an old algorithm run and click on the ‘Edit configuration’ button to run it again, as shown in the video below:Edit and re-run an algorithmLearning the Graph Data Science LibraryThe whole point of NEuler is to help users learn how to use the underlying Graph Data Science Library. We’ve always had a ‘Code’ tab that showed the queries used by each algorithm, and this section has been updated as well.The code viewWe’ve added the ‘Named Graph’ section, which shows how to reproduce the algorithm run using a named graph.Named graphs are projected graphs that are created before we want to run an algorithm and are stored in memory until we delete them. They are usually used in production environments where we want to run multiple algorithms against the same projected graph.We’ve also added named graphs to the Neo4j Browser guide, which can be generated by clicking on the ‘Send to Neo4j Browser’ button at the top of the Code view.Improved start-up flowWe’ve made some changes to the startup-flow of NEuler to try to explain some of the common reasons why it might not start.I hadn’t realised that it’s possible to launch NEuler without having a database running, so now we’re checking for this condition before the app launches:No active databaseNEuler depends on the GDSL and APOC plugins, so we have a check for both of those as well. The screenshot below shows the error that you’ll see if GDSL is missing:GDSL is missingHopefully, we’ve now covered all the way that startup can go wrong, but if you know of any more please let me know!Hosted NEulerFinally, we’re now publishing a hosted version of NEuler at neuler.graphapp.io. From this version of the app, you can connect to any Neo4j database from your web browser.We’ve also updated the Neo4j Sandbox so that you can connect to NEuler from the Graph Data Science sandbox:Graph Data Science SandboxIf you launch this sandbox, you’ll now see the following button to launch NEuler:Clicking this link will auto-populate hosted NEuler with the credentials of your sandbox, as seen in the screenshot below:Connecting to NEuler from the GDS SandboxIn SummaryWe hope you like the changes that we’ve made to NEuler. Enjoy trying it out and if you have any feedback let us know on the Neo4j Community site or via the in-app feedback form.;Sep 23, 2020;[]
https://medium.com/neo4j/lets-graph-explore-your-steam-library-in-neo4j-6d9d133f571;Alexander ErdlFollowMay 14, 2020·12 min readFlorian Olivo on unsplashLet’s graph: Explore your Steam Library in Neo4jIn this post we explain how you can easily import your Steam library into Neo4j and explore your data for interesting insightsIntroductionGenerally this guide is meant for beginners who want to get to grips with graph technology. Neo4j graph database offers huge possibilities to explore all kinds of data and you probably already know the movie graph that comes with every installation of Neo4j. I thought why not explore video games instead to get some interesting recommendations and maybe even learn something about my video game library.I did a live stream how I imported my data and explored it with a few queries as described below.If you rather watch me do it, go ahead. And explore our YouTube Channel.Data loadYou won’t need much more than Neo4j Desktop as well as a tool called Depressurizer to export your games library from Steam in order to follow along.First you need to launch Depressurizer and connect it with your Steam Library. It will then grab all games you own and list them. There will be some data missing, so go to Tools -> Auto-Name All and then Tools -> Auto-Cat All (pick any, I used Genre). This process will take a few minutes (depending on the size of your library). Once it is done make sure you have the following fields visible: Title, Genres, Tags, Review, Year, Last Played, Hours PlayedSteam library as CSV fileYou can now copy all of this and paste it into an Excel file (or Google Sheet). In order for Neo4j to properly import this data you need to make a few adjustments:Remove spaces from the columns Genre and TagReplace , with  in the columns Genre and TagHave Score as an integer value (i.e. 0.8 instead of 80%)Change date to YYYY-MM-DDMake sure Score and Playtime values use a . and not a , to separate decimalsIn the end it should look something like the image above. Let’s save this as a .csv file and we are ready to go into Neo4j Desktop or Browser and create our graph.As a first step we want to create a user, so let’s do that:CREATE (a:Account {User_Name: YourUserName”})RETURN aAnd here we have our first node in the graph! Yay! We gave it a label (Account) and a Property {User_Name}Let’s check out our csv file from earlier and see that we get this also in our graphLOAD CSV WITH HEADERS FROM ‘file:///Steam.csv’ AS rowWITH row.Game AS Game, row.Genre AS Genre, row.Tag AS Tag, toFloat(row.Score) AS Score, toInteger(row.Release) AS Release, datetime(row.Played) AS Last_Played, toFloat(row.Playtime) AS PlaytimeRETURN Game, Genre, Tag, Score, Release, Last_Played, PlaytimeLIMIT 5We are telling Neo4j how to treat each row in our dataset. If we don’t indicate anything Neo4j will just treat everything as String values (meaning text).Game. Genre and Tag are string values, so we don’t have to do anything here. Score is a float value, so we need to tell Neo4j that. Release is an integer value and Played is a date.Once you run the query, this should give you a preview of the csv file as Neo4j interprets it, so you can double check if there are any errors or things aren’t they way they are supposed to be.CSV Preview of our Steam libraryDouble check for errors now. Before we import this though, we need to set a few constraints to avoid duplicates in our graph.CREATE CONSTRAINT ON (g:Game) ASSERT g.Title IS UNIQUECREATE CONSTRAINT ON (t:Tag) ASSERT t.Tag IS UNIQUECREATE CONSTRAINT ON (n:Genre) ASSERT n.Genre IS UNIQUECREATE CONSTRAINT ON (a:Account) ASSERT a.User_Name IS UNIQUEThese prompts make sure that every game, tag, genre and user is unique in our graph.Now we can import our gamesLOAD CSV WITH HEADERS FROM file:///Steam.csv AS rowWITH row.Game AS Game, row.Genre AS Genre, row.Tag AS Tag, toFloat(row.Score) AS Score, toInteger(row.Release) AS Release, datetime(row.Played) AS Last_Played, toFloat(row.Playtime) AS PlaytimeMERGE (g:Game {Title:Game})SET g.Title = Game, g.Score = Score, g.Release = ReleaseRETURN count(g)We can have a look at our set, byMATCH (g:Game) RETURN g LIMIT 25A few games imported in Neo4jNext we want to import our genresLOAD CSV WITH HEADERS FROM file:///Steam.csv AS rowWITH row.Game AS Game, row.Genre AS Genre, row.Tag AS Tag, toFloat(row.Score) AS Score, toInteger(row.Release) AS Release, datetime(row.Played) AS Last_Played, toFloat(row.Playtime) AS PlaytimeMERGE (g:Game {Title:Game})SET g.Title = Game, g.Score = Score, g.Release = ReleaseWITH g, split(Genre,) AS genresUNWIND genres AS genreMERGE (n:Genre {Genre:genre})MERGE (g)-[:IN_GENRE]->(n)As genres are in a list for each game we need to make sure every genre becomes an individual node. This is done with the split part of the query. We do the same with tagsLOAD CSV WITH HEADERS FROM file:///Steam.csv AS rowWITH row.Game AS Game, row.Genre AS Genre, row.Tag AS Tag, toFloat(row.Score) AS Score, toInteger(row.Release) AS Release, datetime(row.Played) AS Last_Played, toFloat(row.Playtime) AS PlaytimeMERGE (g:Game {Title: Game})WITH g, split(Tag,) AS tagsUNWIND tags AS tagMERGE (t:Tag {Tag:tag})MERGE (g)-[:HAS_TAG]->(t)Next step we want to add the playtime and when we played a game for the last time to our graphLOAD CSV WITH HEADERS FROM file:///Steam.csv AS rowWITH row.Game AS Game, row.Genre AS Genre, row.Tag AS Tag, toFloat(row.Score) AS Score, toInteger(row.Release) AS Release, datetime(row.Played) AS Last_Played, toFloat(row.Playtime) AS PlaytimeMATCH (g:Game {Title:Game}), (a:Account {User_Name: YourUserName })WITH a, g, Playtime, Last_PlayedWHERE Last_Played <>   MERGE (a)-[:PLAYED]->(p:Date {Date:Last_Played})-[:PLAYED_GAME {total_time:Playtime}]->(g)Here we need to make sure that we connect the nodes to your account and we only want to do this for rows where we have played a game (this is why we added where Last_Played <> ” to exclude empty rows).Finally we need to add our games to the userMATCH (a:Account {User_Name:  YourUserName }), (g:Game)MERGE (a)-[:OWNS]->(g)We can now look at our schema with the following codeCALL db.schema.visualization()and it should look like thisOur schemaInitial queriesNow that we have our data in the graph we can start exploring and write our first queries. Let’s start simple and find a gameMATCH (g:Game)WHERE g.Title =  Borderlands 3 RETURN gIf we double-click on the Borderlands node we can expand all the other relationships and nodes attached to it and see all genres, tags, who owns the game and when was the last time it was played:Borderlands 3 node with all its neighboursWe can also look for specific tags and return a list of all games that are connected to the tagGames with the tag RPG”MATCH (t:Tag)WHERE t.Tag =  RPG MATCH (g:Game)MATCH (t)-[:HAS_TAG]-(g)RETURN t.Tag, g.Title, g.ScoreORDER BY g.Score DESCWe can also explore by Top Scores and see what games have the highest score in our libraryTop score gamesMATCH (g:Game)WHERE g.Score <>   RETURN g.Title, g.ScoreORDER BY g.Score DESCNext up lets see what games we played recentlyMATCH (a:Account {User_Name: YourUserName })-[:PLAYED]-(n:Date)-[:PLAYED_GAME]-(g:Game)RETURN g.Title, n.DateORDER BY n.Date DESCIf we want to add how long we played each game, we can amend the queryMATCH (a:Account {User_Name: YourUserName })-[:PLAYED]-(n:Date)-[p:PLAYED_GAME]-(g:Game)RETURN g.Title, n.Date, p.total_timeORDER BY n.Date DESCRecently played games and total durationNow we can also calculate how long we played games in totalMATCH (a:Account {User_Name: YourUserName })-[:PLAYED]-(:Last_Played)-[n:PLAYED_GAME]-(g:Game)RETURN sum(n.total_time) AS playtimeI have a total of over 1200 hours. What is your total?Advanced QueriesWith this done we can now step it up a bit and go into a bit more advanced queries. I was able to import next to my library (games I own) also the games a friend of mine owns as well as the games I have on my wishlist. For some of the following queries this is needed to have these extra datapoints, but if you don’t have it available you can still do most of it.With this new data you can show some kind of user overview which gives you the total amount of relationships from your user basically telling you how many games you own, have played etc.MATCH (a:Account {User_Name: YourUserName })-[p]-()RETURN type(p) AS User, count(*) AS total ORDER BY total DESCAll relationships from userLet’s continue with something fun and depressing at the same time — my pile of shame. Meaning these are games I own but have never played since I bought them. Lets just limit it to the top 20 by Score otherwise this will get out of hand…The query isn’t too complicated. We are looking into games we own and check that they don’t have a [:PLAYED_GAME] relationship and return the top 20 by Score (having made sure to exclude games that don’t have a score):MATCH (a:Account {User_Name: YourUserName })-[:OWNS]-(g:Game)WHERE NOT (g)-[:PLAYED_GAME]-() AND g.Score <>   RETURN g.Title, g.Score ORDER BY g.Score DESCLIMIT 20My pile of shameOuch! To my defence I did play Half-Life 2: Episode 2, but there seems to be data missing. Still, I have nothing to say about XCOM or Civilization.Moving on… We can now also get a list of the top genres of games we own. In the following query we find the path from the games we own to the genres of these games and then we collect all games that belong to a genre and count those. With that we can return all the games in a genre and sort in descending order. In my graph the genre Indie, Adventure, Action, Strategy and RPG were the Top 5 by number of games.MATCH (a:Account {User_Name: YourUserName })-[:OWNS]-(g:Game)-[:IN_GENRE]-(p:Genre)WHERE p.Genre IN [p.Genre]WITH p.Genre AS genre, collect(distinct g.Title) AS games, count(g.Title) AS titlesunwind games AS gameWITH genre, game, titles ORDER BY titles, genre, gameRETURN titles, genre, collect(game) AS gamesORDER BY titles DESCLIMIT 10Top genres in game libraryYou can do the same with tags by just replacing a few bits of the query. Try it!Let’s take it one step further and see if we can create some initial recommendations on what game to play next by looking at my history as well as our wishlist.In the first part of the query we look for games we own and played and grab the tags of those. By ordering this by number of titles and limiting this to 5 we get our top 5 tags across our library. With these tags now defined we take them and look into our wishlist and see which games here do also have these tags associated with them. Lets see what the top 10 by score are.MATCH (a:Account {User_Name: YourUserName })-[:PLAYED]-(n:Date)-[:PLAYED_GAME]-(g:Game)-[:HAS_TAG]-(t:Tag)MATCH (a:Account {User_Name: YourUserName })-[:OWNS]-(g:Game)WHERE t.Tag IN [t.Tag]WITH t.Tag as wishtag, collect(distinct g.Title) AS games, count(g.Title) AS titlesORDER BY titles DESCLIMIT 5MATCH (a)-[:WISHLIST]-(p:Game)-[:HAS_TAG]-(u:Tag)WHERE p.Score <>    AND u.Tag=wishtagWITH wishtag, collect(DISTINCT p.Title) AS game, p.Score AS ScoreRETURN collect(distinct wishtag) AS TopTags, game, ScoreORDER BY Score DESCLIMIT 10Top tags in library vs wishlistInteresting… Across the 130 or so games I have in my wishlist this is a new way of filtering (Steam itself lets you only filter by name, price, discount but doesn’t give you such a recommendation). I probably should get a VR headset in order to play Half-Life: AlyxIf you remember I had asked a friend to give me his data from his library so with that we can now see if I have some games on my wishlist that he played already. The query itself is pretty straightforward where we define the games I have on my wishlist as Gamename and use this variable against my friends library.MATCH (a:Account {User_Name: YourUserName })-[:WISHLIST]-(g:Game)WITH g.Title AS GamenameMATCH (a:Account {User_Name: FriendsName })-[:OWNS]-(g:Game {Title:Gamename})-[z:PLAYED_GAME]-(p:Date)WITH g.Title AS Game, g.Score AS Score, z.total_time AS totalRETURN Game, Score, total ORDER BY total DESCLIMIT 10Wishlist recommendation from friend’s libraryAs a result we see that he played Planet Coaster for over 30 hours. Also the game has a pretty good score of 89% — overall a very strong recommendation in my opinion.Let’s say we don’t want a recommendation for a game to buy but for a game we can play together with a friend. Again, the tag node is a very good point of entry as tags like Multiplayer or Co-op will tell us that these titles are meant to be played with more people. In this case we look for games we own as well as our friend and filter the results of games by tags that contain Co-op. As a result we want individual games and sort them by year of release so we see the latest titles first.MATCH (a:Account {User_Name: YourUserName })-[:OWNS]-(g:Game)-[:HAS_TAG]-(t:Tag)WHERE t.Tag CONTAINS  Co-op MATCH (b:Account {User_Name: FriendsName })-[:OWNS]-(g)RETURN DISTINCT(g.Title), g.Release, g.Score ORDER BY g.Release DESCLIMIT 20Co-Op games to play with friendWe could even specifically define that we want to see games that we haven’t played yet, so to discover something new from our libraries. This is done by just adjusting one lineWHERE t.Tag CONTAINS  Co-op  AND NOT (g)-[:PLAYED_GAME]-(:Date)AlgorithmsI don’t want to go into Algorithms too much, but this just should be used as an example how good the documentation on these are, so that even I was able to take some initial queries from the documentation and adapt them to my data set.First example just takes the two users nodes (my and my friend) and by looking at the two nodes calculates a similarity score of the two users.MATCH (a1:Account {User_Name: YourUserName })MATCH (a2:Account {User_Name: FriendsName })RETURN gds.alpha.linkprediction.adamicAdar(a1, a2) AS scoreThe algorithm tells us that these two users overlap about 81%. Which makes sense as my friend and I do play similar games by genre and 220 games are in both of our games libraries.Second example is using the Jaccard algorithm and takes one specific game (Borderlands 3) and looks at all games we own and return games that are similar to that game in the library by comparing genres.MATCH (g1:Game {Title: Borderlands 3 })-[:IN_GENRE]->(genre1)WITH g1, collect(id(genre1)) AS g1genreMATCH (a:Account {User_Name: YourUserName })-[:OWNS]-(g2:Game)-[:IN_GENRE]->(genre2) WHERE g1 <> g2WITH g1, g1genre, g2, collect(id(genre2)) AS g2genreRETURN g1.Title AS from, g2.Title AS to, gds.alpha.similarity.jaccard(g1genre, g2genre) AS similarityORDER BY similarity DESCLIMIT 10Similar games to Borderlands 3It is amazing that it finds lots of other titles from the Borderlands franchise without us defining anything beyond the name of the game.ConclusionThis should give you a good overview of how easy it is to start your first graph database and run a few queries. I hope you had as much fun with it as I had!You can now take it further and explore the data some more or even come up with your own queries (let me know what you thought of with this data set).Also take the opportunity to visualise your graph in Neo4j Bloom (included in Neo4j Desktop since 1.2.7) and explore it there without the need to write queries at all. You can even predefine some queries (including the powerful Graph Algorithms) so you only need to change some variables and get interesting results.My friend’s and my Steam library in Neo4j BloomResourcesA good initial source to import data and how to treat your data set is the developer guide on the topicCypher Manual helps with examples and syntax on writing new queriesMark Needham’s guide on the World Cup data helped me with some of the more advanced queriesShout outs also go to Michael Hunger for helping with a few stumbling blocks!;May 14, 2020;[]
https://medium.com/neo4j/graph-a-possible-solution-for-environmental-pollution-cb85fc618b6;Shaani Arya SrivastavaFollowNov 12, 2021·3 min readGraph: A Possible Solution for Environmental Pollution!Building a solution model for a real-life problem has always been an inspiration for me. There are many graph based solution models running in different domains such as healthcare, finance, fraud, Covid tracking, etc.Today, environmental pollution has become a serious challenge for all of us.Nations are participating and pledging in COP26, SDG 13 for net zero carbon emissions.And as a matter of fact, global warming, water pollution, dramatic changes in water cycles, and weather are all somehow connected to each other. From here I got my idea to present these Environmental Pollution Connected Patterns in a Graph Based Solution Model and the Neo4j Leonhard Euler Idea Contest provided us an amazing platform to present it globally.Just having an idea is not enough — we must show how it works and if it is valid enough to implement. Hence I gathered some sample datasets from the internet, which you can find on my Git.Graph Model for Industrial Pollution AnalysisThe graph model is pretty simple. Just by looking at it, you can see how the connected patterns (relationship arrows) can help us to find the possible industries responsible for the pollution in an area by linking them with their respective area’s air/water quality measures and disease cases, so that immediate actions can be taken on root level with proof of patterns.Did I Say Proof of Patterns? Why not?? If every industry is working as per norms, then why are we not able to track them down easily. What is making it so complex? Why does it take so much time? The answer may be the connections”… We are not able to find connections with proof. Maybe Graph can help us to simply the whole process itself.Below is the brief of the solution model data points. City and Pollutant nodes are linked via HAS_POLLUTANTS relationship — the air quality measures will be there at every fixed interval of time.Pollutant nodes found and Disease nodes reported in City.Industry nodes must be using some RawMaterials and following manufacturing Process which can be linked/related to the pollutants and hazardous chemicals.Below, a simple query helps reveal the industries that are responsible for the pollution and disease in the city.MATCH (c:City)<-[r:REPORTED_IN]-(d:Disease)-[:RELATED_TO]->(p:Pollutants)<-[r1:HAS_POLLUTANTS]-(c) WHERE r.reportedYear=r1.reportedOnYr  AND r1.reportedLevel>r1.maxPermissibleLevelMATCH (p)-[:LINKED_TO]->()<--(i:Industries)-[:IS_IN]->(c)RETURN DISTINCT r.reportedYear AS Year, c.city AS City, p.pollutant as Pollutant, d.disease AS Disease, r.patientCount AS DiseaseCaseCount, i.industry AS IndustryResultSo you can see how easy it is to query the proof’s.. :)What’s the next possible update in the solution? Bringing more factors such as Industry domains as nodes contributing to pollution. Linking the process and raw materials. Country wise analysis, etc.This is all from me about my solution. Suggestions and improvements are most welcome — you can mention them in comments :)You can also reach out to me on LinkedIn to discuss any cool ideas. );Nov 12, 2021;[]
https://medium.com/neo4j/transform-mongodb-collections-automagically-into-graphs-9ea085d6e3ef;Andrea SanturbanoFollowNov 29, 2019·5 min readTransform MongoDB collections automagically into GraphsUsing the apoc.graph.fromDocument procedureThe apoc.graph.* are a set of procedures that allow transforming custom data formats into Graphs. The last procedure that we created is apoc.graph.fromDocument that allows creating graphs from maps, JSON objects and JSON strings. In this article, we’ll talk about, why you should use it and what are the benefits of using this procedure while you’re dealing with JSON-like formats.From JSON to Graph?A JSON is basically a tree-graphSo the following JSON:{    id : 1,    type :  artist ,    name :  Genesis ,    albums : [{       type :  album ,       id : 1,       producer :  Jonathan King ,       title :  From Genesis to Revelation    }]}Can be turned into a graph like this:The Graph representation of the JSON abovewhich is composed of:2 nodes (Artist) and (Album)and 1 relationship ALBUMS between themAnd that’s what the apoc.graph.fromDocument does!How does it work?The procedure signature is quite simple:apoc.graph.fromDocument({json},{config})json, type Object: the JSON that must be transformed. Every entry must have an `id` and a `type` (the label attached to the node), configurable via the config paramsconfig, type Map: the configuration params.So given the following JSON as input:{  id : 1,  type :  Person ,  name :  Andrea ,  sizes : {   weight : {    value : 70,    um :  Kg   },   height : {    value : 174,    um :  cm   } },  books : [{   title :  Flow My Tears, the Policeman Said ,   released : 1974 }, {   title :  The man in the High Castle ,   released : 1962 }]}And the following configuration:{    write: false,    idField:  id ,    mappings: {      `$`: Person:Reader{*,@sizes},      `$.books`: Book{!title, released}    }}the first two are quite straightforward:write, type boolean: persist the graph otherwise return a Virtual Graph, default falseidField, type String: the document field name that will become the id field of the created nodes (used for node resolution when you create relationships between nodes), default idmappings, type Map: you can use a JSON path like syntax for include properties, defining document properties as value objects (by prepending the @ to the property name) and define custom/composite keys per Labels, by prepending the ! to the property name.Let’s describe the mappings fields:`$`: ‘Person:Reader{*,@sizes}’: this means that to the root object will be applied two labels Person and Reader, all properties are included and the size property will be transformed into a value object, this means that it will be flattened, so the node will have properties like: siezes.weight.value , siezes.weight.um and so on… as you can see no id is specified so we will consider as id the property defined into the idField property`$.books`: ‘Book{!title, released}’: this means that at the books property of the root object will be transformed into a node with label Book composed of two properties title considered as id (it’s marked with !) and released moreover, the property will be connected to the parent node of type Person:Reader via the BOOKS relationshipThe output will be the following:The Graph representation of the provided JSONThere are other config params:labelField, type String: the field name that became the label of the node, default typegenerateId, type boolean: in case of missing id-field value it generates an UUID for it, default truedefaultLabel, type String: in case of missing label-field value is uses the provided default label, default is emptyskipValidation, type boolean: in case you want to skip the validation process into the `apoc.graph.fromDocument` procedure, default falseWhy should you use it?You can leverage this procedure while you are dealing with document-based data, like:Web-APIsDocument-based databases like Couchbase, MongoDB, ElasticSearch, etc…So you can turn your JSON data into a graph in a very easy way.A Real Scenario: Transform MongoDB documents into GraphsTransform MongoDB JSON document in Graphs into Neo4jThe only prerequisite is to have Docker and Docker Compose. You can download the example from GitHub.Neo4j just launched Aura its Graph-Database-As-Service which simplifies the deployment and the scaling of your database letting you focus only on your application, you can also use it in combination with MongoDB Atlas in order to test a full-cloud scenario.In order to spin-up the whole stack you must execute the following command:$ docker-compose up -dThis will start 3 services:MongoDB with a Twitter datasetMongo Express: a GUI for MongoDBan empty Neo4j instanceThe goal is to leverage the APOC procedures and the JSON tree-graph structure in order to automatically” transform MongoDB documents into a Graph structure.The first step is to sample the MongoDB dataset, with the apoc.mongodb.first procedure:call apoc.mongodb.first(mongodb://mongo:neo4j@mongo:27017, test, tweets, {}) yield valuereturn valueThe output should be something like this:If you dive into the document you’ll find some fields like:text : the tweet contentuser : the user that published the tweetSo lets leverage these two fields in order to create a graph like this:In order to do that we can use the mappings configuration field to extract exactly this structure:{...   mappings: {      `$`: Tweet{!id_str,text},      `$.user`: User{!id_str,screen_name,description}   }...}Let’s describe the fields:`$`: ‘Tweet{!id_str,text}’: transforms the root object into the Tweet node, and applies to it two fields id_str (considered as id) and text`$.user`: ‘User{!id_str,screen_name,description}’: transforms the user field into the User node, and applies to it three fields id_str (considered as id), screen_name and description.At this point we can chain the mongo procedure with the apoc.graph.fromDocument procedure in order to import our documents as a graph with the following query:CALL apoc.mongodb.get(mongodb://mongo:neo4j@mongo:27017, test, tweets, {}, true) YIELD valueCALL apoc.graph.fromDocument(value, {write: true, skipValidation: true, mappings: {`$`: Tweet{!id_str,text},`$.user`: User{!id_str,screen_name,description}}}) YIELD graph AS g1return g1The output should be something like this:The imported MongoDB collection into Neo4jFinal thoughtsWe saw how to leverage te apoc.graph.fromDocument procedure in order to transform any JSON data into a graph, and in particular we saw how to chain this procedure in order to simply import MongoDB collections into Neo4j by automatically transform the document into a graph.Please try it by yourself by cloning the demo repository and feel free to fill an issue into the APOC repository if you want new features!;Nov 29, 2019;[]
https://medium.com/neo4j/importing-rdfs-owl-ontologies-into-neo4j-23e4e28ebbad;Ljubica LazarevicFollowJan 10, 2019·5 min readImporting RDFS/OWL ontologies into Neo4jAs RDFS and OWL are becoming increasingly popular for larger audiences to create and describe ontologies and models, it is becoming frequently more asked how would one import such an ontology into Neo4j.To demonstrate how an RDFS/OWL ontology can be imported into Neo4j, we are going to show how we import the W3C Organizational Ontology using the neosematics tools.There will be other approaches to achieve this, of course, but this is a good starting point to explore.What is neosemantics?Before we dive straight into our worked example, what exactly is neosematics? Neosemantics is a Neo4j plugin written by Jesus Barassa to efficiently work with previewing and ingesting RDF/semantic data, as well as publishing Label Property Graph data as RDF.jbarrasa/neosemanticsConnecting connected data: Importing RDF into Neo4j & exposing the LPG in Neo4j as RDF - jbarrasa/neosemanticsgithub.comAccompanying this plugin, Jesus has written a series of blogs describing the different ways one can import not only data, but RDFS/OWL models too. An excellent starting point to get more in-depth information on this would be the following post:Building a semantic graph in Neo4jThere are two key characteristics of RDF stores (aka triple stores): the first and by far the most relevant is that…jbarrasa.comGetting started — W3C Organizational OntologyFor reference, the ontology we are looking to import is the one below (available from: https://www.w3.org/TR/vocab-org/).We are going to use the Turtle serialization of this model and use neosemantics procedures to ingest it. To do this, we’ll run the following Cypher query in the Browser:CALL semantics.importRDF( https://www.w3.org/ns/org.ttl ,                          Turtle ,{ languageFilter:  en  })A limitation with neosematics to be aware of is how neosemantics handles multivalued properties during import the process. In this scenario, the last of the multivalue properties imported will be the one available in the resulting model. Multivalued properties are most commonly used with different languages, so by using the languageFilter parameter as above, we will ensure we import the language type we want in our model.Occasionally there will be multivalued properties that are not language-based. A LPG will happy accommodate multivalue properties via arrays. Having said that, multivalued properties outside of languages are very rarely used.Once we’ve imported the Organization ontology, we can have a look at it running the following Cypher query:MATCH (n) RETURN *Which will return the following (nodes coloured already):As we can see there’s a lot of interconnected relationships going on. A brief exploration of the model shows a lot of elements linking to ‘org’. We can show these relationships, along with which nodes being joined by running the following Cypher query:MATCH (n:Resource {uri:’http://www.w3.org/ns/org})-[r]-()WITH type(r) AS rel, startnode(r) AS snRETURN rel, sn.uri ORDER BY snThis isDefinedBy relationships effectively links every single resource to this specific definition, so we can safely ignore this. These types of statements are sometimes used to indicate an RDF vocabulary in which a resource is described (https://www.w3.org/TR/rdf-schema/#ch_isdefinedby).There are a couple of way we can go about doing this we could delete the node and relationships. However the approach we’re going to take instead is to remove the Resource label from this node, so we’re still keeping all of the information, but we can now easily bring back the rest of the model. We’ll use the following two Cypher queries to do this://Remove the Resource labelMATCH (n:Resource {uri:http://www.w3.org/ns/org})REMOVE n:Resourceand://Return only the nodes with Resource labels and associated relationshipsMATCH (n:Resource) RETURN *With a bit of node dragging to move the classes in roughly the same locations as the above diagram, we get the following:Imported Organizational Ontology in Neo4jAnd there is the imported RDFS/OWL ontology with no loss of data. We can take this diagram and re-serialize it back to an RDF format.Some comments about the imported Organizational ontology. As will be clear quite quickly, what we’d consider to be relationships and properties in Neo4j (edges, data type properties) are all represented as nodes, along with relationships between these nodes to indicate range, domain and so forth.This is the case as there are certain relationship types and definitions in RDF that Neo4j doesn’t provide for out of the box. This is not a problem, but just requires the user to have understanding of these patterns within Neo4j.For example, take the RDF concept of showing that two relationship types are inverses of each other:The left image illustrates a user-friendly view of what this diagrammatically looks like in an RDF ontology. This same concept has been replicated in Neo4j on the right as an RDF formalisation of this view, preserving all of the information integrity.Tailoring viewsOf course, we may well want to make what we bring back to view via query look more similar like what we have modelling in our graphical interface, and worry less about the relationship consistency constraints.This can be achieved in an number of ways one approach being through the use of APOC’s virtual relationships function. For example, a starter may look like this:MATCH (n:ns0__Class)-[:ns1__range]-(r)-[:ns1__domain]-(m:ns0__Class)WITH n, r.ns1__label as lab, m, count(*) as countCALL apoc.create.vRelationship(n, lab, {count:count}, m) YIELD relRETURN n, m, relHere we collapse the intermediate relationship” node into a virtual relationship for visualization.This is a virtual view, and does not in any way change or affect the data underneath.SummaryWe have demonstrated an approach of how you can import your RDFS/OWL-based ontology into Neo4j, preserving the detail, and how to make user-friendly views for exploration. For further information on working on RDFS and OWL ontologies and models in Neo4j, please do check out the excellent series of posts written by Jesus Barassa.;Jan 10, 2019;[]
https://medium.com/neo4j/lets-write-a-stored-procedure-in-neo4j-part-i-155ad08bad80;Matt HolfordFollowOct 1, 2021·12 min readLet’s Write a Stored Procedure in Neo4j — Part IThe journey continuesI am frequently asked by Neo4j customers how to improve query performance on large graphs.Achieving good performance depends on:An effective model for the dataEfficiently designed queriesThe Neo4j Java APINeo4j offers two main ways of querying a graph — through the Cypher query language and via a Java API. A solid model and well-designed Cypher query is sufficient in the majority of cases, but for those cases where we want the finest level of control, the Java API provides a powerful and intuitive interface.Sometimes, even here at Neo4j, people get the sense that writing a Stored Procedure is some arcane process that must be executed by an engineering wizard or super-coder.Well, I am here to show you that this is hardly the case!We will work through a simple, yet powerful example and you will see that there’s not much more to it than any other Java program. The Java API has been around longer than Cypher it was the original Neo4j query interface (well before my time…) When I was learning about graph databases, I found the imperative constructs of the Java API more familiar and easier to grasp than the declarative format of Cypher. I suspect this is true for other experienced programmers new to graph technology.The Java API is great but let me make a couple disclaimers:If you can get good results with Cypher queries, it is usually better to design your application that way. There are a few reasons for this but a compelling one is that Stored Procedures are compiled and require a database restart if they are modified. Thus, Cypher queries have an advantage in portability and flexibility.The Java API can’t save you from a bad model. You may be able to get some results faster, but for an effective graph application you need an efficient model. The design must consider the model and query framework in conjunction to be successful.An Example Stored ProcedureOkay, now that that is out of the way, let’s start banging out some code!Journey/Event ModelsWe’re going to look at a problem that graph databases are terrific at, but that requires the Java API to truly perform at scale. I’m talking about the Patient Journey” use case which I wrote about here.An individual’s interactions with some larger domain are represented as sequential events connected to each other. In the linked example, the individuals are patients and the events are their interactions with the medical domain (prescriptions, operations, doctor visits, etc).The Patient Journey modelAnother example would be a customer journey,” where the individuals are shoppers and the events are things like purchases, returns, and so on.The model allows us to observe behaviors across large populations without losing the context of the actions of an individual. Even as the number of individual events climbs into the billions, we don’t need to resort to sampling or summarizing results.The Java API is key here as it gives us the ability to narrow in on the individual’s event path, retaining the bare minimum of information needed to answer the question.A simplified Patient Journey modelFor the purposes of this demonstration, we will be looking at a considerably simplified version of the Patient Journey model. This will let us focus on the basics of building a stored procedure without getting too bogged down in complexity.Simplified Model for Patient JourneyWe’ve reduced our model to just three types of nodes and three types of relationship. We’ve eliminated any doctor or billing details and the only events we are recording are Conditions(i.e. diagnoses).Moreover, we are no longer modeling a start and end of an Encounter we are treating it as a single moment in time. However, we are still representing a patient journey that reflects events in sequential order. That is enough to build the foundation of a journey traversal procedure.To follow along, you can create a sample graph using the program here. The program generates 100,000 simulated Patients with Conditions using Synthea and transforms the data so that it can be loaded into Neo4j using the neo4j-admin import utility.Designing a Stored ProcedureThe stored procedure we are building will show us patterns that occur after a Patient is diagnosed with a particular Condition. The user of the procedure will provide two input parameters:the id of the Conditionthe number of subsequent ConditionsWe will aggregate on the patterns so that we can see which occur most frequently. The output will be:the pattern — a list of subsequent Conditionsthe number of times the pattern occurredHere’s an example invocation of the procedure we will build:CALL org.mholford.neo4j.findJourneys(55822004, 3)YIELD path,countRETURN path, count ORDER BY count DESC LIMIT 10The query asks for the most common patterns of three subsequent Conditions for patients diagnosed with hyperlipidemia (‘55822004’). (To avoid skewing the results, we will ignore patients with fewer than three subsequent Conditions.)Before we start coding, let me say a few words about how Stored Procedures work in Neo4j.Whereas in many relational databases there is some sort of procedural language that provides imperative, lower level control over database operations. Examples include Oracle’s PL-SQL and T-SQL for SQLServer.In Neo4j, this is done directly in Java via the plugin architecture. While this gives us the convenience of programming in Java with all of its powerful tools, it necessitates a deployment step before our code can run.We package our stored procedure code in a jar file which gets specially compiled” so that it can run safely within Neo4j’s kernel. This process is conveniently automated via Maven and, as with Heidegger’s broken hammer, we only notice it when it’s not working.Solution StrategyWe should also lay out the steps of how our procedure will solve the problem of finding the most common patterns:The steps of our Stored ProcedureWe find the Condition node by matching the specified startCondition against the index of Condition nodesWe loop over all the FOUND_CONDITIONrelationships that connect the Condtion node to EncountersFor each Encounter , we hop to the next Encounter by walking the NEXT relationshipWe walk the FOUND_CONDITION relationship to see what Condition was found for this EncounterWe repeat steps 3 & 4 twice more to get 3 subsequent Conditions in all. If there aren’t 3 more, we exclude the resultWe now have a list of 3-step paths which we can aggregate and get frequencies fromBuilding blocks — MavenWe can get started by either cloning the github project for this demo or by starting a new project from the pom.xml file. Here are a few highlights from the pom.<dependency>    <groupId>org.neo4j</groupId>    <artifactId>neo4j</artifactId>    <version>${neo4j.version}</version>    <scope>provided</scope></dependency>This gets the main neo4j dependency. Be sure to specify the provided scope- our plugin will be running within a Neo4j instance, so we don’t need to build a lot of those classes into our jar.<dependency>     <groupId>org.neo4j</groupId>     <artifactId>procedure-compiler</artifactId>     <version>${neo4j.version}</version>     <optional>true</optional> </dependency>This handles the procedure compilation I mentioned earlier. It’s flagged as optional but it’s convenient to include because it can flag certain problems with our procedure before we fully deploy it.<dependency>    <groupId>org.neo4j.test</groupId>    <artifactId>neo4j-harness</artifactId>    <version>${neo4j.version}</version>    <scope>test</scope></dependency><dependency>    <groupId>org.neo4j.driver</groupId>    <artifactId>neo4j-java-driver</artifactId>    <version>${neo4j-java-driver.version}</version>    <scope>test</scope></dependency><dependency>    <groupId>junit</groupId>    <artifactId>junit</artifactId>    <version>4.12</version>    <scope>test</scope></dependency>Here are our testing dependencies. I’m using old-school JUnit 4 for this demo since it’s good enough for our purposes. Our tests will make use of the Neo4j test harness, an awe-inspiring piece of software that stands up a temporary in-memory instance of Neo4j we can test against. We will be issuing Cypher statements (to call the procedure) in our tests. So we need the Neo4j Java driver in our test scope.(A quick note about ${neo4j.version} and ${neo4j-java-driver.version}. I would recommend using the latest stable version of each. At the time of writing this is 4.3.4. There were some changes in how Stored Procedures are handled between Neo4j 3.x and 4.x. I am presenting the current (4.x) methodology here.)Here’s the fun part:<build>    <pluginManagement>        <plugins>            <plugin>                <artifactId>maven-shade-plugin</artifactId>                <version>3.2.2</version>                <executions>                    <execution>                        <phase>package</phase>                        <goals>                            <goal>shade</goal>                        </goals>                    </execution>                </executions>            </plugin>        </plugins>    </pluginManagement></build>This stanza calls on Maven’s Shade plugin to create a shaded” jar file which includes both our code and whatever non-provided/test dependencies we include. This jar (in a coarser age, we called it a fat jar” or uber jar”) gets copied into the plugins folder of our Neo4j installation. Upon restart of the server, our procedure will be available to the kernel so that we can run it via a Cypher call.A Java Stored Procedure classNow let’s start writing the class that will execute our Stored Procedure. In your new project, create a package (org.mholford.neo4j in our example adjust as you like). In this package, create a class JourneyProcedure (actual name doesn’t matter). I’m going to step you through the code line-by-line. You can type as we go, or following along with the file from github. I will assume that you are familiar with Java concepts and have some coding experience.public class JourneyProcedure {  @Context  public Transaction txn  @Context  public Log logThere are a couple things to notice here:First is the @Context annotation before our two fields. This means that the object gets passed (injected) in from the context of the executing Neo4j kernel. At the time our procedure gets executed, a fresh Transaction object and a reference to the Neo4j log are made available to our procedure. The Transaction object is the implicit surrounding transaction that gets created around any Cypher call. (Remember that our Stored Procedure gets executed via Cypher). It is possible to create, commit and rollback other transactions within a Stored Procedure I will demonstrate this in a later blog. The Log object is a handle to Neo4j’s logging subsystem it’s a wrapper around slf4j logging library (part of the 4j family… just kidding). There are a couple other @Context objects we’ll see in later blogs and it’s even possible to define your own.The second thing is that these fields must have public access otherwise the procedure compiler will fail. We do not need to supply a constructor for the JourneyProcedure class Java’s implicit no-arg constructor is sufficient.Stored Procedure signature and resultsMoving on..@Procedure(name= org.mholford.neo4j.findJourneys )@Description( Finds the next numSteps of a patients journey after               the provided condition )public Stream<JourneyResult> findJourneys(      @Name( startCondition ) String condition,       @Name( numSteps ) long numSteps) {The @Procedure annotation is how the Neo4j kernel identifies the entry point for our Stored Procedure. When we say CALL org.mholford.neo4j.findJourneys , it finds this procedure and passes it the parameters provided. The @Name annotation lets us refer to these parameters by name when we’re calling the procedure. The @Description annotation is optional but it provides documentary metadata for Neo4j tools such as Browser.Be sure to notice that we’re defining the datatype of numSteps as a long . Does that mean we’re expecting to support journeys with more than 2,147,483,647 steps?? Well, no although theoretically we could… The reason is that Cypher has its own datatypes which govern how it represents data. As such, it defines only one integer type which corresponds to Java long(There are no short or inttypes). This page from our documentation explains Neo4j’s native types and how they correspond to Java primitives and classes.Our procedure returns a Stream of a custom data class objects (DTO) we are calling JourneyResult . This is another requirement of the procedure compiler. It must return a Stream of objects whose fields are of types that correspond directly with Cypher native types. Keep a bookmark for that page I linked above! We’ll see some other subtle gotchas with these later in the series. Here’s the definition of our JourneyResult class:public class JourneyResult {  public long count  public List<String> path  public JourneyResult(List<String> path, long count) {    this.path = path    this.count = count  }}Our result has two fields: a list of Strings which represents the path of the journey (i.e. the 3 Conditions encountered) and the count of how many Patients had that journey. Note that here again we must use long as our integer type. Finally, note that the fields must have public access. Encapsulation? What’s that?”, asks the Neo4j procedure compiler. (If, like me, you were taught to view public fields with suspicion, don’t worry. This requirement is just on the portions of code that interface directly with the procedure compiler. Within our program, we can encapsulate to our hearts’ content).The first stepAlright, let’s start the journey:var conditionNode = txn.findNode(Label.label(CONDITION_LBL),     ID_PROP, startCondition)Here, we are using the Transaction object to find the node for our start Condition . We use constants for our string variables because it’s good practice. Those are defined earlier in the class:private static String CONDITION_LBL =  Condition private static String ID_PROP =  id (Oh by the way, you can only define static fields or fields that are injected via @Context for the Stored Procedure class. This has to do with how the procedure compiler instantiates the class. The limitation does not hold for classes called by the Stored Procedure class. We’ll see examples of this later in the series.)Traversing the graphSo now we have reached the first step- we have the node for the start Condition . I’ll parcel out the remaining tasks to helper functions to keep our procedure method compact and readable:var journeys = getJourneysFrom(startNode, numSteps)var journeyResults = collectPaths(journeys)return journeyResults.stream()The getJourneysFrom method will collect all journeys from the start Condition and return them as a list of list of strings. Then thecollectPaths method will aggregate these into a list of JourneyResult . Finally, we return a Stream over these.Let’s fill in getJourneysFrom :private List<List<String>> getJourneysFrom(Node n, long numSteps) {  var journeys = new ArrayList<List<String>>()  n.getRelationships(Direction.INCOMING,        RelationshipType.withName(FOUND_CONDITION_REL)).       forEach(r -> {    var startPoint = r.getStartNode(n)    var journey = computeJourney(startPoint)    if (journey != null) {      journeys.add(journey)    }  })  return journeys}What’s going on here? We start by creating an empty list to hold our journeys. Then we find all the incoming relationships of type FOUND_CONDITION and iterate over them. For of these start points, we call the computeJourney function to get the subsequent Conditions . We add it to the list if the journey is not null. We add the null check because not all of the start points will have 3 or more subsequent Conditions . We can have computeJourney return null in those cases.So let us define the computeJourney method:private List<String> computeJourney(Node n, long numSteps) {  var output = new ArrayList<String>()  var next = n.getSingleRelationship(          RelationshipType.withName(NEXT_REL),          Direction.OUTGOING)  while (output.size() < numSteps && next != null) {    n = next.getOtherNode(n)    var nextConditionRel = n.getSingleRelationship(            RelationshipType.withName(FOUND_CONDITION_REL),             Direction.OUTGOING)    var nextCondition = nextConditionRel.getOtherNode(n)    var conditionName = (String)               nextCondition.getProperty(NAME_PROP)    output.add(conditionName)    next = n.getSingleRelationship(            RelationshipType.withName(NEXT_REL),            Direction.OUTGOING)  }  return output.size() == numSteps ? output : null}We need to keep a running list of the Conditions as we walk through this patient’s Encounters . From our starting point, n , we ask for the outgoing NEXT relationship. We check that this node exists and if so we hop from it to the Condition node attached to it. We put this logic in a while loop, and reuse the n and next variables. The loop exits either when we have the desired number of subsequent Conditions or there are no more Encounters . At this point, we return the steps if there are enough otherwise, null.Collecting the resultsNow that we have our journeys, we just need to aggregate and count them and we’re done! The collectPaths method takes care of this:private List<JourneyResult> collectPaths(List<List<String>> paths) {  var results = new ArrayList<JourneyResult>()  var pathMap = new HashMap<List<String>, Integer>()  for (var path : paths) {    if (!pathMap.containsKey(path)) {      pathMap.put(path, 0)    }    int newValue = pathMap.get(path) + 1    pathMap.put(path, newValue)  }  for (var e : pathMap.entrySet()) {    results.add(new JourneyResult(e.getKey(), e.getValue()))  }  return results}We create a map of paths to counts. Then we loop over all our input paths. For each, we either create a new entry in the map or if it’s already there, increment its count by 1. I’m spelling it out in a fairly long-winded way here so that you can see the logic. There is probably a one-or-two-liner using Java 8 streams that does this- I’ll leave that as an exercise for the reader.Et voila…Wow! We’re done! That wasn’t too bad was it? We can run the procedure by executing mvn clean package in the root directory of our project. Your IDE probably has options for this too. Copy the resulting jar file into the plugins folder of your Neo4j installation, restart the instance and you are good to go!Sample execution of Journey Stored Procedure(Note: If you run the procedure on the sample dataset, you will notice that some Conditions are a bit over-represented in the Synthea data, which seems to reflect some dystopian future. Examples include chronic sinusitis, acute viral pharyngitis and COVID-19. Among my 100,000 synthetic patients, it showed over 87,000 Covid cases. Yikes! Stay safe out there folks…)Well, that’s all for now! In our next installment, I will show you how to write tests for a stored procedure and how we can exploit Java tooling (e.g. live debugging, thread dumps, JFR) to help troubleshoot issues. We’ll also add more event types to our model and enable more options for the user. Hope you had as much fun as I did! Please check out all the code at our github repo here.Happy coding!;Oct 1, 2021;[]
https://medium.com/neo4j/spring-into-neo4j-with-spring-data-5-spring-boot-2-and-neo4j-3962fb1ea067;Jennifer ReifFollowApr 4, 2018·11 min readImage courtesy of docs.spring.ioSpring into Neo4j with Spring Data 5, Spring Boot 2, and Neo4j!* updated April 9, 2020 — example code has been archived (still available for read and clone) and replaced with new example project. Updated walkthrough guide also available on Neo4j developer site.There have been some exciting new updates to Spring Data, Spring Boot, and Spring Data Neo4j in the last few months. Of course, the goal of any change is to make the initial experience with all of these technologies a breeze!To give developers the guidance and examples that they need to use popular and valuable technologies, we want to take this opportunity to step through the process of using the Neo4j graph database and Spring tools to create an application that will allow users to interact with the graph data from any web interface.Some of you reading this may be familiar with these components already, or you may be looking for an on-boarding ramp to learn them. With this post, we will discuss some of the highlights of the changes to Spring Data, Neo4j, and Spring Boot 2 and build an application in Java to see how they work together. Let us take a quick inventory of what Spring Data Neo4j and Spring Boot bring to the table.Spring Boot handles much of the boilerplate code for application configuration and bootstrap, giving developers the opportunity to skip much of that labor and focus on business logic and the data itself.Spring Data Neo4j helps developers write code as they would normally in Spring, but work with a graph database. It does this by using the Neo4j object-graph mapper (OGM), which takes a role similar to Hibernate in JPA, to interact with the drivers, and ultimately, the Neo4j graph database.An architecture image is included below to better show this. There is an excellent blog post that explains the new features of Spring Data Neo4j, Spring Boot 2, as well as the updated OGM in more detail, and the post also includes a live webinar video with experts explaining and showing some code (where the below screenshot was taken).Without further ado, let’s start building our application! For those who want to follow along, our code repository is on GitHub here. You can also create your own version by going to start.spring.io, where the Spring Initializr will help you put the first pieces of your project in place.If you begin at start.spring.io, you can search and add the Web, DevTools, and Neo4j dependencies and choose your group and artifact name (can be more creative than the defaults in mine below), then click Generate Project. This will download a .zip file that you can open with your preferred editor or IDE.Opening the pom.xml file (for Maven project), you will already see most of the dependencies that you need. However, we need to add our OGM support to this, as well. The additional dependencies are shown below.<dependency>   <groupId>org.neo4j</groupId>   <artifactId>neo4j</artifactId>   <version>3.2.9</version>   <scope>runtime</scope></dependency><dependency>   <groupId>org.neo4j</groupId>   <artifactId>neo4j-ogm-embedded-driver</artifactId>   <version>${neo4j-ogm.version}</version>   <scope>test</scope></dependency><dependency>   <groupId>org.neo4j</groupId>   <artifactId>neo4j-ogm-bolt-driver</artifactId>   <version>${neo4j-ogm.version}</version></dependency>Now we have our project set up, and we are ready to start coding! If you are following along from the GitHub repository, you will see that we created folders for each type of classes in the project, such as a folder for our domain classes, a folder for our repositories, etc. You can create these subfolders to organize your classes, or leave them all under the same folder.First, we start with our domain model classes (entities). In our domain, we have entities for movies, and we have entities for persons. These entities will be connected by an ACTED_IN relationship, where a Person will have acted in a Movie with a particular Role.The Movie and Person entities will be nodes in our graph, and the Role will be a relationship entity, per Neo4j data modeling practices. The below simplified graph shows a piece of our data model, where Tom Hanks” is a Person who ACTED_IN the Movie Cloud Atlas”.Let’s create the Person class first. We annotate this class with @NodeEntity to specify that this class is an entity node in our graph. We also add the @Id and @GeneratedValue annotations to a Long id field because we are telling Spring Data Neo4j that this field is the internal ID and that we want the database to generate the value for us. The other fields are mapped directly to properties of our person nodes.The next annotation @Relationship deals with the relationship that connects the Person and the Movie entities. We have specified the type as ACTED_IN because this is the relationship type that we want Neo4j to define or find. The movies field we define on the line below the annotation shows that a Person can act in one or more Movies.The model shown above depicts this concept clearly, but it will make more sense once we get a little further in our code.@NodeEntitypublic class Person {      @Id @GeneratedValue private Long id   private String name   private int born   @Relationship(type =  ACTED_IN )   private List<Movie> movies = new ArrayList<>()   ....}You can complete the Movie class, as outlined below, similarly to the Person class. The @NodeEntity annotation is used again because Movie is also another entity node in our graph, and we want to have an id field, with the database generating the value, so the @Id and @GeneratedValue annotations are also the same as in Person.Just as with Person, other fields are mapped directly to properties of our movie nodes. Though there are more options for attribute mapping available, this example uses the title field as the basis for retrieving results from the database.@NodeEntitypublic class Movie {      @Id @GeneratedValue private Long id   private String title   private int released   private String tagline   @JsonIgnoreProperties( movie )   @Relationship(type =  ACTED_IN , direction = Relationship.INCOMING)   private List<Role> roles....}We also have the @Relationship annotation similar to (but not exactly) what we had in the Person class. Notice that the Movie class has more information in the relationship annotation.In this @Relationship annotation, we have specified the direction as incoming so that the relationship will come from the Person class to the Movie class. The default value for the direction attribute is OUTGOING, so we specifically set this relationship direction as INCOMING.Right below the annotation, we’ve also included a third entity called Role. This is because we have made the relationship between Movie and Person a rich relationship, where a Person who ACTED_IN a Movie plays a specific Role. The Role is a separate entity (relationship entity) with its own properties and only becomes an entity when the relationship exists — where a Person has ACTED_IN a Movie.Now, we create the Role entity class and annotate it with @RelationshipEntity and define the type of ACTED_IN. Just like with our other entity classes, we use the same syntax to create an id field. Then, we define the start node and end node that the ACTED_IN relationship connects — in this case, the Person node and Movie node. Getters and setters allow us to return and insert nodes with this relationship or add role names to the string collection on an existing relationship.@RelationshipEntity(type =  ACTED_IN )public class Role {   @Id @GeneratedValue private Long id   private List<String> roles = new ArrayList<>()   @StartNode   private Person person   @EndNode   private Movie movie....}Adding Spring Data RepositoriesNow that we have defined our entity classes using OGM and Spring Data to annotate and map to our graph model, we are ready to create our repositories.We will start with a MovieRepository interface. Here, we extend the Neo4jRepository (which itself is an extension of the CRUDRepository from Spring Data) and set up the queries that we want to retrieve our data.Note that we have annotated one method with @Query and left two without. This is because the query syntax of the first two methods are derived for us, so we do not have to write the queries manually. The annotated method defines a custom query.In the first and second queries (findByTitle and findByTitleLike), we pass in a particular movie title for the search. In the last query, we will return graph results and can optionally pass in a limit for how many results should be returned from the graph.public interface MovieRepository extends Neo4jRepository<Movie, Long> {   Movie findByTitle(@Param( title ) String title)   Collection<Movie> findByTitleLike(@Param( title ) String title)   @Query( MATCH (m:Movie)<-[r:ACTED_IN]-(a:Person) RETURN m,r,a LIMIT {limit} )   Collection<Movie> graph(@Param( limit ) int limit)}Next, the PersonRepository interface is set up similarly to our MovieRepository, although we do not use any custom Person-finder methods in this example. It still offers the useful methods that are provided by the Neo4jRepository. It can be your playground to explore and write your own queries to retrieve Person-related data from the graph!We are ready to use our repository finders in the MovieService class. We annotate this class to be a service with @Service and set up our logger. Then, we inject our MovieRepository via the constructor.Annotating the public service methods with @Transactional and specifying them as read-only will allow enterprise applications using causal clustering to route these transactions to replica servers and focus the write traffic to the core servers, improving performance. In each method, we query the appropriate method from the MovieRepository interface and return it to the service caller. You can imagine multiple repositories being called in each service method to handle more complex use cases.@Servicepublic class MovieService {   private final static Logger LOG = LoggerFactory.getLogger(MovieService.class)   private final MovieRepository movieRepository   public MovieService(MovieRepository movieRepository) {      this.movieRepository = movieRepository   }   @Transactional(readOnly = true)   public Movie findByTitle(String title) {      Movie result = movieRepository.findByTitle(title)      return result   }   @Transactional(readOnly = true)   public Collection<Movie> findByTitleLike(String title) {      Collection<Movie> result = movieRepository.findByTitleLike(title)      return result   }   @Transactional(readOnly = true)   public Map<String, Object>  graph(int limit) {      Collection<Movie> result = movieRepository.graph(limit)      return toD3Format(result)   }   private Map<String, Object> toD3Format(Collection<Movie> movies) {      ....   }   private Map<String, Object> map(String key1, Object value1, String key2, Object value2) {      ....   }}The two helper methods toD3Format() and map() are just used to create the visualization of our graph in the Neo4j browser.The REST ControllerOur controller class exposes the endpoints we want to use for retrieving data over HTTP. First, we annotate the class with @RestController to point this out to Spring, which is the short version of @Controller and @ResponseBody annotations that will render the results as JSON (default) in the response.We then set up the initial entry point to our application with the @RequestMapping. Below that, we pull in our MovieService and set up our MovieController constructor.Next, we define three endpoints for our application, so that we can access them from a web browser or client. The @GetMapping sets up the url path we want to use in order to reach that data. Each method then calls the related method from the MovieService and returns the results to the end point.In the last method to retrieve the graph representation, notice the parameter in the return statement. As mentioned earlier, this is where we can optionally set the limit for the number of results returned in the graph. If the user calling the endpoint does not specify a value (limit == null), then the code sets the limit to 100 (100 : limit). This way, we ensure that the browser is not overloaded with the visualization, and UI performance is not degraded.@RestController@RequestMapping( / )public class MovieController {      private final MovieService movieService   public MovieController(MovieService movieService) {      this.movieService = movieService   }   @GetMapping( /movie )   public Movie findByTitle(@RequestParam String title) {      return movieService.findByTitle(title)   }   @GetMapping( /movies )   public Collection<Movie> findByTitleLike(@RequestParam String title) {      return movieService.findByTitleLike(title)   }   @GetMapping( /graph )   public Map<String, Object> graph(@RequestParam(value =  limit ,required = false) Integer limit) {      return movieService.graph(limit == null ? 100 : limit)   }}The last class to fill in is the SampleMovieApplication class. We start off by annotating that it is a SpringBootApplication and that we need to enable the Neo4j repositories, pointing to our repository class location. Our main method simply runs the application, passing in any command-line arguments.@SpringBootApplication@EnableNeo4jRepositories( movies.spring.data.neo4j.repositories )public class SampleMovieApplication {   public static void main(String[] args) {      SpringApplication.run(SampleMovieApplication.class, args)   }}Our last piece of code will be updated once we start an instance of Neo4j. If you have not already, you can download Neo4j Desktop. Installation and setup steps are provided on the download page.Once you open Neo4j Desktop and create a project and a database (with password), you then need to update the password in the application.properties file to match the password you set for your database.spring.data.neo4j.uri=bolt://localhostspring.data.neo4j.username=neo4jspring.data.neo4j.password=secretThen, start the Neo4j database from Neo4j Desktop, go to Manage, and click Open Browser. From here, type :play movies in the command line at the top and click the play button to the right.Once a result pane appears below the command line, click the right arrow to move to the next slide, click on the Cypher query statement (will populate the command line), and click the play button again to run the query and populate your graph with movies. Another pane will appear below the command line showing the graph data.Finally, we are ready to run the application! You can build and run the application in your IDE or execute mvn spring-boot:run at the command line.You can also interact with this data by opening a browser and going to http://localhost:8080. You should see results populate like the image shows below.A third option is to use curl commands to request and retrieve data at the command line. Going back to the Neo4j Desktop, there are tabs across the top with one saying Terminal. If you click on the Terminal tab, you can run the curl statements below and should see JSON results returned. You can also copy/paste the url from the command line (so, minus the curl) to the web browser, and it will return the same.// JSON object for single movie with castcurl http://localhost:8080/movie?title=The%20Matrix// list of JSON objects for movie search resultscurl http://localhost:8080/movies?title=*matrix*// JSON object for whole graph viz (nodes, links — arrays)curl http://localhost:8080/graphCongratulations, you have successfully written and run an application using the latest Spring Data Neo4j, Spring Boot, and the Neo4j object-graph mapper (OGM)! For further adventures in other programming languages or more information on Neo4j, follow the resource links below. Happy coding!ResourcesNeo4j Developer ResourcesNeo4j Language Guides/Supported DriversSpring Data Neo4j DocumentationSpring Boot 2 Documentation;Apr 4, 2018;[]
https://medium.com/neo4j/zendesk-to-neo4j-integration-better-control-over-your-reporting-needs-and-building-a-ui-7ba7f17380b9;Dana CanzanoFollowDec 17, 2018·5 min readZendesk to Neo4j Integration — Building the UIThis document will explain how the Zendesk dashboard was developed using the graph-based application development framework Structr from our partner of the same name.This is a continuation of Zendesk to Neo4j Integration : Better control over your reporting needsNow that we have demonstrated how to utilize the Zendesk REST API along with Neo4j APOC to read data from Zendesk and build the associated nodes/labels and relationships in Neo4j, reporting becomes a very simple task given the Cypher language.But like so many Neo4j experiences the next step becomes displaying the data in a UI other than the default Neo4j browser. More than likely you are going to have users that are not familiar with Cypher or want a user interface more than what the Neo4j browser offers.It should be noted that everything presented was built by me. Whereby my development skill set is best summed up as having a good grasp of Linux and Cypher, and effectively HTML 101 skills (i.e. I know the difference between a <a href> vs a <table> tag) and an ability to minimally read Javascript. And with such skill set, I was able to create professionally looking HTML pages in a fairly short amount of time.The example was built using Neo4j 3.4.x, the APOC library and Structr 3.0. The entire Structr experience is hosted with a Jetty webserver that is included with Structr.A simple page which allows the user to get a one-click experience to view details on a specific organization will look similar toTo which upon initially arriving on this page the user will select the Organization defined in Zendesk from the drop down in the top left corner.Once selected the data in the right frame is then populated.As to the construction of the UI, Structr provides an IDE which allows you to start byimport webpage templates,add your HTML elementsbind the HTML elements to a Cypher statement resultThe Structr tutorials suggest a good starting point for importing a Web page template is to use those which are provided at GetBootstrap.com. I thus imported their Dashboard as a starting template. To perform the Import, within the Structr UI click on the Import icon as depictedThe following dialogue will then prompt for the URL of the page to Import.Once imported, your page is now in Structr and you can make edits as required. My goal in importing the GetBootstrap.com Dashboard page was to define the look of the page since I’m not an expert on CSS etc.After importing the Structr IDE should then appear similar toAnd upon clicking on an object in the right frame (i.e. the web page) should cause the tree-view structure on the left frame to advance to the corresponding element.For each HTML element in the left frame/tree-view, and to the right of the object name were 4 options (‘Clone’, ‘Edit Properties’, ‘Remove’, ‘Access Control’). Initially, I used ‘Remove’ to clean up the web page and remove elements that I was not going to use.Eventually, my page became a series of HTML tables. To bind a table row to a Cypher statement, click the ‘Edit Properties’ option next to the <tr> HTML object which should then produce the following dialogue.By choosing the ‘Cypher Query’ tab I could then enter the Cypher statement to be bound to the table row.And finally, the results of this query are then bound to the ‘Data Key’ which I have named ‘ZendeskUser’.It should be noted that the Cypher Query approach needs results returned in a Cypher map, and thus the return {oName: o.name, uName: u.name, ...} unlike what one might normally return for example, return o.name as oName, u.name as Uname,....After clicking the ‘Save‘ buttons above I can then express elements of the <tr><td> using this ‘Data Key’.From the display below the <tr> object has an icon ofindicating the HTML item is bound to a Cypher statement.Further using the DataKey I can then define each <td> element under the <tr> row. For each <td> I have added a Content Element and added the placeholder of${ZendeskUser.uName}. I could refer to each element of my Cypher map with the ZendeskUser key prefix.It’s as easy as that.As for my final dashboard — It was simply a repetition of the above in terms of creating new tables, and defining the corresponding Cypher to populate the table data within each table row.And Structr made this real easy through the use of the ‘Clone’ functionality which appeared next to every HTML element. So if I cloned the <table> element, it took the current <table> and made an exact clone and placed it immediately underneath the existing <table>. And then I only had to adapt the columns that needed to change.Other Noteable Features of StructrUser AccessStructr has an ability to define its own list of users who can access the UI and which HTML elements the user can see. I did not entirely utilize this functionality but rather simply fronted all my pages with a Google OAuth plugin.Content StorageMost all of the Structr content (i.e. the HTML that makes up the page you have designed) is actually recorded in the Neo4j graph. This has the benefit that a Neo4j backup will save all your work performed within Structr.Additional HTMLStructr allows you to define HTML items as required within the UI. So if you want to add some Javascript into your page, simply right mouse within the tree view and add a new <script> tag and then define its content.In my dashboard example I utilized this functionality so as to support dynamic table sorting. Clicking on a column header will locally sort the table by that column header. To do this I added 2 <script> objects which utilized the example from http://luiliom.free.fr/The Structr team was very helpful with my questions and I had quite some fun building this application. I can recommend you try it yourselves, starting with a similar example as mine.;Dec 17, 2018;[]
https://medium.com/neo4j/git-commit-history-discover-auradb-week-44-2ea2337abc86;Michael HungerFollowMar 27·8 min readGit Commit History — Discover AuraDB: Week 44Let’s explore the output of git log as a graph in Neo4j AuraDB Free.This time Alex, couldn’t make it, hope he gets better soon.Some interesting things that happened since last weekendWent to watch Dungeons and Dragons: Honor among Thieves with the kids (was really good)Played a lot with GPT-4Missed the asteroid DZ2 2023, due to the cloudy sky :(Been running Arc browser, which is really neat Invite here in the show-notesWent to a local whisky festival which was great for tasting rare spiritsBetween everything I saw the TIL post by Simon Willison about his GPT-4 coding exercise to turn git logs into JSON output.He uses the swiss army knife of JSON processing jq, which is awesome.His post inspired me to today’s session on looking at git commit history as a graph.If you rather want to see the recording, check it out here:Data Source and PreparationWe’re using Neo4j’s open-source repository here for our experiment: https://github.com/neo4j/neo4jClone the repository (I limited it to the 5.6 branch) and follow along.Simon did two interesting thingsUsing git log --pretty with NULL-bytes as separators (instead of commas or tabs)Using jq to parse split the raw string by those NULL bytes and output JSON for the fieldsgit log --date=iso --pretty=format:%H%x00%an%x00%ad%x00%s%x00 | head -2 | \  jq -R -s [split( \n )[:-1] | map(split( \u0000 )) | .[] | {     commit : .[0],     author : .[1],     date : .[2],     message : .[3]  }]The output is:[  {     commit :  5ad4387ed521f169a737f9836402dbac8759a9fc ,     author :  Johannes Donath ,     date :  2023-03-08 16:17:12 +0100 ,     message :  Corrected an issue in which the number of writable bytes is incorrectly calculated. ()   },  {     commit :  c53e91519eca145c6879d5c9be9a421fdd223338 ,     author :  Tobias Johansson ,     date :  2023-03-07 13:34:12 +0100 ,     message :  Do polling of the fabric transaction lock in terminate   }]The placeholders in the pretty-print string are a bit ominous but the man page and the cheat-sheet here help a lot.I wanted to add the parent commit via %P to see the git commit history chain. Also tried to get one of the files of the commit in but we’ll leave that for another time.The other change that we need to do is to turn the data into a CSV, not a JSON file. Fortunately, jq also supports CSV as an output so we can select our 5 fields and tell it to send it through the @csv processing step.echo commit,parent,author,date,message > ~/Downloads/neo4j-git.csvgit log --date=iso --pretty=format:%H%x00%P%x00%an%x00%ad%x00%s%x00 | \  jq -r -R -s split( \n )[:-1] | map(split( \u0000 )) | .[] | [   .[0],   .[1],   .[2],   .[3],   .[4]  ] | @csv >> ~/Downloads/neo4j-git.csvWe can check the resulting CSV file with the xsv tool and see that it has the 76k commits and the right fields, it’s about 14MB of data.xsv count ~/Downloads/neo4j-git.csv76567xsv stats ~/Downloads/neo4j-git.csvfield,type,sum,min,max,min_length,max_length,mean,stddevcommit,Unicode,,0000257bb06e29e15c11b6bc5ad4f8253deed4a4,ffff9ee399d15bd834e4aeb4719a02e5e39308d5,40,40,,parent,Unicode,,0000257bb06e29e15c11b6bc5ad4f8253deed4a4,ffff9ee399d15bd834e4aeb4719a02e5e39308d5,0,81,,author,Unicode,,@fbiville,wujek srujek,2,30,,date,Unicode,,2007-05-24 01:34:45 +0000,2023-03-08 16:17:12 +0100,25,25,,message,Unicode,,     - BeansAPITransaction handles multiple calls to tx.finish(),zoo_keeper_servers example changed to something useful (the default ZK port),1,3217,,ls -lh ~/Downloads/neo4j-git.csv14M 27 Mar 10:17 /Users/mh/Downloads/neo4j-git.csvOk, now we have everything to turn those commits into a graph.Are you ready?Let’s first spin up a Neo4j AuraDB database and get the data imported.Create a Neo4j AuraDB Free InstanceGo to https://dev.neo4j.com/neo4j-aura to register or log into the service (you might need to verify your email address).After clicking Create Database you can create a new Neo4j AuraDB Free instance.Choose the  Empty Instance  option as we want to import our data ourselves.On the Credentials popup, make sure to save the password somewhere safe, best is to download the credentials file, which you can also use for your app development.The default username is always neo4j.Then wait 2-3 minutes for your instance to be created.Afterward, you can connect via the  Open  Button with Workspace (you’ll need the password), which offers the  Import  (Data Importer),  Explore  (Neo4j Bloom), and  Query  (Neo4j Browser) tabs to work with your data.On the database tile, you can also find the connection URL: neo4j+s://xxx.databases.neo4j.io (it is also contained in your credentials env file).If you want to see examples of programmatically connecting to the database go to the  Connect  tab of your instance and pick the language of your choiceAfter opening Neo4j Workspace via the  Open  button, and logging in with the downloaded credentials, we can go to the  Import  tab and get started.Data ModelingThe data model is pretty straightforward, we just have Commit and Author nodes connected by a WROTE relationship.The commits are also pointing to their parent commit via a PARENT relationship.Minimalistic Data ModelNow we can add our CSV file and map the attributes and select our id-fields.Commit (hash, date, parent, subject)Author (author)Data Importer with MappingWith the data mapped we can  Preview  our import and inspect nodes and relationship attributes and structure to see if we messed anything up.Import PreviewAs we’re good we can click  Import  and, after a few seconds, the results are presented. There you can also inspect the Cypher statements used to create the data which you can use in your own code or scripts.The image below is from a 2nd run, so the data is already in the graph.Import ResultsExploreNow with  Start Exploring  we head over to the  Explore  tab and see data in the graph.There we can also style our nodes and for instance determine the shortest paths between commits (select two nodes and right click for the context menu).Now let’s get our hands dirty and write some Cypher statements.Query & EvolveOpening the left sidebar shows us the labels and relationship-types in the graph, we can click on any of them to see a subset of our data.Query the GraphNow we can start to explore a bit,First looking at the total number of commits and authors:MATCH (:Commit) RETURN count(*)// 76kMATCH (:Author) RETURN count(*)// 338Contributor ActivityLet’s find the most active authors, we can find out their outgoing relationships.MATCH (a:Author)WITH a, count { (a)-[:WROTE]->() } as commitsORDER BY commits DESC LIMIT 10RETURN a.author, commitsWhich gives us these folks:a.author commitsMattias Persson 6592Anders Nawroth 4106Pontus Melke 4030MishaDemianenko 3989Andres Taylor 3640Chris Vest 2983Jacob Hansson 2521Anton Persson 2212Mattias Finné 2148Davide Grohmann 1987The usual suspects :)Oh, Mattias has been with us for 15 years but got married in between, so he shows up with two different names.Let’s fix that and move the 6592 relationships from his old alter-ego to the new one:MATCH (new:Author { author:  Mattias Finné })MATCH (old:Author { author:  Mattias Persson })MATCH (old)-[rel:WROTE]->(c:Commit)CREATE (new)-[:WROTE]->(c)DELETE relRETURN count(*)So that should fix the results:a.author commitsMattias Finné 8740Anders Nawroth 4106Pontus Melke 4030MishaDemianenko 3989Andres Taylor 3640Chris Vest 2983Jacob Hansson 2521Anton Persson 2212Davide Grohmann 1987Satia Herfert 1896If we wanted to limit the people by commit-date, so we see who has been more active recently, we can do that too:MATCH (a:Author)WITH a, count { (a)-[:WROTE]->(c:Commit) WHERE c.date > datetime( 2019-01-01T00:00:00 )} as commitsORDER BY commits DESC LIMIT 10RETURN a.author, commitsNow we have a different set of people with fewer total contributions:a.author commitsMishaDemianenko 1892Pontus Melke 1570Mattias Finné 1284Satia Herfert 1126Anton Persson 920Chris Vest 746Louise Söderström 578Tobias Johansson 463Therese Magnusson 439Georgiy Kargapolov 415Fixing ParentsAnother thing we need to fix is that some commit multiple parents not just one so that their parent-hash was not found in the database to connect them.We have roughly 15k commits of that kind. Here is what the property looks like.parent:  e5697a0900ff849f92d0ae3c88bd8e31e3163024 d26c6ab67c34a5f91cc6cfadfc75ca4b1def3bef We can fix it by:Finding these commitsSplitting the parent field by spaceTurning that list of hashes into rowsFind the parent commit with the hashCreate the relationshipsOr in Cypher:// find commits without PARENT relationship but with multiple parent hash valuesMATCH (c:Commit)WHERE NOT exists { (c)-[:PARENT]->() }  AND c.parent contains  // split by space into list of hashesWITH c, split(c.parent, ) as parents// turn list into rowsUNWIND parents as parent// find parent commitMATCH (p:Commit {hash:parent})/// create relationshipMERGE (c)-[:PARENT]->(p)RETURN count(*)Now our graph is better connected, and we can run a few long path queries:MATCH path = (c:Commit)-[:PARENT*100]->(p:Commit)RETURN path LIMIT 1Which gives us this beautiful long, flowery chain of commitsWe can also look at  root  commits and see which ones have the most children.With PROFILE we can sneak under the hood and see how the query planner optimizes this query, it completes in about 2 seconds checking 10-hop paths for all 76k commits.profilematch (c)-[:PARENT*..10]->(p:Commit)with p, count(distinct c) as childrenreturn p {.*} as parent, children order by children desc limit 10Some of them have more than 600 children even only up to level 10!We can do the same without an upper limit but for a subset of commits:PROFILEMATCH (p:Commit) WITH p LIMIT 1000MATCH (c)-[:PARENT*]->(p)WITH p, count(distinct c) as childrenRETURN p, children ORDER BY children DESC LIMIT 10And with that, we were out of time and had covered a lot of ground.If you want to see videos, write-ups, and data for past livestreams check out our overview page:Discovering AuraDB Free with Fun DatasetsWe run these  Discover AuraDB Free  live-streams alternating every Monday alternating at 9am UTC and 2pm UTC.neo4j.comOr our repository: https://github.com/neo4j-examples/discoveraurafreeHope this was as much for you as it was for me,Happy Graphing!;Mar 27, 2023;[]
https://medium.com/neo4j/real-time-systems-landscape-using-neo4j-c043a574cf8;Murali Krishnan ManiFollowNov 6, 2018·3 min readReal-time Systems Landscape using Neo4jWhat are the challenges in understanding Systems Landscape?In any larger organisation, where there are more than thousands of systems (internal / external), it is imperative to understand & co-relate systems and their integrations with different downstream and upstream systems considering environment (development / production) constraints. In addition, following are the challenges faced by organisations to maintain System integrity:Systems Repository, Configuration Management Database / CMDB or other equivalent systems in any organisation are not real-time or not updated.Interface not available for querying & visualizing the system landscape for Project Managers / Business stakeholders for better planning.What is the impact of not having a Real-time System Landscape view?Typically, any larger business programs would have impact on one or more systems and would have been impacted due to the above mentioned challenges. If there are number of parallel ongoing programs, then the impact would be significant. Impact on the projects are typically:Project Planning — To identify impacted systems change due to data model / feed change, it would involve repeated data collection exercise. Redundant effort & budget spent on repeated task.Testing phase — Environment constraints reducing the agility of the program, resulting in delayed delivery of projects. Example: SystemA has only 1 dev and 1 production instance, which needs to be tested for 3 large programs, which has impact on SystemA. Understanding the system dependencies and environment constraints would result in better project planning. It enables to respond to ever changing business needs quickly.Stakeholder Management — Often, it becomes difficult to align the business stakeholders on the complexity of environment / system constraints. A significant effort is spent by the Project Manager, repeating the details to different stakeholders.How these challenge can be mitigated using Neo4j?Neo4j can augment the current system landscape details in a graphical visualization model, that can be queried through intuitive tools. The above mentioned challenges were mitigated using Neo4j. On a high level, the solution implementation for Real-time System Landscape is as shown below.Real-time Systems Landscape ArchitectureCritical features of Neo4j that enable this solution are:Graph database: Capturing, persisting and searching capability on the complex system and integration details is possible through underlying Property Graph model (Entity [Node, Relationship], Path, Token, Property).Data Model for Real-time System LandscapeExtensible: Neo4j provides an extension capability through custom procedures or functions development. The data extraction, cleansing and applying ML / NLP for extracting system information from structure / unstructured data can be implemented specific to the organisation needs, with limited development effort.Cypher: An intuitive query interface provided for querying using SQL like interface. The repeatedly used queries can be saved for future purpose through bookmarks.Drivers availability: The availability of APIs in different programming languages (like Java, C#, JavaScript, Python) for Neo4j, enables rapid development for integrating with the database. Vizjs was used to visualize the Neo4j data, as highlighted in the section below.A sample real-time system landscape, providing system integration details with querying capabilities can be readily used by IT project team member and business users. The visualization helps identify environment conflicts through different color coding for resolving the Project planning issues.Real-time Systems Landscape Neo4j VisualizationConclusion:Neo4j provides required tool sets for quickly developing, deploying & visualizing graph database for the organisations with limited development resources.I had developed above real-time systems landscape dashboard using Neo4j and Visjs for a customer in less than a week. The dashboard provides the change in system landscape as you move the project timeline.This visualization helps in better planning and avoid environment conflicts and resulted in saving significant effort & dollars spent on planning and testing phases of the project.;Nov 6, 2018;[]
https://medium.com/neo4j/create-a-data-marvel-part-9-building-the-webpage-for-comics-1ceb26f8a5be;Jennifer ReifFollowMar 21, 2019·9 min readCreate a Data Marvel — Part 9: Building the Webpage for Comics*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)We now reached the point to create the webpage for interacting with the Marvel comic data! Through the past 8 blog posts, we have seen how to start a development project from scratch — from a data set to a functioning application interacting with the data. It is this step where we build a prettier user interface that allows users to search for comics, see their details, and interact with a graph visualization of the data.To catch up with the most recent post of this series where we built the endpoints for retrieving comic issues and related entities, you can read Part 8. Let’s get to it!Creating the HTML pageThere is quite a bit of code that goes into the issues.html file in this project, so we will show some (but not all!) of the code for that file in this post. We will walk through the bits of code that handle and render our data, then spend a bit of time on the visualization section of the page. Of course, the full code can be found on our GitHub repository. Feel free to check that out!First, let’s get a feel for what the end result will look like in the image below.Pretty cool, right?! Let’s find out how to build it!Handling the list of comicsFirst, let’s start by analyzing the search functionality and the populated list of comics that show up in the pane on the left-hand side of the page, which is in the image below.The code retrieves comics based upon the values users put in the search box at the top. Most of the code is contained in a search() function and then added to the html content on our page.function search() {  var query = $(#search”).find(input[name=search]”).val()  $.get(/comicissues/findbynamelike?name=*” +        encodeURIComponent(query) + *”,  function (data) {    var t = $(table#results tbody”).empty()        if (!data) return    data.forEach(function (issue) {      $(<tr><td class=’issue’>” + issue.name +         </td><td>” + issue.issueNumber +         </td><td>” + issue.pageCount +         </td></tr>”).appendTo(t)        .click(function () {          showIssue($(this).find(td.issue”).text())        })    })    showIssue(data[0].name)  }, json”)return false}The starting line inside the function finds the value of the search input box and sets that to the query variable. The next line hits our fuzzy search endpoint query to search our database for comics with a name like the value in our search box. The next few lines call another function to loop through the results and create the html tag to encompass each value. Those dynamically-generated <tr> and <td> tags get inserted into the below html block.<div class=”panel panel-default”>  <div class=”panel-heading”>Search Results</div>  <table id=”results” class=”table table-striped table-hover”    <thead>      <tr>        <th>Name</th>        <th>Issue Number</th>        <th>Page Count</th>      </tr>    </thead>    <tbody>      //new, dynamic content tags will go here!    </tbody>  </table></div>Great! That takes care of our list of search results. I set my default search input box text to Captain America” to retrieve those comics (he’s my favorite character!), but you can default that value to anything you like in the <input value= Captain America > tag. :) A default value ensures you see some values and not empty results when you first load the webpage.Next, we will cover the top panel on the right to show the details of each comic as it’s selected.Displaying ComicIssue DetailsIn this section of the page, we want to show how we pulled each section of content (Characters, Creators, Series, Stories, Events, plus the ComicIssue image) and display it on our webpage. The following code is for the part of the page shown below.Similar to what we saw above, we use a JavaScript function (showIss()) to retrieve the values from our database and dynamically generate the surrounding html tags. That content then gets inserted between html content sections where we see the nice blocks of bulleted lists. Let’s take a look.function showIssue(name) {  $.get(/comicissues/findbyname?name=” + encodeURIComponent(name),    function (data) {    if (!data) return      var issue = data    $(#name”).text(issue.name)    $(#poster”).attr(src”, issue.thumbnail)    var $charList = $(#characters”).empty()    issue.characters.forEach(function (issueChar) {      var person = issueChar.name      var thumbnail = issueChar.thumbnail      $charList.append($(<li><img src=’” + thumbnail +         ‘ id=’comicChar’ style=’height: 50px width: auto’/>  +        person + </li>”))    })    var $creList = $(#creators”).empty()    issue.creators.forEach(function (issueCre) {      //similar code here for creators    })    var $seriesList = $(#series”).empty()    issue.series.forEach(function (issueSeries) {      //similar code here for series    })    var $storyList = $(#stories”).empty()    issue.stories.forEach(function (issueStor) {      //similar code here for stories    })    var $eventList = $(#events”).empty()    issue.events.forEach(function (issueEvent) {      //similar code here for events    })    render()  }, json”)return false}In the function call on the first line, we pass in the name of the ComicIssue, which is whatever comic the user clicks on from the left-side pane we talked about above. Each time the user clicks a different comic from the list on the left, this function gets called and retrieves the relevant content for that comic name. The first line within the function simply calls the findByName() method from our application. We verify we received data back from the call, and then set that data to the issue variable.Next, we set the name (though we currently don’t display that) and poster values. For the poster, notice that we use the image url from our node in the database as the src property on the html tag. This allows the tag to retrieve that image from the url and display it on the page!The next blocks of code loop through each of the nested JSON structures that contain the related Characters, Creators, Events, Series, and Stories that are connected to the chosen ComicIssue. We will take a closer look at these blocks below for clarity.var $charList = $(#characters”).empty()issue.characters.forEach(function (issueChar) {  var person = issueChar.name  var thumbnail = issueChar.thumbnail  $charList.append($(<li><img src=’” + thumbnail +     ‘ id=’comicChar’ style=’height: 50px width: auto’/>  +    person + </li>”))})In the above segment, we set a variable $charList to the empty contents of the html tag with an id of characters. Then, we loop through our nested JSON of Characters for that ComicIssue (issue.characters.forEach). For each Character object in the list, we set the character name and character thumbnail. Finally, we append to our $charList variable, adding the values wrapped in the appropriate html tags.You might have noticed that certain images do not render for the Series section, and some of the Story values might seem meaningless. This is due to bad values coming from the API that were then transferred to our database. I think there should be some data cleanup discussions and activity from the owners of the API. However, since that is not something we can control, we could determine how to clean up the data in the database or render bad data in a friendlier manner on the front end. Either way, we have tabled that discussion for later, as it could be a topic of its own!This same process occurs for each of our other lists (Creator, Event, Series, and Story), and then triggers our render() event, which we will discuss in the next section.Rendering the Data in a Graph VisualizationOur last component of the webpage is the graph visualization, representing our nodes and relationships from our Neo4j database as circles and lines. This visualization is rendered using the D3 library, which we discussed in the last post for getting our graph data into the Map<> format D3 expects. This piece of our webpage is where all those pieces come together!function render() {    d3.select(svg”).remove()  var width = 500, height = 500  var force = d3.layout.force().charge(-200)              .linkDistance(30).size([width, height])  var svg = d3.select(#graph”).append(svg”)            .style(width”, 100%”).style(height”, 100%”)            .style(background-color”, white”)            .attr(pointer-events”, all”)  d3.json(/comicissues/buildgraph”, function (error, graph) {    if (error) return    force.nodes(graph.nodes).links(graph.links).start()    var link = svg.selectAll(.link”)               .data(graph.links).enter()               .append(line”)               .attr(class”, link”)    var node = svg.selectAll(.node”)               .data(graph.nodes).enter()               .append(circle”)               .attr(class”, function (d) {                 return node  + d.label               })               .attr(r”, 10)               .call(force.drag)    //html title attribute    node.append(svg:title”)        .text(function(d) {          return Type:  + d.label + \nName:  + d.name         })    node.append(name”)        .text(function (d) {          return d.name        })    ....  })}The first line inside our render() function ensures the field is cleared out and no residual data is in the html svg tag. It then sets the height and width of the pane and sets values for how far and fast the nodes and relationships should bounce away from one another (var force). Once that is set, we set our svg variable to the graph html element on our page with some basic styling and attributes.The next block actually populates our data within the visualization. First, we call d3.json that will go to our /buildgraph endpoint in our application and then call a function. Remember that our /buildgraph endpoint will execute our graph() method to gather all of the data and format it as two maps. If the call errors, it will simply return otherwise, it will continue.In the force.nodes().links().start() code, it is pulling the nodes (from graph.nodes) and the relationships (graph.links) and using the force-directed graph approach, which means that the view will center on them and the nodes will bounce away from one another and not sit on top of one another. We set variables for each node and relationship and loop through, returning values and building circles and lines for each one dynamically.The last couple of blocks shown in this section were for extra features. This creates a tooltip that displays a node’s type (whether a character, event, or other) and the name of the node (creator’s name, comic issue title, or other). We thought this added a bit more helpful information, so users could explore the graph and see which node colors belonged to each category. Of course, we could definitely continue to improve the user experience here, and we will talk more about possibilities and upcoming ventures in our next blog post!What I LearnedI’m not a skilled frontend developer, so much of this was used from examples I had dealt with in the past and then customized for our application. I can still definitely make improvements, but I enjoyed the learning experience and seeing some interesting and simple things I could do that made a big difference on the page.Though the user experience is not stunning, this approach blends simple with a few nicer features. It provides a fun way to adjust certain settings and formatting to see how it impacts all of our data and the rendering. The examples I had seen allowed us to build a simple and nice interface for us and other users to interact with the data, search for comics, and navigate the page.I got more in-depth exposure to D3 and how to render data with html tags and css. Styling has not been my forte, but I’ve always enjoyed learning about it and seeing the tangible changes in coloring or formatting. It also got me thinking about ways to continue improving the experience and interaction with the different panes.Next StepsWe have now progressed from initial project seed stages through a full-stack application! Congratulations for making it this far on the journey with me. In the next post, we will take a look at some of the ways where this application could be improved, as well as the full list of project resources and thoughts for future projects we tackle.Stay tuned!ResourcesGithub source codeFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifSimilar webpage from Spring/Neo4j movie applicationSpring Data Neo4j docsSpring Data Neo4j GuidePrevious parts of this blog series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8;Mar 21, 2019;[]
https://medium.com/neo4j/new-and-noteworthy-neo4j-developer-tools-bfef5a41b41f;GregFollowJul 8, 2021·5 min readNew and Noteworthy: Neo4j Developer ToolsNeo4j Browser melds new with old and Neo4j Desktop gets more capable on WindowsHot on the heels of the Neo4j 4.3 release last month, we also recently released the latest versions of our Neo4j DevTools Browser and Desktop. Check out this installment to find out what you can expect with the latest updates!From Sweden with ♥ Midsummer Photo by Mikael Kristenson on UnsplashNeo4j BrowserEarlier this year, we added the ability to edit queries in situ from within a query result frame. We’ve received great feedback from old and new users alike, and we’re not standing still. Read more to learn about how we’ve brought back some long-established interaction and are improving new ones, too.Send your queries back to the main editor againSince the beginning of Neo4j Browser, clicking on the text of a previously run query populated the main editor with the Cypher query. This gave you the freedom to quickly copy a query, edit it, and create a new result frame.With the advent of reusable frames we had changed that, and the default behavior became editing the query within the result frame. But there are still occasions when the old way of doing things makes more sense, so we’re now giving you the best of both worlds.The most recent release now allows you to ⌘ + click (Mac) or ctrl + click (Windows/Linux) on any query to send it to the main editor.⌘ + click (Mac) or ctrl + click (Windows/Linux) a query to send it to the main editorQuery editing gets more spaciousWhile editing a reusable frame is a great addition to Browser, the way the frame buttons encroach on your queries can leave wide queries looking a bit cramped.To address this, we’ve made some tweaks to the way frame buttons are laid out so your queries can breathe again.Ahh, that’s betterThe frame buttons for pinning, collapse, full-screen, and close now appear together in an elevated section, similar to other window-management systems. The play button has been moved into the query editor area.More screen width is freed up to give your queries room to breathe.We’ve also made improvements to the way the result frame query collapses so it gets out of your way when you don’t need it.Improvements to guidesA number of improvements have been made to the guides sidebar introduced in the last release, including:More scalable pagination controls for large guidesOption to immediately execute queries from Cypher snippets by clicking the play iconSupport for Neo4j 4.3With the release of Neo4j 4.3 last month, we’ve made some tweaks to support the latest and greatest Neo4j release, including:New Cypher keyword supportUse of the home 🏠 emoji to denote the user’s Home/Default databaseWelcome home!And finally, we made a number of bug fixes, including:Fixing an issue that prevented browser rendering very large property values in the Graph visualization (such as embeddings generated by the Graph Data Science library)Focusing the reusable editor properly at the start of editingNeo4j DesktopDump and upgrade migrations now available on WindowsNew releases of Neo4j now contain graceful shutdown procedures for Windows. This allows our Windows users to generate dump files, load databases, and upgrade DBMSs that require data migrations safely for the first time.The graceful shutdown has been available since the 4.3 release and for patch releases 4.1.10+ and 4.2.8+. Any pre-existing DBMS installations unfortunately won’t be able to take advantage of this new procedure, but the future is brighter for new installs!General Improvements to DBMS upgrade experienceWe’re now giving you clearer guidance on the recommended steps to take when attempting a DBMS upgrade:Upgrade guidanceWhile we aren’t enforcing this guidance in the UI, following it should help more of your upgrades go smoothly.When things don’t go to plan, logs are now available for troubleshooting, as well as better signposting of backup locations to help get you back on track.Clearer Backup and Log signpostingImproved log viewerWe’ve updated the log viewer for better cross-platform reliability. You can now also inspect all DBMS logs, so you’re no longer restricted to using this handy log viewer just for the neo4j.log.Multiple log viewer supportFolder collapsingFolders within a DBMS’s files section can now be expanded/collapsed for easier viewing. Note that files and folders continue to only appear in this view when they are relevant to Neo4j, such as dump files and cypher queries.We look forward to seeing you at the next installment when we’ll be back with more DevTools goodness. Thanks for reading!Please continue to send us feedback for Neo4j Browser and Desktop and be sure to read the changelog for updates.Neo4j Browser FeedbackGive feedback to the Neo4j Browser team so we can make more informed product decisions.neo4j-browser.canny.ioNeo4j Desktop FeedbackGive feedback to the Neo4j Desktop team so we can make more informed product decisions.neo4j-desktop.canny.io;Jul 8, 2021;[]
https://medium.com/neo4j/brexit-debate-on-twitter-week-ending-march-27th-2a18cf442878;John SwainFollowMar 28, 2016·5 min readBrexit debate on Twitter Week Ending March 27thDuring the period 21st March to 27th March 2016 we collected approximately 300k Tweets relating to the Brexit” referendum in the UK.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comHere is a summary of our analysis of the most influential Twitter Users during the period together with some commentary on interesting observations.Top InfluencersNetwork MapIn a previous analysis we noticed that the two main groups were a strongly united Leave group and a group consisting of Remain and a wider set of mainstream media Users.Here we see the same split which can be easily observed from the network map.The colours in the map represent the communities that are detected by our community detection software. At a high level the colours show the two main groups.However, communities are detected which consist of smaller groups of Users who share more narrow interests.Here the colours are representing a more detailed view of the sub groups within the main communities.This provides further evidence that the Leave campaign is a more tightly knit group with less diverse sub groups than the Remain campaign.Some Interesting ObservationsFirstly let’s look at the Top Overall top 10.There are three very intriguing names in that list.Polly Toynbee (@pollytoynbee) | TwitterThe latest Tweets from Polly Toynbee (@pollytoynbee). Polly Toynbee is a Guardian columnist, formerly BBC social…twitter.comKatie Hopkins (@KTHopkins) | TwitterThe latest Tweets from Katie Hopkins (@KTHopkins). Columnist @MailOnline. LBC Radio Host. CBB Runner Up. Gained & lost…twitter.comAllison Pearson (@allisonpearson) | TwitterThe latest Tweets from Allison Pearson (@allisonpearson). Telegraph columnist, apprentice mother, words nut, author I…twitter.comWhy were these three powerful women influential in the Brexit debate this week?First point to note is that the network map provides an indication that they might all have differing views on the debate.Polly ToynbeeHere is what we can quickly see from our dashboard on the Tribe that grouped around Polly Toynbee.Here is a list of more of the top Tweets.And here is the article that Polly Toynbee published.Katie HopkinsKatie Hopkins also made a Tribe and this time we can get an indication of the main topics of interest from the Topics section.The top Tweets in this TribeOne of the Tweets Katie Hopkins sent which had a large impact.Allison PearsonAllison also formed a community and was also focused on the security issue here is an example Tweet.Different StylesWe noticed something quite interesting about how the three women achieved their influence.Here are the three Ego Networks for each of them. An Ego Network is a network which shows all the other Users in the network that are just one jump away from them.Polly ToynbeeThis is a small network with one other Influential User in the network The Guardian” which has 5m followers. The influence of Polly Toynbee would seem to be inextricably linked to the Guardian Newspaper.Katie HopkinsThis is a much bigger network. The second most visible user in this network is David Vance who has 20k followers. In Katie Hopkins case this look very much more like a popular following of ‘ordinary’ people.Allison PearsonThis network is also quite big and has one other main User in it — Nigel Farage who has 260k followers and is a key player in this debate. The conclusion you might draw from this is that Allison Pearson is an ally of Nigel Farage.Conclusion.Everything is Obvious *Once You Know the Answer.The fact that Polly Toynbee is an influential journalist working for the Guardian does not seem like much of a revelation if you are familiar with the domain of UK political journalism.However, this analysis was done from a starting point of zero knowledge of Polly and her role in this debate. The analysis is done mostly through machine learning with the network maps and dashboard tool illustrated provided to a very inexpert analyst — me!I hope this illustrates the power of the tools and how information can be uncovered about influencers and the stories they are influencing.For an expert analyst of the UK Brexit campaigns there are much deeper insights which can be gained rapidly using these tools.For more information about the methodology see this presentation which I prepared for Oxford University.Social Network Analysis PresentationFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Mar 28, 2016;[]
https://medium.com/neo4j/neo4j-apoc-release-export-streaming-support-delete-custom-procedures-new-string-functions-82224c68be78;Mark NeedhamFollowNov 19, 2019·5 min readNeo4j APOC Release — Export Streaming support, Delete custom procedures, New string functionsThe latest release of APOC introduces streaming support for exporting to JSON and GraphML, as well as other features.This week we released version 3.5.0.6 of APOC, Neo4j’s standard library.APOCtldrThis release contains the following features and bug fixes:streaming support for export JSON and GraphMLstring multiplication and fill functionsremoving custom procedures and functionsnull issues with apoc.coll.containsAll and Mongo procedureapoc.periodic.repeat has improved reporting for bad queriesapoc.graph.fromDocument considers whitelist property mappingsYou can install the latest release directly from the Neo4j Desktop in the ‘Plugins’ section of your project. Jennifer Reif also has a detailed post explaining how to install plugins if you haven’t used any yet.Installing the APOC LibraryIf you’re installing the library on a server installation of Neo4j you can download the JAR from the GitHub releases page. This page also includes the release notes.GitHub releases pageStreaming support for export JSON and GraphMLAPOC supports exporting data to a variety of different formats, including JSON, GraphML, CSV, and Cypher script.It can export data in those formats into files or as a stream of results.Exporting to a file is a useful feature, but only works if we are able to write to the file system on the server where Neo4j is hosted. If we don’t have that ability then the streaming approach is the way to go.This release adds missing stream functionality for the JSON and GraphML export formats.Assuming that we’ve imported the movies graph (:play movies), the following query exports the nodes and relationships around Tom Cruise and the movies that he acted in:CALL apoc.export.json.query(   MATCH path = (p:Person {name: $name})-[:ACTED_IN]->(movie)    RETURN path ,  null,  {stream: true, params: {name:  Tom Cruise }})YIELD dataRETURN dataIf we execute that query, we’ll see the following output:Export streaming JSONWe could then write that stream to disk on the client, use the data to update our application, or send it into a tool like jq to do further filtering and analysis.Repeating TextThis release also adds a couple of new functions that can be used to generate arrays or strings of repeated text.apoc.text.repeat concatenates a string a certain number of times. For example, the following function call will create a string with the value Aura repeated 5 times:RETURN apoc.text.repeat( Aura , 5) AS result╒══════════════════════╕│ result               │╞══════════════════════╡│ AuraAuraAuraAuraAura │└──────────────────────┘apoc.coll.fill is similar, but creates an array of repeated values. For example, the following function call will create a list with the value Neo4j Aura repeated 3 times:RETURN apoc.coll.fill( Neo4j Aura , 3) AS result╒════════════════════════════════════════╕│ result                                 │╞════════════════════════════════════════╡│[ Neo4j Aura , Neo4j Aura , Neo4j Aura ]│└────────────────────────────────────────┘Deleting custom procedures and functionsLet’s declare a function that tells us how long it is until 2020:CALL apoc.custom.asFunction( countdown ,    RETURN duration.inDays(date(), date(2020-01-01)) AS duration ,   Duration ,   [],   true)We can now list all the custom functions and procedures on our database:CALL apoc.custom.list()List of functions and proceduresLet’s run the function to see how many days until the new year:RETURN custom.countdown().days AS days╒══════╕│ days │╞══════╡│43    │└──────┘Not long to go! Now let’s use the new apoc.custom.removeFunction to remove the function:CALL apoc.custom.removeFunction( countdown )Now if we try to run it we won’t be able to:RETURN custom.countdown()Unknown function custom.countdownIf we want to recreate a function with the same name, we’ll also need to flush the query cache to make sure references to any old version of the function are removed:CALL apoc.custom.asFunction( countdown ,    RETURN duration.inDays(date(), date(2020-01-01)) AS duration ,   Duration ,   [],   true)CALL dbms.clearQueryCaches()Bug FixesThis release also contains several bug fixes, including:apoc.graph.fromDocumentThe apoc.graph.fromDocument procedure now considers the mappings field when creating a graph structure from a JSON document. The following query creates the graph structure (:User)<-[:USER]-(:Tweet):CALL apoc.graph.fromDocument({  id : 1,  text :  Very interesting tweet ,  data :  02-11-2019 ,  user : {  id : 1,  screenName :  conker84 ,  name :  Andrea  },  geo : {  latitude : 11.45,  longitude : -12.3 } }, {  mappings: { `$`:  Tweet{!id, text} ,               `$.user`:  User{!screenName, name}  },  write: true})Executing this query will result in the following graph:Small twitter graphBefore this release the user field would not have been properly processed.apoc.periodic.repeatThe apoc.periodic.repeat procedure also has better error reporting when the query provided has syntax errors. For example, the following query submitted to this procedure passes a String rather than a Map to the apoc.periodic.iterate procedure:CALL apoc.periodic.repeat(    brokenJob,    CALL apoc.periodic.iterate(       MATCH (n) RETURN n ,        WITH n SET n.foo = \mark\ ,         ),    120)Failed to invoke procedure `apoc.periodic.repeat`: Caused by: org.neo4j.cypher.internal.v3_5.util.SyntaxException: Type mismatch: expected Map, Node or Relationship but was String (line 4, column 9 (offset: 108))There are also some fixes around handling of null values for the apoc.mongo.* and apoc.coll.containsAll procedures.We hope these changes improve your experience with the library, and if you have any questions or suggestions please let us know in the APOC category of community.neo4j.com.Cheers, Mark and Jennifer Reif;Nov 19, 2019;[]
https://medium.com/neo4j/summer-of-nodes-week-2-the-online-day-out-944a57c6b7b4;Ljubica LazarevicFollowAug 10, 2020·6 min read*Now updated with hints and solutions!*Summer of Nodes: Week 2 — The online day outMissed the live stream? Catch up here!Missed the hints stream? Catch up here!Missed the solutions stream? Catch up here!Series playlistHello everybody!Summer of Nodes 2020 is now over. If you’ve not had a chance to look at the challenges, you can always have a go at your leisure:The socially distanced barbecueThe online day out (this post)The whodunitExploring the areaThis week’s theme — the online day outUnsurprisingly, during the summer months, many of us take the opportunity to get those holidays in, be it at the beach, or to indulge in some city getaway. Many of us whilst away will take the opportunity to learn more about the history of the area we are visiting, or soak up some culture in a local museum.During these unprecedented times, many museums have either been opening up or reminding us all about their collections, many of them being viewable online.This week’s Summer of Nodes challenge is going to be about exploring such online collections. We’ll learn how graphs make it easy for us to find objects we want to look at, based on our preferences.These challenges should take approximately an hour each, depending on experience. Of course, you are welcome to try both challenges!Don’t forget, on Thursday we’ll be providing some hints and tips, just in case you need them.Beginner’s challenge — Blooming discoveryIntroductionThere are many museums that have made their collections and catalogues available online. The Metropolitan (Met) in New York provides data on their collection, as well as some images, under their Open Access. With this data at your fingertips, it is a fantastic opportunity to explore a wealth of art from the comfort of your sofa.This week’s challenge, you will be building a Bloom perspective to answer some questions!You will be:Following instructions on how to load a data set.Building a Bloom perspective for that data set.Answer some questions using Bloom!Using BloomIf you’re new to Bloom, don’t panic! Bloom is very user-friendly and intuitive to use. This is a great opportunity to get comfortable with this visualisation tool.We recommend that you use the Neo4j Sandbox for this challenge, as everything will be set up for you to get started. You can access the blank sandbox here.The following resources might be helpful in getting up and runningSummary blog post on Bloom 1.3Getting started with BloomBuilding a Bloom perspective Twitch stream videoBloom tips and tricksI am completely new to Neo4j, and Sandbox and Bloom, and I don’t know how to get started. Help!Here’s a short video showing you the ropes!Blooming discoveryWe would like you to:Load some data into a Neo4j database using this load script. If you are not using the Sandbox, make sure you enable multi statement query editor.Create a Bloom perspective — you will submit a screenshot from one of the questions belowOnce you have completed that, answer the following questions:How many different classifications are there?How many paintings have more than one artist?What instrument is in the painting found in the Musical Instruments department?How many painting titles start with ‘Harbor’?Add a search phrase within Bloom to help you find all paintings that contain a specific word.With the search phrase, how many paintings have ‘Harbor’ anywhere in their title?You may find the following data model useful for reference when exploring with Bloom.What are we looking for?Suitable icons assigned to the node categoriesSensible captions on display for the node categoriesAnswers to the questions!Can I have some hints please?Of course! We suggest you watch the catch up. If you need a bit more help, you can always post on the forum.SolutionsHere are the answers to the questions:How many classifications? 23Paintings with more than artist? 103 (you may get slightly less due to limits)Instrument in the painting? GuitarPaintings that start with Harbor: 5Paintings with Harbor in the title: 18 (you may get 17 due to limits)Experienced challenge — a voyage of discoveryIntroductionAs mentioned earlier, there is a wealth of art to explore from the comfort of home through the Open Access scheme. We are going to build upon the paintings data, importing more data and look at the different collections from the amazing collections from The Met.You and a group of friends are planning to host a virtual viewing of The Met paintings, but before you can figure out what you’re going to look at, you need to do some research! You all have different things that you like to look at, so you’re going to have to use the graph to determine what paintings you are going to be viewing together!You will be loading the data we give you. You will need to do a bit of processing along the way. You will need to decide what is a suitable data model, based on the demanding requirements your friends provide for the virtual viewing!The friends viewing requirementsFreya likes the use of gold as a medium.Grace is a fan of extra large (XL) paintings.Tareq likes paintings from the romantic period (assume Artist endYear range of 1725–1890).Mateo likes paintings with birds.Identify all the paintings that meet at least 3 out of the 4 of the above criteria!The dataThe data we are using is a reduced set from The Met’s Open Access. Please only use this cut for the challenges.What we’re expectingYou may use standard Neo4j plugins.Tokenize Mediums, Tags, and Artists.Use the regular expression in the below code block for dimensions.Process sizes into buckets (cm²): XS: < 100, S:>= 100, <500 , M:>=500,<2800, L:>=2800, <11150, XL:>=11150.Only include paintings that have at least 2 dimensions, assume first two are height and width.Where there’s multiple measurements, use the first one.Avoid any paintings that don’t have at least one clearly-named artist.You may reuse and adapt the import script from the beginner’s challenge.Answer as a list of Painting IDs in ascending order.\\(( *[0-9]+(\\.[0-9]+)*( *[×x] *[0-9]+(\\.[0-9]+)*)) *cm\\)Please use the regular expression above — if it doesn’t work, check the hyphens, Medium does strange things to them…A hint for me too, please!Indeed! We suggest you watch the catch up. If you need a bit more help, you can always post on the forum.Kingfisher and Bamboo, Sesshu Toyo, The Metropolitan Museum of ArtSolutionsThis is the data model we went for. There are many different ways to do this!You can load the the data using this script.The approach we used here was to set a label of Interest to all the nodes that meet our viewing criteria:MATCH (i:Interest) remove i:InterestMATCH (n:Tag {name: Birds }) SET n:InterestMATCH (n:MediumWord {name: gold }) SET n:InterestMATCH (n:PaintingSize {size: XL }) SET n:InterestMATCH (n:Artist) WHERE n.endYear>=1725  and n.endYear<= 1890  SET n:InterestThen find the paintings that meet 3 out of 4 of those criteria:MATCH (p:Painting)-->(i:Interest)WITH p, collect(i) AS col WHERE size(col) >2RETURN p.id AS id ORDER BY idThe list of painting ids (as requested!): 12944”, 65595”, 39894”, 65594”, 193951”, 193992”, 436007”, 436545”, 436607”This is your voyage of discovery tooDon’t rush through!Whilst we’ve provided a subset of the data, we encourage you to not only explore this, but perhaps also consider importing the complete, latest set from the Metropolitan Museum after you’ve finished the challenge. You can, of course, continue to explore the 4000+ paintings in the reduced data set.If you have the time and opportunity, why not look through the artwork that is of interest to you!Both the original data and the reduced version used for the challenges have URLs to the object’s description. These are clickable both within Neo4j Browser and Bloom!;Aug 10, 2020;[]
https://medium.com/neo4j/solve-problems-virtually-in-neo4j-163e0498defc;Shaani Arya SrivastavaFollowOct 20, 2022·2 min readSolve Problems Virtually in Neo4jIn this blog, we will present the solution for real-world scenarios virtually using a Neo4j graph.Many times, we developers come across scenarios where we try to show the graph in such a way that it hides all the intermediate nodes/relationships in the model, revealing the abstracted information relevant to business users (without using any visualization tool), without modifying the graph model.To be precise, this means showing the direct relationship between any two nodes based on the intermediate patterns (which may not need to be exposed).The answer is hidden in the Neo4j APOC library. It has the capability to virtually project nodes and relationships.First, let’s ingest some sample data:MERGE(n:Traveller{name:JustAnotherTraveller})MERGE(a:Place{place:Munnar})MERGE(b:Place{place:Himalayas})MERGE(c:Place{place:Mount Abu})MERGE(d:Place{place:Mount Everest})MERGE(n)-[:TRAVELLED{travelDate:DATE(2021–10–10)}]->(a)MERGE(n)-[:TRAVELLED{travelDate:DATE(2021–12–11)}]->(b)MERGE(n)-[:TRAVELLED{travelDate:DATE(2022–02–10)}]->(c)MERGE(n)-[:TRAVELLED{travelDate:DATE(2021–05–20)}]->(d)The ingested data will look like this.Real problem statement:In this blog, we will show the timeline journey of any given traveler in the database (in this case, a traveler with the name ‘JustAnotherTraveler’). How can we do that without modifying the model?Below is the Cypher code snippet to output the entire journey of the traveler.The general syntax for the function is:apoc.create.vRelationship(startNode,REL_TYPE, {key:value,…},endNode)WITH JustAnotherTraveller as travellerNameMATCH (t:Traveller{name:travellerName})-[r:TRAVELLED]->(p:Place)WITH t,p,r.travelDate as travelDate ORDER BY r.travelDateWITH t, collect([p,travelDate]) as journeysWITH t,journeyList,size(journeys) as lRETURN t,       [x IN range(0,l-1)|journeys[x][0]],  apoc.create.vRelationship(t,TRAVELLED_TO,                    {travelledOn:journeys[0][1]},journeys[0][0]),       [x IN range(0,l-2)|  apoc.create.vRelationship(journeys[x][0],TRAVELLED_TO,                   {travelledOn:journeys[x+1][1]},journeys[x+1][0])]Journey of the travellerWITH ‘JustAnotherTraveler’ as travelerName — we can parameterize the traveler name.In the Return statement, we are returning all the nodes from the collect list journeys using range [x IN range(0,l-1)|journeys[x][0]] and then creating virtual relationship at each level using another range [x IN range(0,l-2)|apoc.create.vRelationship(journeyList[x][0],’TRAVELLED_TO’,{travelledOn:journeys[x+1][1]},journeys[x+1][0])].The best part is that it’s generalized for any traveler present in your database. All we need to do is run this query and his/her entire journey will be on our screen. Also, we can use this Cypher as a source query to render the virtual graph on the visualization tool as well (if it supports Cypher graph results).The virtual concept is actually being used as a popular feature in many of the graph visualization tools like Bloom, Hume etc., which actually helps developers answer certain business problems keeping graph abstraction in place.You can reach out to me on my Linkedin profile for any discussions/ideas on Neo4j or graph use cases!;Oct 20, 2022;[]
https://medium.com/neo4j/stay-connected-be-a-part-of-global-graph-celebration-day-2021-d11539b1224d;Jennifer ReifFollowMar 26, 2021·4 min readStay Connected: Be a Part of Global Graph Celebration Day 2021!On April 15, 2021, Neo4j is again leading the Global Graph Celebration Day for the 3rd year! We have some exciting activities planned for our event this year and would love to have you there. If you are experienced in all things Neo4j or brand new to graphs, Global Graph Celebration Day 2021 provides you with knowledge and community collaboration. We hope it will inspire you to build amazing things with connected data!What is Global Graph Celebration Day (GGCD), you might ask? To get the most out of the day, it probably helps to know the backstory!What Is GGCD?We don’t know exactly when the concept of graph theory was born, but Leonard Euler (pronounced OY-lər) studied and documented the ideas of graph theory with the Seven bridges of Königsberg problem. Euler was an influential mathematician, pioneering many areas in mathematics and the sciences. In his work on the Seven bridges of Königsberg, he attempted to discover a way to visit the areas of the city (mainland and two islands) by crossing each of the seven bridges only once and returning to the starting point. Unfortunately, the solution to this problem is not possible. However, his work documented the science of graph theory that is the foundation of so much today!Caption: Königsberg map, seven bridges diagram, translated to graphGraph theory paved the way for graph databases as the solution for complex problems in connected data. The growth of graph databases continues its upward trend over the last several years. Check out the full list of charts measuring graph technologies on DB-Engines.com.To celebrate graph theory and its contribution to the world, we highlight Leonard Euler’s birthday (April 15) and his study of graph theory! Graph theory has made such an impact on our world, not only in Neo4j as a product, but also in the general principles of network science. After all, graph theory is much older than the database, but the principles of connected data exist and have been explored in social studies, technology, medicine, economics, mathematics, business, and so much more!How Does Neo4j Celebrate?In past years, we have helped our graph enthusiasts in communities around the world organize meetups, viewing parties, and other types of events. These groups share excitement, inspiration, projects, and learning experiences with each other, and Neo4j supports and assists in any way we can.This year is no different, as we encourage our graph communities to help us celebrate the contributions graphs have made in our personal and professional lives. Neo4j is hosting an extended meetup, so that everyone around the world can all join in together! We will have guests from around the world to share their stories, and there will be channels on Discord and the community forum for everyone to interact and chat. Stay tuned for more details in the near future. We are still looking into options for virtual get-togethers and will update you soon on ways to participate!Throughout the livestream, there will be plenty of room for sharing stories and experiences of graphs and Neo4j. We are looking forward to featuring some amazing community members for interviews, providing ways for everyone to participate in the community. There will also be some announcements around upcoming events and Neo4j technologies!Here’s the planned schedule for the event:History of the graph, followed by a fun quiz with **prizes**Interviews with the community members from around the worldGraphQL Community Meeting with a very exciting announcement!Wrapping Up!We are looking forward to virtually connecting with you all and hearing about all the amazing things the community is doing with graphs. Join us and share in the fun, excitement, and learning!Day-of-event important links:Event page for GGCD 2021!Discord event linkCommunity Forum event linkOther resources on Global Graph Celebration Day:Past GGCDs: 2019 recap and 2020 announcement2020 GGCD activities: community graphHistory: Leonard EulerGraph theory: 7 bridges of Königsberg;Mar 26, 2021;[]
https://medium.com/neo4j/experimental-a-networkx-esque-api-for-neo4j-graph-algorithms-4002baac45be;Mark NeedhamFollowJun 8, 2018·4 min readA NetworkX-esque API for Neo4j Graph AlgorithmsA few years ago when I first started learning Python I came across the NetworkX library and always enjoyed using it to run graph algorithms against my toy datasets.Nowadays Neo4j has its own Graph Algorithms library but we have to call that via Cypher procedures which isn’t quite as nice as calling it from Python functions.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comAs a result, acouple of months ago I started writing a NetworkX-esque API that would provide a nice wrapper around Neo4j’s algorithms.It’s still in the experimental stages so if you want to install it you’ll have to do so directly from GitHub. The following command will do the trick:pip install git+https://github.com/neo4j-graph-analytics/networkx-neo4j.git#egg=networkx-neo4jOne you’ve done that you’ll need to create a driver that points to your Neo4j server.Launch Graph of Thrones SandboxI’m going to show you how to point to a Neo4j sandbox instance that has a preloaded Game of Thrones dataset that we can play with. If you want to follow along you’ll need to go to neo4j.com/sandbox and install the Graph Algorithms Sandbox.Graph Algorithms SandboxOnce that’s launched you should see something like this:Click on the ‘Code’ and then ‘py’ tabs:Here we have the details that we can use to create our driver. Copy the lines down to ‘driver =’ into your Python script or execute them in your Python terminal.from neo4j.v1 import GraphDatabase, basic_authdriver = GraphDatabase.driver(     bolt://54.174.242.100:36186 ,    auth=basic_auth( neo4j ,  invention-airship-gunnery ))The values for your server will be different than mine so make sure you update them appropriately.Using networkx-neo4jNow we’re ready to start using networkx-neo4j. Let’s first import the module:import nxneo4jConfiguring our graphNext we’re going to create a map explaining the node labels, relationship types, and properties used in the Graph of Thrones.config = {     node_label :  Character ,     relationship_type : None,     identifier_property :  name }G = nxneo4j.Graph(driver, config)We set:node_label toCharacter so that we’ll only consider nodes with that labelrelationship_type toNone so that we’ll consider all relationship types in the graphidentifier_property is the node property that we’ll use to identify each node from the networkx-neo4j APIPageRankNow it’s time to start running some algorithms! We’ll start with the famous PageRank algorithm. Let’s find out who the most influential characters in Game of Thrones are:sorted_pagerank = sorted(nxneo4j.centrality.pagerank(G).items(), key=lambda x: x[1], reverse=True)for character, score in sorted_pagerank[:10]:    print(character, score)Tyrion-Lannister 11.6290205Stannis-Baratheon 7.2328375Tywin-Lannister 6.8489435Varys 6.144811999999999Theon-Greyjoy 4.654753499999998Sansa-Stark 4.237233499999999Walder-Frey 3.2422405Robb-Stark 3.0707785Samwell-Tarly 2.9794970000000007Jon-Snow 2.920541Hopefully there aren’t too many surprises there!Shortest PathWhat about if we want to find the shortest path between two characters?nxneo4j.path_finding.shortest_path(G,  Tyrion-Lannister ,  Hodor )[Tyrion-Lannister, Robb-Stark, Hodor]Notice that we refer to nodes by their name property — this is where the identifier_property that we defined in our config map is used.Finding CommunitiesWe can also partition the characters into communities using the label propagation algorithm:communities = nxneo4j.community.label_propagation_communities(G)sorted_communities = sorted(communities, key=lambda x: len(x), reverse=True)for community in sorted_communities[:10]:    print(list(community)[:10])[Josmyn-Peckledon, Belwas, Rafford, Polliver, Petyr-Frey, Tristifer-IV-Mudd, Jeyne-Heddle, Urswyck, Falyse-Stokeworth, Hoster-Blackwood][Trystane-Martell, Blue-Bard, Matthos-Seaworth, Marya-Seaworth, Mors-Umber, Jaehaerys-I-Targaryen, Myrcella-Baratheon, Justin-Massey, Denys-Mallister, Clayton-Suggs][Oberyn-Martell, Nurse, Tommen-Baratheon, Tanda-Stokeworth, Garlan-Tyrell, Morgo, Qavo-Nogarys, Moon-Boy, Leonette-Fossoway, Allar-Deem][Owen, Jon-Snow, Gerrick-Kingsblood, Lanna-(Happy-Port), Maekar-I-Targaryen, Gorne, Arron, Arson, Satin, Rast][Asha-Greyjoy, Palla, Squirrel, Tristifer-Botley, Yellow-Dick, Lorren, Jason-Mallister, Benfred-Tallhart, Kyra, Gynir][Harras-Harlaw, Baelor-Blacktyde, Dunstan-Drumm, Ralf-Stonehouse, Gorold-Goodbrother, Rodrik-Harlaw, Talbert-Serry, Sigfryd-Harlaw, Rodrik-Sparr, Wulfe][Alliser-Thorne, Othell-Yarwyck, Jaremy-Rykker, Ragwyle, Craster, Clubfoot-Karl, Blane, Donal-Noye, Halder, Mag-Mar-Tun-Doh-Weg][Tomard, Horton-Redfort, Lothor-Brune, Myranda-Royce, Grisel, Merrett-Frey, Loras-Tyrell, Nestor-Royce, Anya-Waynwood, Marillion][Marq-Piper, Rickard-Karstark, Margaery-Tyrell, Senelle, Hallis-Mollen, Harren-Hoare, Nan, Colen-of-Greenpools, Desmond-Grell, Edmure-Tully][Koss, Woth, Meralyn, Mad-Huntsman, Dobber, Ravella-Swann, Ternesio-Terys, Yoren, Amabel, Waif]I’ve included more information about which algorithms are available in the README of the networkx-neo4j GitHub repository. There’s also a Jupyter notebook which has examples of all the available algorithms against this dataset.If you get the chance to play around with this and have any feedback please let me know in the Issues section of the repository or send us an email to devrel@neo4j.com.Free download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Jun 8, 2018;[]
https://medium.com/neo4j/open-measurement-full-text-search-in-197m-chemical-names-graph-database-3266300cd339;Tom NijhofFollowJul 28, 2022·6 min readFull-Text Search in 197M Chemical Names Graph DatabasePubChem is a database with millions of chemical compounds. All these can be downloaded and put into your graph database as a basis for your project.I downloaded 197M synonyms related to 57M compounds for the Open Measurement project.Open MeasurementThis project is being built as part of Open Measurement”, a platform to share measurements of biological experiments with others. The challenge comes from the many different formats and structures people use. This makes it hard to find out if other people did a similar experiment to yours.In this blog, I will look at building a compound searcher. The goal is that different synonyms of the same compounds can be linked to each other.Techniques: Neo4j graph database, full-text search, Lucene, data wranglingLoading the DataThe data is given in turtle (.ttl) format. For the synonyms, there are 18 files with each over 10M rows. Each row connects a synonym value (name as a string) and an ID (MD5 encoding of the name).Next to those are 11 turtle files that link synonyms to compounds. The first 10 files also have 10M rows with the synonym ID and the compound ID (PubChem ID).The data has a few challenges:Empty strings for synonyms (957 synonyms)Duplicates of synonyms (1,591,056 synonyms)Synonyms with different md5 encoding as their ID (5,865 synonyms)There are more synonyms than compounds linked to synonymsNeosemantics (n10s) is a great tool to load in RDF files like turtles, but these files are too big (for my computer)To solve these I made a python script that read all synonyms, then deleted the MD5 mismatches, duplicates, and empty strings. It saved them as smaller CSV files and load those into neo4j with LOAD CSV”. Because I already dropped the duplicates I can use CREATE” instead of MERGE” to save time. This process only takes 2 to 3 hours. This results in 197M synonyms.For the compounds, I do a similar thing. I take all compounds from the synonym2compound” files and remove the duplicates. Using the same CSV trick this process takes 1 to 2 hours and results in 57M compounds.Both synonyms and compounds are constrained to be unique and indexed on ID.Next come the connections between synonyms and compounds, loading them all in smaller CSV, using MERGE” to be sure no doubles are introduced. This process is taking days, maybe weeks (not finished when I started to write this).This takes so long because of 2 factors, for every connection the synonym and compound need to be found. Given they are both in a binary tree those are log(197M)=28 and log(57M)=26 searches. This already makes 50 times more operations than creating the nodes, merging can slow it down a bit more but the biggest one is the fact that I only had space on my HDD, not SSD meaning the I/O operations are killing me here.Neo4j setting:dbms.memory.heap.initial_size=2Gdbms.memory.heap.max_size=6Gdbms.memory.pagecache.size=2GData explorationSynonyms can have a relation to multiple compounds. An example is destim” which is related to compounds 9306 and 66124. These two compounds are stereoisomerism but the name refers to both of them.PS, this does NOT apply to the compound 12213639, which is also a stereoisomerism of destim but destim does not revert to this compound.Not all synonyms have a relation to a compound. This can already be seen by the mismatch in synonym files (18), and synonym-to-compound files (11).Synonyms have, besides a name, also a type. For instance, oxydrene has the type PubChem depositor-supplied molecular entity name”. This is not yet used in this version of the database.Compounds can have 2d and 3d similarity metrics to each other. This holds great potential for finding a cluster of similar chemicals.Full-Text Search QueryThe first part is a standard tutorial we create a full-text index, this will use Lucene for us in the background.CREATE FULLTEXT INDEX synonymsFullText FOR (n:Synonym) ON EACH [n.name]Basic Fuzzy Full-Text QueryThis means I can now use the Lucene query languages to find Synonym nodes by their name property. Lucene splits every index word into a given string. A normal search will return all nodes with an exact match of the word. When you add ~ after the word it will become a fuzzy match. Resulting in spelling mistakes being corrected.If I do this for pitavastatan” while we mean pitavastatin it will work.CALL db.index.fulltext.queryNodes( synonymsFullText ,  pitavastatan~ ) YIELD node, scoreRETURN node.name, score limit 5One Synonym Per Compound QueryWhen I do this for pitavastatin” we find 5 synonyms that match pitavastatin” the best, but 3 of those are synonyms connected to the same compound. Meaning the autocomplete just gives you 3 unique compounds as options instead of 5.To solve this we are going to use the graph properties. We still are going to look for the synonyms but now take the compounds related to it. We only take one synonym per compound (the one with the highest full-text score). And then apply the limit of 5. This results in 5 unique compounds per search.CALL db.index.fulltext.queryNodes( synonymsFullText ,  pitavastatin~ )YIELD node, scoreMATCH (node)-[:IS_ATTRIBUTE_OF]->(c:Compound)WITH DISTINCT c as c, collect({score: score, node: node})[0] as sRETURN s.node as name, s.score, c as compoundId limit 5One Unique Synonym Per Compound QueryIf a synonym has multiple compounds (like destim”) it will return all compounds as a result. For example, destim will result in destim cid9306” and destim cid66124”.To combat this we will collect all the compounds related to the same synonym. Meaning the user will not have duplicates in their options. All related compounds are returned so users can be warned about it.CALL db.index.fulltext.queryNodes( synonymsFullText ,  destim~ )YIELD node, scoreMATCH (node)-[:IS_ATTRIBUTE_OF]->(c:Compound)WITH DISTINCT c as c, collect({{score: score, node: node}})[0] as sWITH DISTINCT s as s, collect(c.pubChemCompId) as compoundIdRETURN s.node.name as name, s.node.pubChemSynId as synonymId, compoundId limit 2Results:{‘name’: ‘destim’,‘synonymId’: ‘37d400b013a5db675a6fa80c629b8552’,‘compoundId’: [‘compound:cid66124’, ‘compound:cid9306’]},{‘name’: ‘bestim’,‘synonymId’: ‘9c6c93d3b365a6768fb5a1f4285b7c1a’,‘compoundId’: [‘compound:cid3038501’]}Optimize QueryAt the moment the query will match all nodes that have some match with the full-text search. This can become a lot if multiple words are involved or the matches afterwards do not add up to the limit of 5.To reduce the time we first call the full-text search and limit the results to 50. This means the matching afterward will never have to deal with more than 50 nodes. Speeding up the query and reducing its memory usage.CALL {  CALL db.index.fulltext.queryNodes( synonymsFullText ,  hexahydro~   AND aminophenyl~ AND indeno~ AND pyridine~ )  YIELD node, score  RETURN node, score limit 50}MATCH (node)-[:IS_ATTRIBUTE_OF]->(c:Compound)WITH DISTINCT c as c, collect({score: score, node: node})[0] as sWITH DISTINCT s as s, collect(c.pubChemCompId) as compoundIdRETURN s.node.name as name, s.node.pubChemSynId as synonymId, compoundId limit 5Splitting the WordsWhen calling the query above we need to find out what words we need to search for. For example, in (+)-2,3,4,4a,5,9b-hexahydro-5-(4-aminophenyl)-1h-indeno(1,2-b)pyridine” everything which is not a combination of letters or numbers is discarded by Lucene full-text search, all letters that are not in the English alphabet will be replaced with English letters (öáì will become oai).This splitting is done in the backend with regex [\p{L}\d]{2,}”. It will search for groups of Unicode characters (\p”) that are classified as letters ({L}”) or a digit (\d”) with a length of 2 or more ({2,}”). All of these will be followed by a ~” and joined with an AND” so Lucene knows to fuzzy search for every word.Because I remove all special characters it will also be more protected against Cypher injections.Result front endAll of this together with a bit of front-end magic gives us the following application. We can see that asprin” and aspirine” are not detected at the same time, and spelling mistakes are fixed.;Jul 28, 2022;[]
https://medium.com/neo4j/from-nodes-to-rows-a-guide-to-querying-graph-databases-in-pandas-sql-style-using-cypher-fae25be8c1f9;Saloni GuptaFollowMar 4·9 min readFrom Nodes to Rows: A Guide to Querying Neo4j Graph Database in Pandas/SQL Style Using CypherPhoto by Galina Nelyubova on UnsplashWell, you might be wondering: Why would one want to write Pandas or SQL-style queries while working with a graph database like Neo4j? Does it not defeat the purpose of having data stored and ingested in a graph DB in the first place? No, it doesn’t.Graph databases excel at storing and querying complex, connected data that is difficult to represent in a traditional tabular database. However, not all data analysts/scientists who wish to analyze a graph database are fluent in Cypher, the query language used by many graph databases.They may be more comfortable performing data analysis in SQL or Pandas, and wish to perform tabular-style data analysis on a graph. In such cases, writing SQL or pandas-style queries in Cypher provides a way to leverage the benefits of the graph database while working within a familiar framework.Additionally, there may be a need to first perform exploratory data analysis in SQL or pandas before leveraging the graph structure to gain additional insights.For instance, suppose you have a graph database with millions of nodes and relationships, and you need to perform data analysis or visualization tasks that require working with tabular data. In that case, you can use pandas-style queries to extract data from the graph database and manipulate it in a way that’s more suitable for your needs.This can help you gain a better understanding of the data and uncover patterns or relationships that may not be immediately apparent from a graph structure. Once you have extracted the necessary data, you can use it to make informed decisions about how to interact with the original graph database.Therefore, pandas or SQL-style queries in Cypher do not defeat the purpose of using a graph database. Instead, they provide a way to work with graph data in a more flexible and convenient way, without sacrificing the benefits of using a graph database.Now, let’s get to the fun part — writing Cypher queries for common analysis operations performed on tabular data. We will be running queries on the sample Northwind Graph provided by Neo4j for exploration purposes.Northwind ModelTo load this dataset into Neo4j please follow the instructions here: https://neo4j.com/graphgists/northwind-recommendation-engine/.To run the pandas code, I used the dataset available here: https://github.com/graphql-compose/graphql-compose-examples/tree/master/examples/northwind/data/csv.[Note by Publication] There is a dedicated graph example dataset for Northwind available here:GitHub - neo4j-graph-examples/northwind: From RDBMS to Graph, using a classic datasetDescription: From RDBMS to Graph, using a classic retail dataset MATCH (p:Product)- [ : PART_OF ] ->(:Category)- [ …github.comGetting value counts of a list of values:Sample Question: List the counts of the unique list of product categories provided by each supplier.Pandas equivalent code:# read the data from the CSV filescategories = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/categories.csv)products = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/products.csv)suppliers = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/suppliers.csv)# merge the dataframes on the necessary columnsmerged = pd.merge(pd.merge(products, categories, on=categoryID), suppliers, on=supplierID)# group by supplier and category and count the occurrencesmerged_groupby = merged.groupby([companyName])[categoryName].unique().reset_index()# display the resultsprint(merged_groupby[categoryName].value_counts())SQL equivalent query (if data was stored in a relational DB):SELECT array_agg(DISTINCT c.categoryName) AS Categories, COUNT(*) AS value_countsFROM supplier sJOIN product p ON s.supplierID = p.supplierIDJOIN category c ON p.categoryID = c.categoryIDGROUP BY s.companyNameORDER BY value_counts DESCCypher query:MATCH (s:Supplier)-->(:Product)-->(c:Category)WITH s.companyName as Company, collect(distinct c.categoryName) as CategoriesRETURN Categories, COUNT(*) AS value_countsORDER BY value_counts DESCIf we just wanted the categories as a list (this query is already given in part 4/7 of the playbook), the query would be:MATCH (s:Supplier)-->(:Product)-->(c:Category)RETURN s.companyName as Company, collect(distinct c.categoryName) as CategoriesHowever, to get the value counts of an array, the simple trick is to first write a query to return the array, and then change the RETURN to WITH. Then add a return statement to treat this array as a single entity and just do a COUNT(*). Also, notice how we can easily perform multiple joins in just one MATCH line in Cypher.Cypher output:Snippet of Neo4j Output from Cypher query to List the value counts of unique product categories provided by each supplier2. Joining, grouping, and aggregating data:Sample Question: Which customers have made orders with a total cost greater than $1000?Pandas equivalent code:# read the data into pandas dataframesproducts = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/products.csv)order_details = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/order_details.csv)orders = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/orders.csv)customers = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/customers.csv)# merge the dataframes on common columnsmerged_data = pd.merge(pd.merge(order_details, orders, on=orderID), products, on=productID)merged_data = pd.merge(customers, merged_data, on=customerID)# calculate the total cost for each ordermerged_data[totalCost] = merged_data[unitPrice_x] * merged_data[unitsOnOrder]# group the data by customer and sum the total costcustomer_order_totals = merged_data.groupby([customerID,companyName]).agg({totalCost: sum})# filter for customers with total cost greater than $1000high_spending_customers = customer_order_totals[customer_order_totals[totalCost] > 1000]# print the list of high spending customersprint(high_spending_customers)SQL equivalent query (if data was stored in a relational DB):SELECT CustomerID, CompanyName, TotalCostFROM (  SELECT c.CustomerID AS CustomerID, c.CompanyName AS CompanyName, SUM(p.UnitPrice * p.UnitsOnOrder) AS TotalCost  FROM Customers c  JOIN Orders o ON c.CustomerID = o.CustomerID  JOIN `Order Details` od ON o.OrderID = od.OrderID  JOIN Products p ON od.ProductID = p.ProductID  GROUP BY c.CustomerID, c.CompanyName  HAVING TotalCost > 1000) tCypher query:MATCH (c:Customer)-[:PURCHASED]->(o:Order)-[:ORDERS]->(p:Product)WITH c, SUM(p.unitPrice * p.unitsOnOrder) AS TotalCostWHERE TotalCost > 1000RETURN c.customerID AS CustomerID, c.companyName AS CompanyName, TotalCostAgain, notice how we can easily perform multiple joins in just one MATCH line in Cypher.In this query, we first connect the data belonging to all the relevant nodes, i.e., products, categories, orders, and customers. Then, we group the data by category and calculate the total revenue generated by summing up the product of the quantity and unit price for each order.Cypher output:Snippet of Neo4j Output from Cypher query to get a list of customers who have made orders with a total cost > $10003. Filtering using negation of subquery:Sample Question: What are the product names of all products that have not been ordered by customers from Germany?Pandas equivalent code:# read the data from the CSV filesorders = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/orders.csv)customers = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/customers.csv)products = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/products.csv)order_details = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/order_details.csv)# merge the dataframes on the necessary columnsmerged = pd.merge(pd.merge(pd.merge(products, order_details, on=productID), orders, on=orderID), customers, on=customerID)# find the product names not ordered by customers from Germanynot_ordered_by_germany = merged[merged[country] != Germany][productName].unique()# filter the products dataframe to get the product namesresult = products[products[productName].isin(not_ordered_by_germany)][productName]# display the resultsprint(result)SQL equivalent query (if data was stored in a relational DB):SELECT productNameFROM ProductWHERE productName NOT IN (  SELECT p.productName  FROM Product p  JOIN Orders o ON p.productId = o.productId  JOIN Customer c ON o.customerId = c.customerId  WHERE c.Country = Germany)Cypher query:Version 1: Subquery style using CALL and WHERE NOT {}CALL {  MATCH (p:Product)  WHERE (p)<-[:ORDERS]-(:Order)<-[:PURCHASED]->(:Customer {Country: Germany})  RETURN collect(p.productName) AS productName_list}MATCH (p:Product)WHERE NOT p.productName IN productName_listRETURN p.productName AS ProductNameVersion 2: Concise without CALL {} subquery.MATCH (p:Product)WHERE NOT exists {(p)<-[:ORDERS]-(:Order)<-[:PURCHASED]->(:Customer {Country: Germany}) }RETURN p.productName  AS ProductNameAgain, notice how we can easily perform multiple joins in just one MATCH”line in Cypher. Furthermore, we can see that using CALL{} to write a subquery-style query in Cypher while being a valid option, is not necessary. It may be possible to make the query even more concise without an explicit subquery.Cypher output:Snippet of Neo4j Output from Cypher query to get a list of product names of products that have not been ordered from Germany4. Concatenating data (vertically):Sample Question: List all customers and suppliers in one combined list with their respective names and types of entity.Pandas equivalent code:# Define data for Customerscustomers = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/customers.csv)customers[type] = Customercustomers = customers.rename(columns={customerID: id, companyName: name})customers = customers[[id, name, type]]# Define data for Supplierssuppliers = customers = pd.read_csv(https://raw.githubusercontent.com/graphql-compose/graphql-compose-examples/master/examples/northwind/data/csv/suppliers.csv)suppliers[type] = Suppliersuppliers = suppliers.rename(columns={supplierID: id, companyName: name})suppliers = suppliers[[id, name, type]]# Concatenate dataresult = pd.concat([customers, suppliers], ignore_index=True)# display the resultsprint(result)SQL equivalent query (if data was stored in a relational DB):SELECT customerID AS id, companyName AS name, Customer AS typeFROM CustomersUNION ALLSELECT supplierID AS id, companyName AS name, Supplier AS typeFROM SuppliersCypher query:MATCH (c:Customer)RETURN c.customerID AS id, c.companyName AS name, Customer AS typeUNION ALLMATCH (s:Supplier)RETURN s.supplierID AS id, s.companyName AS name, Supplier AS typeIn this case, both the SQL and Cypher queries are very similar with only minor syntax differences. However, the Pandas version is a bit more verbose.Cypher output:Snippet of Neo4j Output from Cypher query to get a combined list of all customers and suppliers with their respective names and types of entityTakeaways:We can perform the same data analysis that we do using Pandas or SQL , using Cypher queries as well, if the data is loaded into Neo4j in the right way. It is not necessary that Cypher queries are always simpler than their equivalent Pandas code or SQL queries, but many times, they indeed might be.This is because of the following advantages of querying using Cypher:Easier joins: Connecting data is generally easier in Cypher as compared to SQL because Cypher is specifically designed for querying graph patterns and focuses on the relationships between nodes rather than just the properties of nodes. In Cypher, we can traverse the graph and follow relationships to join different nodes together, which is a more natural and intuitive way of joining data when working with graphs. On the other hand, in SQL, joining data involves a lot of complex syntaxes and can be challenging to understand.Flexible pattern matching: Another advantage of Cypher is that it allows for easier pattern matching. You can use the MATCH clause to specify the nodes and relationships you’re interested in, and then use the WHERE clause to filter the results. This can be more flexible than SQL, which requires you to specify the exact columns and tables you want to query. With Cypher, you can focus on the patterns in the data and write queries that capture them more easily.However, it is important to keep in mind that using a graph DB like Neo4j to store and query your data might not be the best choice in all scenarios, especially when data is more structured and less interconnected relationships.But, if your data has complex relationships, and you need to perform complex queries, scale your analysis, or want greater flexibility in your data modeling, then, a graph database like Neo4j would likely be a better choice than a relational database.Writing queries in Cypher might be uncomfortable to begin with, but practicing regularly will make you confident in your Cypher data analysis skills in no time. GraphAcademy courses are a great place to learn Neo4j, with their amazing hands-on in-built exercises. I personally used these a lot on my Cypher learning journey.Neo4j GraphAcademy CoursesThank you for reading my article. If you liked this article, and wish to see more such content, please show some support by following me on Medium.If you would like more specific content on Neo4j or any other graph DB, please comment below with details. Cheers!;Mar 4, 2023;[]
https://medium.com/neo4j/graph-algorithms-release-random-walk-and-personalized-pagerank-80160db3757;Mark NeedhamFollowJul 27, 2018·5 min readNeo4j Graph Algorithms Release — Random Walk and Personalized PageRankEarlier this week we released a new version of the Neo4j Graph Algorithms library, almost a year to the day since the library was launched on 24th July 2017.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comtldrThis release sees the addition of the Random Walk algorithm, as well as support for a basic variant of Personalized PageRank.You can install the latest release directly from the Neo4j Desktop in the ‘Plugins’ section of your project. Jennifer Reif also has a detailed post explaining how to install plugins if you haven’t used any yet.If you’re installing the library on a server installation of Neo4j you can download the JAR from the releases page.Random WalkA random walk means that we start at one node, choose a neighbor to navigate to at random or based on a provided probability distribution, and then do the same from that node, keeping the resulting path in a list. It’s similar to how a drunk person traverses a city.Let’s see how this algorithm works on the movies dataset that comes with Neo4j. If you want to play along you’ll need to run the :play movies command and then click through to import the data.The following query will find 1 random walk of 10 hops starting from Tom Hanks:MATCH (person:Person {name: Tom Hanks”})CALL algo.randomWalk.stream(id(person), 10, 1, {path: true})YIELD pathRETURN pathWe can return a path as we have above, or we can return a list of nodeIds. In the following query we do this and then find the corresponding nodes and output either the name property if one exists (i.e it’s a Person node) or if not then the title property.MATCH (person:Person {name: Tom Hanks”})CALL algo.randomWalk.stream(id(person), 10, 1, {path: true})YIELD nodeIdsUNWIND nodeIds AS nodeIdMATCH (node) WHERE id(node) = nodeIdRETURN coalesce(node.name, node.title) AS nodeSince this algorithm has an element of randomness we’ll get back a different set of nodes than in the first example:By default the algorithm chooses the next neighbour to visit randomly, but you can choose to use a node2vec style probability distribution instead by passing in the parameter mode:node2vec. You can learn more about this algorithm in the documentation.Thanks to Freya Behrens, Sebastian Bischoff, Pius Ladenburger, Julius Rückin, Laurenz Seidel, Fabian Stolp, Michael Vaichenker and Adrian Ziegler of the MetaExp-Project for their work implementing the algorithm.Personalized PageRankThe library has supported the PageRank algorithm since it was released, but we didn’t have support for a common variant called Personalized PageRank.With Personalized PageRank, rather than getting a list of the most influential nodes in a graph as a whole, we get the most influential nodes relative to a set of source nodes that we provide.Personalized PageRank can be used in recommender systems, and the paper WTF: The Who to Follow Service at Twitter explains how they use it to present users with recommendations of other accounts that they may wish to follow.We still use the same PageRank algorithm as before, but we can now pass in a sourceNodes parameter to which we should provide the list of nodes that we want to run Personalized PageRank around.In the following example we’ll run it for Keanu Reeves, but we could pass in more that one node if we wanted:MATCH (p:Person {name: Keanu Reeves”})CALL algo.pageRank.stream(null, null, {direction: BOTH”, sourceNodes: [p]})YIELD nodeId, scoreMATCH (n) WHERE id(n) = nodeId AND n <> pRETURN coalesce(n.name, n.title) AS node,        labels(n)[0] AS label,        scoreORDER BY score DESCLIMIT 5This query return the following results:Not too surprising — those are the movies that Keanu Reeves acted in so we’d expect them to show up.We could filter those results to show the top ranking people:MATCH (p:Person {name: Keanu Reeves”})CALL algo.pageRank.stream(null, null, {direction: BOTH”, sourceNodes: [p]})YIELD nodeId, scoreMATCH (n:Person)WHERE id(n) = nodeId AND n <> pRETURN coalesce(n.name, n.title) AS node,       labels(n)[0] AS label, score,       [node in nodes(shortestpath((p)-[*]-(n))) |        coalesce(node.name, node.title)] AS pathORDER BY score DESCLIMIT 5This query returns the following output:All these people have worked directly with Keanu. What if we want to get some recommendations for people that he might want to work with?We could write the following query:MATCH (p:Person {name: Keanu Reeves”})CALL algo.pageRank.stream(null, null, {direction: BOTH”, sourceNodes: [p]})YIELD nodeId, scoreMATCH (n:Person)WHERE id(n) = nodeIdAND n <> pAND length(shortestpath((p)-[*]-(n))) > 2RETURN coalesce(n.name, n.title) AS node,       labels(n)[0] AS label, score,       [node in nodes(shortestpath((p)-[*]-(n))) |        coalesce(node.name, node.title)] AS pathORDER BY score DESCLIMIT 5This query only returns people that are at least 2 hops away from Keanu Reeves.We’ve added the most basic variant of this algorithm so please let us know if it works for you or if you need some other features supported by sending an email to devrel@neo4j.comBug fixesThis release also contains bug fixes for the formula of the Wasserman Faust version of the Closeness Centrality algorithm, and an issue with the Delta Stepping algorithm when returning distances for unreachable nodes. We’ve also fixed an edge case when loading nodes which link to themselves.The Yens k-shortest path algorithm has also been updated to store weights along with the stored shortest path relationshipsWe hope you enjoy using these new algorithms and if you have any questions or suggestions please send us an email to devrel@neo4j.comFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Jul 27, 2018;[]
https://medium.com/neo4j/build-with-neo4j-and-graphql-at-the-leonhard-euler-idea-contest-cb3d7aa30a69;Tara Shankar JanaFollowApr 15, 2021·5 min readBuild with Neo4j and GraphQL at the Leonhard Euler Idea ContestSubmit Breakthrough Solutions to Win $10k Worth of PrizesAll of us have ideas: the ideas that must be heard, the ideas that improve our lives and perhaps even millions of others. But how often do we act on turning those ideas into a reality?Well, most good ideas don’t go beyond those initial creative thoughts in our mind, and other times we feel like we lack a platform to build them out or showcase them.Announcing the Leonhard Euler Idea ContestIf you’re a professional, aspiring developer working in the field of application development, databases, graphs, artificial intelligence (AI), full-stack, or you’re someone who is passionate about technology, Neo4j is excited to offer you an opportunity to transform your most creative ideas into reality using Neo4j graph database and the Neo4j GraphQL Library.The Leonhard Euler Idea Contest gives you a chance to win $10,000 worth of prizes and get your idea heard by the entire world. The winning project will be featured at NODES 2021, an online developer expo and summit attended by close to 13,000 developers across the globe last year.Check out the rules and terms & conditions of the idea contest, and register right away!Register now!Contest TimelineThe Leonhard Euler Idea Contest is now open for registrations. Submissions will be accepted starting April 27, and closed on June 4. The winning entries will be announced on June 17 at NODES 2021.The ChallengeThe Leonhard Euler Idea Contest is seeking breakthrough ideas and solutions from developers, data scientists, professionals, and students from across the globe. The mandatory criteria for a valid submission is to build the application prototype using Neo4j graph database and the Neo4j GraphQL Library.The ideas you submit are judged on the following parameters:Half the weight (50%) is for the originality of your idea.Complexity accounts for 30% (i.e. level of sophistication yet easy to implement).Feasibility and completeness of your solution accounts for 20%.Idea Contest CriteriaQualifying CriteriaTo qualify for the competition, individuals or teams are required to submit:A working application code on GitHub for judges to check the quality of code (MUST use the Neo4j GraphQL Library).A video of the idea, code walkthrough on YouTube or Vimeo, no longer than two minutes, as a pitch for the competition showcasing your idea.Demo application (if it exists).We encourage you to register early to maximize the time you can spend planning and building out your solution. We are looking for solutions across the whole spectrum of use cases. For inspiration, check out the GRANDstack Starter Project and this podcast application built using GRANDstack.Did you know?Our previous winners did not have Neo4j and or GraphQL experience they used the hackathon as an opportunity to learn.It was key ideas, practicality, applicability, and usefulness that won, not developer skills.This hackathon is a great opportunity to learn all about graph databases, GraphQL, and building modern applications.Get StartedYou are not alone when you start coding and building your ideas. You will have access to:Fill out this formFirst 100 get Aura Free Tier! With this contest, we are also giving the fastest 100 registrants of this contest access to Aura Free Tier for building applications. Neo4j Aura is a fully managed cloud database service. Fill out this form.An in-depth GraphAcademy workshop on how to build applications with the Neo4j GraphQL Library and Neo4j graph database (this is coming soon, and will be ready on April 27, when the contest starts accepting submissions).The GRANDstack starter kit contains sample apps and sample app GitHub code for inspiration, built with the Neo4j GraphQL Library (Neo-Push, Movies, and a Twitter dashboard using Retool & GraphQL).Neo4j Sandbox where you can build your project, see here for step-by-step instructions, or you can download Neo4j Desktop, which comes with a development license for Neo4j Enterprise Edition for developing locally.Access to experts within Neo4j via Discord (#hackathon), as well as in the community to answer questions when you need help.Access to rich documentation on how to get started with the Neo4j GraphQL Library to help you build your ideas further.Also, do see our previously hosted Global GraphHack Contest for ideas and inspiration. Some of these apps and prototypes were built in less than a month.Thinking process and workflow from ideation to submissionJudgesAmy Hodler (Graph Analytics and AI Program Director, Neo4j)Cristina Escalante (COO, Silver Logic)Jennifer Reif (DevRel, Neo4j)Ljubica Lazarevic (DevRel, Neo4j)William Lyon (DevRel, Neo4j)JudgesPrizesThe winners of the Leonhard Euler Idea Contest (individual or as a team) in the contest will respectively receive:Grand Prize: USD $3,500 (Cash)1st Runner Up: $2,500 (Cash)2nd Runner Up: $1,500 (Cash)Top 5 most creative ideas: $500 (Cash)Rookie of the Idea Contest: Neo4j Swag bagWe hope the contest is motivating enough to get you started today. Good luck!Register Now;Apr 15, 2021;[]
https://medium.com/neo4j/understanding-alliances-exploring-structural-balance-with-neo4j-71fc08f10985;Nathan SmithFollowOct 26, 2019·7 min readUnderstanding alliances: Exploring structural balance with Neo4jIn 1919 the citizens of Kansas City raised $2.5 million in ten days to fund the construction of a monument to soldiers who fought in World War I. Today the National World War I Museum and Memorial encourages visitors thoughtfully reflect on the causes of the war, the impact on those who lived through it, and the consequences for the rest of the twentieth century. It’s one of the places I recommend that Kansas City visitors see while they are in town.National World War I Museum and Memorial in Kansas City, Missouri [CC0], via Wikimedia commonsChapter five in David Easley and Jon Kleinberg’s book Networks, Crowds, and Markets discusses international politics in the run-up to World War I. They apply a graph theory concept called structural balance. When considering structural balance in a social network, we allow the edges in the network to represent either a positive or a negative relationship between the participants. The relationships are undirected. We assume that if one node feels positively towards another, the feeling is mutual. Structural balance theory says that for groups of three nodes, certain configurations of positive and negative relationships are stable, while others are unstable.Triangles with one or three positive relationships are structurally balanced. This is because two people who like each other tend to share the same attitude towards a third person. Either the people who like each other will both like a third person, leading to three positive edges, or they will both dislike the third person, leading to one positive, and two negative edges.Structurally balanced trianglesTriangle configurations with two or zero positive relationships are structurally unbalanced. If you have two friends who don’t get along with each other, you know things can get awkward. In a group of three mutual enemies, an alliance of convenience will tend to form between two against the third. The enemy of my enemy tries to become my friend.Structurally unbalanced trianglesWe can extend this local property of triangles to a larger graph. We say that a graph is structurally balanced if all of the triangles that it contains are structurally balanced. Easley and Kleinberg provide a proof of the balance theorem. This theorem states that in a graph, there are only two configurations that are structurally balanced. In the first scenario, all members of the graph like each other. In the second scenario, the graph is divided into two groups. Members of each group like everyone else in their own group and dislikes everyone in the other group. At first, it might seem surprising that enforcing this local property on triangles has such a profound impact on the global graph.Two examples of structurally balanced networksEasley and Kleinman cite a paper by Antal, Krapivsky, and Redner that tracks the changing diplomatic relationships among European countries during the years 1872 to 1907. At the beginning of the period, the relationships were structurally unbalanced. Alliances shifted, seeking balanced triangles. Finally two groups formed: Great Britain, France, and Russia allied against Austria-Hungary, Germany, and Italy. With two powerful, stable groups opposing each other, the stage was set for World War I.We can explore structural balance and create an alternative history with Neo4j. You can set up a free Neo4j sandbox to execute this code.After I set up a starting scenario with countries and relationships, here are the steps I want to execute:Identify all of the edges participating in unbalanced triangles.Select one of those edges at random and flip it’s type from :LIKES to :DISLIKES or from :DISLIKES to :LIKES. This will balance at least one triangle.Increment the age on the countries.Take a snapshot of the state of the graph so that I can see it’s evolution over time.I will repeat these steps until my graph converges to a balanced state.First, I create six country nodes. I give each country an age property that I’ll increment when the graph changes.UNWIND [Italy, Germany, Great Britain, Russia,      Austria- Hungary, France] AS countryNameCREATE (c:Country {name:countryName, age:0})RETURN cSince this is alternative history, I will set up random relationships among the countries. I set up :LIKES relationships between a random set of about half of the countries. Since Neo4j requires directed relationships, we choose the source node for the relationship to be the first one in alphabetical order. I’m going to want to add up the number of positive relationships in the triangles, so I add a likeCount property to make that easy.MATCH (c1:Country), (c2:Country)WITH c1, c2, Rand() AS randWHERE c1.name < c2.name AND rand < .5MERGE (c1)-[:LIKES {likeCount:1}]->(c2)RETURN c1, c2I set up :DISLIKES relationships between the countries that don’t already have a :LIKES relationship.MATCH (c1:Country), (c2:Country)WHERE c1.name < c2.name AND NOT (c1)-[:LIKES]-(c2)MERGE (c1)-[:DISLIKES {likeCount:0}]->(c2)RETURN c1, c2Initial state of countries graphI would like to be able to trace the evolution of the relationships in the network over time. To do that, I will create a series of :Snapshot nodes that capture the current state of the graph as properties. I set up the code as a custom procedure to make it easier to call repeatedly later on.CALL apoc.custom.asProcedure( makeSnapshot ,  MATCH (c1:Country)-[rel]->(c2:Country)WITH c1.age as age,      COLLECT(c1.name + | + c2.name) AS keys,      COLLECT(type(rel)) AS valuesMERGE (s:Snapshot {age:age})WITH s, keys, valuesCALL apoc.create.setProperties(s, keys, values)     YIELD nodeRETURN node ,  write , [[ node ,  NODE ]], [],       Create a snapshot of the current state of the countries. )Call this function once and look at the results in table view to see the properties that were created.CALL custom.makeSnapshot()      YIELD node RETURN nodeNext, I write a function to find all of the edges that participate in an unbalanced triangle.CALL apoc.custom.asFunction( findUnbalanced ,  MATCH p1 = (c1:Country)-[rel1]->(c2:Country),       p2 = (c1:Country)-[rel2]->(c3:Country),       p3 = (c2:Country)-[rel3]->(c3:Country)WHERE (rel1.likeCount + rel2.likeCount + rel3.likeCount) % 2 = 0RETURN apoc.coll.flatten(collect([p1, p2, p3])) AS         unbalancingPaths ,  LIST OF PATH , [], true,       Get the edges that participate in unbalanced triangles. )Finally, I’ll write a procedure to pick one edge from my list at random and change it’s type.CALL apoc.custom.asProcedure( flipEdge ,  WITH apoc.coll.randomItem(unbalancingPaths) AS pWITH head(relationships(p)) AS relSET rel.likeCount = (rel.likeCount + 1) % 2WITH relCALL apoc.refactor.setType(rel, CASE type(rel)                                 WHEN LIKES                                 THEN DISLIKES ELSE LIKES END)      YIELD outputRETURN output ,  write , [[ output ,  RELATIONSHIP ]], [[ unbalancingPaths ,      LIST OF PATH ]],      Pick a random edge from the list and flip it. )Now we can assemble the pieces like this.CALL apoc.periodic.commit( WITH custom.findUnbalanced() AS unbalancingPathsCALL custom.flipEdge(unbalancingPaths)      YIELD outputWITH size(unbalancingPaths) AS pathCountMATCH (c:Country) SET c.age = c.age + 1WITH max(pathCount) AS maxPathCountCALL custom.makeSnapshot()      YIELD nodeRETURN maxPathCountLIMIT 1 )Because the code block is wrapped in a apoc.periodic.commit statement, my code executes repeatedly until there are no unbalanced triangles. When you examine the countries, you will find a structurally balanced graph.MATCH (c:Country) RETURN cStructurally balanced resultNow let’s explore the snapshots. Turn the snapshots into a linked list.MATCH (s1:Snapshot), (s2:Snapshot)WHERE s1.age = s2.age -1MERGE (s1)-[:LEADS_TO]->(s2)You can query the list of snapshots to see the points in the simulation where certain countries liked each other.MATCH(s:Snapshot) WHERE s.`France|Germany` =  LIKES RETURN sIn my simulation, France and Germany only liked each other for the final three steps.You can use apoc.diff.nodes to see which edge switched at each step of the simulation.MATCH (s1:Snapshot)-[:LEADS_TO]->(s2:Snapshot)WITH apoc.diff.nodes(s1, s2) AS diffRETURN diff.differentORDER BY diff.different.age.leftIn the first step of my simulation, Great Britain and Italy switched from DISLIKES to LIKES.The query below will reset the relationships to match any of the snapshots in our chain of history.MATCH (s:Snapshot {age:0}),      (c1:Country)-[rel]-(c2:Country)     (c1:Country)-[rel]-(c2:Country)WITH c1, rel, c2, properties(s) AS propsCALL apoc.refactor.setType(rel, props[c1.name +  |  + c2.name])     YIELD outputWITH output, c1, c2SET output.likeCount = CASE type(output)                        WHEN  LIKES                         THEN 1 ELSE 0 ENDRETURN c1, c2I used the query to replay the last six steps in my simulation. One edge changes in each picture below until the graph reaches the structurally balanced state in the bottom right.Replay of the last steps of the simulationThe code in the blog post switches a random edge that participates in an unbalanced triangle. However, each edge is part of n - 2 triangles, where n is the total number of nodes. By switching the type of a given edge, we might bring balance to one triangle, but unbalance several others. We’ll explore what happens when we flip the edge that balances the most triangles in the next blog post in this series.It’s also important to recognize that this process of flipping edges shown here is not deterministic. I would probably get a different result if I reset the simulation to zero and ran it again. I believe that the real world can reach a structurally balanced state where all nations like each other, but it will take more than a graph database to achieve that result.;Oct 26, 2019;[]
https://medium.com/neo4j/making-sense-of-news-the-knowledge-graph-way-d33810ce5005;Tomaz BratanicFollowFeb 2, 2021·12 min readMaking Sense of News, the Knowledge Graph WayHow to combine Named Entity Linking with Wikipedia data enrichment to analyze the internet news.A wealth of information is being produced every day on the internet. Understanding the news and other content-generating websites is becoming increasingly crucial to successfully run a business. It can help you spot opportunities, generate new leads, or provide indicators about the economy.In this blog post, I want to show you how you can create a news monitoring data pipeline that combines natural language processing (NLP) and knowledge graph technologies.The data pipeline consists of three parts. In the first part, we scrape articles from an Internet provider of news. Next, we run the articles through an NLP pipeline and store results in the form of a knowledge graph. In the last part of the data pipeline, we enrich our knowledge with information from the WikiData API. To demonstrate the benefits of using a knowledge graph to store the information from the data pipeline, we perform simple network analysis and try to find insights.AgendaScraping internet newsEntity linking with WikifierWikipedia data enrichmentNetwork analysisGraph ModelWe use Neo4j to store our knowledge graph. If you want to follow along with this blog post, you need to download Neo4j and install both the APOC and Graph Data Science libraries. All the code is available on GitHub as well.Graph schema. Image by authorOur graph data model consists of articles and their tags. Each article has many sections of text. Once we run the section text through the NLP pipeline, we extract and store mentioned entities back to our graph.We start by defining unique constraints for our graph.Uniqueness constraints are used to ensure data integrity, as well as to optimize Cypher query performance.CREATE CONSTRAINT IF NOT EXISTS ON (a:Article) ASSERT a.url IS UNIQUECREATE CONSTRAINT IF NOT EXISTS ON (e:Entity) ASSERT e.wikiDataItemId is UNIQUECREATE CONSTRAINT IF NOT EXISTS ON (t:Tag) ASSERT t.name is UNIQUEInternet News ScrapingNext, we scrape the CNET news portal. I have chosen the CNET portal because it has the most consistent HTML structure, making it easier to demonstrate the data pipeline concept without focusing on the scraping element. We use theapoc.load.html procedure for the HTML scraping. It uses jsoup under the hood. Find more information in the documentation.First, we iterate over popular topics and store the link of the last dozen of articles for each topic in Neo4j.CALL apoc.load.html( https://www.cnet.com/news/ ,   {topics: div.tag-listing > ul > li > a }) YIELD valueUNWIND value.topics as topic  WITH  https://www.cnet.com  + topic.attributes.href as linkCALL apoc.load.html(link, {article: div.row.asset > div > a }) YIELD valueUNWIND value.article as articleWITH distinct  https://www.cnet.com  + article.attributes.href as article_linkMERGE (a:Article{url:article_link})Now that we have the links to the articles, we can scrape their content as well as their tags and publishing date. We store the results according to the graph schema we defined in the previous section.MATCH (a:Article)CALL apoc.load.html(a.url,{date: time , title: h1.speakableText , text: div.article-main-body > p , tags:  div.tagList > a }) YIELD valueSET a.datetime = datetime(value.date[0].attributes.datetime)FOREACH (_ IN CASE WHEN value.title[0].text IS NOT NULL THEN [true] ELSE [] END |            CREATE (a)-[:HAS_TITLE]->(:Section{text:value.title[0].text}))FOREACH (t in value.tags |   MERGE (tag:Tag{name:t.text}) MERGE (a)-[:HAS_TAG]->(tag))WITH a, value.text as textsUNWIND texts as rowWITH a,row.text as textWHERE text IS NOT NULLCREATE (a)-[:HAS_SECTION]->(:Section{text:text})I did not want to complicate the Cypher query that stores the results of the articles even more, so we must perform a minor cleanup of tags before we continue.MATCH (n:Tag)WHERE n.name CONTAINS  Notification DETACH DELETE nLet’s evaluate our scraping process and look at how many of the articles have been successfully scraped.MATCH (a:Article)RETURN exists((a)-[:HAS_SECTION]->()) as scraped_articles,       count(*) as countIn my case, I have successfully collected the information for 245 articles. Unless you have a time machine, you won’t be able to recreate this analysis identically. I have scraped the website on the 30th of January 2021, and you will probably do it later. I have prepared most of the analysis queries generically, so they work regardless of the date you choose to scrape the news.Let’s also examine the most frequent tags of the articles.MATCH (n:Tag)RETURN n.name as tag, size((n)<-[:HAS_TAG]-()) as articlesORDER BY articles DESCLIMIT 10Here are the results:Image by authorAll charts in this blog post are made using the Seaborn library. CNET Apps Today is the most frequent tag. I think that’s just a generic tag for daily news. We can observe that they have custom tags for various big companies such as Amazon, Apple, and Google.Named Entity Linking: WikificationIn my previous blog post, we have already covered the Named Entity Recognition techniques to create a knowledge graph. Here, we will take it up a notch and delve into Named Entity Linking.First of all, what exactly is Named Entity Linking?Image by Aparravi on Wikipedia. Licensed under CC BY-SA 4.0Named Entity Linking is an upgrade to the entity recognition technique. It starts by recognizing all the entities in the text. Once it finishes the named entity recognition process, it tries to link those entities to a target knowledge base. Usually, the target knowledge bases are Wikipedia or DBpedia, but there are other knowledge bases out there as well.In the above example, we can observe that the named entity recognition process recognized Paris as an entity. The next step is to link it to a target entity in a knowledge base. Here, it uses Wikipedia as the target knowledge base. This is also known as the Wikification process.The Entity Linking process is a bit tricky as we can see that many entities exist in Wikipedia that have Paris in their title. So, as a part of the Entity Linking process, the NLP model also does the entity disambiguation.There are a dozen Entity Linking models out there. Some of them are:http://wikifier.org/https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/ambiverse-nlu/aidahttps://github.com/informagi/RELhttps://github.com/facebookresearch/BLINKI am from Slovenia, so my biased decision is to use the Slovenian solution Wikifier [1]. They don’t actually offer their NLP model, but they have a free-to-use API endpoint. All you have to do is to register. They don’t even want your password or email, which is nice of them.The Wikifier supports more than 100 languages. It also features some parameters you can use to fine-tune the results. I have noticed that the most dominant parameter is the pageRankSqThreshold parameter, which you can use to optimize either recall or accuracy of the model.If we run the above example through the Wikifier API, we get the following results:We can observe that Wikifier API returned three entities and their corresponding Wikipedia URL as well as WikiData item id. We use the WikiData item id as a unique identifier for storing them back to Neo4j.The APOC library has the apoc.load.json procedure, which you can use to retrieve results from any API endpoint. If you are dealing with a larger amount of data, you will want to use the apoc.periodic.iterate procedure for batching purposes.If we put it all together, the following Cypher query fetches the annotation results for each section from the API endpoint and stores the results in Neo4j.CALL apoc.periodic.iterate( MATCH (s:Section) RETURN s , WITH s,  http://www.wikifier.org/annotate-article?  +         text=  + apoc.text.urlencode(s.text) +  &  +         lang=en&  +         pageRankSqThreshold=0.80&  +         applyPageRankSqThreshold=true&  +         nTopDfValuesToIgnore=200&  +         nWordsToIgnoreFromList=200&  +         minLinkFrequency=100&  +          maxMentionEntropy=10&  +         wikiDataClasses=false&  +         wikiDataClassIds=false&  +         userKey=  + $userKey as urlCALL apoc.load.json(url) YIELD valueUNWIND value.annotations as annotationMERGE (e:Entity{wikiDataItemId:annotation.wikiDataItemId})ON CREATE SET e.title = annotation.title, e.url = annotation.urlMERGE (s)-[:HAS_ENTITY]->(e),{batchSize:100, params: {userKey:$user_key}})The Named Entity Linking process takes a couple of minutes. We can now check the most frequently mentioned entities.MATCH (e:Entity)RETURN e.title, size((e)<--()) as mentionsORDER BY mentions DESC LIMIT 10Here are the results:Image by authorApple Inc. is the most frequently mentioned entity. I am guessing that all dollar signs or USD mentions get linked to the United States dollar. We can also examine the most frequently-mentioned-by-article tags.MATCH (e:Entity)<-[:HAS_ENTITY]-()<-[:HAS_SECTION]-()-[:HAS_TAG]->(tag)WITH tag.name as tag, e.title as title, count(*) as mentionsORDER BY mentions DESCRETURN tag, collect(title)[..3] as top_3_mentionsLIMIT 5Here are the results:Image by authorWikiData EnrichmentA bonus to using the Wikification process is that we have the WikiData item id of our entities. This makes it very easy for us to scrape the WikiData API for additional information.Let’s say we want to define all business and person entities. We will fetch the entity classes from WikiData API and use that information to group the entities. Again we will use the apoc.load.json procedure to retrieve the response from an API endpoint.MATCH (e:Entity)// Prepare a SparQL queryWITH SELECT *      WHERE{        ?item rdfs:label ?name .        filter (?item = wd: + e.wikiDataItemId + )        filter (lang(?name) =  en  ) .      OPTIONAL{        ?item wdt:P31 [rdfs:label ?class] .        filter (lang(?class)= en )      }} AS sparql, e// make a request to WikidataCALL apoc.load.jsonParams(     https://query.wikidata.org/sparql?query=  +     apoc.text.urlencode(sparql),     { Accept:  application/sparql-results+json }, null)YIELD valueUNWIND value[results][bindings] as rowFOREACH(ignoreme in case when row[class] is not null then [1] else [] end |         MERGE (c:Class{name:row[class][value]})        MERGE (e)-[:INSTANCE_OF]->(c))We continue by inspecting the most frequent classes of the entities.MATCH (c:Class)RETURN c.name as class, size((c)<--()) as countORDER BY count DESC LIMIT 5Here are the results:Image by authorThe Wikification process found almost 250 human entities and 100 business entities. We assign a secondary label to Person and Business entities to simplify our further Cypher queries.MATCH (e:Entity)-[:INSTANCE_OF]->(c:Class)WHERE c.name in [ human ]SET e:PersonMATCH (e:Entity)-[:INSTANCE_OF]->(c:Class)WHERE c.name in [ business ,  enterprise ]SET e:BusinessWith the added secondary label, we can now easily examine the most frequently-mentioned business entities.MATCH (b:Business)RETURN b.title as business, size((b)<-[:HAS_ENTITY]-()) as mentionsORDER BY mentions DESCLIMIT 10Here are the results:Image by authorWe already knew that Apple and Amazon were discussed a lot. Some of you already know that this was an exciting week on the stock market, as we can see lots of mentions of GameStop.Just because we can, let’s also fetch the industries of the business entities from the WikiData API.MATCH (e:Business)// Prepare a SparQL queryWITH SELECT *      WHERE{        ?item rdfs:label ?name .        filter (?item = wd: + e.wikiDataItemId + )        filter (lang(?name) =  en  ) .      OPTIONAL{        ?item wdt:P452 [rdfs:label ?industry] .        filter (lang(?industry)= en )      }} AS sparql, e// make a request to WikidataCALL apoc.load.jsonParams(     https://query.wikidata.org/sparql?query=  +     apoc.text.urlencode(sparql),     { Accept:  application/sparql-results+json }, null)YIELD valueUNWIND value[results][bindings] as rowFOREACH(ignoreme in case when row[industry] is not null then [1] else [] end |         MERGE (i:Industry{name:row[industry][value]})        MERGE (e)-[:PART_OF_INDUSTRY]->(i))Exploratory Graph AnalysisOur data pipeline ingestion is complete. Now we can have some fun and explore our knowledge graph. First, we will examine the most co-occurrent entities of the most frequently-mentioned entity, which is Apple Inc. in my case.MATCH (b:Business)WITH b, size((b)<-[:HAS_ENTITY]-()) as mentionsORDER BY mentions DESC LIMIT 1MATCH (other_entities)<-[:HAS_ENTITY]-()-[:HAS_ENTITY]->(b)RETURN other_entities.title as entity, count(*) as countORDER BY count DESC LIMIT 10Here are the results:Image by authorNothing spectacular here. Apple Inc. appears in sections where iPhone, Apple Watch, and VR are also mentioned. We can look at some more exciting news. I was searching for any relevant tags of articles that might be interesting.CNET has many specific tags, but the Stock Market tag stood out as more broad and very relevant in these times. Let’s check the most frequently-mentioned industries in the Stock Market category of articles.MATCH (t:Tag)<-[:HAS_TAG]-()-[:HAS_SECTION]->()-[:HAS_ENTITY]->(entity:Business)-[:PART_OF_INDUSTRY]->(industry)WHERE t.name =  Stock Market RETURN industry.name as industry, count(*) as mentionsORDER BY mentions DESCLIMIT 10Here are the results:Image by authorRetail is by far the most mentioned, next is the video game industry, and then some other industries that are mentioned only once. Next, we will check the most mentioned businesses or persons in the Stock Market category.MATCH (t:Tag)<-[:HAS_TAG]-()-[:HAS_SECTION]->()-[:HAS_ENTITY]->(entity)WHERE t.name =  Stock Market  AND (entity:Person OR entity:Business)RETURN entity.title as entity, count(*) as mentionsORDER BY mentions DESCLIMIT 10Here are the results:Image by authorOkay, so GameStop is huge this weekend with more than 40 mentions. Very far behind are Jim Cramer, Elon Musk, and Alexandria Ocasio-Cortez. Let’s try to understand why GameStop is so huge by looking at the co-occurring entities.MATCH (b:Business{title: GameStop })<-[:HAS_ENTITY]-()-[:HAS_ENTITY]->(other_entity)RETURN other_entity.title as co_occurent_entity, count(*) as mentionsORDER BY mentions DESCLIMIT 10Here are the results:Image by authorThe most frequently-mentioned entities in the same section as GameStop are Stock, Reddit, and US dollar. If you look at the news you might see that the results make sense. I would venture a guess that AMC (TV channel) was wrongly identified and should probably be the AMC Theaters company.There will always be some mistakes in the NLP process. We can filter the results a bit and look for the most co-occurring person or business entities of GameStop.MATCH (b:Business{title: GameStop })<-[:HAS_ENTITY]-()-[:HAS_ENTITY]->(other_entity:Person)RETURN other_entity.title as co_occurent_entity, count(*) as mentionsORDER BY mentions DESCLIMIT 10Here are the results:Image by authorAlexandria Ocasio-Cortez(AOC) and Elon Musk each appear in three sections with GameStop. Let’s examine the text where AOC co-occurs with GameStop.MATCH (b:Business{title: GameStop })<-[:HAS_ENTITY]-(section)-[:HAS_ENTITY]->(p:Person{title: Alexandria Ocasio-Cortez })RETURN section.text as textHere are the results:Image by authorGraph Data ScienceSo far, we have only done a couple of aggregations using the Cypher query language. As we are utilizing a knowledge graph to store our information, let’s execute some graph algorithms on it. Neo4j Graph Data Science library is a plugin for Neo4j that currently has more than 50 graph algorithms available. The algorithms range from community detection and centrality to node embedding and graph neural network categories.We have already inspected some co-occurring entities so far. Next, we infer a co-occurrence network of persons within our knowledge graph. This process basically translates indirect relationships, where two entities are mentioned in the same section, to a direct relationship between those two entities. This diagram might help you understand the process.Image by authorThe Cypher query for inferring the person co-occurrence network is:MATCH (s:Person)<-[:HAS_ENTITY]-()-[:HAS_ENTITY]->(t:Person)WHERE id(s) < id(t)WITH s,t, count(*) as weightMERGE (s)-[c:CO_OCCURENCE]-(t)SET c.weight = weightThe first graph algorithm we use is the Weakly Connected Components algorithm. It is used to identify disconnected components or islands within the network.CALL gds.wcc.write({    nodeProjection:Person,    relationshipProjection:CO_OCCURENCE,    writeProperty:wcc})YIELD componentCount, componentDistributionHere are the results:Image by authorThe algorithm found 134 disconnected components within our graph. The p50 value is the 50th percentile of the community size. Most of the components consist of a single node.This implies that they don’t have any CO_OCCURENCE relationships. The largest island of nodes consists of 30 members. We mark its members with a secondary label.MATCH (p:Person)WITH p.wcc as wcc, collect(p) as membersORDER BY size(members) DESC LIMIT 1UNWIND members as memberSET member:LargestWCCWe further analyze the largest component by examining its community structure and trying to find the most central nodes. When you have a plan to run multiple algorithms on the same projected graph, it is better to use a named graph. The relationship in the co-occurrence network is treated as undirected.CALL gds.graph.create(person-cooccurence, LargestWCC,     {CO_OCCURENCE:{orientation:UNDIRECTED}},   {relationshipProperties:[weight]})First, we run the PageRank algorithm, which helps us identify the most central nodes.CALL gds.pageRank.write(person-cooccurence, {relationshipWeightProperty:weight, writeProperty:pagerank})Next, we run the Louvain algorithm, which is a community detection algorithm.CALL gds.louvain.write(person-cooccurence, {relationshipWeightProperty:weight, writeProperty:louvain})Some people say that a picture is worth a thousand words. When you are dealing with smaller networks it makes sense to create a network visualization of the results. The following visualization was created using Neo4j Bloom.Node color represents communities and node size represents the PageRank score. Image by authorConclusionI really love how NLP and knowledge graphs are a perfect match. Hopefully, I have given you some ideas and pointers on how you can go about implementing your data pipeline and storing results in a form of a knowledge graph. Let me know what do you think!As always, the code is available on GitHub.References[1] Janez Brank, Gregor Leban, Marko Grobelnik. Annotating Documents with Relevant Wikipedia Concepts. Proceedings of the Slovenian Conference on Data Mining and Data Warehouses (SiKDD 2017), Ljubljana, Slovenia, 9 October 2017.;Feb 2, 2021;[]
https://medium.com/neo4j/everything-you-need-to-know-about-our-neo4j-certifications-a71dbaec049c;Elaine RosenbergFollowMar 29, 2021·4 min readPhoto by Vasily Koloda on UnsplashEverything You Need to Know About Our Neo4j CertificationsWhether you want to assess your skills, improve your CV, impress your boss or your family, the Neo4j Certifications will help you demonstrate your expertise with the most widely-used graph database.Neo4j is committed to making your application development journey successful. Through our free online resources and also our Professional Services training offerings, we strive to provide you the opportunity to learn and gain experience with Neo4j.As the products offered by Neo4j evolve, we continue to update these resources. The Neo4j certification process is also evolving, so we wanted to use the opportunity to give you an update, on what you can be certified on and how.We now have three free Neo4j certification exams available that you can take to demonstrate that you have gained knowledge and experience with Neo4j:Neo4j Certified ProfessionalNeo4j 4.x CertifiedNeo4j Graph Data Science CertifiedThese exams are multiple choice and the passing grade to earn your certificate is 80%. You can take one exam every 24 hour.Neo4j Certified Professional80 questions, 60 minutesThis exam has been available for several years, but we have recently updated the questions in this exam (on March 16, 2021) to include concepts and usage of Neo4j 4.x.This is the certification exam that everyone should take to demonstrate their overall knowledge of Neo4j 4.xThe areas that this exam covers include:The Neo4j 4.x Graph PlatformNeo4j Graph Database ConceptsImporting Data in Neo4j 4.xGraph Data ModelingHere are the recommended training courses to prepare for this exam:Introduction to Neo4j 4.x Series:Overview of Neo4j 4.xQuerying with Cypher in Neo4j 4.xCreating Nodes and Relationships in Neo4j 4.xUsing Indexes and Query Best Practices in Neo4j 4.xImporting Data with Neo4j 4.x2. Graph Data Modeling with Neo4jPhoto by Firmbee.com on UnsplashNeo4j 4.x Certification30 questions 45 minutesThis exam was added on November 20, 2020 as a means for you to demonstrate your knowledge of new Neo4j 4.x features.If you have passed the Neo4j Certified Professional exam prior to March, 16 2021, you can take this exam that tests only Neo4j 4.x features.This Neo4j 4.x Certified exam tests your knowledge of Cypher and multi-database of Neo4j 4.x, as well as Role Based Access Control and Fabric — features used in deployed Neo4j Enterprise applications.Note: Neo4j 4.x additions to Cypher and multi-database are now covered in the latest Neo4j Certified Professional exam.The areas that this exam covers include:New Cypher features: Subqueries, Fulltext schema indexes, Query plannerMulti-database managementRole Based Access ControlFabricThere are no specific online courses that teach new Neo4j 4.x features, but subqueries and multi-database are used in the Introduction to Neo4j 4.x SeriesNeo4j multi-database is covered in depth in the, Basic Neo4j 4.x Administration course.You can also learn about the new Neo4j 4.x features in the Neo4j Developer Guides.Photo by Moritz Kindler on UnsplashNeo4j Graph Data Science Certification40 questions 60 minutesThis certification exam was added on February 24, 2021. It tests your knowledge of using the Neo4j Graph Data Science Library for analyzing data in the graph.The areas that this exam covers include:General use of the Neo4j Graph Data Science Library.Graph Data Science workflow used during analysis.Using specific graph algorithms.Here are the recommended courses and resources to prepare for this exam:Introduction to Neo4j 4.x SeriesIntroduction to Graph Algorithms in Neo4j 4.xUsing a Machine Learning Workflow for Link PredictionApplied Graph Data Science for Web ApplicationsDeveloper Guides for Graph Data ScienceWe wish you a lot of success with the Neo4j certifications you choose to take!Each of the online training courses has means for you to provide feedback and you can also ask questions in our online developer community. We’re looking forward to your feedback which will allow us to continue to improve our learning resources.;Mar 29, 2021;[]
https://medium.com/neo4j/nxneo4j-networkx-api-for-neo4j-a-new-chapter-9fc65ddab222;Yusuf Baktir, Ph.D.FollowSep 2, 2020·7 min readnxneo4j: NetworkX-API for Neo4j — A new chapterPhoto: Clint AdairRecently, I have had the opportunity to work with nxneo4j and I am excited to share it with the world!What is nxneo4j?nxneo4j is a python library that enables you to use networkX type of commands to interact with Neo4j.Neo4j is the most common graph database. NetworkX is the most commonly used graph library. Combining the two brings us the nxneo4j!Neo4j has the following advantages:Neo4j is a database which means it persists the data. You can create the data once and run graph algorithms as many times as you want. In networkX, you need to construct the graph every time you want to run it.Neo4j graph algorithms are scalable and production-ready. Neo4j algorithms are written in Java and performance tested. NetworkX is a single node implementation of a graph written in Python.The response time is much faster in Neo4j.Neo4j supports graph embeddings in the form of Node Embeddings, Random Projections, and Graph Sage. These are not available in nxneo4j yet but it will be available in the future versions.So, why not use Neo4j?nxneo4j is designed to help users interact with Neo4j quickly and easily. It uses the famous networkX API. You will use your well-accustomed scripts and but the scripts will run against Neo4j. Cool!Just to be clear, Mark Needham had already created the nxneo4j and you might have used it in the past. This version updates the entire library for Neo4j 4.x and new Graph Data Science library since the older Graph Algoritm library is not supported with Neo4j 4.x. More importantly, it significantly improves the core functionality with property support, node and edge views, remove node features etc. So, this is more a like A new chapter” or Welcome back” for the library and it will have continuous support.If you are like me and prefer Jupyter Notebooks instead, here is the link:https://github.com/ybaktir/networkx-neo4j/blob/master/examples/nxneo4j_tutorial_latest.ipynbPrerequisite 1: Neo4j itselfYou need to have an active Neo4j 4.x running. Make sure you have Neo4j 4 and above. You have four options here:Neo4j Desktop: It is a free desktop application that runs locally on your computer. It is super easy to install. Follow the instructions on this page: https://neo4j.com/download/Neo4j Sandbox: This is a free temporary Neo4j instance and it is the fastest way to get started with Neo4j. So fast that, you can have your Neo4j instance running under 60 seconds. Just use the following link: https://neo4j.com/sandbox/Neo4j Aura: This a pay-as-go cloud service. You can start as low as $0.09/hour. This is most suitable for long term projects where you don’t have to worry about the infrastructure. In case you want to give it a try: https://neo4j.com/aura/Your enterprise Neo4j instance. You know it when you have it. Otherwise, you can ask your architect to run an instance for you. Be careful during the experimentation.Prerequisite 2: APOC and GDS pluginsIn Neo4j Desktop, you can easily install them like the following:Image by the AuthorImage by the AuthorThe libraries come pre-installed in the Sandbox and Aura.Connect to Neo4jNo matter which option you choose, you need to connect to the Neo4j. The library to use is neo4j”pip install neo4jThen, connect to the Neo4j instance.from neo4j import GraphDatabaseuri      =  bolt://localhost  # in Neo4j Desktop                              # custom URL for Sandbox or Aurauser     =  neo4j             # your user name                               # default is always  neo4j                                # unless you have changed it. password = your_neo4j_passworddriver = GraphDatabase.driver(uri=uri,auth=(uri,password))If everything went smoothly so far, you are ready to use nxneo4j!nxneo4jTo get the most up to date version, install it directly from the Github page.pip install git+https://github.com/ybaktir/networkx-neo4jThis will install nxneo4j 0.0.3. The version 0.0.2 is available on pypi but it is not stable. 0.0.3 will be published on pypi soon. Until then, please use the above link.Then create the Graph instance:import nxneo4j as nxG = nx.Graph(driver)   # undirected graphG = nx.DiGraph(driver) # directed graphLet’s add some data:G.add_node(1)                   #single nodeG.add_nodes_from([2,3,4])       #multiple nodesG.add_edge(1,2)                 #single edgeG.add_edges_from([(2,3),(3,4)]) #multiple edgesCheck nodes and edges:>>> list(G.nodes())[1, 2, 3, 4]>>> list(G.edges())[(1, 2), (2, 3), (3, 4)]To add nodes and edges with features:G.add_node(Mike,gender=M,age=17)G.add_edge(Mike,Jenny,type=friends,weight=3)Check individual nodes data:>>> G.nodes[‘Mike’]{gender: M, age: 17}>>> G.nodes[Mike][gender]MCheck all nodes and edges data:>>> list(G.nodes(data=True))[(1, {}), (2, {}), (3, {}), (4, {}), (Mike, {gender: M, age: 17}), (Jenny, {})]>>> list(G.edges(data=True))[(1, 2, {}), (2, 3, {}), (3, 4, {}), (Mike, Jenny, {type: friends, weight: 3})]Visualize with like the following:>>> nx.draw(G)Image by the AuthorTo delete all the dataG.delete_all()Config fileNeo4j has some additional requirements for data storage. In Neo4j, the relationships have to have a relationship label. The labels of the nodes are highly recommended. Since the NetworkX syntax has no room for label modification, we store this knowledge in the config” file.The config file is a python dictionary, and the default config file has the following statements:{node_label: Node,relationship_type: CONNECTED,identifier_property: id}You can easily change this dictionary and create an instance with new modifications. For example:config = {node_label: Person,relationship_type: LOVES,identifier_property: name}G = nx.Graph(driver, config=config)You can also change the default values after the instance creation:G.direction = UNDIRECTED #for Undirected GraphG.direction = NATURAL    #for Directed GraphG.identifier_property = ‘name’G.relationship_type = ‘LOVES’G.node_label = ‘Person’To check the config file:>>> G.base_params(){direction: NATURAL, node_label: Person, relationship_type: LOVES, identifier_property: name}Built-in Data Setsnxneo4j has 3 built-in datasets:Game of ThronesTwitterEurope Road1.Game of Thrones dataCreated by Andrew Beveridge, the data set contains the interactions between the characters across the first 7 seasons of the popular TV show.There are 796 nodes and 3,796 relationships.All nodes are the TV characters labeled Character”. The relationship types are INTERACTS1”, INTERACTS2”, INTERACTS3” and INTERACTS45”The only node property is name”The relationship properties are book” and weight”.You can load it with the following command:G.load_got()2. Europe RoadsCreated by Lasse Westh-Nielsen, the data set contains the European cities and the distances between them.There are 894 nodes and 2,499 relationships.All nodes are labeled Place” and the relationships types are all EROAD”Node properties are name” and countryCode”Relationship properties are distance”, road_number” and watercrossing”.You can load the data with the following code:G.load_euroads()3. TwitterCreated by Mark Needham, the data contains Twitter followers of the graph community.There are 6526 nodes and 177,708 relationships.All node labels are User” and all relationship types are FOLLOWS”Node properties are name”, followers”, bio”, id”, username”, following”. Relationships don’t have any property. To get the data, run:G.load_twitter() Graph Data ScienceIt is algorithm time!There are at least 47 builtin graph algorithms in Neo4j. nxneo4j will expand to cover all of them in the future versions. For now, the following networkX algorithms are supported:pagerankbetweenness_centralitycloseness_centralitylabel_propagationconnected_componentsclusteringtrianglesshortest_pathshortest_weighted_pathLet’s clear the data and load Game of Thrones data set:G.delete_all()G.load_got()Visual inspection:nx.draw(G) # You can zoom in and interact with the nodes           # when running on Jupyter NotebookImage by the AuthorCentrality Algorithms:Centrality algorithms help us understand the individual importance of each node.>>> nx.pagerank(G){Addam-Marbrand: 0.3433842763728652, Aegon-Frey-(son-of-Stevron): 0.15000000000000002, Aegon-I-Targaryen: 0.3708563211936468, Aegon-Targaryen-(son-of-Rhaegar): 0.15000000000000002, Aegon-V-Targaryen: 0.15000000000000002, Aemon-Targaryen-(Dragonknight): 0.15000000000000002, Aemon-Targaryen-(Maester-Aemon): 1.1486743815905878,... }>>> nx.betweenness_centrality(G){Addam-Marbrand: 0.0, Aegon-Frey-(son-of-Stevron): 0.0, Aegon-I-Targaryen: 0.0, Aegon-Targaryen-(son-of-Rhaegar): 0.0, Aegon-V-Targaryen: 0.0, Aemon-Targaryen-(Dragonknight): 0.0, Aemon-Targaryen-(Maester-Aemon): 186.58333333333334,... }>>> nx.closeness_centrality(G){Addam-Marbrand: 0.3234782608695652, Aegon-Frey-(son-of-Stevron): 0.0, Aegon-I-Targaryen: 0.3765182186234818, Aegon-Targaryen-(son-of-Rhaegar): 0.0, Aegon-V-Targaryen: 0.0, Aemon-Targaryen-(Dragonknight): 0.0, Aemon-Targaryen-(Maester-Aemon): 0.33695652173913043,... }2. Community Detection AlgorithmsCommunity Detection algorithms show how nodes are clustered or partitioned.>>> list(nx.label_propagation_communities(G))[{Addam-Marbrand,  Aegon-I-Targaryen,  Aerys-II-Targaryen,  Alyn,  Arthur-Dayne,... ]>>> list(nx.connected_components(G))[{Raymun-Redbeard}, {Hugh-Hungerford}, {Lucifer-Long}, {Torghen-Flint}, {Godric-Borrell},... ]>>> nx.number_connected_components(G)610>>> nx.clustering(G){Colemon: 1.0, Desmond: 1.0, High-Septon-(fat_one): 1.0, Hodor: 1.0, Hosteen-Frey: 1.0,... }>>> nx.triangles(G){Addam-Marbrand: 0, Aegon-Frey-(son-of-Stevron): 0, Aegon-I-Targaryen: 0, Aegon-Targaryen-(son-of-Rhaegar): 0, Aegon-V-Targaryen: 0,... }3. Path Finding AlgorithmsPath Finding algorithms show the shortest path between two or more nodes.>>> nx.shortest_path(G, source= Tyrion-Lannister , target= Hodor )[Tyrion-Lannister, Luwin, Hodor]>>> nx.shortest_weighted_path(G, source= Tyrion-Lannister , target= Hodor ,weight=weight)[Tyrion-Lannister, Theon-Greyjoy, Wyman-Manderly, Hodor]Resources:Project Github Page:neo4j-graph-analytics/networkx-neo4jNetworkX API for Neo4j Graph Algorithms. Contribute to neo4j-graph-analytics/networkx-neo4j development by creating an…github.comJupyter Notebooks:https://github.com/ybaktir/networkx-neo4j/blob/master/examples/nxneo4j_tutorial_latest.ipynbChangelog:https://github.com/ybaktir/networkx-neo4j/blob/master/CHANGELOG.mdCredits:Mark Needham for creating the library.David Jablonski for adding the functionalities while improving the core functionality.;Sep 2, 2020;[]
https://medium.com/neo4j/learning-a-taxonomy-of-yelp-categories-using-overlap-coefficient-a00ea2410142;Mark NeedhamFollowOct 15, 2018·4 min readLearning a Taxonomy of Yelp Categories using Overlap CoefficientUpdate: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.com18 months ago my colleague Jesús wrote a blog post describing a technique for learning a taxonomy from tagged data (aka the Barrasa Method), and a couple of weekends ago I realised that his approach describes a similarity measure called the Overlap Coefficient.Over the last couple of months, Michael and I have been adding similarity algorithms to the Neo4j Graph Algorithms library and this seemed like it could be useful one to add.As of the 3.4.8.0 release of the Graph Algorithms Library this algorithm is now available for you to try out on your tagged data.Yelp Dataset ChallengeI wanted to see how it’d work on a reasonably large dataset, and the Yelp Dataset Challenge is perfect for that.A couple of times a year Yelp release a dataset containing businesses, reviews, categories, users and more, and they invite people to conduct research or analysis on their data and share their discoveries.My colleague Will Lyon has an excellent video that introduces the dataset:An introduction to the Yelp datasetYelp Graph ModelWe have the following graph model for the Yelp dataset:If you want to follow along at home you can find the code to import the data into Neo4j in my yelp-graph-algorithms GitHub repository.Inferring a category taxonomyBusinesses have categories, but each of the categories stands on their own — we don’t have any links between them to indicate how those categories are related.We can use the algo.similarity.overlap procedure to help us out. The following query calculates the Overlap coefficient between all categories and orders the results based on similarity:MATCH (category:Category)MATCH (category)<-[:IN_CATEGORY]-(business)WITH {item:id(category),       categories: collect(id(business))} as userDataWITH collect(userData) as dataCALL algo.similarity.overlap.stream(data)YIELD item1, item2, count1, count2, intersection, similarityRETURN algo.getNodeById(item1).name AS from,        algo.getNodeById(item2).name AS to,       count1, count2, intersection, similarityORDER BY similarity DESCIf we run that query we’ll see this output:from is the smaller category, and to is the larger category.If we take a couple of examples:Campgrounds is a subset of Hotels & TravelCideries is a subset of FoodThere is a latent taxonomy here, where (from)-[:NARROWER_THAN]-(to) . We can run the non streaming version of the algorithm to reify this taxonomy. We’ll also set a similarityCutoff of 0.75 so that we only create relationships between categories that have at least 75% similarity.MATCH (category:Category)MATCH (category)<-[:IN_CATEGORY]-(business)WITH {item:id(category),       categories: collect(id(business))} as userDataWITH collect(userData) as dataCALL algo.similarity.overlap(data, {  write: true, similarityCutoff: 0.75 })YIELD nodes, similarityPairs, p50, p75, p90, p99RETURN nodes, similarityPairs, p50, p75, p90, p99After we’ve done that we can write a query to view the taxonomy that we’ve created:MATCH p=()-[r:NARROWER_THAN*..2]->()RETURN pLIMIT 25This works well but there are some unnecessary relationships. For example:Conveyor Belt Sushi -> Seafood -> Restaurants andConveyor Belt Sushi -> RestaurantsThe relationship from Conveyor Belt Sushi to Restaurants is unnecessary as we can infer that relationship via the Seafood relationship.Let’s now apply the 2nd part of the Barrasa Method, where we delete direct relationships between categories if a transitive link already exists:MATCH (g1:Category)-[:NARROWER_THAN*2..]->(g3:Category),       (g1)-[d:NARROWER_THAN]->(g3)DELETE dNow if we run the same query from above we’ll see this output:We could use this taxonomy to help users browse through Yelp’s businesses.For example, if we’re in the Arts & Entertainment category and want to drill down to something more specific, the following query would help us learn what those more specific categories and reveal the businesses in those categories:MATCH (category:Category {name:  Arts & Entertainment })MATCH path = (category)<-[:NARROWER_THAN*]-(subCategory)WHERE not((subCategory)<-[:NARROWER_THAN]-())WITH path, subCategoryORDER BY length(path) DESCLIMIT 10MATCH businessPath = (subCategory)<-[:IN_CATEGORY]-(business)RETURN path, businessPathLIMIT 50If we run that query we’ll get the following output:Jesus describes other uses of such a taxonomy in his blog post, but I’m sure there are others that we haven’t thought of.If you have any ideas or tagged datasets that would well with this approach let us know — devrel@neo4j.comFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Oct 15, 2018;[]
https://medium.com/neo4j/from-understanding-to-setup-neo4j-on-azure-vm-linux-46b1d064cff6;Siddhartha SehgalFollowDec 30, 2019·6 min readFrom understanding to setup — Installing Neo4j on an Azure virtual machine (Linux/Ubuntu)What is Neo4j?The world’s most flexible, reliable and developer friendly graph database as a service.” It is an online database management system with Create, Read, Update and Delete (CRUD) operations that stores data as a graph.What is a graph database?A graph database, also called a graph-oriented database, is a type of NoSQL database that uses graph theory to store, map and query relationships. A graph database is essentially a collection of nodes and edges.A graph is composed of two elements: a node (or vertex) and a relationship (or edge). Each node represents an entity (a person, place, thing, category or other piece of data), and each relationship represents how two nodes are associated. This general-purpose structure allows you to model all kinds of scenarios — from a system of roads, to a network of devices, to a population’s medical history or anything else defined by relationships.Why would you ever require a graph database anyway?A graph database, unlike a relational database management system (RDBMS), treats relationships as first class citizens. It is not required to use approaches such as complex join queries or accessing foreign keys to get data related to each other. We join entities as soon as we know they’re connected, so these mapping methods are unnecessary. Since graph databases employ object oriented thinking at their core, the data model you draw on your whiteboard is the model of data you can store in your database.Modern data has, implicitly, lots of relationships. In order to leverage these data connections, organizations need a database technology that stores relationship information as a first-class entity. That technology is a graph database. Unfortunately, legacy RDBMS are poor at handling data relationships. Also, their rigid schemas make it difficult to add different connections or adapt to new business requirements.Not only do graph databases effectively store data relationships they’re also flexible when expanding a data model or conforming to changing business needs. You can read more about advantages of using Graph databases.Neo4j 4.0 was recently released, so do bear in mind which version you are installing. The Neo4j 3.5.x series requires Java 8, whereas Neo4j 4.0 uses Java 11. Also, there have been some changes to the way you connect to the database in 4.0, with the new connection schema of neo4j://.Let us get started with Neo4j on an Microsoft Azure virtual machine.Create a virtual machine on which you can host a Neo4j community version serverThe process to create a virtual machine on Azure is quite straight forward. I am using a Linux (Ubuntu 18.04) virtual machine for this post.Here is a little refresher — go to your Azure portal. Create a resource from the home page itself:Azure portal homepageSearch ‘virtual machine’ and look under the ‘Compute’ section:Search for virtual machine in Azure MarketplaceEnter your preferences for name, disk size, region, resource group, authentication, and finally review and create it:Review settings and create the virtual machineVirtual machine is createdOnce the virtual machine is created (it will start automatically), connect to it via a client software of your choice , such as Putty or MobaXterm. I will be using MobaXterm for this post.Connect to the virtual machine and start installingUse the public or private IP to connect to the virtual machine using the client of your choice. Remember, the public IP is at risk of changing when the virtual machine is restarted. You can, alternatively, setup a domain name system (DNS)for the virtual machine and connect using that.Once the connection is established to the virtual machine, let us install Java, which is necessary for Neo4j. Install Java using the command below:sudo apt-get install openjdk-8-jreDo bear in mind if you’re installing Neo4j 4.0 you will need Java 11.Installing the latest Neo4j community versionAdd Neo4j to list of repositories in Debian:wget -O - https://debian.neo4j.org/neotechnology.gpg.key | sudo apt-key add -echo deb https://debian.neo4j.org/repo stable/ | sudo tee /etc/apt/sources.list.d/neo4j.listUpdate the apt-get repositories list:sudo apt-get updateFinally, install Neo4j:sudo apt-get install neo4jThis should set up Neo4j on your Linux machine, and be available to use. You can start the Neo4j service using the following command:sudo service neo4j restartWait a few seconds and check to see if the server started. Note that the default port Neo4j is configured to run on is 7474. Since Neo4j is currently only accessible to the Linux machine , i.e., local, use the following command to check if it is working:sudo curl localhost:7474The sudo command will only work if you have root level access to the Linux machine.If the service is active, the result should be something like this:{ data” : http://localhost:7474/db/data/ , management” : http://localhost:7474/db/manage/ , bolt” : bolt://localhost:7687”}Be aware that if you’re using Neo4j 4.0, the bolt:// connection schema is replaced with neo4j://.Setting up Neo4j to be accessible over the internetNow we will make the Neo4j service available to anyone over the internet. We accomplish this by changing few things in the Neo4j config file.Access the Neo4j config file from the following path on the Linux machine where we installed Neo4j:/etc/neo4j/neo4j.confI am going to access the conf file using the vim MobaXterm command:sudo vim /etc/neo4j/neo4j.confAdd/edit the following lines to the config fileOnce the neo4j.conf file is open in Vim editor, press key ‘I’ to enter ‘INSERT’ mode and make the relevant changes. Once complete, click the ‘esc’ key and ‘:wq’ to save changes and quit the editor:dbms.connector.bolt.enabled=truedbms.connector.bolt.listen_address=0.0.0.0:7687dbms.connector.http.enabled=truedbms.connector.http.listen_address=0.0.0.0:7474The above commands will enable the connector to bolt and http (not secure) for the IP addresses and the ports we mentioned. Note that mentioning the IP part as 0.0.0.0 will allow all IP addresses to hit your machine at port 7474 for Neo4j access. This is not secure, and I suggest you to use the IP address of the machine you intend to access the Neo4j server from.To access port 7474 from outside the Linux machine, you will have to add it to the inbound port rules for the virtual machine. See how to do that here.Restart the Neo4j service after making these changes:sudo service neo4j restartWe can now access Neo4j browser by using:http://<IP address/DNS for linux machine>:7474/browser/When connected to the Linux machine from your local/other machine, the browser looks like this:Neo4j browserThe default credentials for Neo4j browser are:Username: neo4jPassword: neo4jYou will be prompted to change the password on the first login. Set a new password and you are set! You can start writing Cypher queries to generate data, and create some graphs.You can further increase the heap size of your Neo4j setup to store more data to improve performance. This is done by uncommenting and updating the following parameters in the conf file:dbms.memory.heap.initial_size=4gdbms.memory.heap.max_size=4gThat’s all folks.Edit: Got Neo4j up and running? Read about next steps to get started on modeling your data in a graph database here:Neo4j Data modelling 101Get started with moving your data into Neo4jmedium.com;Dec 30, 2019;[]
https://medium.com/neo4j/creating-api-in-nestjs-with-graphql-neo4j-and-aws-cognito-cf92cf40b355;Konrad KalicińskiFollowOct 25, 2022·7 min readCreating API in NestJS With @graphql/neo4j and AWS CognitoData sample from the neo4j movies databaseWhen I discovered Neo4j Graph Database, it was love at a first sight. It solves a lot of problems of highly relational databases. Besides, its visual representation of the data model is much easier to read and understand. I was looking for a solution that will allow me to quickly set up a safe backend, but I didn’t find anything mature enough and fast at the same time.Then I discovered the @neo4j/graphql module, which allows you to have GraphQL API in your node application just by defining models and relationships between them. Additionally, I wanted my app to be secured with AWS Cognito. Luckily, @neo4j/graphql covers it all.GraphQL, with all its flaws, seems to be a perfect tool to work with a graph database, even better than REST — it kind of uses graphs as an interface, which aligns ideally with Neo4j data model.I‘m sharing my experience as a blog post so that anyone with a similar need will find something that can be used as a starting project (and which is included at the end of the article as link to Github repo).What you’ll need:- Any tool that makes HTTP requests to get JWT token (postman, curl etc)- AWS Cognito account with a client without the secret- Docker — for Neo4j db, or Neo4j Desktop installedStep 1: Create NestJS ApplicationNest has a pretty powerful CLI, so we can use it to generate the app and later on to generate modules, controllers etc.To install nest cli tool it run the command:$ npm i -g @nestjs/cliAnd then, just create new nest project (GANN is just pronounceable acronym for GraphQL+AWS+Neo4j+NestJS):$ nest new gannThen you need to set the preferred package manager and you’re ready to go. You may verify it by starting the server with npm start and visiting http://localhost:3000 — you should see Hello World!Step 2: Install Neo4j on Your Local MachineIf you’re using docker, you may run graph database container from official Neo4j docker image. It is super easy. The only thing that we need to take care of, is to run it with APOC library enabled.To be sure that your docker container starts every time you work on your app, you may want to add this commands to npm scripts:// package.json// initialize the database with APOC plugins enabled db:init :  docker run -d --publish=7474:7474 --publish=7687:7687 -e NEO4J_apoc_export_file_enabled=true -e NEO4J_apoc_import_file_enabled=true -e NEO4J_apoc_import_file_use__neo4j__config=true -e NEO4J_AUTH=neo4j/password -e NEO4JLABS_PLUGINS=[\ apoc\ ] --name neo4j --restart=always neo4j ,// start database container container db:start :  docker start neoj4 // start database and run nest in watch mode start:dev :  npm run db:start && nest start — watch ,Then just start the app and check if database is running under port 7474 in your browser.npm run start:devOf course, you can use Neo4j Desktop to start local database, or even use external provider like Neo4j AuraDB. We will stick with docker for the tutorial purposes.Step 3: Add Required Neo4j and GraphQL Node ModulesWe need to add all modules required to run GraphQL API and connect to Neo4j database. Let’s start with:npm i @neo4j/graphql @nestjs/graphql @nestjs/apollo neo4j-driverStep 4: Adding Required ModulesWe will put everything related our GraphQL setup in a separate module, imported in app.module.ts. We will name it gql since graphql name is already taken by the official nest module.nest g module gqlIt needs to be imported in app.module.ts.We want to use environment variables to provide database credentials. NestJS recommends to use @nestjs/config module to provide env variables in the process.env global scope. It need to be installed separately.npm i @nestjs/configWhen adding it, just make sure that config module is on the top of every other modules — only after it bootstraps variables from .env file are accessible.// app.module.ts...imports: [  ConfigModule.forRoot({ isGlobal: true }),   GqlModule]...Create .env file in the project root directory that contains:NEO4J_USERNAME=neo4jNEO4J_PASSWORD=passwordNEO4J_URI=bolt://localhost:7687We can’t run GraphQL without the model, so we need to define it — let’s create something very basic for now just to run the module. We will follow the example from Neo4j movie database so create type-defs.ts file in the gql module directory.// type-defs.tsimport { gql } from  apollo-server-express export const typeDefs = gql`type Movie {  title: String!  released: Int!}`Then, we need to provide a factory method that will return the GraphQL configuration and use it in the GqlModule:After server restart we should see GraphQL log entries. To verify it works, we can use GraphQL playground under localhost:3000/graphql and run a query to get all movies.query {  movies {    title  }}As a result, we will see an empty movies array.Step 5: Model DefinitionFor this tutorial purposes, we will use a model definition taken from the official docs and update the type-defs.ts file.This allows us to create 2 node types: Person and Movie. Person can act in the movie, but also it may be a director of the movie.We are ready to test GraphQL API. The sample mutation taken from the docs will create a Forest Gump” movie with the release year property, as well as one Person who is director of the movie (Robert Zemeckis) and one actor (Tom Hanks), and it will connect them with proper relations (ACTED_IN and DIRECTED).mutation CreateMovieAndDirector {  createMovies(    input: [      {        title:  Forrest Gump         released: 1994        director: {          create: {            node: {name:  Robert Zemeckis , born: 1951}          }        }        actors: {          create: {            node: {name:  Tom Hanks , born: 1956}          }        }      }    ]  ) {    movies {      title      released      director {        name        born      }      actors {        name      }    }  }}And it should return response:{   data : {     createMovies : {       movies : [        {           title :  Forrest Gump ,           released : 1994,           director : {             name :  Robert Zemeckis ,             born : 1951          },           actors : [            {               name :  Tom Hanks             }          ]        }      ]    }  }}Pretty neat, in my opinion 🙂Step 6: JWT AuthorizationThe setup is not ready yet — all mutations can be called by anyone. That’s why we will provide a basic JWT authentication/authorization mechanism. We will test it with one of the popular identity providers — AWS Cognito.In this tutorial I’m skipping the step describing how to create proper AWS Cognito user pool. I just assume that you’ve created one pool with app, that has no client secret and you’ve got the required information: user poll id, region name and client id. We will put all of that in env variables:// .envCOGNITO_USER_POOL_ID=<REGION>-<ID>COGNITO_REGION=<REGION>COGNITO_CLIENT_ID=<ID>Step 7: Authentication EndpointFor the login we will use simple rest controller. We add /login endpoint, where user sends login credentials. We will use the amazon-cognito-identity-js module.npm i amazon-cognito-identity-jsAnd now we create auth module and corresponding controller, service and config provider.# generate modulenest g m auth# generate controllernest g co auth --project auth --no-spec# generate service nest g s auth --project auth --no-spec# generate confignest g --project auth --no-spec cl auth/auth.configWe will put required env variables into the config class:Don’t forget to add the config class as module provider.import { Module } from @nestjs/commonimport { AuthController } from ./auth.controllerimport { AuthService } from ./auth.serviceimport { AuthConfig } from ./auth.config@Module({  controllers: [AuthController],  providers: [AuthConfig, AuthService],})export class AuthModule {}Now, in controller, we create the login endpoint:Finally, we create the authenticateUser method in the auth.service.ts that is responsible for the login flow.Now it’s time to test our endpoint. Make sure that at this point you’ve got an user in the Cognito user pool, since it’sre quired to run this step.Hint: If you created user, but you don’t know how to get rid of Force change password status just run this command (official aws cli with python is required):aws cognito-idp admin-set-user-password --user-pool-id <your-user-pool-id> --username <username> --password <password> --permanentSending request like:{  name :  user@example.com ,  password :  password }should return response like below.{   idToken : {     jwtToken : <token-hash>,     payload : <user-id-data-json>  },   refreshToken : {     token : <token-hash>  },   accessToken : {     jwtToken : <token-hash>     payload : <user-access-data-json>  },   clockDrift : 0}Step 8: Securing GraphQLHaving authentication in place, we can secure our GraphQL setup. We will use an @auth directive provided by @neo4j/graphql, we can extend our model and secure it. Let’s append below configuration to the type-defs.ts file:extend type Person @auth(  rules: [    { operations: [READ], allowUnauthenticated: true }    { operations: [CREATE, DELETE, UPDATE], isAuthenticated: true }  ])It says that for any read operation on the Person model we allow user to be unauthenticated. For CUD operations it’s expected that user is logged in.We also need to provide the authentication information in the gql module somehow. For that, we will use auth plugin in Neo4jGraphQL configuration:const {COGNITO_REGION, COGNITO_USER_POOL_ID} = process.envconst jwksEndpoint = https://cognito-idp. +                           COGNITO_REGION +                     .amazonaws.com/ +                      COGNITO_USER_POOL_ID +                     /.well-known/jwks.json`const neoSchema = new Neo4jGraphQL({  typeDefs,  driver,  plugins: {    auth: new Neo4jGraphQLAuthJWKSPlugin({jwksEndpoint}),  },})Great, Let’s try it with query — we expect it to return some results.query {  people {    name    born  }}It works, great. Now let’s try the same mutation we’ve used before:mutation {  createPeople(    input:{      name:”Bob”,      born:1982    }  )  {    people {      name    }  }}As we expected, does not work — we get Unauthenticated response.So now let’s try with the token and put in in Authorization header:{   Authorization :  Bearer <access-token> }And that’s it. Your GraphQL API is secured. From now the only thing you need to change to expose new models is to modify the type-defs.ts file.I recommend reading the entire auth documentation, since it’s pretty powerful and customizable — you can create role based authentication, control access certain fields, combine with Cypher WHERE queries, combine with roles, and more.Useful links: - gann-starterkit epository (the starter project for the example above)- @neo4j/graphql repository- @neo4j/graphql (official documentation);Oct 25, 2022;[]
https://medium.com/neo4j/introducing-the-graph-app-gallery-81aa3e63567b;Mark NeedhamFollowApr 5, 2019·4 min readIntroducing the Graph App GalleryIn October 2017 we launched the Neo4j Graph Platform, and with it the concept of the Graph App. Since then a lot of Graph Apps have been created, but there’s been no easy way to discover them. Until now!What is the Neo4j Graph Platform?The Neo4j Graph Platform is built around the Neo4j native graph database and includes the following components:Emil introduces the Neo4j Graph PlatformNeo4j Desktop is developers’ mission control console for all things Neo4jNeo4j Native Graph Database supports transactional applications and graph analyticsNeo4j Graph Analytics helps data scientists gain new perspectives on dataData integration tools expedite distilling RDBMS data and other big data into graphsGraph visualization and discovery help communicate graph benefits throughout the organizationThe Neo4j Graph PlatformWhat is a Graph App?A Graph App is a single-page application that takes advantage of some services provided by Neo4j Desktop — primarily the management of Neo4j Databases. That makes it super convenient to try out ideas.The Neo4j Browser Graph AppThe most well know Graph App is the Neo4j Browser, a graphical user interface (GUI) that can be used for adding data, running queries, creating relationships, and moreThis is the only Graph App installed by default on the Neo4j Desktop.In the 18 months since the launch, we’ve seen the creation of Graph Apps to solve all sorts of different problems. To name just a few:The Neo4j ETL Tool provides an easy to use UI for importing data from relational databases.Halin makes it easy to monitor your Neo4j servers and runs diagnostics over your configuration files.Neo4j Database Analyzer lets you get a quick understanding of the data structures in your Neo4j DatabaseGraph Apps are everyUnless you keep a close eye on the Neo4j Developer Blog it’s hard to keep track of all the Graph Apps that have been released, so this week the Neo4j Labs team are happy to launch the first version of the Graph Apps Gallery.The Graph Apps GalleryThe Graphs App Gallery is an online Graph App that aims to make it easier for users to discover Graph Apps. It contains Graph Apps that have been approved by the Neo4j Labs team.If you have a Graph App that you’d like to us to add to the gallery, please send us an email to devrel@neo4j.comGraph Apps GalleryAt the moment it’s a HTML page that contains descriptions of Graph Apps and a deep link that will pop up the installer in the Neo4j Desktop for each of the apps.Please note that the neo4j:// URLs for installing the Graph Apps don’t work yet on Linux, there please copy the install URL and use the Neo4j Desktop sidebar (see below) to install the individual Graph Apps.We’ll evolve this over time based on the feedback we get.How do I use the Graph Apps Gallery?There are two ways that you can use the Graph Apps Gallery:You can navigate to it at install.graphapp.io/, and install all your Graph Apps directly from your web browser.You can install the Graph Apps Gallery as an ‘online graph app’ by copy/pasting https://install.graphapp.iointo the ‘Install Graph Application’ box in the Neo4j Desktop, as shown in the print screen below.Installing the Graph Apps GalleryThen you have the Gallery available within Neo4j Desktop.This is also the general way to install Graph Apps by pasting their file, npm-repository or online-URL in this field.Note that you will need to be connected to the internet for the graph app to work — it does not download anything locally.We hope you like it and have fun trying out the various Graph Apps. If you have any feedback or questions, let us know!;Apr 5, 2019;[]
https://medium.com/neo4j/the-world-wide-web-is-like-a-bow-tie-discovering-graph-structure-with-neo4j-5d1b684cd4ee;Nathan SmithFollowNov 28, 2019·5 min readThe world wide web is like a bow tieDiscovering graph structure with Neo4jBy dabasco — Private photo., Attribution, LinkIn chapter 13 of Networks, Crowds, and Markets, authors David Easley and Jon Kleinberg provide an interesting survey of the early history of the world wide web. They highlight the work of Andrei Broder and his colleagues. They used Alta Vista search engine results to describe the structure of the web in a paper published in 2000.Broder et al. considered the web as a structured graph. Each page is a node, and the hyperlinks leading from one page to another are directed relationships. They found that there was a large strongly connected component at the center of their graph. When two nodes are part of a strongly connected component, you can start from either node and reach the other along a path that follows the directed relationships.Nodes A and B are part of the same strongly connected component because A can reach B and B can reach A. Nodes D and E are not part of the same strongly connected component because E cannot reach D.Broder et al. classified some nodes that were not part of the giant strongly connected component as IN and OUT. The IN group could reach the giant component, but there was no path back to the IN nodes from the giant component. The OUT group could be reached from the giant component, but there was no path back to the giant component from the OUT nodes.Broder and his colleagues classified the remaining pages as tendrils if they could be reached from IN or could reach OUT, and they were not part of the giant component. Nodes that could be reached from IN and could reach OUT, but bypassed the giant component were called tubes. There were also a relatively small number of pages in disconnected components that were isolated from the giant component.The analysis by Broder and his colleagues has held up well over the past twenty years. In 2017, the paper received the Soeul Test of Time Award as a paper that has demonstrated significant scientific, social, or technical impact.In the exercises for chapter 13, Easley and Kleinberg offer a small data set with which we can test our understanding of the concepts they presented. You can find the giant component and in and out segments in their data by visual examination, but the exercise also gives us a good opportunity to practice our Neo4j skills.First, we create the sample data in Neo4j. You can use Neo4j desktop or a Neo4j sandbox. Turn on the setting Enable multistatement query editor” to make the code below work.UNWIND range(1,18) AS numCREATE (p:Page {pageNumber:num})MATCH (p1:Page {pageNumber:1}), (p2:Page {pageNumber:8}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:3}), (p2:Page {pageNumber:8}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:4}), (p2:Page {pageNumber:1}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:4}), (p2:Page {pageNumber:3}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:4}), (p2:Page {pageNumber:5}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:5}), (p2:Page {pageNumber:10}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:6}), (p2:Page {pageNumber:2}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:6}), (p2:Page {pageNumber:7}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:7}), (p2:Page {pageNumber:8}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:8}), (p2:Page {pageNumber:13}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:9}), (p2:Page {pageNumber:3}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:9}), (p2:Page {pageNumber:4}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:9}), (p2:Page {pageNumber:15}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:11}), (p2:Page {pageNumber:7}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:12}), (p2:Page {pageNumber:13}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:13}), (p2:Page {pageNumber:14}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:14}), (p2:Page {pageNumber:9}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:15}), (p2:Page {pageNumber:14}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:15}), (p2:Page {pageNumber:16}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:15}), (p2:Page {pageNumber:18}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:16}), (p2:Page {pageNumber:10}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:17}), (p2:Page {pageNumber:16}) MERGE (p1)-[:LINKS_TO]->(p2)MATCH (p1:Page {pageNumber:18}), (p2:Page {pageNumber:13}) MERGE (p1)-[:LINKS_TO]->(p2)The first exercise for the chapter is to find the largest strongly connected component. Neo4j graph algorithms has a procedure to do exactly that. You can find more about the algorithm in Mark Needham and Amy Holder’s blog post and in their excellent book available for download from Neo4j. The code to apply the algorithm to our data looks like this.CALL algo.scc( Page ,  LINKS_TO , {write:true, partitionProperty: partition })YIELD maxSetSizeRETURN maxSetSizeThis will add a new property called partition” to our nodes indicating the component to which they belong. It will also return the number of nodes in the largest component.To see the nodes that are part of the giant component we can run this code.MATCH (p:Page) WITH p.partition as partition, collect(p) AS pagesRETURN partition, pagesORDER BY size(pages) DESCLIMIT 1We want to easily find the members of the giant component, so lets add a label to those nodes.MATCH (p {partition :0})SET p:SCCNow that we know the nodes in the giant component, we can find the IN and OUT groups. We add labels to these groups of nodes as well.MATCH (p:Page)-[:LINKS_TO*]->(gc:SCC)WHERE NOT p:SCCSET p:InMATCH (gc:SCC)-[:LINKS_TO*]->(p:Page)WHERE NOT p:SCCSET p:OutNow we can visualize the bow tie structure of our example data.MATCH (p:Page) RETURN pIf you enjoyed this post, find a copy of Networks, Crowds, and Markets or read the online version. Also check out my previous posts about Chapter 5 and Chapter 10.;Nov 28, 2019;[]
https://medium.com/neo4j/playing-around-league-of-legends-with-neo4j-prologue-87650ceae1c;Jimmy CrequerFollowAug 22, 2019·5 min readPlaying around League of Legends with Neo4j — PrologueIt has been a while (few years actually) since I wanted to use Neo4j, but until now I couldn’t find an interesting dataset to get my hands on.A few days ago, the League of Legends European Championship (LEC) 2019 Summer Regular Season just ended and this is the time, looking into playoffs coming up next week, to look back on what happened during the last ten weeks. I came across a site which gathers a lot of interesting information about the League of Legends competitive scene and decided to scrape some data from here.The League of Legends European ChampionshipThis post aims to import simple data and to build a small graph in Neo4j, then execute some Cypher queries and see it we can get some insights from the data.1. Build the graph1.1. Import the dataYou can find the data in the Github repo.Copy the 3 files into the import folder of your Neo4j installation, then run the following queries.Note : I didn’t include the scraping part in this post because I am planning to write a dedicated post about it in the future (after I clean up my code).The teamsLOAD CSV WITH HEADERS FROM  file:///teams.csv  AS rowMERGE (t:Team { nameShort: row.nameShort, nameFull: row.nameFull })10 teams are participating in the 2019 LEC Summer Season, namely G2 Esports, Fnatic, Splyce, FC Schalke 04, Rogue, Team Vitality, SK Gaming, Origen, Misfits Gaming and Excel Esports.The matchesLOAD CSV WITH HEADERS FROM  file:///matches.csv  AS rowMATCH (t1:Team { nameShort: row.team1 }),      (t2:Team { nameShort: row.team2 }),      (winner:Team { nameShort: row.winner })MERGE (m:Match { date: row.date, team1: row.team1, team2: row.team2 })MERGE (t1)-[:PLAYED]->(m)<-[:PLAYED]-(t2)MERGE (winner)-[:WON]->(m)91 matches were played this season so far over 9 weeks, with an additional tiebreaker game at the end. We link the match to the teams with a PLAYED relation and add an additional relationship representing which team WON .The castersLOAD CSV WITH HEADERS FROM  file:///casters.csv  AS rowMATCH (m:Match { team1: row.team1, team2: row.team2 })MERGE (c:Caster { name: row.casterName, type: row.casterType })MERGE (c)-[:CASTED]->(m)Lastly, we create the caster nodes and add a relationship to the matches they CASTED .1.2. Check the graphThe schema is very simple, so let’s look a bit closer to one match.MATCH (m:Match)WITH mLIMIT 1RETURN m, (m)--(:Team), (m)--(:Caster)A single match and its relationshipsA match (purple node) was played on June 7 by Splyce and G2 Esports (orange nodes), and eventually won by G2 Esports. Ender and Quickshot (blue nodes) were casting this match.2. Sample Queries2.1. Number of matches casted per CasterMATCH (m:Match)WITH COUNT(m) AS numberOfMatchesTotalMATCH (c:Caster)-[:CASTED]-(m:Match)RETURN c.name AS casterName,        COUNT(m) AS numberOfMatchesCasted,        COUNT(m) * 100.0 / numberOfMatchesTotal AS percentageOfMatchesORDER BY numberOfMatchesCasted DESCLooking from the bottom of the table, PapaSmithy and Foxdrop actually casted only one week worth of matches, so we expected to see them at the bottom. Though, I was surprised that Quickshot casted so few matches compared to the others casters (20 matches compared to 30+ matches of the top 5 casters).2.2. Teams casted by Medic & VediusMATCH (:Caster {name:  Medic })-[:CASTED]->(m:Match)<-[:CASTED]-(:Caster {name:  Vedius })WITH mMATCH (t:Team)-[:PLAYED]->(m)RETURN t.nameFull AS teamName,        COUNT(*) AS numberOfTimesCastedORDER BY numberOfTimesCasted DESCIn League of Legends, usually two casters comment together and the better their synergy, the more they make the game entertaining. I have a personal preference for the MediVedi” duo, composed of Medic and Vedius.I was interested to know if we could find something interesting among the teams they casted together. We can see that they casted more SK Gaming and Origen compared to Excel Esports and Team Vitality.2.3. Matches which had a tri-castMATCH (t:Team)-[:PLAYED]->(m:Match)<-[:CASTED]-(c:Caster)WITH m,      COLLECT(DISTINCT t.nameFull) AS matchup,      COLLECT(DISTINCT c.name) AS castWHERE size(cast) = 3RETURN m.date AS date,        matchup[0] +   VS   + matchup[1] AS Match,       cast AS CastORDER by date ASCSince tri-cast usually happen when a match is important, it is understandable that the more you progress into the season, the more matches matter. By looking at the results from the Cypher query, we notice that the number of matches that was tri-casted was higher in August, which also corresponds to the last month of the season.Noticeably, Vedius was part of 14 among the 21 casting trios.2.4. Casters who never casted togetherMATCH (c1:Caster),      (c2:Caster)WHERE id(c1) < id(c2)   AND NOT( (c1)-[:CASTED]->(:Match)<-[:CASTED]-(c2) )RETURN c1.name,       c2.nameBecause Neo4j is built around relationships between entities, we can also look for relationships that don’t exist. This is useful when building a recommendation engine for example.In this case, casters who never casted together” means that there is no relation (c1)-[:CASTED]->(:Match)<-[:CASTED]-(c2).3. ConclusionUsing Neo4j to build a graph from simple data, I was able to play around simple Cypher queries and get some interesting insights from the data. Some of the queries are, I think, actually very hard to do in SQL, but that would deserve its own post.In the next steps, I will build a more complex graph using richer data such as players, champions, picks & bans, items builds, detailed stats for each match, and add other regional leagues and international competitions to do even more interesting analysis.;Aug 22, 2019;[]
https://medium.com/neo4j/resotrack-exploring-the-resonate-api-with-django-neomodel-on-aura-da0eef97e65a;CristinaFollowAug 13, 2021·6 min readResotrack: Exploring the Resonate API with Django-Neomodel on AuraResotrack (demo, code) uses Neo4j to suggest popular, relevant tracks from Resonate’s music catalogResonate is a relatively young, (almost all) open source music streaming platform with a catalog featuring mostly but not exclusively ambient and electronic music, as categorized by Tags at the album level.Photo by Lee Campbell on UnsplashWhile many users enjoy exploring in random mode, listeners interested in learning more about the seemingly endless subgenres within Resonate would benefit from having a simple recommendation tool to find the most popular tracks for each tag.In an attempt to provide a solution to this discovery issue, Resotrack (demo, code) uses Neo4j to suggest popular, relevant tracks for each tag in Resonate’s music catalog.Resonate - the ethical music streaming co-opThe community-owned music network. Support the artists you love. What takes other services 200 plays, we do in 9 with…resonate.isResotrack also happens to be built using Django-Neomodel and deployed on Aura Free and Heroku. This example will present a brief overview of the project and provide simple examples of apoc.periodic.commit and importing JSON Data from a REST API into Neo4j using apoc.load.json.The Resotrack Data ModelResotrack’s base data modelWhile the creators of the tracks can add their tracks to a variety of types of TrackGroups (lp, eps, et cetera) and add Tags to these TrackGroups, non-artists can add tracks to TrackGroups of type playlist, and have limited (or no) tagging capabilities.Resotrack’s data model, with tagsAs Resotrack users search and interact with the data, the system calculates a Top Track” for each tag and presents the Top Track” for a particular tag to the user.Using ResotrackIn the example below, the user has searched for techno and is presented with the top track for tags containing the string techno. In the image below, since ambienttechno has techno as a substring, we can see that Daibutsu is the TOP_TRACK for the Tag ambientechno.Resotrack’s FrontendIn the Django admin, the superuser can view the project’s R(esonate) Users, Tags, Track groups, and Tracks.Resotrack’s AdminConnecting the Django-Neomodel App to Aura FreeIf you haven’t already, clone the repo to your local machine:git clone git@github.com:whatSocks/reso-tag-charts.gitGitHub - whatSocks/reso-tag-charts: Example Movie app for Neo4j and DjangoSearch Resonate ( code) tags for top tracks. python3 -m venv venv source venv/bin/activate pip install -r…github.comCreate or log in to your Aura accountNeo4j Aura - Fully Managed Cloud SolutionNeo4j Aura™ is a fast, reliable, scalable and completely automated graph database as a cloud service. Aura enables you…neo4j.comTap Create a database then select Aura Free.Follow the prompts to create the database and take note of the username (neo4j) and password.Log in to the neo4j browser and verify the database exists with the Open” button.In your terminal, navigate to the project’s root directory and create the database environment variable:export NEO4J_BOLT_URL=neo4j+s://neo4j:password@host-or-ip:portRun migrations and create your superuser (for the admin, this is using an SQLite database)./manage.py migrate./manage.py createsuperuserRun the server:python manage.py runserverNow you should be able to access http://localhost:8000 and view the empty app.Load the DataAn empty database is no fun. We can use apoc.load.json (Strava example) and apoc.periodic.commit to load the data from the Resonate API.Importing JSON Data from a REST API into Neo4j - Developer GuidesWe are interested in importing the activities for the athlete who is logged in. That endpoint takes the following…neo4j.comYou can easily create a Listener account on Resonate and create your own playlists before you start. While not required, accounts are free and are helpful in understanding the lay of the data.Create ConstraintsCREATE CONSTRAINT ON (a:Ruser) ASSERT a.uuid IS UNIQUECREATE CONSTRAINT ON (a:TrackGroup) ASSERT a.uuid IS UNIQUECREATE CONSTRAINT ON (a:Track) ASSERT a.uuid IS UNIQUEAdd the first page of Playlists (a type of TrackGroup)WITH https://api.resonate.coop/v2/ AS uriCALL apoc.load.json(uri + ‘trackgroups?type=playlist’) // in this example, grabbing listener-generated playlistsYIELD valueUNWIND value[data”] as dataMERGE (u:RUser {uuid:toString(data[user][id])})MERGE (t:TrackGroup {uuid:toString(data[id])})MERGE (u)-[:OWNS]->(t)SET t.title = data[title]SET t.type = data[type]SET t.slug = data[slug]SET t.tracks_imported = falseAdd more TrackGroupsWITH https://api.resonate.coop/v2/ AS uriCALL apoc.load.json(uri + ‘trackgroups’) // in this example, grabbing listener-generated playlistsYIELD valueUNWIND value[data”] as dataMERGE (u:RUser {uuid:toString(data[user][id])})MERGE (t:TrackGroup {uuid:toString(data[id])})MERGE (u)-[:OWNS]->(t)SET t.title = data[title]SET t.type = data[type”]SET t.slug = data[slug]SET t.tracks_imported = falseAdd TracksCALL apoc.periodic.commit( MATCH (tg:TrackGroup)WHERE NOT tg.tracks_importedSET tg.tracks_imported = trueWITH tg limit $limitWITH https://api.resonate.coop/v2/ AS uri, tg.uuid as tg_idCALL apoc.load.json(uri + trackgroups/ + tg_id )YIELD valueUNWIND value[data][items] as itemsMERGE (u:RUser {uuid:toString(items[track][creator_id])})MERGE (track:Track {uuid:toString(items[track][id])})MERGE (t)-[:HAS_TRACK]->(track)MERGE (track)<-[:CREATED]-(u)SET track.title = items[track][title]SET track.tags_imported = falseRETURN count(*) ,{limit:10})Add TagsCALL apoc.periodic.commit( MATCH (u:RUser)-[:CREATED]->(track:Track)WHERE not u.uuid in [7212,4315,’4414] // bad dataAND NOT track.tags_importedSET track.tags_imported = trueWITH u as artist, u.uuid as user_id, count(DISTINCT track) as tracks,https://api.resonate.coop/v2/ as uriORDER BY tracks descLIMIT $limitCALL apoc.load.json(uri + artists/ + user_id + /releases) // grabbing allYIELD valueUNWIND value[data] as dataUNWIND data[tags] as tagsMERGE (t:TrackGroup {uuid:toString(data[id])})MERGE (user:RUser {uuid:toString(user_id)})-[:OWNS]->(t)MERGE (tag:Tag {name:toLower(tags)})MERGE (tag)<-[:HAS_TAG]-(t)SET tag.uuid=apoc.create.uuid()SET t.title = data[title]SET t.type = data[type]RETURN count(*) ,{limit:10})Now you should be able to explore the data locally, with your data safe and sound in Aura.About the AuthorsCristina and Alisson work at The SilverLogic, a software development company based in Boca Raton.Learn Moreapoc.periodic.commit - APOC Documentationapoc.periodic.commit(statement,params) - runs the given statement in separate transactions until it returns 0…neo4j.comImporting JSON Data from a REST API into Neo4j - Developer GuidesWe are interested in importing the activities for the athlete who is logged in. That endpoint takes the following…neo4j.comLoad JSON - APOC DocumentationWeb APIs are a huge opportunity to access and integrate data from any sources with your graph. Most of them provide the…neo4j.comResonate - the ethical music streaming co-opThe community-owned music network. Support the artists you love. What takes other services 200 plays, we do in 9 with…resonate.isGitHub - whatSocks/reso-tag-charts: Example Movie app for Neo4j and DjangoSearch Resonate ( code) tags for top tracks. python3 -m venv venv source venv/bin/activate pip install -r…github.comNeo4j Aura - Neo4j AuraNeo4j Aura is a fully managed cloud graph database service. Built to leverage relationships in data, Aura enables…neo4j.comGetting Started with NeomodelNeomodel is an Object Graph Mapper (OGM) for the Neo4j graph database built on py2neo and provides a Django ORM style…thobalose.co.za;Aug 13, 2021;[]
https://medium.com/neo4j/hands-on-graph-visualization-keylines-neo4j-9c5aeb7a8d3a;Dan WilliamsFollowMay 30, 2018·6 min readHands-on graph visualization: KeyLines & Neo4jAt Cambridge Intelligence, we build graph visualization technologies that make the world safer. KeyLines is our JavaScript SDK for interactive graph visualization. It’s ideal for hands-on Neo4j developers, offering the same power, scalability and flexibility for which the world’s most popular graph database has become known.A KeyLines visualization, using the ‘combos’ node grouping functionalityOver the last 7 years, hundreds of developers have used KeyLines to build visualization web applications for Neo4j. As a result, we’ve honed our integration code, documentation and support to make the process as simple as possible.Build from scratch or use a templateAs KeyLines is an SDK, there are limitless ways to customize your application. You have complete control over every aspect — UI, UX, functionality, visual appearance and branding. The most common approach is to use KeyLines’ API to harness functionality like the time bar, combos, SNA and geospatial visualization. For that, more advanced JavaScript developers can follow our tutorial over here.Another popular option — which we’ll cover in this post — is to use one of our demos to kick-start the process. With this approach, we’ll use KeyLines’ Cypher integration and a one-to-one mapping between Neo4j and the chart, so switching the schema and producing your own visualization is fast and simple. Let’s get started.A few prerequisites…To follow this tutorial, you’ll need:a KeyLines SDK login (request an account here)a Neo4j instance, loaded with some databasic knowledge of JavaScript and web applicationsThe visualization architectureThe KeyLines-Neo4j architecture is simple, but secure and flexible enough to work in any setup:The standard KeyLines / Neo4j visualization architecture.KeyLines itself is a JavaScript file, deployed to a web server. The KeyLines chart is then embedded as an HTML element somewhere within your application (or as a standalone component) which allows the user to interact with, and explore, the graph.Graphics rendering is powered by HTML5 Canvas or WebGL — KeyLines will switch between the two depending on the browser — so it performs well even with larger graphs. Also, everything stays inside your corporate firewall so data security shouldn’t be an issue.Step 1: Download our Neo4j demoOnce you’re in the KeyLines SDK, take a few minutes to look around the site and review the documentation. The ‘Basics’ section of the Documentation tab is the best place to start.Next, head to the Demos tab. Here you’ll find a gallery of KeyLines apps, ready to download and start customizing:KeyLines’ gallery of demosThere’s a few different demos designed for Neo4j integration, but ‘Integrate with Neo4j’ is the most straightforward. It uses Cypher queries and KeyLines’ incremental layout to explore a Neo4j dataset of movies and actors (appropriately starting from The Matrix).Exploring movie data with KeyLines, Neo4j and CypherTo the right of the demo KeyLines chart we’ve included an information box, containing the Cypher queries being raised, and some UI to run layouts from the KeyLines API.From the demo page, click ‘Download’.Step 2: Run the demo locallyThe demo will download as a Zip folder. Inside you’ll find the following files:KeyLines — Neo4j demo folder structureYou don’t need to know what each of these files does, but there are a few important ones:assets contains images for different components of the KeyLines chart, like the navigation controls and the chart watermark.css (as you’d expect) contains the different CSS files for the rest of your webpage. You can use ours, or write your own.images contains the image files for nodes, glyphs, etc.neo4j-db.js is the database driver, including Cypher integration and JSON parsing code.neo4j.js adds KeyLines functionality, as well as tying some of that functionality to general UI (e.g. run a layout when the user clicks on this button).To get the demo running locally, open ‘index.htm’ in your preferred browser. You should see something like this:A blank KeyLines chart, waiting for data from a Neo4j instanceAt this point, the demo is running from our local filesystem. The chart is blank because we haven’t connected it to our database yet.Step 3: Point the demo at your Neo4j databaseIn your ‘demo’ folder, find the neo4j-db.js file and add your instance login credentials to the following:headers: {Authorization: ‘Basic ‘+btoa(‘username:password’)},Obviously, this approach is only recommended during KeyLines prototyping. When you deploy your app in production, you’ll run Neo4j on a secure web server and only expose certain endpoints to KeyLines.Using Bolt?In this example, we’ve assumed you’re using Neo4j’s REST API to send queries back and forth. If you’re using Neo4j’s Bolt protocol, you’ll need to make some adjustments to the KeyLines demo at this stage:Set up the JavaScript Bolt library — either in your browser (using the browser version of the Neo4j driver) or on your server (for example by requiring Neo4j-driver in Node)Set up the driver — by adding your credentials and creating sessions to run KeyLines’ Cypher queriesProcess the Bolt response into KeyLines JSON — in a single loop you can: a) convert the node IDs, b) do the same for link/edge start and endpoints, c) convert other numeric properties.More detailed instructions can be found in this blog post.Step 4: Update the schemaNext you need to map your Neo4j data to the KeyLines schema, and choose the visual model you want to use.This stage can be as simple or as complex as you like, but our demo uses one-to-one mapping between the chart and database, and follows a typical user workflow (expand and explore). For most developers, a few simple ‘find and replace’ commands should do the trick.E.g. open neo4j-db.js, find e: item.d.type === ‘movie’ ? 1.7 : 2.5 , which enlarges movie nodes in our demo, and replace movie with whichever node type from your schema you want represented as a larger size.You’ll also need to adjust the Cypher queries in-line with your own schema. That’s in the queryTemplates object in neo4j-db.js.Finally, you can also change node and link styles at this stage, for example by finding the ‘dbDemoTheme’ object in neo4j.js and changing the font icons by editing the ‘fi’ property.Step 5: Run the demoThe final step is to start your database and load KeyLines. If you’re using Node, that’s a simple `npm install` followed by `npm start` in the root folder.Now navigate to http://localhost:8080 to find your interactive KeyLines — Neo4j demo:Our populated KeyLines chartOptional step 6: incorporate advanced functionalityBy now, you’ll have a simple demo, running from your database with your own visual styling. It’s a great starting point for you to visually explore and understand your Neo4j data.But if you want to take your insight a step further, the KeyLines API offers a huge library of advanced graph analysis and visualization functionality. With a few lines of JavaScript code (take a look at our 80+ demos for inspiration) you can incorporate graph layouts, SNA centrality measures, temporal / dynamic graph visualization, geospatial analysis, filtering or our advanced ‘combos’ functionality.Try it for yourselfThis tutorial is just the starting point. KeyLines and Neo4j are a powerful combination, and make it easy for you to build custom, flexible and high-performance graph exploration tools.Start a trial over here, or contact us for more information.;May 30, 2018;[]
https://medium.com/neo4j/working-with-neo4j-date-and-spatial-types-in-a-react-js-app-5475b5042b50;William LyonFollowJun 11, 2018·10 min readWorking With Neo4j Date And Spatial Types In A React.js AppBuilding A Dashboard App With Neo4j, Mapbox, React, and Nivo ChartsThis simple dashboard React app allows users to search for businesses within a radius of a specific point that has received reviews within a specified date range. Try it here.OverviewNeo4j 3.4 includes support for new spatial and temporal functionality, including Point and Date types, and indexes that enable querying using those types. I thought it would be fun to build a simple app that uses both the spatial and temporal features.In this post we walk through the steps to build a React.js dashboard type application that allows a user to search for businesses by location that have reviews within a certain date range and display some charts based on aggregations of these reviews. We’ll explore how to use neo4j-import with the new spatial and temporal types, how to use the new types in Cypher queries and with the Neo4j drivers, and how that all fits into a React app.DataFor this example I wanted a dataset that included both space and time components, so I decided to use the Yelp Open Dataset. This dataset released by Yelp includes a subset of the data powering the online reviews site, including over 5 million reviews of 174 thousand businesses in 11 metropolitan areas throughout the US and Europe. You can download the data here.The Graph Data ModelThe graph data model for the Yelp Open Dataset.There have been a few examples showing how to use the Yelp Open Dataset in Neo4j. From how to model and import the dataset, including the super fast neo4j-import tool, how to query the dataset using Cypher to find recommended businesses, and how to apply graph algorithms to the dataset.ImportWe first need to import this dataset into Neo4j. The Yelp data is provided in streaming JSON format (one JSON object per line). We have several options for how we could import this into Neo4j:Use apoc.load.json to import the JSON filesRead each JSON line and pass as parameters to a Cypher statementConvert to CSV and use LOAD CSVConvert to CSV and use neo4j-import for fast bulk loading importI opted for the last approach (using neo4j-import). The dataset is large enough that it will be faster to use the bulk import functionality instead of LOAD CSV or the other options. And my colleague Mark Needham had already written most of what I needed. I just extended his script to support the Point and Date types.Neo4j-import makes use of header files that define the properties and types for the import. So we’ll need to update the headers files for Business and Review nodes. First Business :id:ID(Business),name,address,city,state,location:Point(WGS-84)and Review :id:ID(Review),text,stars:int,date:DateWe don’t need to change the CSV file for reviews, since Neo4j is able to parse date strings in the format already used (YYYY-MM-DD), but we will need to add out Point type in the Business csv, we write the data to the csv as a map (or dictionary) that contains latitude and longitude: FYWN1wneV18bWNgQjJ2GNg , Dental by Design , 4855 E Warner Rd, Ste B9 , Ahwatukee , AZ , {latitude: 33.3306902, longitude: -111.9785992} We then use the neo4j-admin import command, passing in the CSVs to import the data into Neo4j.It’s important to note that neo4j-admin import does not create indexes for us, so we’ll need to explicitly create any indexes we want to use for initial data lookups. In this case we will create an index on the location property of our Business nodes and the date property of our Review nodes:CREATE INDEX ON :Business(location)CREATE INDEX ON :Review(date)You can find the full code for the import here.Hosting Neo4j In The CloudSince we’re building a web app we need to host our Neo4j database somewhere. Also, since we’ll probably want to serve our webpage over a secure HTTPS connection we’ll need to generate some trusted certificates for Neo4j that our web browser will accept. By default Neo4j will use a self-signed certificate, but that’s not good enough for most configurations.Fortunately, we can use the certbot tool from Let’s Encrypt to easily generate Certificate Authority signed certificates for Neo4j:# Use certbot to generate certificatescertbot certonly# Copy certs to Neo4j directorycp /path_to_certs/fullchain.pem /var/lib/neo4j/certificates/neo4j.certcp /path_to_certs/privkey.pem /var/lib/neo4j/certificates/neo4j.keyI initially used this process to secure my Neo4j connection on a VPS instance (see this page for the myriad options for deploying Neo4j), but ultimately I used Neo4j Cloud, which takes care of all the hassle of obtaining certificates :-) You can sign up for early access to Neo4j Cloud here.QueriesNow that we’ve created our database, we can write some Cypher queries to work with the data in Neo4j. We’ll focus on looking up businesses within some distance of a specific point and finding reviews within a specified date range.Spatial QueriesThe first bit of functionality we want to support is searching for businesses by location when the user selects a point on the map we need to search for businesses within a user-defined distance (say 1000 meters). Here’s how we can do that taking advantage of the spatial index on our Point type that we just created:MATCH (b:Business)WHERE distance(        b.location,         point({latitude:33.329 , longitude:-111.978})      ) < 1000RETURN COUNT(b) AS num_businesses-------------------------------------------------------num_businesses117We make use of the distance function to filter for businesses within 1000 meters of a point that we specify by latitude and longitude.If we prepend our query with PROFILE we can see the execution plan to verify that we are indeed using the spatial index.PROFILE results of querying for Businesses within 1km of a point, using the spatial index.This query takes about 7ms on my laptop to find businesses within 1km of a point in the Phoenix area.Date QueriesThe next query we want to write will search for reviews within a certain date range, say between March 23, 2015 and April 20, 2015. Here’s how we can write that query:MATCH (r:Review)WHERE date( 2015-03-24 ) < r.date < date( 2015-04-20 )RETURN COUNT(r) AS num_reviews-----------------------------------------------------num_reviews63527We have a few options for constructing dates in Cypher. We could pass each date component (year, month, day) as integers or, as we’ve done here, pass a string to be parsed to the date function. Again, we can PROFILE our query to ensure it is using the temporal index. This query takes ~12ms on my laptop, certainly better than if we had to scan over all 5 million Review nodes.Searching for reviews within a time range using the temporal index.We only touched on a small piece of the new temporal functionality in Neo4j, there are also other types, likeDateTime and LocalDateTime that take timezone into account, and durations for working with time periods. You can learn more about these features in the Neo4j documentation. My colleague Adam Cowley has put together a couple of great posts showing more detail on the Neo4j temporal types and using them with JavaScript.Putting It TogetherNow that we’ve seen how to search through space and time, we can combine the queries above to search for businesses by location that have reviews within a certain time range:MATCH (b:Business)<-[:REVIEWS]-(r:Review)WHERE distance(        b.location,         point({latitude:33.329 , longitude:-111.978})      ) < 1000       AND date( 2015-03-24 ) < r.date < date( 2015-04-20 )RETURN COUNT(b) AS num_businesses_with_reviews---------------------------------------------------------num_businesses_with_reviews69This query is just giving us the count of businesses in our radius that have any reviews within our date range. To populate our UI we actually need a bit more information. You can see the full query in section below Querying Neo4j From Our React App”.If we inspect the PROFILE of this query we’ll see that the query planner chooses to use the temporal index on :Review(date). This makes sense since it should be the most selective index as this index contain 5 million entries (reviews), while the spatial index only contains less than 200 thousand (businesses).Sometimes we want to force the use of one index over another using an index hint, for example if we wanted to use the spatial index instead of the temporal index. Currently index hints aren’t supported for spatial indexes, but this will be added in the next Neo4j release.React AppNow that we have our database and queries, we’re ready to start building our web app.Create React AppCreate React App is the easiest way to start a React project. It’s a tool for creating React application skeletons without having to configure build tools like Babel and Webpack. Creating a React app with create-react-app is as easy as:npx create-react-app spacetime-reviewsComponentsA React-based UI is made up of components that encapsulate logic for how to render a piece of the interface given some data (props). Here’s an overview of the components we’ll create for this application:App ComponentThis is our main component which will handle sending queries to the database using the Neo4j JavaScript driver, storing the results in (and maintaining other application) state. All other components in our application will be children of the App component.Map ComponentThe job of the Map component is to show a map that allows the user to select a point and display businesses as map markers.I’ve previously used Leaflet.js and Mapbox for a few different projects, like this Panama Papers address geocode example and this US Congressional district map, but not in a React app. So the first thing I looked for was a React component wrapper for Mapbox GL JS. I found react-mapbox-gl, published by Uber’s data science team, which seemed like what I needed.After starting to work with react-mapbox-gl however I felt constrained by working with only the props that the library makes available. Fortunately I found this blog post from Tristen Brown that helps explain how to use Mapbox GL JS alongside React. The basic idea is to use the Mapbox GL JS library inside our Map component, encapsulating the use of the library within this component. This felt less constraining to me, but I’m sure I could have gotten the app to work with react-mapbox-gl since Kepler.gl is built using it.We also can use React’s two-way data binding approach for triggering a call to fetch fresh data from Neo4j when the user selects a new location on the map.Here’s the event handler for our draggable marker in the Map component. The user drags it around the map and then releases it to select the center of a new location to search:In addition to updating the circle showing our search area, we grab the latitude and longitude from the map (as well as the zoom level), and call this.props.mapSearchPointChange(viewport). This function is actually defined in the App component and is passed to our Map component through props as a kind of callback. This is how React’s two way data binding pattern works, as callback functions passed to child components as props. Here’s the implementation of mapSearchPointChange in the App component:mapSearchPointChange simply updates state in the App component, which will trigger a re-render and thus a call to Neo4j to update data for the new location selected.ReviewSummary and CategorySummaryComponentsReviewSummary and CategorySummary are purely presentational components, responsible only for drawing charts based on the data received as props. They make use of the Nivo chart component library.As data is retrieved from Neo4j in the App component it is passed to these two presentational components to render the charts showing the histogram of review counts by stars and a pie chart of business categories.I found it useful to use the AutoSizer component from the React Virtualized library to allow the chart components to resize, taking the full height and width of their containers.Querying Neo4j From Our React AppTo fetch data from Neo4j we’ll query the database directly from our client application. This might not be an ideal architecture for a real world application, but will work fine for our app. A more realistic architecture might be to create a GraphQL API that queries our Neo4j database.To actually fetch data from Neo4j we create a function fetchBusiness that contains the Cypher query we want to execute, and updates state in the App component with the result of the query. We can call this function from the appropriate component lifecycle functions, such as componentWillUpdate and componentDidMount . Note that we import the Neo4j Date temporal type and instantiate a Date object to pass as a parameter to the query.Deploy With NetlifyBuild settings for deploying on Netlify.To deploy our React app we just need to be able to serve static content, so we we have lots of options. I used Netlify, which makes it easy to build and deploy our app.We can integrate a Netlify project with Github and create a git commit hook that triggers a build on each commit. We can also specify our environment variables for our Mapbox token and Neo4j credentials with Netlify.You can find the code for this project on Github and try it live here.The SpaceTime Reviews dashboard. Search for businesses by location that have reviews within a time range. Try it here.;Jun 11, 2018;[]
https://medium.com/neo4j/exploring-supervised-entity-resolution-in-neo4j-daed28cdce80;Zach BlumenfeldFollowNov 5, 2021·20 min readExploring Supervised Entity Resolution in Neo4jPhoto by Alina Grubnyak on UnsplashWhile Supervised Entity Resolution (ER) can be immensely valuable, it is sometimes difficult to apply and scale in the real-world enterprise setting.In this post, I explore how the Neo4j Graph Data Science (GDS) library can be applied to rapidly develop supervised ML pipelines for ER and walk through an example to demonstrate how it could be applied to your own data.GDS is designed to be intuitive and easy to use, reducing friction for graph analytics so that junior and senior data scientists alike can make rapid progress, scale, and deliver value quickly.We will start this post with a quick overview of Entity Resolution (ER), talk about ER in Graph, then dive into an example where we will use GDS Link Prediction Pipelines to train an entity linkage model and predict new entity links in the graph.From there we will go over quick procedures for generating resolved entity ids and query resolved entities out of the graph. At the end, I will link you to follow-up resources, including a GitHub repository* that should have everything you need to reproduce the example.*Note: This blog was initially written with GDS version 1.7 . While the workflow for the latest GDS 2.x should be the same, some of the procedure names and syntax have changed. If you are using GDS 2.x, the GitHub repository contains a compatible version of the code. Please see the Readme to locate it.What is Entity Resolution (ER)?Entity Resolution (ER) is the process of disambiguating data to determine if multiple records actually represent the same real world entity such as a person, organization, place, or other type of object.For example, say you have information on persons coming from different e-commerce platforms. They may have slightly different contact information, with addresses formatted differently, using different forms/abbreviations of names, etc.A human may be able to tell if the records actually belong to the same underlying entity, but given the number of possible combinations and matching that can be had, there is a need for an intelligent automated approach to doing so, which is where ER systems come into play.Why is ER Important for Data Science?We see application for ER across industries, including Online Advertising, Marketing, Retail, Anti-Money Laundering, and Law Enforcement.Oftentimes our analysis requires integrating multiple data sources together, and we aren’t always in control of the quality and consistency of those data sources. For example, integrating additional e-commerce information with Customer Relationship Management (CRM) data can provide better understanding of a customers preferences, which could result in better user experiences, product recommendation, and ultimately sales revenue. Being able to correctly identify and merge entities will be key to providing actionable insights and business value — otherwise any modeling and analysis we do on users could be incomplete and suffer from poor accuracy and biases.In other situations we may have just have one data source where identities are purely transient and/or severely obfuscated. One example of this is web browsing activity, where users may be browsing on the go, potentially on multiple devices without a traceable persistent id from account login.Without some sort of ER solution, an online advertiser would have a limited and fragmented view of user activity and preferences which could significantly hinder their ability to accurately target and drive engagement.ER in GraphGraphs are well suited for many ER problems because they can represent associated information between subjects with paths made up of nodes and relationships. The resulting connectivity between subjects can be used to make entity resolution inferences.If we believe two subjects actually represent the same underlying entity, we can represent it with a relationship in the graph. I will refer to this pairwise matching as Entity Linking” and label it with a SAME_AS” relationship type.In cases where we believe more than two subjects belong to the same entity, we can draw multiple entity linkage relationships. This forms small connected subgraphs, or components, where each disjoint component represents a single underlying entity.Assigning a unique key to each component gives us a resolved entity id that we can then use to query aggregate resolved entity information. It is not 100% necessary in every circumstance, but it is a helpful id to have if you want to move information to downstream processes outside the graph.It can be helpful to think about this ER workflow in these two distinct steps:Linking step, where you do pairwise entity linking, and theResolution step, where you identify resolved entities via the linkages so downstream processes can easily access information aggregated by resolved entity.The mechanism used for the resolution step is conceptually straightforward, though not always easy to implement depending on the underlying system. In our example below, we will go over an algorithm called Weakly Connected Components (WCC) that can be used to help create the resolved entity ids. Luckily, Neo4j and the Graph Data Science (GDS) Library are extremely well optimized to perform this step at scale.The Mechanism used for Entity Linking, on the other hand, can vary greatly by use case and project maturity. In some cases, the relationships that determine entity linkages will be known and provided ahead of time, allowing you to jump directly to the resolution step. Other times, there may be a set of deterministic business rules governing the process, where entity linkages are determined based on matching known connection patterns in the graph.This can be a common starting point for many ER projects and it can work well for simpler, more deterministic, ER problems. In these cases, queries can be used to scan the graph and determine where entity linkages should be inserted. In yet other cases where it is difficult to determine a known set of business rules, similarity algorithms, such as K-Nearest Neighbor (KNN) or Jaccard based Node Similarity, can be used to score potential pairwise entity linkages in an unsupervised manner. Depending on the techniques used, this can leverage a combination of both relationships and node attribute information.If you are fortunate enough to have a set of pre-identified, known, entity linkages, perhaps manually identified by an analyst or subject matter expert, then you also have the option of taking a supervised approach, which can be extremely powerful.This is what we are going to talk about today. In graph, we can use Link Prediction to solve this problem, which we will cover below. Like similarity scoring, this can leverage both relationship and node attribute information.If you are interested in more ER use cases for Neo4j outside of Link Prediction, I encourage you to review this use case paper which go over these at a high level.As with most things in Data Science and ML, there are many other variants of this approach that one can take for ER and deduplicating entities, many of which involve merging subjects together or creating new resolved entity objects.However, the pattern explained above can be a great place to start due its simplicity, maintainability, and separation from the original source data. Because original nodes are never merged/deduplicated and new nodes are never created, you can be agile from a Data science perspective, easily updating, backtracking, redoing, or dumping your ER analysis as needed without having to worry about reversing changes to original subject data or costly mutations to resolved entity objects.Cross Device Entity Linking ExampleTo demonstrate, we will use a codalab challenge dataset focused on cross device entity linking. The dataset contains a browsing log for a set of anonymized userIDs at the level of devices and corresponding click-streams. User clicks are linked to site URLs and HTML titles.They also release a subset of known existing entity linkages between userIds, which is intended for use in supervised learning. The challenge is to predict new entity linkages, effectively resolving the same user across multiple devices. This type of entity linking is useful for marketing and online advertising since generating more complete user profiles can enable better targeting. As explained in the challenge, persistent user ids are not always available, so companies often rely on these weaker device level or browser cookie ids. If you are interested you can find more details about the dataset and challenge here.One interesting thing about this dataset is that it does not have any user identifiers outside the transient userId. I think this is actually somewhat unique for an ER problem, usually you have some other identifiers, like usernames, email addresses, etc. In this case, we will need to train a model to predict underlying person identities just based on user browsing behavior.Importing Into Neo4j GraphFor this example I am just using 10% of the training set provided from the competition. I recommend keeping it around there for experimentation on the average laptop. That said, I haven’t had any issues loading in more of the data so long as the hardware limitations on your machine, particularly RAM, can handle the additional data. If you are interested in how to optimize GDS for analyses like these on your machine, please see the GDS system requirements for a high level, particularly around setting heap space. The GDS configuration guide covers this in even deeper detail.There are a couple different ways to construct a graph out of this data, and in fact, if you are interested in these sorts of problems, I would encourage you to try other schemas. But for sake of a quick example, we will go with the following, just leveraging the URLs (not even touching the HTML titles):There are two node labels, a User for each unique userId, and a Website for each unique URL. A VISITED relationship represents a user event with the URL. The SAME_AS relationships represent the provided known entity linkages between userIds.Web Hierarchy and CHILD_OF RelationshipsI am also making a CHILD_OF relationship to capture the hierarchical structure of the Websites. In the Codalab dataset, the words within each URL segment and query string are hashed to preserve anonymity, but the ordering is preserved. Below is an example — notice the forward slashes and question mark denoting the path segments and query string respectively:Example from https://competitions.codalab.org/competitions/11171As a result, if two URLs share a common domain or partial path pre-anonymization, they will also share it post-anonymization. This allows us to create a hierarchical structure like this:This structure reflects how web pages relate together in the real world, with high-level website domains and other sub pages underneath those. This structure adds richer interconnected context to the graph which will further help our model correlate user behavior.Exploring the GraphI will spare you much of the initial EDA, but here are a couple quick queries of the graph just to give you an idea for what things look like.Aggregate Tallies: Loading approximately 10% of the CodaLab dataset, we get almost 2.5 million total nodes and a bit over 5 million total relationships. The great majority of the nodes are Websites with only ~34k Users. The relationships are split mostly between ~2.8M VISTED and ~2.3M CHILD_OF relationships, with only ~5K SAME_AS relationships.$ CALL apoc.meta.stats() YIELD stats RETURN stats.nodeCount AS nodeCount, stats.relCount as RelationshipCount╒═══════════╤═══════════════════╕│ nodeCount │ RelationshipCount │╞═══════════╪═══════════════════╡│2433861    │5176804            │└───────────┴───────────────────┘$ CALL apoc.meta.stats() YIELD labels RETURN labels.User AS userCount, labels.Website AS WebsiteCount╒═══════════╤══════════════╕│ userCount │ WebsiteCount │╞═══════════╪══════════════╡│33941      │2399920       │└───────────┴──────────────┘$ CALL apoc.meta.stats() YIELD relTypesCount RETURN relTypesCount.VISITED AS `VISITED Count`, relTypesCount.CHILD_OF AS `CHILD_OF Count`, relTypesCount.SAME_AS AS `SAME_AS Count`╒═══════════════╤════════════════╤═══════════════╕│ VISITED Count │ CHILD_OF Count │ SAME_AS Count │╞═══════════════╪════════════════╪═══════════════╡│2841391        │2330404         │5009           │└───────────────┴────────────────┴───────────────┘Connected Browsing Activity: Users can be connected through browsing activities in multiple ways. They can be connected directly by visiting the exact same URL/Website but also in multi-hop ways by visiting websites linked through the hierarchical CHILD_OF relationships. Below is a good example:MATCH (u)-[v:VISITED]->(w:Website) WHERE u.userId in [ 1b13bc4a2b79448c462e4f3817e6d470 ,  f05fe874d83d346f4993ce5ea872f20b ]RETURN u,w,vImage by authorSAME AS Relationships: Sampling the same-as relationships it appears most components just consist of pairs of two userIds, with fewer components of three and more.MATCH (u1:User)-[r:SAME_AS]->(u2:User) RETURN u1,u2 limit 400We can calculate the component sizes globally across the graph using Cypher and the GDS Weakly Connected Components (WCC) algorithm. You will see that the majority of components consist of single userIds (no same-as relationships) with 2,995 (a little over 10%) having a size of 2 userIds, around 413 (~1%) with 3 userIds, and less than 1% of the components having more than that. The largest component has 8 userIds.CALL gds.graph.create(same-as-subgraph,User, SAME_AS)CALL gds.wcc.write(same-as-subgraph, {writeProperty: entityId})MATCH(u:User) WITH DISTINCT u.entityId as eid, count(*) as componentSize RETURN distinct componentSize, count(*) as numberOfComponents ORDER BY componentSize╒═══════════════╤════════════════════╕│ componentSize │ numberOfComponents │╞═══════════════╪════════════════════╡│1              │26259               │├───────────────┼────────────────────┤│2              │2995                │├───────────────┼────────────────────┤│3              │413                 │├───────────────┼────────────────────┤│4              │81                  │├───────────────┼────────────────────┤│5              │18                  │├───────────────┼────────────────────┤│6              │4                   │├───────────────┼────────────────────┤│7              │1                   │├───────────────┼────────────────────┤│8              │1                   │└───────────────┴────────────────────┘Pre-Processing & Feature EngineeringA couple steps before we create the the Pipeline:First, we need to create a graph projection for the pipeline to operate on. We did this above really quickly when we ran the WCC algorithm. This time we are going to project the entire graph. Graph projections copy nodes and relationships from the database to an analytics environment optimized for global traversals where we can run algorithms and machine learning models quickly over the entire graph or large portions of it.//create named graph - project entire graph with undirected relationshipsCALL gds.graph.create(  er-projection,  [User, Website],  {    SAME_AS: {      type: SAME_AS,      orientation: UNDIRECTED    },    CHILD_OF: {      type: CHILD_OF,      orientation: UNDIRECTED    },    VISITED: {      type: VISITED,      orientation: UNDIRECTED    }  }) YIELD nodeCount, relationshipCount, createMillis╒═══════════╤═══════════════════╤══════════════╕│ nodeCount │ relationshipCount │ createMillis │╞═══════════╪═══════════════════╪══════════════╡│2433861    │10353608           │931           │└───────────┴───────────────────┴──────────────┘When we specify orientation as ‘UNDIRECTED’ we effectively create an undirected graph for the projection. This distinction is important for upcoming algorithms, particularly for Link Prediction, which expects undirected relationships.Next, we are going to do some feature engineering — specifically we are going to generate node embeddings. It is usually best to generate features inside the pipeline if you can, as the steps are automatically tracked with the data splitting and model evaluation. However, in this specific instance, we will be further filtering the graph during model training and the relationships and nodes we need will be filtered out.We will use a Node Embedding technique called Fast Random Projection or FastRP” for short. If you are interested in the technical details, please check out the documentation. To summarize for our intents and purposes, the job of FastRP is to take the complex interconnected structure of the user browsing activity and web hierarchy, and translate it into fixed length numeric vectors, one for each node in the graph. Executing FastRP successfully here means that users with similar browsing activity will have embedding vectors that are relatively close, while users with very different browsing activity will have relatively distant embeddings. You can imagine just how useful such features will be for model training.Of the multiple node embeddings techniques available in GDS, FastRP is chosen here because it it is relatively easy to execute and also VERY performant, taking only seconds to complete for this dataset. I did adjust the iteration weights and dimensionality a bit from defaults in attempt to capture more of the rich graph structure. This was just done by hand (could probably be optimized further).//create fastRP embeddings based on VISITED and CHILD_OF relationshipsCALL gds.fastRP.mutate(  er-projection,  {    mutateProperty: embedding,    relationshipTypes: [CHILD_OF , VISITED],    iterationWeights: [0.0, 1.0, 0.7, 0.5, 0.5, 0.4],    embeddingDimension: 128,    randomSeed: 7474  }) YIELD nodePropertiesWritten, computeMillis╒═══════════════════════╤═══════════════╕│ nodePropertiesWritten │ computeMillis │╞═══════════════════════╪═══════════════╡│2433861                │11130          │└───────────────────────┴───────────────┘Configuring the Link Prediction (LP) PipelineWe will use Link prediction in GDS to accomplished supervised entity linking. As the name implies, Link Prediction is a machine learning technique to predict whether or not a relationship should exist between two nodes. A training example consists of input features generated from two nodes, with a binary class output representing whether or not a link exists between them.GDS supports a Pipeline for Link Prediction. The Pipeline enables you to configure a series of steps for engineering features, data sampling and splitting, and hyper-parameters for training, in addition to procedures for prediction. It abstracts away many of the in-the-weeds technical challenges traditionally associated with link prediction, including sampling techniques for severe class imbalance and dealing with data leakage problems. As an ML-pipeline, it also saves your steps and the resulting trained model, making it easier to track and version your work.To create the pipeline for our example we can execute the calls below:Instantiate the pipelineCALL gds.alpha.ml.pipeline.linkPrediction.create(er-pipe)2. Calculate link features, specifically features that represent a measure of distance between the FastRP embeddings. As stated above, users with similar browsing activity should also have close embeddings, so the distance metric can provide a really good signal for our model. In this case I am adding two variants of distance, Least Squares (L2) and Cosine link feature combiners.//add L2 link featureCALL gds.alpha.ml.pipeline.linkPrediction.addFeature(  er-pipe,  l2,   {nodeProperties: [embedding]})//add cosine link featureCALL gds.alpha.ml.pipeline.linkPrediction.addFeature(  er-pipe,   cosine,   {nodeProperties: [embedding]})3. Configure data splitting.CALL gds.alpha.ml.pipeline.linkPrediction.configureSplit(  er-pipe,  {    testFraction: 0.3,    trainFraction: 0.7,    negativeSamplingRatio: 10,    validationFolds: 5  })We accomplish a couple things here. First, we decide on split ratio between training and test sets, just like a traditional ML workflow. A big difference to be aware of in this pipeline is the introduction of a third, feature input, set. If you allocate trainFraction=0.4 and testFraction=0.2 , then the remaining 1 — 0.4 — 0.2 = 0.4 or 40% will be allocated to the feature input set. The Feature input set is used to avoid data leakage while generating node properties inside the pipeline, such as centrality, node embeddings, or other similar metrics. We will not really be leveraging it in this example (our embeddings do not leverage the SAME_AS relationships), but it is critical to use when you are generating node properties off the links you are trying to predict.Second, we configure the negativeSamplingRatio. Negative examples in this context are any pairs of user nodes which do not already have a SAME_AS link between them. The negative sampling ratio dictates the rate at which these pairs are sampled relative to the known SAME_AS pairs during training. Link Prediction problems tend to be highly imbalanced with way more negative examples possible in the graph than positive ones — it is an O(n²) problem.Neo4j’s recommended value for negativeSamplingRatio is the true class ratio of the graph. However, the higher the negative sample ratio is, the larger your dataset for training and testing becomes, which translates to longer evaluation time. So for larger graphs, a tradeoff exists between evaluation accuracy and speed, and I have personally found that setting the negative sampling rate to the true sample ratio is not always feasible as the graph grows in size. In this example, the true class would be calculated as:true_class_ratio = (q - Number_Of_SAME_AS_Rels) / Number_Of_SAME_AS_Rels where q = Number_Of_Users * (Number_Of_Users-1)/2Which comes to almost 115,000. Here I choose negativeSamplingRatio=10 as it evaluates relatively quickly for purposes of running through an example and demonstrating how things work.Hyper-parameters and training configuration. These cover many of the ordinary things you would expect for ML training — i.e. bias features, penalties, batch sizes, tolerance, epochs etc.. Each set of parameters below will correspond to a model. All of the models will be trained and the best performing one will be automatically selected. For this I am just adjusting the penalty between the three models.//configure model parametersCALL gds.alpha.ml.pipeline.linkPrediction.configureParams(er-pipe, [  {      penalty: 0.0,      patience: 3,      maxEpochs: 1000,      tolerance: 0.00001  },  {      penalty: 0.01,      patience: 3,      maxEpochs: 1000,      tolerance: 0.00001  },    {      penalty: 1.0,      patience: 3,      maxEpochs: 1000,      tolerance: 0.00001  }])Training the LP ModelWhen we run the training step, we filter the graph down to just Users and SAME_AS relationships, as this is what we are trying to predict.//train the model CALL gds.alpha.ml.pipeline.linkPrediction.train( er-projection, {    modelName: entity-linkage-model,    pipeline: er-pipe,    randomSeed: 7474,    concurrency: 4,    nodeLabels: [User],    relationshipTypes: [SAME_AS],    negativeClassWeight: 1.0/10.0}) YIELD modelInfoRETURN  modelInfo.bestParameters AS winningModel,  modelInfo.metrics.AUCPR.outerTrain AS trainGraphScore,  modelInfo.metrics.AUCPR.test AS testGraphScore╒═════════════════════════════════════════════════════════════════╕│  winningModel             │  trainGraphScore  │ testGraphScore  │╞═════════════════════════════════════════════════════════════════╡│ {... penalty :1.0,...}    │ 0.937             │ 0.942           │└─────────────────────────────────────────────────────────────────┘The scores we get back are Area Under the Precision Recall Curve (AUCPR) scores. AUCPR is a metric bound between [0,1] which is particularly well-suited for scoring models on imbalanced data sets. In this case, we want to identify as many of the underlying entity links as possible while keeping the ratio of false positives low, and this is exactly what Recall and Precision measure respectively. Generally, the more entity linkages we want to identify, the wider net we need to cast, and the higher the ratio of false positives will become. This tradeoff tends to be expensive by default in severely imbalanced datasets where positive observations rarely occur. One way to think about AUCPR is that as it gets closer to 1, the tradeoff gets less expensive, i.e. we can identify more of the entity links with less of an increase in false positive rate.It is critical to note that the way we set negativeSampleWeight has a big effect on the AUCPR score and it’s interpretation. Generally speaking, there are two approaches to setting the negativeSampleWeightTraditional Evaluation Approach: Set the negativeSampleWeight to the reciprocal of the negativeSamplingRatio so that negativeSamplingRatio * negativeSampleWeight = 1 and the total probability mass in the test set is balanced 1-to-1. This is what we did above. In this case, AUCPR reflects how the model performs on a balanced dataset.True Class Ratio Approach: Set negativeSampleWeight such that the total probability mass reflects the true class ratio, i.e. negativeSamplingRatio * negativeSampleWeight = trueClassRatio. This AUCPR can more accurately reflect how the model predicts on new unseen data, but the score will almost always be drastically smaller, so temper expectations.Here is what the AUCPR looks like in our example using the second approach:CALL gds.alpha.ml.pipeline.linkPrediction.train( er-projection, {    modelName: entity-linkage-model-imb,    pipeline: er-pipe,    randomSeed: 7474,    concurrency: 4,    nodeLabels: [User],    relationshipTypes: [SAME_AS],    negativeClassWeight:11499}) YIELD modelInfoRETURN  modelInfo.bestParameters AS winningModel,  modelInfo.metrics.AUCPR.outerTrain AS trainGraphScore,  modelInfo.metrics.AUCPR.test AS testGraphScore╒═════════════════════════════════════════════════════════════════╕│  winningModel            │  trainGraphScore  │  testGraphScore  │╞═════════════════════════════════════════════════════════════════╡│ {... penalty :0.01,...}  │ 0.045             │ 0.052            │└─────────────────────────────────────────────────────────────────┘This lower score suggests that our model may lack precision when predicting overall node pairs in the graph (i.e. bring back a lot of false positives). While I won’t be tackling that in-depth in this post, there are multiple things we could adjust to try and improve this, including bringing in more of the source data, increasing the negative sampling rate, or tuning the feature engineering and/or hyper-parameters. That being said, some of these issues are difficult to avoid in ER and ultimately you need to analyze the business cost for false positives and false negatives within the context of your specific problem — i.e. how many false positives can be tolerated to identity a true entity linkage? And conversely, what is the cost of missing a true entity linkage? These costs may vary greatly between different business applications — for example, the cost of false positives may be significantly higher in marketing applications than in, say, anti-fraud.Additionally, it is not always straightforward which of the weighting approaches is best to optimize for an ER problem. Often in ER, datasets are only partially labeled, i.e. there may be many unidentified entity linkages present in the current data. In this context, some lack of precision may not be that bad if it helps identify more of the unknown entity linkages and uncover patterns not yet fully understood by analysts. Again, it really depends on the problem domain and the use case at hand.Creating Entity Linkage PredictionsOnce the model is trained we can use it to try and predict new entity linkages in the graph. In the below command we scan the graph for the top 20 most probable missing entity links (a.k.a — highest predicted probabilities according to the model) and create the predicted links in the projected graph. (Note: As all potential user pairs in the graph are scanned, this step may take a while to complete. For this example, it takes me ~5–10 minutes)//make link predictionsCALL gds.alpha.ml.pipeline.linkPrediction.predict.mutate(er-projection, {  modelName: entity-linkage-model,  mutateRelationshipType: SAME_AS_PREDICTED,  nodeLabels: [User],  relationshipTypes: [SAME_AS],  topN: 20,  threshold: 0.0})Once the above is complete we can easily write the links back to the database with the predicted probabilities.//write predicted relationships back to DBCALL gds.graph.writeRelationship(er-projection, SAME_AS_PREDICTED, probability)When we write undirected relationships back to the database we will get 2 relationships between each node, one for each direction. In this application we do not care about direction so these are effectively duplicates. We can deduplicate them with the below:MATCH (u1:User)-[r:SAME_AS_PREDICTED]->(u2:User) WHERE id(u1) < id(u2) DELETE rNow we can visualize the predicted entity linkages:MATCH (u1:User)-[r:SAME_AS_PREDICTED]->(u2:User) RETURN u1,r,u2We can also see the sorted predicted probabilities for each link going from one user node to the other. User nodes with multiple predicted links will be listed multiple time in the output.//show predicted entity links ordered by probabilityMATCH (u1:User)-[r:SAME_AS_PREDICTED]->(u2:User) RETURN u1.userId AS user1, u2.userId AS user2, r.probability as entityLinkageProbabilityORDER BY entityLinkageProbability DESCGenerating Resolved Person Ids and Querying Resolved Person InfoWe can generate resolved person ids with both the SAME_AS and SAME_AS_PREDICTED relationships using the Weakly Connected Components (WCC) algorithm. we will use the GDS write API to write these ids directly back to the database.CALL gds.wcc.write(  er-projection, {  nodeLabels: [User],  writeProperty: personId  }) YIELD componentCount, nodePropertiesWritten, writeMillis, computeMillis╒════════════════╤══════════════════╤═════════════╤═══════════════╕│ componentCount │ nodePropsWritten │ writeMillis │ computeMillis │╞════════════════╪══════════════════╪═════════════╪═══════════════╡│29760           │33941             │37           │33             │└────────────────┴──────────────────┴─────────────┴───────────────┘We can then easily query and aggregate information by these ids. Cypher is extremely performant when it comes to relationship traversals, so oftentimes these queries can be replicated using just the SAME_AS* relationship types, but the id is still convenient to have. For example, in the below query we return aggregated browsing activity for all persons that had a newly predicted entity link.//get all resolved persons who have a newly predicted entity linkMATCH (n:User)-[:SAME_AS_PREDICTED]-() WITH DISTINCT n.personId AS personIdList//get all browsing activity for those personsMATCH (n:User)-[r:VISITED]->(w:Website) WHERE n.personId IN personIdList//return each person with userIds and summary browsing activityWITH n.personId as personId, collect({  website:w.url,  firstTimeStamp: r.timeStamps[0],  lastTimeStamp: last(r.timeStamps)}) AS webActivity, collect(DISTINCT n.userId) AS userIdsRETURN personId, webActivity, userIds ORDER BY personId DESCAnd that will give results like those below, which includes a resolved person id, their aggregated browsing activity, and all their userIds:[  {     personId : 10644,     webActivity : [      {         firstTimeStamp :  2016-06-06T22:10:53.779000000Z ,         lastTimeStamp :  2016-06-06T22:11:23.779000000Z ,         website :  bddd393cb02101a/203b292b93c374e5       },...    ],     userIds : [       3005d862d7f6a60f2a19fa10a70dd24d ,       3e4217f722986af322333746f5f132c0 ,       50e9121a2f9f30c83b071e9adf9951c3     ]  },...]These sorts of queries can be incredibly useful for downstream processes that need comprehensive resolved views for persons, whether that be for further analytics, reporting, or a front-end application.What’s Next?In this post, we demonstrated how we can rapidly develop supervised machine learning pipelines for entity linking, evaluate performance, make new entity linkage predictions, and quickly create resolved entity ids that allow downstream processes to pull resolved person information.Here is a GitHub repository* that should have everything needed to reproduce the example we just walked through, including a notebook for subsampling and formatting the source data, and a Cypher script for ingesting into Neo4j.GitHub - zach-blumenfeld/demo-lp-for-entity-resolution: Demo of a supervised machine learning…Demo of a supervised machine learning approach for Entity Resolution in graph using Neo4j GDS Link Prediction Pipelines…github.comIf you are new to Neo4j, I recommend using Neo4j Desktop with GDS for this example. Please use GDS version 1.7.2 or higher for compatibility.As stated above, if you want to get serious with this, it is probably a good idea to ingest more of the data and fine-tune the feature engineering, sampling, and hyper-parameter configuration. Please reach out and let me know how it goes for you or if you have questions related to Entity Resolution in Neo4j!*Note: This blog was initially written with GDS version 1.7 . While the workflow for the latest GDS 2.x should be the same, some of the procedure names and syntax have changed. If you are using GDS 2.x, the GitHub repository contains a compatible version of the code. Please see the Readme to locate it.;Nov 5, 2021;[]
https://medium.com/neo4j/a-market-for-matches-finding-prices-with-neo4j-71ab085f8cd2;Nathan SmithFollowOct 12, 2019·6 min readA market for matches: Finding prices with Neo4jI have been using Neo4j tools to explore the concepts in the book Networks, Crowds, and Markets by David Easley and Jon Kleinberg. In a previous story, I shared how we can use Cypher and APOC to find a solution to the bipartite matching problem. Easley and Kleinberg also discuss another slightly more complicated matching scenario in chapter 10.In the basic bipartite matching problem, we imagined a set of college students that needed to be matched with an equal number of dorm rooms. Each student was interested in only a subset of rooms, and we looked for a matching that paired each student with a room they were interested in.We can make the problem more realistic by allowing the students to have different levels of interest in each room. Some rooms they might really like, and some they might find only OK. We’ll give a numeric valuation to the student’s interest in each room. We can model these valuations as relationship properties in a Neo4j graph.The original statement of the bipartite matching problem is really a special case of this new version where all the valuations were set to 1 or 0.We make one more change to the original problem by allowing each room to come with a price. We’ll subtract the price from a student’s valuation of the room to calculate each student’s potential payoff from being assigned to a room. Students are motivated to select a room that offers them the maximum payoff. In market terminology, we call the seller offering the room with the best payoff for a student that student’s preferred seller.I did not study economics in school. Thinking of prices and valuations this way was eye opening to me. The price can represent how much of the enjoyment of an object I am willing to trade away to be able to possess the object. There’s a germ of a sermon about Matthew 16:26 there somewhere.Easley and Kleinberg show that there is always a set of market clearing prices at which every student is able to be matched with a preferred seller. This is even true in a case of a constricted set in the basic bipartite matching scenario.Consider a very simple case in which two students who both want room 1 and both don’t want room 2. If we think of it with valuations, they both assign room 1 a valuation of 1 and room 2 a valuation of 0. If we place a cost of 0 on both rooms, room 1 is the only preferred seller for both students. This means we have a constricted set of preferred seller relationships. If we increase the cost of room 1 to 1, then the payout for both rooms is now 0. Both rooms have become preferred sellers for both students. With two preferred sellers and two students, we no longer have a constricted set of preferred seller relationships.Easley and Kleinberg describe the algorithm for finding market clearing prices this way.At the start of each round, there is a current set of prices, with the smallest one equal to 0.We construct the preferred-seller graph and check whether there is a perfect matching.If there is, we’re done: the current prices are market-clearing.If not, we find a constricted set of buyers, S, and their neighbors N(S).Each seller in N(S) simultaneously) raises his price by one unit.If necessary, we reduce the prices: The same amount is subtracted from each price so that the smallest price becomes zero.We now begin the next round of the auction, using these new prices.Let’s convert this into Neo4j code.We’ll start with seven students and seven rooms.UNWIND [Alexis, Brandon, Chloe, Daniel, Emily, Faith, Gabriel] AS studentNameCREATE (s:Student {name:studentName})RETURN sUNWIND RANGE(1,7) AS roomNumberCREATE (r:Room {roomNumber:roomNumber})RETURN rAdd an :INTERESTED_IN relationship from each student to each room, and assign each relationship a random valuation property. I chose valuations as integers between 1 and 10.MATCH (s:Student), (r:Room)MERGE (s)-[rel:INTERESTED_IN]->(r)SET rel.valuation = ceil(rand() * 10)For step 1 of the algorithm, we add a price property to each room with an initial value of 0.MATCH (r:Room) SET r.price = 0 RETURN rFor step 2, we construct the preferred seller graph that matches each student to the room with his or her maximum payoff. We add :HAS_PREFERRED_SELLER relationships to the graph with this statement.MATCH (s:Student)-[rel:INTERESTED_IN]->(r:Room)WITH s, max(rel.valuation - r.price) AS maxPayoffMATCH (s)-[rel2:INTERESTED_IN]->(r2:Room)WHERE rel2.valuation - r2.price = maxPayoffMERGE (s)-[:HAS_PREFERRED_SELLER]->(r2)In step 3, we check for a perfect matching between students and preferred sellers. You could visually inspect the graph. That’s pretty easy with just seven students and seven rooms. In the Neo4j browser settings, turn off Connect result nodes” so that only relationships you explicitly return in the query will show in the visualization.MATCH (s:Student), (r:Room) OPTIONAL MATCH (s:Student)-[rel:HAS_PREFERRED_SELLER]->(r:Room) RETURN s, rel, rIn a bigger graph, it would be hard to spot all the connected components and count the students and rooms in each component. In my last blog post, I wrote about an algorithm to find either a perfect matching assignment or a constricted set. However, it required several steps. Also, we don’t really care about the matching here, just the fact that there is a constricted set.Fortunately, the Neo4j Graph Algorithms library has a procedure called unionFind that returns the connected components in a graph. We can find the components, and then count the number of students and rooms in each component. If the students outnumber the rooms, we have a constricted set. This block of code covers steps 4 and 5 of the algorithm.//Find connected componentsCALL algo.unionFind.stream(null, HAS_PREFERRED_SELLER, {})YIELD nodeId, setId//Turn node IDs into nodesWITH algo.asNode(nodeId) AS setNode, setId//Count room nodes and student nodes in each set //and return sets with more students than roomsWITH setId,     sum(CASE WHEN labels(setNode) = [ Room ]               THEN 1 ELSE 0 END) AS roomCount,     sum(CASE WHEN labels(setNode) = [ Student ]               THEN 1 ELSE 0 END) AS studentCount,     collect(setNode) AS setNodes     WHERE studentCount > roomCount//Pick 1 constricted set and increase the price of each room in the constricted set by 1WITH [node in setNodes where labels(node) = [ Room ] | node]      AS rooms LIMIT 1UNWIND rooms AS roomSET room.price = room.price + 1RETURN roomFinally, for step 6, we check to see if there is still a room with a zero price. If not, we decrease all the room prices by the same amount to get back to a state where at least one room has a zero price.MATCH (r:Room)WITH min(r.price) AS minprice, collect(r) AS roomsUNWIND rooms AS roomSET room.price = room.price - minpriceRETURN minpriceNow we need to repeat this process until there are no constricted sets of preferred seller relationships. We’ll combine all the steps above into a single large Cypher statement so that it will run as a single transaction. Then we’ll wrap it in the apoc.periodic.commit function that will run the statement repeatedly until it returns a value of 0. Here’s the finished code.CALL apoc.periodic.commit( MATCH (s:Student)OPTIONAL MATCH (s)-[ps:HAS_PREFERRED_SELLER]->(:Room) DELETE psWITH sMATCH (s)-[rel:INTERESTED_IN]->(r:Room)WITH s, max(rel.valuation - r.price) as maxPayoffMATCH (s)-[rel2:INTERESTED_IN]->(r2:Room)WHERE rel2.valuation - r2.price = maxPayoffMERGE (s)-[ps:HAS_PREFERRED_SELLER]->(r2)WITH collect(s) AS studentsCALL algo.unionFind.stream(null, HAS_PREFERRED_SELLER, {})YIELD nodeId, setIdWITH algo.asNode(nodeId) AS setNode, setIdWITH setId,     sum(CASE WHEN labels(setNode) = [Room]               THEN 1 ELSE 0 END) AS roomCount,     sum(CASE WHEN labels(setNode) = [Student]               THEN 1 ELSE 0 END) AS studentCount,     collect(setNode) AS setNodes     WHERE studentCount > roomCountWITH [node in setNodes where labels(node) = [Room] | node]      AS rooms LIMIT 1UNWIND rooms AS roomSET room.price = room.price + 1WITH COUNT(room) AS updatedRoomCountMATCH (r:Room)WITH min(r.price) AS minprice, collect(r) AS allRooms,     updatedRoomCountUNWIND allRooms AS roomSET room.price = room.price - minpriceRETURN updatedRoomCountLIMIT 1 )Now we can query the rooms to see the market clearing prices.MATCH (r:Room) RETURN r.roomNumber, r.price ORDER BY r.roomNumberThis code worked for me when I tested 1000 students and 1000 rooms in my Neo4j sandbox, but I bet there are ways to make it more efficient. I’d love to hear your suggestions in the comments!;Oct 12, 2019;[]
https://medium.com/neo4j/updating-my-graph-database-using-pubchems-endpoints-1a767f8f2e18;Tom NijhofFollowSep 3, 2022·7 min readUpdating My Graph Database Using PubChems EndpointsCombining PubChem and Neo4j even furtherIn my last blog, I downloaded 197M nodes of all the chemical synonyms out of PubChem. This number was of course too low to cover every chemical synonym ever used.I do want to connect the compounds from NCI (National Cancer Institute, USA) to the rest of the database. NCI gives every compound an NSC number (Cancer Chemotherapy National Service Center number), these are seen as synonyms by PubChem. Some of these NSC numbers are missing in the database, others are not connected to a compound.Also, the data will get out of date, PubChem will update but my database will not. Besides that, some mistakes could have been introduced when loading the data. This means there is a need for a method to update synonyms and compounds in my local database.PubChem EndpointsLuckily PubChem has two great endpoints to work with. Less luckily, these two endpoints do not always give the same results.For this, we need to keep in mind how PubChem is structured. We are interested in three elements.Synonym: one and only one name. This can be related to a compound and substance. Neither is limited.Compound: A standardized description. It can be related to multiple synonyms.Substance: A description found in a single record. It too can have multiple synonyms.Because substances are seen as less scrutinized compounds, we only use the compounds. This does mean we have synonyms linking to nothing. This document explains the difference between substances and compounds better.PUG (Power User Gateway)PUG is a great tool to get direct information about a single object. If you look for a synonym name (like nsc815330), you will get the compound related to it together with the other known synonyms.RDF (Resource Description Framework)RDF (Resource Description Framework) is not focused on single objects, but on relationships. Searching by name is not possible, only by the id of the object. Searching for compound 46850181 gives us every object related to this compound and how they are related, from synonyms is-attribute-of this compound, to other compounds that are stereoisomers of this compound.DifferenceIf we look for nsc815330 in PUG we get 4 synonyms: Melodinine K, CHEMBL1163260, NSC815330, and NSC-815330. All are part of compound 46850181. PUG does not allow me to search for synonyms based on the compound.Using RDF to search for the same compound 46850181 we find a link to 2 synonyms (Melodinine k, and CHEMBL1163260).Searching for the md5 encoded ID of NSC815330, NSC 815330, or NSC-815330 gives no results in RDF.This means both endpoints are different, not only in their use case but also in their data, and so should both be utilized.Create Synonym IDWe can only search by id for synonyms in the RDF. But for that, we need to get the id from the name. A synonym ID is MD5_ + MD5 hex hashing of the lowercase string of the name”. E.g. CHEMBL1163260 in lower case is chembl1163260, add MD5 hashing 388ea4f67de9a93d40f412a0fe85d044, add the prefix MD5_388ea4f67de9a93d40f412a0fe85d044 and we got the synonyms object in RDF found by id.This was also used in my last blog, where I found that 5,865 of 197,000,000 synonyms did NOT follow this logic. This is close enough for our use case.Backend FunctionIn order to use this logic more than once, I made a backend function out of it. This will be living in the same place as the search function.Retrieving DataThis function will check the PUG endpoint to find the compound and other synonyms, by name.For example nsc63357” will give us this:{CID”: 247901,Synonym”: [2,4-dichloro-3-ethyl-5-methylphenol”,1127–60–2”,Phenol,2,4-dichloro-3-ethyl-5-methyl-”,3-ethyl-2,4-dichloro-5-methyl-phenol”,NSC63357”,m-Cresol,6-dichloro-5-ethyl-”,CHEMBL1999840”,DTXSID90920869”,ZINC393690”,NSC-63357”,AKOS024332244”,MCULE-9983242334”,DA-27321”,NCI60_011195”,FT-0729068”]}If none are found the function returns an empty list. This is very rare for NSC numbers.All the PUG results are given a constructed synonym ID (lowercase md5 hashing).The RDF is called with the compound ID. This gives us a list of synonym IDs connected to the compound (no names only IDs).Next up we combine these two lists and remove all duplicates based on ID. We give priority to PUG because RDF does not have names yet.If one of the synonyms does not have a name we make an HTTP call to the RDF synonym endpoint.We are now left with a list of compounds. Each of these compounds has the given name (e.q. nsc63357”) as a synonym. The compounds do hold all other synonyms too.Updating the DatabaseFor every compound, we will update the database. This will be done in two steps, removing wrong synonyms, and adding the right ones.Removing incorrect synonymsThe parts between curly brackets are python code or variables. We first find the compound by id and find all synonyms currently connected to it.Next up we filter them by the whole list of synonyms id we got from PUG and RDF calls.For everything not in the list, we remove the connection to this compound.MATCH (c:Compound {{pubChemCompId: {compound_id_str}”}})<-[r:IS_ATTRIBUTE_OF]-(s:Synonym)WHERE NOT s.pubChemSynId IN {[i.id for i in synonyms]}DELETE rWe only remove the connection because a synonym can be connected to other compounds.Adding correct synonymsFor this part, the python code does a bit more. Every synonym needs to be merged to prevent duplicates.We create a query with one match to get the compound. Followed by a line for every synonym to be merged. And then merge the relation between the synonym and compound.This creates a long query if there are a lot of synonyms but luckily python does that for us.The code to add the correct synonymsUpdater in ActionFor the next step, I want to add NCI60 data, in which every compound is given by its NSC number only. For a total of 56,685 unique NSC numbers in this dataset. But the synonym related to the NSC number 687296 can be nsc687296, nsc 687296, or nsc-687296 (capital letters do not matter for now). But there are also false positives like nsc 687296/nsc 19893 combination”.With Lucene, I searched for each NSC synonym, and if they have a compound, using the following query.CALL {{ CALL db.index.fulltext.queryNodes(‘synonymsFullText’, nsc687296 OR (nsc AND 687296)”) YIELD node, score return node limit 10 }}OPTIONAL MATCH (node)-[:IS_ATTRIBUTE_OF]->(c:Compound)RETURN node.name as name, node.pubChemSynId as synonymId, c.pubChemCompId as compoundId limit 5This query has a very high recall but precision is lower, meaning it will find all synonyms we are looking for plus some more.Next up we are going to filter the response, every synonym name that does not follow the nsc687296, nsc 687296, or nsc-687296 naming will be removed. Some of the synonym names we had to remove:n-(1-b-d-arabinofuranosyl-1,2-dihydro-2-oxo-4-pyrimidinyl)docosanamide, behenoylcytosine arabinoside, bh-ac, nsc-239336, sunrabinlentaron(r)17-dionecgp-32349nsc 282175nsc-d-611615nsc 625987, >=97% (HPLC)nsc 708781 incorporated in micelles, 0.55% (w/w) of drugnsc 708782 incorporated in micelles, 0.45% (w/w) of drugnsc 708783 incorporated in micelles, 0.20% (w/w) of drugnsc 708788 incorporated in micelles, 0.20% (w/w) of drugnsc-651016 free acidThis shows the 2 kinds of most common false positives we get, the nsc compound is used as part of a compound, and a list of synonyms that is not split into their own synonyms.All of the nsc numbers also do have a true positive.52847 of 56685 nsc numbers matched to at least one synonym with compound.Update Synonyms With RDFSome synonyms do NOT have a connection to a compound. We will make a call to the RDF endpoint to find out if we can find if this synonym has a known compound.If it does we update the database with this new compound. There is a chance we need to create a new compound in the database. For that, we use MERGE.8620 synonyms missed a compound, but after these RDF calls, only 3128 did. Note that every NSC number can have up to 3 synonyms.These synonyms do not have to mean the NSC number was without a good synonym.Use full updateFor every NSC number without a good synonym, we make a call to the backend. We try 3 different synonyms before giving up, nsc123, nsc 123, and nsc-123.2808 out of 3838 problematic NSC numbers are fixed this way.Updated compound 247901 linked to NSC 63357 and all other synonymsConclusionIf we now match all NSC numbers again we get 55578 out of 56685 NSC numbers matching at least one synonym with a compound. This means we can call this method a clear improvement of the current state.;Sep 3, 2022;[]
https://medium.com/neo4j/updated-efficient-neo4j-data-import-using-cypher-scripts-ca68dea44ac4;Davide FantuzziFollowOct 22, 2021·4 min readUpdated: Efficient Neo4j Data Import Using Cypher-ScriptsHow the new Cypher parser in Neo4j 4.2 made imports 10x fasterThis is an updated version of this article by Andrea Santurbano. Before proceeding with this post, please check that out.In this updated version we will perform benchmarks for Neo4j 4.2.What’s New in Neo4j 4.2?In the new Neo4j 4.2 release, the Cypher parser has been rewritten (from Parboiled to JavaCC), and there will be significant improvements in the import benchmark.Small RecapWe will be importing and exporting data using three different optimizations types. Just remember that Andrea’s article contains a more in-depth explanation of the three optimizations.No OptimizationThe generated file will contain a CREATE statement for each node to be imported.CREATE (:Foo:`UNIQUE IMPORT LABEL` {name:”foo”, `UNIQUE IMPORT ID`:0})CREATE (:Foo:`UNIQUE IMPORT LABEL` {name:”bar”, `UNIQUE IMPORT ID`:1})...Unwind BatchA more efficient statement structure is achieved by using UNWIND. That turns a batch-list of data entries into individual rows, each of which contains the information for the CREATE statement.UNWIND [{_id:3, properties:{age:12}}] as rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:BarUnwind Batch ParametersSame as the previous one, but we’ll now use the query parameters support from the shell for speeding things up.:param rows => [{_id:4, properties:{age:12}}, {_id:5, properties:{age:4}}]UNWIND $rows AS rowCREATE (n:`UNIQUE IMPORT LABEL`{`UNIQUE IMPORT ID`: row._id}) SET n += row.properties SET n:BarNeo4j 3.5We used the 3.5.22 release. This is the export benchmark.$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(3.5_exportDataCypherShellNoOptimizations.cypher,{format:cypher-shell, useOptimizations: {type: none}, batchSize:100}) real 0m44.871suser 0m1.354ssys 0m0.178s$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(3.5_exportDataCypherShellUnwindBatch.cypher,{format:cypher-shell, useOptimizations: {type: unwind_batch, unwindBatchSize: 20}, batchSize:100}) real 0m29.257suser 0m1.397ssys 0m0.181s$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(3.5_exportDataCypherShellUnwindBatchParams.cypher,{format:cypher-shell, useOptimizations: {type: unwind_batch_params, unwindBatchSize:100}})  real 0m25.333suser 0m1.393ssys 0m0.182sThe import benchmark for Neo4j 3.5:$ time cypher-shell -u neo4j -p davide <  import/3.5_exportDataCypherShellNoOptimizations.cypher real 100m24.805suser 5m39.444ssys 4m7.330s$ time cypher-shell -u neo4j -p davide <  import/3.5_exportDataCypherShellUnwindBatch.cypher real 31m33.870suser 1m12.383ssys 0m30.247s$ time cypher-shell -u neo4j -p davide <  import/3.5_exportDataCypherShellUnwindBatchParams.cypher real 10m28.723suser 8m4.257ssys 0m5.748sNeo4j 4.1We used the 4.1.4 release. Following the benchmark for the export.$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(4.1_exportDataCypherShellNoOptimizations.cypher,{format:cypher-shell, useOptimizations: {type: none}, batchSize:100}) real 0m42.675suser 0m1.437ssys 0m0.218s$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(4.1_exportDataCypherShellUnwindBatch.cypher,{format:cypher-shell, useOptimizations: {type: unwind_batch, unwindBatchSize: 20}, batchSize:100}) real 0m30.574suser 0m1.399ssys 0m0.214s$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(4.1_exportDataCypherShellUnwindBatchParams.cypher,{format:cypher-shell, useOptimizations: {type: unwind_batch_params, unwindBatchSize:100}})  real 0m25.393suser 0m1.376ssys 0m0.221sImport:$ time cypher-shell -u neo4j -p davide <  import/4.1_exportDataCypherShellNoOptimizations.cypher real 135m37.920suser 4m32.836ssys 3m43.420s$ time cypher-shell -u neo4j -p davide <  import/4.1_exportDataCypherShell.cypher real 44m13.016suser 0m53.779ssys 0m28.362s$ time cypher-shell -u neo4j -p davide <  import/4.1_exportDataCypherShellUnwindBatchParams.cypher real 10m8.991suser 8m39.109ssys 0m5.342sNeo4j 4.2We used the 4.2 release. Here’s the export benchmark.$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(4.2_exportDataCypherShellNoOptimizations.cypher,{format:cypher-shell, useOptimizations: {type: none}, batchSize:100}) real 0m42.951suser 0m1.379ssys 0m0.207s$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(4.2_exportDataCypherShellUnwindBatch.cypher,{format:cypher-shell, useOptimizations: {type: unwind_batch, unwindBatchSize: 20}, batchSize:100}) real 0m29.523suser 0m1.392ssys 0m0.213s$ time cypher-shell -u neo4j -p davide  call apoc.export.cypher.all(4.2_exportDataCypherShellUnwindBatchParams.cypher,{format:cypher-shell, useOptimizations: {type: unwind_batch_params, unwindBatchSize:100}})  real 0m25.900suser 0m1.381ssys 0m0.203sImport:$ time cypher-shell -u neo4j -p davide <  import/4.2_exportDataCypherShellNoOptimizations.cypher real 122m23.241suser 4m28.974ssys 3m40.094s$ time cypher-shell -u neo4j -p davide <  import/4.2_exportDataCypherShellUnwindBatch.cypher real 36m51.066suser 0m51.777ssys 0m27.773s$ time cypher-shell -u neo4j -p davide <  import/4.2_exportDataCypherShellUnwindBatchParams.cypher real 2m21.473suser 0m42.900ssys 0m3.190sConclusionsLet’s take a look at the results:As you can see, the export is a bit slower in Neo4j 4.2, but nothing to worry about — it’s just a matter of seconds.Here the results are more interesting. Look at the import with the new shell parameters. It went from 10 minutes in Neo4j 4.1 to a bit over two minutes in Neo4j 4.2. The export is overall faster compared to Neo4j 4.1. Not a tremendous difference from 3.5, except for the shell parameters export.;Oct 22, 2021;[]
https://medium.com/neo4j/generating-test-workloads-on-neo4j-97acc71298e6;David AllenFollowMar 18, 2019·5 min read·Member-onlyGenerating Test Workloads on Neo4jWorking with customers, sometimes we need to simulate various workloads on Neo4j, for many different reasons. Here is how you can do that.For example, we want to measure the response time or latency of queries, or we want to see how many queries a given hardware configuration can handle.To help out with that, I wrote a JavaScript module called graph-workload. I use this for some internal benchmark testing, to help verify that Neo4j’s cloud…;Mar 18, 2019;[]
https://medium.com/neo4j/using-nlp-in-neo4j-ac40bc92196f;David AllenFollowNov 15, 2017·7 min readNeo4j: Natural Language Processing (NLP) in CypherIn Neo4j 3.0, the database introduced the idea of user-defined procedures that you could call directly from the Cypher language. Prior to this, cypher was already a pretty good language, but things really started blowing up, with people writing code on top of neo4j that lets you do just about anything in Cypher directly. This article gives an example of using Natural Language Processing (NLP) inside of Cypher to show how you can draw meaning out of text in graphs, aimed at people who may be new to NLP.As an example, we’ll work through how to find out about positive and negative sentences in Donald Trump’s twitter feed, showing techniques that can be used on any text. To do this, I used both the APOC procedures for neo4j, and GraphAware’s neo4j-nlp procedures and installed the relevant JARs into the plugins directory for neo4j.Neo4j-NLP Setup: make sure to follow the directions on the github page. You will need at least 4 JARs, and to add some configuration to neo4j.conf. Finally, after starting the database you’ll need to create a default pipeline step, which is covered in their setup documentation.For this tutorial, the steps we’ll go through:Load data on tweetsBreak up hashtag / user replies and link them to the tweetsApply some basic NLP approaches to tag which words and concepts he’s tweeting aboutApply some sentiment analysis provided by neo4j-nlp to determine what he’s feeling positive about, and what he’s not so positive about.Step 1: The apoc.load.json function lets us get data into neo4j directly from JSON. The data URLs come from the excellent Trump Twitter Archive and are kept up to date with all of his tweets. By unwinding an array of data URLs, we can load all of the files in a single shot.CREATE INDEX ON :User(name)CREATE INDEX ON :Tweet(text)CREATE INDEX ON :Hashtag(name)UNWIND [ http://www.trumptwitterarchive.com/data/realdonaldtrump/2019.json, http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2018.json,http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2017.json,http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2016.json,  http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2015.json,  http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2014.json,  http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2013.json,  http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2012.json,  http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2011.json,  http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2010.json,  http://trumptwitterarchivedata.s3-website-us-east-1.amazonaws.com/data/realdonaldtrump/2009.json] AS urlCALL apoc.load.json(url) YIELD value as tMERGE (s:Source { name: t.source })CREATE (tweet:Tweet {    id_str: t.id_str,    text: t.text,    created_at: t.created_at,    retweets: t.retweet_count,    favorites: t.favorite_count,    retweet: t.is_retweet,    in_reply: coalesce(t.in_reply_to_user_id_str, )})CREATE (tweet)-[:from]->(s)RETURN count(t)This gives us all 37,000+ tweets that Donald Trump’s account sent, starting in 2009. Our graph right now is extremely simple, only connecting a single tweet to the source it was sent from, like this:Step 2: We can go ahead and punch up the data a little bit by extracting hash tags and user mentions from the data, creating those as separate nodes, and linking them to the relevant tweets, like this:/* Hashtag Analysis */MATCH (t:Tweet) WHERE t.text =~  .*#.*  WITH   t,   apoc.text.regexGroups(t.text,  (#\\w+) )[0] as hashtags UNWIND hashtags as hashtagMERGE (h:Hashtag { name: toUpper(hashtag) })MERGE (h)<-[:hashtag { used: hashtag }]-(t)RETURN count(h)/* User Mention Analysis */MATCH (t:Tweet) WHERE t.text =~  .*@.*  WITH   t,   apoc.text.regexGroups(t.text,  (@\\w+) )[0] as mentionsUNWIND mentions as mentionMERGE (u:User { name: mention })MERGE (u)<-[:mention]-(t)RETURN count(u)Looking at an individual tweet, we can now see it’s linked appropriately. By hashtag topic and by user mention then, we can look at who Donald Trump tweets to the most often, and what topics he’s most often tweeting about.A simple tweet related to hashtags and user mentionsStep 3: OK, to get some meaning out of this raw text, we’ll first need to annotate” the language. What’s happening here is that the text” of the tweet is getting broken up into individual words with certain common words eliminated, by using English grammar it will work out what’s a noun and what’s a verb, and it will store the structure of the sentence in the neo4j graph. Doing all of this is simple because of the neo4-nlp extension in cypher, and works like this./* Detect language and update each tweet with that information */MATCH (t:Tweet)CALL ga.nlp.detectLanguage(t.text)YIELD resultSET t.language = resultRETURN count(t)/* Annotate all text thats detected as English, as the underlying library may not support things it detects as non-English */MATCH (t:Tweet { language:  en  })CALL ga.nlp.annotate({text: t.text, id: id(t)})YIELD resultMERGE (t)-[:HAS_ANNOTATED_TEXT]->(result)RETURN count(result)All of the heavy lifting is done by GraphAware’s ga.nlp.annotate procedure. This will create a large number of new nodes in your graph. Every Tweet will be associated with an AnnotatedText node, which in turn will be linked further to Tag and Sentence nodes. For example, if we annotate the sentence See you in the Supreme Court!”, it will be broken down into tags” like see” (which is a verb) and Supreme Court” which is a noun. We can tell because the associated tag node has a property called pos” (Part of Speech) which has the value NNP (a proper noun). Here’s what the graph looks like for a relatively simple sentence:Showing the results of annotating textTags also occur at particular spots in sentences. So you’ll also see TagOccurance nodes which indicate where in the sentence each tag is seen.OK, this gets us the words and their functions within the paragraph, but still don’t say much about meaning. We need to go two steps further. The first is to enrich” the concepts, using the ConceptNet5 API.MATCH (n:Tag)CALL ga.nlp.enrich.concept({tag: n, depth:2, admittedRelationships:[ IsA , PartOf ]})YIELD resultRETURN count(result)This yields a set of additional relationships that relate the tags together, so we can tell what’s similar to what. It also labels a lot of nodes with additional categories, such as NER_Person, NER_Location, NER_Organization, and so on which allows us to classify our tags by what kind of thing they are. At this point, we’re better off than knowing Supreme Court” is a noun, we know it refers to an organization.This lets us write queries like the below, which will fetch all of the tweets where the language in the tweet is talking about the person concept Clintons”.MATCH (:NER_Person {value: clintons})-[]-(s:Sentence)-[]-(:AnnotatedText)-[]-(tweet:Tweet) RETURN distinct tweet.textTweets about the ClintonsStep 4: OK. At this point we’ve broken down the language, and we have some sense of the concepts being discussed. The last step is sentiment analysis. This can be a very in-depth topic, but for the purposes of this article, it’s quite simple: it just applies a positive, neutral, or negative label to each Sentence node in our graph.MATCH (t:Tweet)-[]-(a:AnnotatedText) CALL ga.nlp.sentiment(a) YIELD result RETURN resultWith this in place, querying for positive and negative tweets is easy. We can simply extend our previous query and tack on a label to find all negative tweets about a topic:MATCH (:NER_Person {value: clintons})-[]-(s:Sentence:Negative)-[]-(:AnnotatedText)-[]-(tweet:Tweet) RETURN distinct tweet.textStep 5: Let’s pull it all together. What can we learn about Donald Trump’s tweeting patterns using Neo4j and NLP techniques? How about let’s try a broader query to find the most frequently mentioned people, in negative contexts. We intentionally exclude tags that could be multi-purpose depending on the sentence context, to focus on just the people.MATCH (tag:NER_Person)-[]-(s:Sentence:Negative)-[]-(:AnnotatedText)-[]-(tweet:Tweet) WHERE    NOT tag:NER_Organization AND    NOT tag:NER_ORETURN distinct tag.value, count(tweet) as negativeTweets ORDER BY negativeTweets DESCThe results are probably not surprising for those who are familiar with the raw tweets.Persons mentioned in negative contextsAlthough it is interesting that Donald Trump” is the first result. This is an artifact of a couple of things in our dataset first that Donald Trump talks about and retweets about himself quite a lot. And second, the sentiment analyzer isn’t perfect, and may rate as negative tweets like @KingOf_Class: @realDonaldTrump Honestly,you can’t find anyone more real than Donald Trump!”.Lastly, let’s take a broad view of positive and negative sentiments about various organizations:MATCH (t:NER_Organization)-[]-(s:Sentence)WHERE   Negative in labels(s) OR   Positive in labels(s)   AND length(labels(t)) = 2 RETURN distinct t.value as orgName,        s.text as sentence,        labels(s) as sentimentOrganizations and the sentiment of the sentences they were mentioned in.ConclusionUsing NLP techniques inside of neo4j, there is quite a lot that you can do. GraphAware has already used this library as part of building Amazon Alexa skills, where the NLP component gets used to decide which skill the user’s input phrase is most like. In this article, we’ve shown a simple way you can programmtically understand the gist of text using the same approach.Many other fun applications are possible. A while ago, the Planet Money podcast built a bot that trades stocks based on positive or negative sentiment about companies seen in Donald Trump’s twitter feed using similar approaches (although I’m not certain whether they used neo4j or not). The limits are only how creative you can get.In this article, I have glossed over a couple points in particular the quality of the results with the sentiment analyzer can be spotty depending on your input data. For top-notch results, you’ll find yourself going deeper into the world of NLP to include training your own sentence analyzer. It’s a deep rabbit hole, but it’s also rewarding and fun if you’re interested in learning modern approaches to dealing with text analysis.For a different approach to doing NLP together with Neo4j, make sure to check out Will Lyon’s post on finding Russian Twitter trolls with neo4j and NLP approaches.;Nov 15, 2017;[]
https://medium.com/neo4j/java-17-explore-the-newly-released-java-version-in-a-graph-database-14fd11d4f4;Jennifer ReifFollowSep 15, 2021·7 min readPhoto by Nolan Issac on UnsplashJava 17: Explore the Newly-Released Java Version in a Graph Database!I’ve recently been playing around with a JDK data set that details the historical library changes of the versions of Java, and with the release of Java 17 today (September 14!), I thought it would be a good time to explore this data set a bit more with others. I invite you to join me and to continue with additional exploration and projects!The data set is pulled from a Marc Hofman’s Github repository for the Java Almanac that also feeds a web browser version for the javaalmanac.io website. All of the JDK data is organized into JSON files for each Java version, then folders for each version contain diff files of the changes between that version and any previous version(s). The information is quite extensive and heavily nested, forming a sort of hierarchy or tree when drawn out or visualized.Trying to put this data into various storage formats could certainly be very complex and likely result in complicated queries (i.e. lots and lots of joins). However, putting this into a graph seemed like a cleaner fit, especially for some of our use cases (detailed in just a minute) and made for some nice visualizations, as well as a fun experiment with a Spring Data Neo4j (SDN) application.Note: There were some modeling decisions I had to test and tweak. Hopefully, I can detail that in another post. :)All of the code for today’s post is available and able to be replicated from one of my Github repositories. You can replicate, follow along, tweak, if you wish. Without further ado, let’s take a look at some of the questions I wanted to solve with this data set in a graph!Use CaseThere are a variety of problems we might want to solve using our data set, but I came up with a few initial ones. Feel free to think of your own or adjust mine! Some questions that we might want to ask of this data set are as follows:What are the changes between any Java version (e.g. 8, 11, or 16) to 17?What has changed in the java.time package between version 16 and 17?What has been added/removed/etc in Java version 17?We will take a look at some of these using queries… but first, let’s look at the data model as a graph and walk through the import.Data Model and ImportWhen I started exploring this data set in the Github repository and on the website, I thought of a few different ways to model it as a graph. I could probably spend an entire post on that separately, but I’ll focus on what ended up being the current model.To start, I went to the whiteboard and drew the Java version entities with diffs, and then basically created a tree of the different types of changes beneath that. As an example, I looked at a page like the one showing the delta between Java 16 and 17 to see how many nesting levels I might need and the types of differences we might see. Honestly, though, I didn’t look at every possible change type — I let Cypher (Neo4j’s query language) do some of the work here. I ended up with something along the lines of the image shown below.I toyed quite awhile with the relationship names between the JavaVersion and Diff entities, and I’m still not super confident about the final versions (shown later). But, this was a good starting point to build my import statements.I ended up just using Cypher for the import because it can be applied with any instance of Neo4j, and is accessible for a wide audience to replicate. You can copy/paste all of the contents of the java-version-import.cypher file directly to a Neo4j Browser UI and import everything exactly as I have it.Note: for Neo4j instances outside of Sandbox or Aura, you will need to add the APOC plugin for the import to succeed.Once those Cypher statements complete, you can run CALL apoc.meta.graph() in the Neo4j Browser to see the data model. It should look like the one shown below.Now that we have our data model, we can run some queries and explore Java 17! Yay!QueryingWe can take our three questions from earlier and write queries to answer those. Let’s take a look!What Are the Changes Between Any Java Version (e.g. 8, 11, or 16) to 17?We could write some pretty specific queries to return the changes to annotations or methods or something else entirely, but I’ve put together a general query that simply returns all the diffs between Java 16 and Java 17.//Find changes from version 16 to 17MATCH (j:JavaVersion {version: 17”}) -[r:FROM_NEWER]->(d:VersionDiff) <-[r2:FROM_OLDER]-(j2:JavaVersion {version: 16”})MATCH (d)-[r3:HAS_DELTA*1..4]->(delta)RETURN d, r3, deltaResults:Now, my browser only limited the results to 300 nodes (for performance of rendering in d3 javascript), so we could actually change our browser settings to return more or view these another way. But, our map of entity types above the image shows that most of the changes are to *methods* in the JDK. We could drill into these a bit more to see exactly what changed, but this gives us a good start. According to our visualization, it shows only a handful of other types of changes are included.Note: a recent update to the Neo4j Bloom visualization tool allows you to see hierarchies in a tree-like depiction. I haven’t tried it yet, but this would be an interesting query to try with that!Ok, this gives us a good overview of what’s involved between Java 16 and 17, but let’s focus on one particular package in the next query.What Has Changed in the java.time Package Between Version 16 and 17?We could pick a variety of packages, but the package for temporal values is probably used a lot, so I’m expecting several projects and businesses might be interested in changes to java.time. We could write a query like the one below to find out.//Find the changes in java.time package between 16 and 17MATCH (j:JavaVersion {version: 17”})  -[:FROM_NEWER]->(d:VersionDiff)  <-[:FROM_OLDER]-(j2:JavaVersion {version: 16”})MATCH (d)-[:HAS_DELTA*1..2]->(p:Package {name: ‘java.time’})  -[r:HAS_DELTA*1..2]->(delta)RETURN *Results:Our results show that a few classes and several methods make up the bulk of the changes for the java.time package between versions 16 and 17. However, there is a field and an interface scattered in there, as well.This could help us better understand which applications would need updated and allow us to write inspection queries to see which of our projects need updated with the latest methods or other types.Let’s wrap up our analysis with finding out what has been added, or removed, or something else in Java 17.What Has Been Added/Removed/etc. in Java version 17?We can look at new features and functionality by seeing what has been added in the latest JDK. Here is our query:MATCH (j:JavaVersion {version: 17”})  -[:FROM_NEWER]->(d:VersionDiff)  <-[:FROM_OLDER]-(j2:JavaVersion {version: 16”})MATCH (d)-[r:HAS_DELTA*1..4]->(delta)WITH delta.status as status,   collect(DISTINCT labels(delta)) as type,   count(delta) as countRETURN status, type, countORDER BY count DESCResults:According to the table results above, we have quite a bit of added functionality, followed by some things that are marked notmodified.” I’m not sure what that means, so it could be interesting to dig a bit more to understand that status.Next StepsThere is quite a bit of info that we could analyze with this data set, such as trends throughout the years of 17 Java versions. I also did not import the tags from the javaalmanac data set, so we could add that and investigate what has been deprecated or marked for removal in future versions.Modeling could also be another topic to discover different ways to model this information in a graph. Would you have modeled it differently from the one we used today?I have also written a Spring Data Neo4j application that is published on Github, so we could dive in-depth on the process for modeling and building that application.Wrapping Up!I hope you enjoyed this quick analysis of the new Java 17 version in a Neo4j graph database! I would love to hear your thoughts or see your ideas and content around this data set in Neo4j, too.Happy coding!ResourcesGithub repository: Java version graph dataData source: Javaalmanac Github repositoryGithub repository: Spring Data Neo4j application with Java data and Neo4j Aura;Sep 15, 2021;[]
https://medium.com/neo4j/getting-started-with-provenance-and-neo4j-b50f666d8656;Stefan BieliauskasFollowFeb 4, 2019·5 min readGetting started with Provenance and Neo4jWe all want to know Where does our meat come from?” or Is this a reliable information or fake news?”. If we ask this kind of questions it is always about the Provenance of information or physical objects. This article proposes the usage of neo4j to store provenance information — originally written in 2017.What is provenanceProvenance describes the origin and history of a thing”. The term provenance originally has been used for paintings and their owner history over time. If there are some missing Provenance information, for example, a decade where you can’t tell who was the owner, the painting value drops immediately. Today we use the term provenance to track processes and responsibilities in general. For example to track the production of food or to track the history of a file.W3C — PROV standardIn 2003 the W3C adopted the official PROV standard to describe provenance structures. I don’t want to explain the complete standard only the key concepts: Entities, Activities, Agents, and Relations.PROV-O conceptActivities — An action that modify / create / delete EntitiesEntities —Pictures, Files, Meat, HorseAgent — Triggers actions and owns” entities, used to track responsibilitiesFurthermore, there is a fixed set of relation types, for example, a generation or usage. You find more information in the PROV-DM spec.The standard provides also several serialization formats for example:PROV-NPROV-JSONPROV-XMLPROV-NCake ExampleThe next picture is a simple example of this concept with an agent, two activities and two entities:PROV bake example (https://provenance.ecs.soton.ac.uk/store/documents/115091/)This example explains that Bob baked a cake, with some additional information like the start- and end time. Now we can follow the digraph and determinate the origin activity that created the ingredients of our Cake, in this case, the Buy activity. Of course in a real-world example, we should split our ingredients activity into multiple entities and track their process of buying or growing separately.The interesting part is that we are able to find the first activities that are somehow influenced our cake activity, without knowing the exact steps in between. In this example, this seems really simple but these digraphs can get really complex:Horsemeat — https://provenance.ecs.soton.ac.uk/store/documents/75490/This example tracks the production- and shipping process for a meat product. This includes each cattle, slaughters and transportation step. You see it is a complex digraph with many entities activities and agents.PROV and Neo4jTo handle PROV graphs it is useful to store the information in some kind of database instead of a single file. Neo4j provides the required features to store PROV data in a property graph:Nodes — Mapped to Entities, Activities, and AgentsRelations — For example, used, wasGenerartedBy …Properties — Predefined PROV — as well as custom properties can be attached to nodes as well as to relations (for example startAtTime, endAtTime)We developed a simple python module to store PROV documents in a neo4j database called prov-db-connector. The following script saves our bake a cake example into the Neo4j database.The script uses the ProvDocument from the prov library. The PROV lib provides basic functionality to serialize PROV documents.The result of this script is a graph structure that represents our PROV document with all relations and properties:PROV-Document in Neo4jIf you compare the example diagram and the actual neo4j graph diagram above you see they are really similar. Within all the information in the graph database, you benefit from Cyphers powerful query mechanisms to extract information from your graph.Maybe you ask yourself Why should I use such a complex standard and put effort to design these PROV data model?”. Yes, you are right this not simple and not done in 1 hour. But if you collect such data you can do fabulous data analyze and answer questions likeWhat is the origin of this order?Who was responsible for installation of product xyz at our client?Is this product produced under good working conditions (based on certifications) ?I saved a complex example PROV document that contains information about the production of meat.Horsemeat — https://provenance.ecs.soton.ac.uk/store/documents/75490/You see the graph is huge and complex. With many different steps and activities. Let me show how you answer a question based on this graph:Who was the slaughter of order 2?The Cypher command to get the slaughter of order 2 (or the slaughters) looks like this:Display location of the slaughterIf I track my data with a traditional data structure I probably would do some joins on tables and find the slaughter. Not complicated if all data is in one database, If not it might be complex but still doable. So why?We don’t care about structures between the order and slaughterIt’s backward compatible in case of a change in the process eg. changing the transportation company.Its interoperable between eg. companiesConclusion — For whomThis technology is not for every use-case. If you just want to display your users a log about the recent changes in a dataset, just save the recent changes in a database and show the list to your user, keep it simple )The PROV standard is interesting if you have a complex process that results in a thing”. Like a food product or a weather prediction. For example, the computation of the temperature prediction for the next day is a complex process. The calculation is based on several satellite images and other data. In this case, it is good to know on which data the temperature calculation was based on. This could be useful for debugging or for transparency reasons.Another use-case could be the global production process and lifecycle of products. To understand the way of products and answer questions like is this product really only build with wood from the EU?”. The PROV-Standard could be a data format to answer such questions and exchange provenance information.FeedbackI’m just started to write my first few blog articles. I’m glad about any feedback — positive and negative! Just let me know if you have any questions or suggestions to improve this article!;Feb 4, 2019;[]
https://medium.com/neo4j/wheres-my-neo4j-cypher-query-results-%EF%B8%8F-%EF%B8%8F-9c3b150e6e19;Dan FlavinFollowJan 14, 2021·13 min readWhere’s My Neo4j Cypher Query Results? 😠 ⚡️ ⁉️Slow Cypher Queries in Neo4j Browser? Why a Cypher query run in the Neo4j Browser may not return in a reasonable amount of time, what is happening, and what you can do about it.Note: T̶h̶e̶ ̶c̶o̶n̶t̶e̶n̶t̶ ̶o̶f̶ ̶t̶h̶i̶s̶ ̶p̶o̶s̶t̶ ̶i̶s̶ ̶r̶e̶l̶e̶v̶a̶n̶t̶ ̶t̶o̶ ̶t̶h̶e̶ ̶N̶e̶o̶4̶j̶ ̶B̶r̶o̶w̶s̶e̶r̶ ̶v̶e̶r̶s̶i̶o̶n̶ ̶4̶.̶2̶.̶0̶ ̶a̶s̶ ̶o̶f̶ ̶J̶a̶n̶u̶a̶r̶y̶,̶ ̶2̶0̶2̶1̶. Your mileage may vary” as the Neo4j Browser is continually being improved. The Neo4j Browser version 4.4.3 has been improved! The cypher statement that returned results fast from the database, but used to overwhelm the browser display now displays the results.Photo by JESHOOTS.COM on UnsplashTLDRThe Neo4j Browser is a javascript-based web browser application. This runtime environment can be the limiting factor in being able to process and render the force-directed layout of a Cypher query output. Often users will assume that the query is slow when it could very well be the processing behind browser rendering results. This post covers how to address this operational issue.The Neo4j Browser is a developer tool and is the number one interface used to write Cypher queries to interact with a Neo4j database.The browser delivers an easy-to-use, metadata-supported environment for developing Cypher queries with the interactive, force-directed graph visualization being the most popular output.The utility of the Neo4j Browser for developers cannot be understated and it will continue to be the main development interface for Neo4j users.It has query history, saving favorites, multi-statement execution, a variety of output formats, browser guides available through the :play command, and is continuously being improved (hint: check out the :edit command if you haven’t already). You can check out the user interface guide here.The majority of Cypher queries run in the Neo4j Browser have no issues displaying an interactive graph visualization. Unfortunately, there are times when queries run through the Neo4j Browser start taking a long time to run” and frustration builds while looking at the spinning dots as shown in figure 2.Figure 2. The Spinning dots”. Where’s me lines, arrows, and bouncing bubbles?This situation is where performance and utility can unknowingly be at cross purposes.Neo4j users often equate the time it takes for the graph visualization or rendering of results of a Cypher query in the Neo4j Browser to database query performance.At first glance, this is a perfectly reasonable assumption. The reality is that it can be the rendering of the query results that is taking time, not the query execution.Assuming that the query response time is measured by when the visualization is produced can result in potentially invalid and costly conclusions, such as I must re-write my query and / or restructure my database”.The goal of this post is to provide Neo4j Browser users with ways to determine if it’s a query’s performance or if the visualization that is the culprit when query execution seems slow, and what other options exist.The workflow is very simple:Determine if it’s the Neo4j Browser or the query execution that is keeping a visualization from being displayed.If Neo4j Browser visualization is the bottleneck, then try different query techniques and/or a different tool.Investigate why the query is slow to return if it’s not the visualization.What the post does not take into consideration is the larger context of concurrency, resource and database utilization, or query design. That’s a much bigger topic!The Neo4j Browser and What It’s Doing50 Nodes -> 70 Relationship: Arrgh! Query!!! 🙀” vs. Yeah! Query!!! 😺”What Is the Neo4j Browser Graph Visualization Really Displaying?Let’s use a simple acyclic graph structure to illustrate how even a small set of data can result in a significant amount of processing needed for visualization. The example graph consists of nodes with a :Node label, that are related to each other by a :PARENT_OF relationship, each with a unique node_id field.Figure 3. Example database schemaThe graph is intentionally small, containing only 50 :Node labeled nodes, and 70 :PARENT_OF relationships. It was generated using a Neo4j apoc procedure that generates a random graph using the Erdos-Renyi model. The schema visualization is shown in figure 3, and the source Cypher statements are on github.dfgitn4j/browserVsCypherShellBlog post is Found Here Neo4j APOC procedures will need to be installed, see this link on how to install There are two…github.comWhy so small? To illustrate how even a small number of nodes and relationships can have a large number of unique paths through the graph.The number of unique paths is not a problem unto itself but can have an impact on the Neo4j Browser graph visualization that is not obvious.Neo4j Browser can take a long time to render information, because what it draws and what the query results are can be different.Figure 4 shows the visual output of the one and two-hop :PARENT_OF relationship traversals between :Node nodes using the query MATCH paths=(:Node)-[:PARENT_OF*1..2]->() RETURN paths. Here’s where the flattened, interactive visual output of the Neo4j Browser can be misinterpreted.Figure 4. One and two traversal Neo4j Browser interactive graph visualizationFigure 4 shows 47 nodes and 70 relationships displayed, but running a path count query MATCH paths=(:Node)-[:PARENT_OF*1..2]->() RETURN count(path) returns 170 paths.Why the Discrepancy Between What’s Visualized and the Underlying Dataset It Represents?The Neo4j Browser transforms the multi-dimensional overlapping path data into a 2D visual. The visualization is very useful for exploring query results with the unfortunate potential side effect of obscuring the actual amount of data returned. (see The Graph Database Chronicles Episode 1” for a deeper discussion if this seems strange to you).Graph Database Chronicles Episode IThe 20 minute introduction to graph databasesvimeo.comThe 50 node / 70 relationship example graph has a total of 5,673 unique paths. A visualization of all the unique paths is not much different from figure 4, but the underlying data is approximately a 3,000% increase over the 170 one and two-hop traversal query paths.The query MATCH path=(:Node)-[:PARENT_OF*]->() RETURN path illustrates returning all the paths for any number of traversals. Running this in the Neo4j Browser would result in the figure 2 spinning dots. Not to worry!Turns out that the query is not the issue.Think about your query carefully. Do you want a picture of that data? Are you sure that rendering that visually is a useful thing to do?Neo4j Browser’s greatest strength (easy visualization) is also it’s greatest weakness. It simply isn’t the case that visualizing results is the first thing you should do always.This could be your inner monologue:⁉️ This is a good time for a reality check. It should be asked what the query is trying to accomplish and will the visualization add any value, or is it just eye candy? 👀 For this example it would be hard to come up with valid reason for needing a flattened visualization of 5,673 unique paths. But wait! I am looking for unrecognized patterns in my graph!” is a common quick response. A flattened force-directed layout is not going to give you this. What’s really being asked for is the ability to find patterns based on the shape of the data in the graph. This would be a good indication that you might get value from Neo4j Graph Data Science algorithms.Three Simple Ways to Determine If the Neo4j Browser Graph Visualization Rendering is Masquerading As a Performance Issue1The count() test.Using the Cypher count() aggregation can be an easy test to understand how much data a query is returning and to get an idea of the query performance characteristics. Running a Cypher queryMATCH path=(:Node)-[:PARENT_OF*]->() RETURN count(path)to count the number of paths rows in figure 5:Figure 5. Path count queryNotice two things:The number of paths can be significant. In my case, there are 5,673 unique paths through the database even though there are only 50 nodes and 70 relationships in the graph.The query starts streaming results in 1ms and completes after 10ms.The aggregation avoids having to return and render the interactive graph visualization. There’s no fun force-directed layout visual (which is of dubious value in this use case), but at least it is known that it’s not the query execution itself that’s the performance issue here.The 5,673 unique paths in my example graph represents a combination of all paths. This is often not what a user wants and is an example of how the graph visualization can obscure the underlying data.This post is focusing on when intentionally or not, a query returns more data than is reasonable for the Neo4j Browser to process. Why there are 5,673 paths returned and why this may not be obvious is a topic for another discussion.2Use the PROFILE query directive to avoid the default force-directed visual layout.Using the PROFILE directive will show how the query was executed, the processing time, and show the query execution steps as the initial output, avoiding the default graph visualization step (until you click on the visualization tab).To see this in action, run the all paths query with the PROFILE command:PROFILE MATCH paths=(:Node)-[:PARENT_OF*]->() RETURN pathswhich shows the query executing in 11ms as shown in figure 6 below.If you then click on the graph visualization icon as shown in figure 7, you’ll likely end up waiting for the visualization to render as in figure 2. The Neo4j Browser might have to be closed and reopened to continue on because your web browser’s JavaScript engine ran out of memory.Figure 6 (left). Abbreviated query PROFILE / Figure 7 (right). Switch to graph visualizationConsider using the approaches for working with the data in the following Options” section if the PROFILE command returns in a reasonable amount of time but takes too long to visualize as a graph. If PROFILE does not return in a reasonable amount of time, then it’s likely the query execution that is the culprit.Try running the query with EXPLAIN directive to see the expected query execution plan. Either optimize the query structure from there, or consider steps similar to those discussed in the It is the query” section below.3 Use a LIMIT clause in your query, or reduce the number of paths traversed.This approach is useful for minimizing the results returned, allowing for the graph visualization to be displayed in the Neo4j Browser. This is not always appropriate as it changes the query.A LIMIT is simply added to the query:MATCH path=(:Node)-[:PARENT_OF*]->() RETURN path LIMIT 200While changing the path length from 1 to 3 hops is accomplished by:MATCH path=(:Node)-[:PARENT_OF*1..3]->() RETURN pathThis obviously is a trial-and-error approach that sometimes elicits interesting observations that can change the premise of the original query.Of course, you can combine both to minimize even more.Options: Neo4j Browser Visualization Is the Performance” CulpritWhat Are My Options If I Want to Work with the Big Old Dataset Returned by the Cypher Query?1Use cypher-shellSimple to use command-line utility for running Cypher queries, cypher-shell works with all Neo4j editions.cypher-shell query results have rudimentary formatting that needs very little processing to create the final output. I often use it from within the Sublime Text Editor when developing Cypher queries¹. This is useful when I have a series of disjoint statements (e.g. create data, indexes, match, merge, etc.) to run in sequence, or I want to use git as a repository for my queries.You can find cypher-shell in the Neo4j install location bin subdirectory, or it can be installed standalone (see Cypher Shell section of the Neo4j Downloads page). Running cypher-shell in a terminal window launched from the Neo4j Desktop on a Macbook is in figure 8. It is the same process for Windows.Figure 8. Launching a terminal window and running cypher-shell on a mac👉 Using cypher-shell with the--format plain option is one of the fastest ways to return query data and execution metrics without writing your own code. Output can be saved to a file or piped through a pager for a better user experience.Cypher Shell - Operations ManualDescribes Neo4j Cypher Shell command-line interface (CLI) and how to use it.neo4j.com2 Use Neo4j Bloom Neo4j Bloom is a graph visualization tool designed for end-users and analysts. Bloom users navigate and query the graph without having to write Cypher while being able to visualize a much larger set of data than is possible in the Neo4j Browser.ツ Update! See the Run a Cypher Statement in Bloom” section of the How to Create Conditional and Dynamic Queries in Neo4j Bloom” blog post for an easier and reusable way to run raw Cypher in Neo4j Bloom.Figure 9. Bloom create search phrase dialog boxHow does that help in this scenario where we want to see the output of a Cypher statement? Bloom allows for Cypher queries to be created and parameterized for use within the interface by creating a custom search phrase (figure 9). We can then execute the query that was problematic for the Neo4j Browser to see the visualization Bloom.A caveat with Neo4j Bloom. Neo4j Bloom requires the Enterprise version of the Neo4j database and does not work with the Neo4j Community Edition. Neo4j Bloom also works best with a dedicated GPU. Users can use Neo4j Bloom in several ways:Using the Neo4j Desktop. A single-user Bloom installation is included with the Neo4j Desktop. Databases created from within the Neo4j Desktop run a free Developer License of the Neo4j Enterprise Edition.Through Neo4j Aura, all Neo4j Aura instances come with Bloom enabled out of the box. There’s even a free version of Aura that you can try.Using a Neo4j Sandbox, which is a free, guided graph database use case walkthroughs that can be accessed for up to 10 days.Install with a Neo4j Enterprise Edition database. This requires Neo4j Bloom to be installed on the server and a Bloom activation key.It Is the Query After all! What to Do If the Cypher Query Is Not Performing as Wanted?✏️ That Is a Big Subject! ☯A single blog post could never even begin to address the subject of the yin and yang of query and database performance. There’s just too much to cover and too many variables. Given that, here are some hints and tips to keep in mind and resources to help out:Check your schemaMonitor your databaseGet help from the communityThe YANGI principleDo you have a graph problem?Check Your SchemaLike any database, the design and how well queries are written can affect performance. The good news is that being schema-less”, it is very easy to provide multiple graph models in a single Neo4j database to meet different query requirements.The twist is understanding when and how to create an efficient graph model and queries. Fortunately, there’s an incredible number of resources to help Neo4j developers and users. There’s a quick introduction to modeling in the Neo4j Developer documentation, and the no-charge online courses from Neo4j Graph Academy. These courses cover Cypher basics, from advanced query writing and optimization to database design and administration, etc.GraphAcademy - GraphAcademyNow is the perfect time to show your employer, customers, and colleagues that you are a Neo4j expert. We currently have…neo4j.comMonitoringRemember that the Neo4j graph database is a database. Even though Neo4j is a very efficient graph database, the universal database resource trifecta of RAM, CPU, and I/O still apply and are constrained by concurrent usage.You can’t fix what you can’t see, and there are many ways to monitor the resource usage of the Neo4j database and queries.👉 Note: Halin is being depreciated and will not support certain features for the Neo4j Graph Database version greater than 4.3.0.Halin - Neo4j Monitoring Tool - Neo4j LabsHalin is a cluster-enabled monitoring tool for Neo4j, that provides insights into live metrics, queries, configuration…neo4j.comThe Neo4j Operations Manual covers production monitoring of database and system metrics. This includes sending metrics to Graphite, and publishing metrics for polling as a Prometheus endpoint.Monitoring - Operations ManualThis chapter describes the tools that are available for monitoring Neo4j. Neo4j provides mechanisms for continuous…neo4j.comGet Help from the CommunityThe Neo4j Community website is a wonderful resource to ask specific questions and take advantage of the collective knowledge of the vast Neo4j user community. I will often go to neo4j.community.com for ideas when I’m trying to solve a problem, or am looking for new approaches to writing a complex Cypher pattern. Good chance that whatever it is you’re asking has already been addressed. On a side note, the Neo4j Community’s This Week in Neo4j” and Featured Community Member” often present interesting user-provided graph use case examples and real-world projects using new approaches and technologies. I would have completely missed the Using Neo4j withPySpark on Databricks” post if I wasn’t a member of the Neo4j community.The YAGNI PrincipleFollow the YagNI (You ain’t gonna Need It) principle and good graph data modeling techniques.A generalized query will return all the properties for each node and relationship to Neo4j Browser for rendering. If every node in our example graph had 512K of property data, that would be ~3MB of data being returned to the Neo4j Browser for displayed in the property value box.That’s a lot of memory and CPU being used just in case a user clicks or hovers on an individual node or relationship visualization to see the property data.Having node properties available in visualization is what you’d expect, but there’s only so much you can really load into our web-based Neo4j Browser.Not only is an unreasonable number of properties a stressor for the Neo4j Browser, but it can indicate an underdeveloped graph model. See this series of blog posts for a quick introduction to modeling concepts and how a good model that is easier to understand and the query will avoid this scenario.Graph Data Modeling: Categorical VariablesProperty graphs provide a lot of flexibility in data modeling but how do you know when to use which feature?medium.comDo You Have a Graph Problem?This should be obvious, but be sure you have a graph use case! It is so much fun and easy working with a Neo4j graph database that it is easy to try and apply it to scenarios where a graph does not add any value. Watch this short video for a good introduction to identifying graph shaped problemsParting Thoughts. Yours and Mine.Thank you for your time if you made it this far. Please post any questions or comments as I am very interested in what readers think and am hoping to gain insight from any responses.¹ More on the Sublime Text editor and cypher-shell coming in another blog post.;Jan 14, 2021;[]
https://medium.com/neo4j/connecting-to-neo4j-from-microsoft-power-bi-using-odbc-a45d2a955cf0;Daniel Heward-MillsFollowOct 6, 2021·3 min readConnecting to Neo4j From Microsoft Power BI Using ODBCFinal ResultNeo4j’s BI Connector enables users to send and receive SQL-centric query data using Java Database Connectivity (JDBC), and JDBC compliant applications.This article shows how to use OpenLink Software’s ODBC-JDBC Bridge to extend the BI Connector’s reach to ODBC-compliant applications, such as Microsoft Power BI.PrerequisitesOpenLink Software ODBC-JDBC Bridge Driver (Free Evaluation, Installation Guide)A running Neo4j instanceKnowledge of the port number used for Bolt requests (Default: 7687)Creating a Data SourceSetting Up an ODBC DSNOpen ODBC Administrator and Click Add… . Select OpenLink Lite for JDK 1.5 (Unicode or ANSI), and click Finish.Provide a name for your Data Source, a description (optional), and click Next.Name your new data source, and click Next.Optionally, you can build your JDBC source connection string by clicking on the … button.Click Next until you reach the final page.Click Test Data Source…If a successful connection has been made, you can now use this ODBC data source name with PowerBI.Connecting and Querying from PowerBIOpen PowerBI, click Get Data, and find ODBC. Then, double click on ODBC, or click on Connect.Click on Advanced Options, enter the SQL Query that you want to return in PowerBI, and click OK.If successful, a table showing results will be populated. Click Load to use the results in PowerBIOnce loaded, you can build visualizations using data from your Neo4j instance.In addition to Power BI, this driver connection can also be used to connect any ODBC-compliant application to Neo4j and its JDBC BI connector (i.e., Excel, Access, Tableau, Qlik Sense, etc.).Related ContentMaking an ODBC connection to Neo4j via Excel for Mac, via ODBCMaking ODBC connections to Neo4j on macOS;Oct 6, 2021;[]
https://medium.com/neo4j/a-new-age-of-data-what-is-graph-and-how-can-it-help-me-fa104a187c6d;Jennifer ReifFollowJul 19, 2018·7 min readGraph technology — connecting the world’s dataA New Age of Data — What is A Graph and How Can It Help Me?I have come across many people who have never heard of a graph database. Don’t be discouraged if you fall in this category. So did I until pretty recently. When taking an initial look, it feels so foreign, yet revolutionary and exciting.In this post, I want to cover what a graph database is and the components that make up a product in this space (specifically, from Neo4j). I also want to talk about further expansion in this area, use cases, and opportunities.Data StorageLet’s go back in time just a bit. Before 1979, all of our data was stored in paper forms in filing cabinets. This made it very difficult to locate information, as well as connect information over silos (for example, finding a person’s information from basic customer record and then cross-referencing them in orders they have made).In 1979, this began to change when the first relational database hit the market. From that point, the amounts of information that could be stored, analyzed, and aggregated exploded. Relational databases are still widely and appropriately used in businesses today, continuing to drive value and help businesses progress.However, our world has changed quite a bit since 1979, hasn’t it? We carry devices that connect our contacts, social media, banking info, health data, the internet, etc. Society and the world is far more connected now, and it continues to increase.How do we best track and analyze all of these relationships among our current data? This is where we at Neo4j believe that a graph database can help. In the next paragraphs, we will define what a graph database is and how to use it to maximize business value for connected data.What is a Graph Database?No, this is not a database to store the charts or exercises you had in math class. :) Instead, a graph database, in this case, means that the data is stored more closely to the conceptual model representation than a relational or NoSQL database often allows. A graph database is designed to show how business and technical data connect and are related to each other — kind of like the way graph charts created a more visual representation of the data in your math class.A graph database stores data with its accompanying relationships. It would be like storing the relational database row of my information from a Person table with the row with Neo4j’s information from the Company table and storing the relationship between them in a memory pointer (inserting the direct connection with the rows, rather than writing a JOIN query to see if a relationship exists). An example ERD of this data is pasted below.This would be like (but not literally) storing the result value of the SQL query below, so that every time you looked for where I worked, you would return Neo4j.SELECT p.fullName, c.companyName FROM company_employee ceJOIN person p on ce.personID = p.personIDJOIN companies c on ce.companyID = c.companyIDWHERE p.lastName = ‘Reif’AND p.firstName = ‘Jennifer’Note: The sql query above assumes a Person table, Company table, and a link table Company_Employee. It also assumes that an person can work for only one company, but a company will have many people employed.In a relational model, you wouldn’t actually know if I worked anywhere, as there isn’t a physical connection between tables, only a theoretical one drawn in models. To find out if I worked at a company, you would need to run a SQL query to see if any results returned.In the Neo4j graph database, the same data would be stored like the below image.The nodes for Person (fullName = Jennifer Reif) and Company (companyName = Neo4j) are stored in the database with the relationship WORKS_FOR to connect them. If you want to find out which company I work for, you tell the query to locate my node and follow the WORKS_FOR relationship. That will return the node on the other end.Graph Data ModelBecause of the storage method we discussed in the paragraphs above, this makes the data model extremely simple and easily understood. The graph data model is called whiteboard-friendly because, instead of organizing the data into a document or table structure (like in an ERD), you draw it like it is stored. The image below shows the data model for the example I gave above.Key Components of Neo4jThere are several graph database products in the market now, but I want to focus on the initial entry to the market back in 2007 — Neo4j. Neo4j has a few defining factors that separate it from other types of databases, as well as other graph databases, but let us cover just a few of the key elements for now.First is the fact that Neo4j uses index-free adjacency. This simply means that relationship information pointing to neighbors is stored with each entity (node). Technically using memory pointer offsets to find the fixed blocks of storage space for each node and each relationship. It sounds complex, but from a node all relevant, related offsets are available, so the database can jump to those blocks and continue from there. This makes the traversal very, very fast, as it only needs to perform a simple multiply-and-add-operation to find the next memory address. No index lookups needed.Another factor in the speed of our database is Neo4j’s own graph query language — Cypher. It was modeled similarly to the powerful SQL for relational databases, but Cypher was designed and built for graphs. It is a very simple, concise language that uses ASCII-art to form the syntax for graph patterns. An example that parallels the earlier SQL query is below.MATCH (p:Person {lastName: Reif})-[r:WORKS_FOR]->(c:Company)RETURN p, r, cNeo4j is also ACID-compliant. ACID stands for Atomic, Consistent, Independent, and Durable. ACID-compliance ensures that the isolated transactions are entirely saved and completed in the database, protecting the data integrity. With ACID, you won’t have a partial transaction commit to the database, losing some of the transaction in the process. Either the entire transaction will commit, or it will entirely fail and rollback.Finally, Neo4j has a fantastic community surrounding the product, usage, and development experience. The community and Neo4j have built (and continue to do so) an amazing suite of extensions, libraries, and tools to help developers integrate the graph database with other vendor tools and also just interact with the data more easily.From language drivers to GitHub project libraries for various use cases, there is probably something out there to improve the developer’s life with Neo4j — no matter your need. And, if not, we can often build it. :) Getting feedback from developers with different backgrounds and skill sets improves Neo4j’s user experience, product functionality, and understanding for what users want and need in our products. This is the beauty of an open source mindset!Use CasesWith so much connected data in the world around us, there are more and more use cases that fit a graph database. Any scenario where you might be looking for connections, analyzing layers of relationships, navigating long paths, or trying to look at disparate entities as a unit are excellent possibilities to use a graph.Many companies apply graphs for fraud detection in financial institution transactions, recommendations for retail products/services, knowledge bases with lessons learned or disease research, identity and access management for user or role-based permissions control, network and infrastructure monitoring to track network interdependencies and impacts, and so many others. The list of opportunities continues to grow as data becomes more and more connected.Note: Though the prospect of limitless possibilities for graph databases is overwhelmingly exciting, we want to note that not every opportunity is the best fit for it. You should evaluate your use case and the data to see if graph would provide the best value for your needs.There is a large variety of companies that have begun to see the value in graph technology — startups to enterprises, researchers, NGOs and governments, as well as across any imaginable industry. From big names to small, Neo4j’s goal is simply to help the world make sense of data.New to graph databases and want to learn more about Neo4j? Check out my resource links below for more information. Happy learning!ResourcesNeo4j websiteDeveloper GuidesTry Neo4j in a SandboxDownload the Neo4j Desktop AppNeo4j Use CasesNeo4j Customers;Jul 19, 2018;[]
https://medium.com/neo4j/visualizing-shortest-paths-with-neomap-0-4-0-and-the-neo4j-graph-data-science-plugin-18db92f680de;Estelle ScifoFollowFeb 27, 2020·5 min readVisualizing shortest paths with neomap ≥ 0.4.0 and the Neo4j Graph Data Science pluginA new version of neomap was released on 27th February (version 0.4.0). It brings a few new features:Support for Neo4j-spatial simple point layersSave/Open existing projectDraw polylinesBetter handling of large datasetsSee the ChangeLog for more details.In the meantime, the Neo4j Graph Algorithm library is being replaced by the Graph Data Science (GDS) plugin. In this post, we are going to see how neomap can be used together with this new library to visualize shortest paths through some London streets.The dataIn this example, we are going to use a subset of the London streets graph extracted from OpenStreetMap (OSM).In order to do this extraction, we can use the awesome osmnx python package. With only three line of codes, we can get a graphml file compatible with Neo4j:import osmnx as oxG = ox.graph_from_point((51.509934, -0.087333), distance=1500, network_type=’all’)ox.save_graphml(G, london.graphml”)This code snippet creates the street network centered around the point with latitude and longitude (51.509934, -0.087333), within a radius of 1500 meters. It also saves the generated graph in london.graphml.The generated graph looks like the following figure, where nodes are displayed in blue and edges in grey:London street network — Image generated by osmnxWe can then import this graph into Neo4j using the APOC plugin:CALL apoc.import.graphml(london.graphml”, {})Since the created nodes have no label, we can assign a label to them (not mandatory, but improves query performance if you happen to have other types of nodes/many nodes in your Neo4j graph):MATCH (n) SET n:NodeOur graph schema is quite simple: only one node Label (Node), related to each other through the RELATED relationship:Neo4j graph schemaAnd we are ready to go.Finding shortest pathIn order to use the GDS shortest path algorithms, we first need to create a projected graph the algorithm will run on. Here is how to create the projected graph we want to use:CALL gds.graph.create.cypher(         projected_graph”,          MATCH (n:Node) RETURN id(n) AS id”,          MATCH (n)-[r:RELATED]->(m) RETURN id(n) AS source, id(m) AS target, toFloat(r.length) AS length”)From this projected graph, we can then find the shortest path between two nodes with:MATCH (startNode:Node {osmid: 7203717542”})MATCH (endNode:Node {osmid: 7203717545”})CALL gds.alpha.shortestPath.stream(        projected_graph”,         {          startNode: startNode,           endNode: endNode,           relationshipWeightProperty: length”        }) YIELD nodeId, costRETURN gds.util.asNode(nodeId).osmid AS osmid, costThe result is displayed below, both with a table listing the node’s osmid and cost(length) of the path between them (only 45 meters!), and with the graph visualization.Nodes in the shortest pathLet’s now see how neomap can help in visualizing this path in a more understandable way, since we have geolocated nodes.Shortest path visualizationWe are going to create two layers:A marker layer indicating each node in the graphA polyline layer able to display the shortest path between two chosen nodes, based on a Cypher queryLondon Street NetworkIn order to visualize the London Street Network, we can use a simple layer configuration. In this setup, we just need to configure:The node label(s) to be displayed. Here we only have one label called NodeThe node property containing the latitude (y in our case)The node property containing the longitude (x in our case)The node property to be used for the popup. We will choose the osmid property to be able to easily identify and find the node locationsFinally, we can choose the color to be used for the markers. Here I choose grey for convenience. The final configuration is shown on the image below:Simple marker layer configurationYou can hit the Update map” button to see where our nodes are. Let’s now create another layer to display the result of a shortest path algorithm.Shortest PathTo visualize shortest path, we will use an advanced layer configuration, selecting the nodes allowed to be displayed, using a Cypher query. This query needs to return two attributes: the latitude and longitude of the nodes. Starting from the shortest path query above, we just need to change our return statement so that it returns the nodes latitude (y attribute) and longitude (x attribute):MATCH (startNode:Node {osmid:  7203717542 })MATCH (endNode:Node {osmid:  7203717545 })CALL gds.alpha.shortestPath.stream(         projected_graph ,         {           startNode: startNode,            endNode: endNode,            relationshipWeightProperty:  length         }) YIELD nodeId, costWITH gds.util.asNode(nodeId) AS nodeRETURN node.x AS longitude, node.y AS latitudeInside the neomap layer configuration, we will also choose the Polyline” map rendering method, in order to draw a line and not markers for this path:Shortest Path advanced layer configurationUpdating the map displays this result nicely:Finding path between other pairs of nodesYou can now also use the London Street Network layer to find the node’s osmid (by clicking on them) and modify the shortest path query. For instance, let’s find the shortest path between St Paul’s Cathedral and Cannon Street Station:NB: since our nodes do have latitude and longitude (y and x), we can also use the A* algorithm (gds.alpha.shortestPath.astar).Have fun with Neo4j, its new GDS plugin and neomap!;Feb 27, 2020;[]
https://medium.com/neo4j/try-and-then-retry-there-can-be-failure-30bf336383da;Michael SimonsFollowAug 24, 2020·17 min readTry. And then retry. There can be failure.With persistent network connections between things, the exceptional case should be expected and not considered to be a surprise. There are dozens of reasons why a remote connection may be closed or be rendered unusable. In such scenarios precautions must be taken so that your logic succeeds eventually.IntroductionGeneral considerationsMost database management systems (DBMS) these days provides client libraries aka drivers that provide stateful connections to the DBMS. Establishing such connections includes among other things acquiring a transport link, a network session on both the client and the server and of course, user authentication. These days connections are usually encrypted — or at least they should be — so the whole TLS ceremony, including certificate verification comes before that. All that makes creating connections expensive. Therefore once created, these network connections are kept intact as long as it’s reasonable.Having established and verified a connection once gives no guarantee that this connection will remain healthy until it is closed. There are plenty of reasons that a connection can become useless:The physical connection failsThe server goes awayParts of the server being in use go away (i.e. the members of a cluster)The server figures that the connection has not been used and terminates itAn intermediate component (firewall, load balancer) decides that the connection should be terminatedIn the Java world JDBC is an established standard for connecting to relational databases. JDBC connections are rarely handled in isolation directly from within an application but most often through connection pools. In connection pooling, after a connection is created, it is placed in the pool. After it was closed” the connection is put back into the pool, so it can be used again so that a new connection does not have to get re-established. If all the connections are being used, a new connection is established and added to the pool. Connection pools can also be configured to verify or test connections before they handle out a lease or to keep them alive.Depending on the pool, this might or might not save you from the pain when a connection goes away. The pool can create a new one, hand it out and you’re good to go.This is all fine when you execute small operations that complete fast or at least way before a connection may fail. It won’t help you with longer running transactions, involving multiple calls on a connection. The first calls may succeed, the next one fails. A pool cannot not do anything here, when it handed out the connection, it worked.To sum this up: Stateful connections are costly to acquire and there are many good reasons to keep them around once created. Connection failures however are not something extraordinary, they happen frequently.As a developer you have to create your application in such a way that it is able to mitigate connection failures. There are various tools that help you with the basic connection management in pools. When things fail in mid flight, it is up to you and your requirements to decide whether you want to fail hard or retry a transaction.Neo4jWhy is this topic important?Neo4j can be run as a database cluster, not only to make it very scalable but also make it very resilient against the occasional loss of a server or changes in infrastructure. Clients are usually routed to one of the members of a cluster that fulfills the needs of that client (either read-only operations or read-and-write operations). If the member to which the client is currently connected goes away, the cluster itself takes care of the cluster’s reorganization, but the client needs to handle the exceptional state: A stateful connection that has become stale.That becomes even more relevant in cloud deployments of the Neo4j database, such as Neo4j Aura, in which the members of a cluster are rotated quite often as part of normal operations (software upgrades, resiliency etc…).The Neo4j DriverI am writing this post from a Java perspective, thus I mainly focus on the Neo4j Java driver, but the core aspects apply to each supported driver.Each Neo4j driver instance does connection pooling for you. You point it to a server or a cluster and it creates an internal connection pool for you. With .session() (or .asyncSession() respectively .rxSession()) on the driver object, a session with an underlying connection will be acquired and handed to you.If there are no more connections in the pool or if all connections are closed, a new connection is created.All the work in that sessions happens inside of transactions. All the time.Transactions are atomic units of work executing one or more Cypher statements. Transactions may contain read or write operations, and will generally be routed to an appropriate server for execution, where they will be carried out in their entirety. In case of a transaction failure, the whole transaction needs to be retried from the beginning.The driver offers two modes in which the transaction management is done for you and one mode where you are responsible.You will find the following information in the driver’s manual under Transactions”.Automatic commits: This is the simplest form: You run one query directly on the session. Opening a transaction before and committing it afterwards is done for you. This is probably also the most short-lived form of a transaction, depending only on how long your single statement takes to run.Either way: Not only is the transaction managed for you but also making sure the session and its underlying physical connection is usable is done for you. This all happens in one go. The statement will however not be retried if the connection breaks down during execution. Which is kind of what you expect from such a kind of operation, in this way it resembles the JDBC autocommit mode.Transactional functions: All the drivers provide an API that takes in a unit of work callback, defined in terms of the programming language. In the Java world that is a functional interface called TransactionWork<T> with a single method called T execute(Transaction tx). T represents a type parameter (the type of the object returned by the unit of work) and the one and only parameter is the ongoing transaction. That way it can be easily implemented using a lambda.Round and round again.Those functions can be passed around inside the driver and they can be retried (re-executed) when things fail during execution. Failure can happen as described before (the acquisition of the physical connection fails) or in-flight (connection is lost) or on transient errors (like deadlocks or timeouts).To allow the driver to safely execute those functions multiple times, there are some hard requirements:The functions must be idempotent and are not allowed to return an unconsumed result set from any ongoing query. Idempotency should be an obvious requirement: The function may be applied multiple times until success and it shouldn’t have a different outcome on the second run than at first try.The second requirement may not be that obvious. The function will use the passed Transaction object to execute queries. The result of those queries is tied to that transaction. The transaction will be closed after the function ran, in both failure and success states. At that very moment, the behaviour of the result will become undetermined, as it is tied to the transaction. If the method returns anything from it, than it must be mapped into a stable object before the end of the functions.You might wonder why 2 of the following examples require additional libraries respective work to instead of relying on the builtin retry-mechansim:Many application frameworks, Spring being one of them, run their own transaction management. Those application level transaction have a bigger scope than the drivers one. A larger unit of work, orchestrated by the application. They may even integrate with another transaction manager by chaining multiple transactions. In such scenarios we cannot just force the outer transaction into something the driver can understand. Even if it was the case, many users enjoy the ease of declarative transaction. They can annotate their business logic with @Transactional and the framework does the heavy lifting of wrapping the annotated functions inside a transaction.Unmanaged transaction: Unmanaged transactions work by opening a session (which will ensure that at this very moment a working connection can be established), and than explicitly opening up a transaction with beginTransaction(), working on it and committing as you see fit.This is the API which many higher-level abstractions/libraries will use with the driver. Such abstractions — like Spring Data Neo4j — use transactions managed by the application (or framework) itself and translate them into database transactions accordingly. Mark Paluch and I spoke at Devoxx 2019 about those topics, find our reactive transaction master class here.Unmanaged transactions provide freedom of interaction with the database transactions as necessary — for example keeping it open as long as a large result set is streamed and processed further — but delegate the responsibility of handling cases of failures back to the client code.ExamplesThe following examples are all Java based and use Spring Boot as an application runtime.The reason for that: I work in the team that is responsible for our Spring integration and have a lot of Java experience so in the end, I can write idiomatic Java much better than say idiomatic Python or Go.The main ideas should be portable to other languages too.The examples all work in the Neo4j Movie Graph. For your convenience I have added Neo4j migrations to the setup of each project. It creates the dataset for you.Each of the different services offer a read-only REST service under http://localhost:8080/api/movies, giving you a list of movies.A second endpoint, http://localhost:8080/api/movies/watched/ takes a movie title records a  watch/view  of the movie. This endpoint requires authentication as couchpotato with password secret.All three example services use the same MovieController to orchestrate a MovieService looking like thispublic interface MovieService {        Collection<Movie> getAllMovies()        Integer watchMovie(String userName, String title)}The implementations of the movie service, especially watchMovie is bloated and complicated on purpose.The general flow isgetting the movie, thengetting the person that is authenticated and thenupdating the number of times the movie is watched.I know how to write this in one Cypher statement, but the idea is to have a slight window of time between operations on the database in which I can kill the connection or introduce arbitrary failure.All the following examples are available on Github: Neo4j Java Driver retry examples.michael-simons/neo4j-sdn-ogm-tipsA curated list of Neo4j SDN and OGM tips developed while answering questions on SO or for customers. …github.comShared configurationThe examples share the following configurationConfig.builder()                .withMaxConnectionLifetime(5, TimeUnit.MINUTES)                .withMaxConnectionPoolSize(1)                .withLeakedSessionsLogging()or expressed as properties in Spring Boot 2.3 with our starter on the classpath in application.properties.org.neo4j.driver.pool.max-connection-lifetime=5morg.neo4j.driver.pool.metrics-enabled=trueorg.neo4j.driver.pool.log-leaked-sessions=trueorg.neo4j.driver.pool.max-connection-pool-size=1or with Spring Boot 2.4 upwards asspring.neo4j.pool.max-connection-lifetime=5mspring.neo4j.pool.metrics-enabled=truespring.neo4j.pool.log-leaked-sessions=truespring.neo4j.pool.max-connection-pool-size=1This is NOT a configuration I recommend in any form in production. Especially the pool size of one effectively disables the pool, but allows for easy testing our retries via Neo4j’s dbms.listConnections() and dbms.killConnection()functions.Application using the Java driverThis describes the application named driver_with_tx_function in the GitHub repository. Not relevant for our example, but it usesspring-boot-starter-web, spring-boot-starter-security and neo4j-java-driver-spring-boot-starter which gives you the Neo4j Java driver.Given the service holds an instance of org.neo4j.driver.Driver like this:@Servicepublic class MovieService {        private final Driver driver        MovieService(Driver driver) {                this.driver = driver        }}the function reading all the movies can be implemented like this:public Collection<Movie> getAllMovies() {    // This is a transactional function, a unit of work    TransactionWork<List<Movie>> readAllMovies = tx -> {            // A mapping function, extracted for readability        Function<Record, Movie> recordToMovie =            r -> new Movie(r.get( m ).get( title ).asString())        // The only interaction with the database        return tx.run(             MATCH (m:Movie) RETURN m ORDER BY m.title ASC         ).list(recordToMovie)    }    try (Session session = driver.session()) {        // The actual moment the unit of work         // is passed to the driver        return session.readTransaction(readAllMovies)    }}The whole unit of work is basically atomic. It doesn’t modify state, so it is safe to retry. The result set is consumed before the unit of work is left (via the list(transformer)method). When passed to readTransaction the driver tries to execute it for a maximum of 30s by default.The ceremony looks very similar for a write scenario:public Integer watchMovie(String userName, String title) {    // The unit of work    TransactionWork<Integer> watchMovie = tx -> {        // Split onto multiple queries to have         // some window for disaster        var userId = tx.run(             MERGE (u:Person {name: $name}) RETURN id(u) ,            Map.of( name , userName)        ).single().get(0).asLong()        var movieId = tx.run(             MERGE (m:Movie {title: $title}) RETURN id(m) ,            Map.of( title , title)        ).single().get(0).asLong()        // With some random delay added as well        InsertRandom.delay()        var args = Map.of( movieId , movieId,  userId , userId)        return tx.run(              +  MATCH (m:Movie), (u:Person)              +  WHERE id(m) = $movieId AND id(u) = $userId              +  WITH m, u              +  MERGE (u) - [w:WATCHED] -> (m)              +  SET w.number_of_times = COALESCE(w.number_of_times,0)+1              +  RETURN w.number_of_times AS numberOfTimes , args)            .single().get( numberOfTimes ).asInt()        }        try (Session session = driver.session()) {            // The actual call, this time            // in a `writeTransaction`            return session.writeTransaction(watchMovie)        }}All the merge-operations (i.e. graph updates) in those statements will be committed or none at all. Care must be taken not calling a stored procedure that does internal commits or using a statement with PERIODIC COMMIT or creating other side-effects.The execution of the watchMovie unit of work will be retried for 30 seconds by default.Now let’s look at Spring’s @Transactional, Object-Graph-Mappers like Neo4j-OGM or for more value-add Spring Data Neo4j.Application using Neo4j-OGM and Spring Data inside Spring transactionsThe following example can be found in the sdn_ogm application of the example repository.michael-simons/neo4j-sdn-ogm-tipsA curated list of Neo4j SDN and OGM tips developed while answering questions on SO or for customers. …github.comSpring offers a declarative way of defining transactional boundaries in the service layer of an application via the @Transactional annotation. This depends on Spring’s TransactionManager. In Spring’s case this TransactionManager is responsible for the scope and propagation of a transaction and also on which type of exceptions the transaction should be rolled back.Spring’s transaction manager has no built-in understanding of retries. That is treated as an application level concern.In addition to @Transactional, Spring transactions can also be used with the TransactionTemplate, which is a similar unit-of-work callback, but the restrictions mentioned just above stay valid.Assume an OGM based service like this@Servicepublic class MovieServiceBasedOnPureOGM implements MovieService {        private final org.neo4j.ogm.session.Session session        public MovieServiceBasedOnPureOGM(Session session) {                this.session = session        }}The session is not a Driver, but an OGM session!Looking at the read method above implemented with OGM we find@Transactional(readOnly = true)public Collection<Movie> getAllMovies() {    return session.loadAll(Movie.class)}There’s no way to use/inject the neo4j-driver’s builtin retries as the operation is just passed through the drivers APIs. The same is true for the write case.Again, please note that this is of course implemented badly to test out retries:@Transactionalpublic Integer watchMovie(String userName, String title) {var user = Optional.ofNullable(            session.queryForObject(                User.class,                 MATCH (u:Person) WHERE u.name = $name   +                 OPTIONAL MATCH (u) -[w:WATCHED] -> (m:Movie)  +                 RETURN u, w, m ,                Map.of( name , userName)            )).orElseGet(() -> new User(userName))    var movie = Optional.ofNullable(            sessiom.queryForObject(                Movie.class,                 MATCH (m:Movie) WHERE m.title = $title RETURN m ,                Map.of( title , title))            ).orElseGet(() -> new Movie(title))    InsertRandom.delay()    int numberOfTimes = user.watch(movie)    session.save(user)    return numberOfTimes}getAllMovies and watchMovie now defines our transactional units of work, as the lambdas in the previous section did before.To avoid defining custom queries completely, we can replace the interaction with the session with Spring Data repositories like that:@Servicepublic class MovieServiceBasedOnSDN implements MovieService {    interface MovieRepository extends Neo4jRepository<Movie, Long> {        Optional<Movie> findOneByTitle(String title)    }    interface UserRepository extends Neo4jRepository<User, Long> {        Optional<User> findOneByName(String name)    }    private final MovieRepository movieRepository    private final UserRepository userRepository    public MovieServiceBasedOnSDN(        MovieRepository movieRepository,        UserRepository userRepository) {        this.movieRepository = movieRepository        this.userRepository = userRepository    }    @Override @Transactional(readOnly = true)    public Collection<Movie> getAllMovies() {        return (Collection<Movie>) movieRepository.findAll()    }    @Override @Transactional    public Integer watchMovie(String userName, String title) {        var user = userRepository.findOneByName(userName)            .orElseGet(() -> new User(userName))        var movie = movieRepository.findOneByTitle(title)            .orElseGet(() -> new Movie(title))        InsertRandom.delay()        int numberOfTimes = user.watch(movie)        userRepository.save(user)        return numberOfTimes    }}The transactional units of work stay the same and it reads better but there’s still no way we can facilitate the drivers builtin retry mechanism.As explained earlier: Expect those things to fail! With the code in place, you can handle this on the calling side like this:@PostMapping( /watched )public Integer watched(    Principal principal, @RequestBody String title) {    try {        return this.movieService           .watchMovie(principal.getName(), title)    } catch(Exception e) {        throw new ResponseStatusException(HttpStatus.I_AM_A_TEAPOT)    }}Or do retries on your own in that catch block. Or you do it in the client at the caller who can decide to auto-retry or leave it to the end user to do so.Regardless of what you do: It is the applications responsibility to handle these errors!One way of doing this is a library named Resilience4j. Resilience4j is a lightweight fault tolerance library inspired by Netflix Hystrix, but designed for functional programming.resilience4jResilience4j is a fault tolerance libraryresilience4j.readme.ioThe library offers not only retries, but also circuit breakers, bulkheads and more. In generally, it offers several ways to make your application more resilient against inevitable exceptional states and behaviors of other services it depends upon.The easiest way add Resilience4j to your Spring project is via a starter: io.github.resilience4j:resilience4j-spring-boot2:1.5.0. In addition, you have to add org.springframework.boot:spring-boot-starter-aop to enable the declarative usage via the@Retry annotation.Those dependencies gives you support to configure Resilience4j via application properties and provides all beans necessary in the Spring context.Resilience4j can be configured programmatically but we are using the provided configuration properties:# This is represents the default configresilience4j.retry.configs.default.max-retry-attempts=10resilience4j.retry.configs.default.wait-duration=1s# Those are the same exceptions the driver itself would retry onresilience4j.retry.configs.default.retry-exceptions=\  org.neo4j.driver.exceptions.SessionExpiredException,\  org.neo4j.driver.exceptions.ServiceUnavailableException# Only to make log entries appear immediateresilience4j.retry.configs.default.event-consumer-buffer-size=1resilience4j.retry.instances.neo4j.base-config=defaultThis creates a retry object named neo4j which tries 10 attempts and waits for a second in between. It only retries on exceptions of the given type.An exponential backoff interval can be enabled by setting resilience4j.retry.configs.default.enable-exponential-backoff=true.How to use this?some proper resilienceIf you want to stick with the declarative approach, all you have to do is annotate the service class as a whole or individual methods with @Retry(name =  neo4j ) like this:import io.github.resilience4j.retry.annotation.Retryimport org.springframework.stereotype.Serviceimport org.springframework.transaction.annotation.Transactional@Service@Retry(name =  neo4j )public class MovieServiceBasedOnPureOGM implements MovieService {    private final org.neo4j.ogm.session.Session session    public MovieServiceBasedOnPureOGM(Session session) {                this.session = session    }    @Transactional(readOnly = true)    public Collection<Movie> getAllMovies() {        // Implementation see above    }    @Transactional    public Integer watchMovie(String userName, String title) {        // Implementation see above    }}And that’s effectively all there is.If you prefer doing it in a programmatic approach without using annotations, you can inject the registry of Retry objects into the calling side and run your transactional unit of work like this:import io.github.resilience4j.retry.RetryRegistryimport java.security.Principalimport java.util.Collectionimport org.neo4j.tips.cluster.sdn_ogm.domain.Movieimport org.neo4j.tips.cluster.sdn_ogm.domain.MovieServiceimport org.springframework.web.bind.annotation.GetMappingimport org.springframework.web.bind.annotation.PostMappingimport org.springframework.web.bind.annotation.RequestBodyimport org.springframework.web.bind.annotation.RequestMappingimport org.springframework.web.bind.annotation.RestController@RestController@RequestMapping( /api/movies )public class MovieController {    private final MovieService movieService    private final RetryRegistry retryRegistry    public MovieController(        MovieService movieService,         RetryRegistry retryRegistry) {        this.movieService = movieService        this.retryRegistry = retryRegistry    }    @GetMapping({   ,  /  })    public Collection<Movie> getMovies() {        // Get the configured retry        return retryRegistry.retry( neo4j )            // Chose one of the fitting methods and             // execute your service            .executeSupplier(this.movieService::getAllMovies)    }    @PostMapping( /watched )    public Integer watched(    Principal principal, @RequestBody String title) {        return retryRegistry.retry( neo4j )            .executeSupplier(() ->          this.movieService.watchMovie(principal.getName(), title))    }}Please note that you cannot do this inside the service method annotated with @Transactional. If you would, you would get the boundaries exactly the wrong way: The retry would happen inside the transaction. You want to have the transaction retried.The Neo4j driver itself does retry on two additional cases: When it receives a transient exception from the server with two well defined error codes. This is rather easy to replicate by a Java Predicate:public class RetryOGMSDNExceptionPredicate implements Predicate<Throwable> {    @Override    public boolean test(Throwable throwable) {        Throwable ex = throwable        if (throwable instanceof CypherException) {            ex = throwable.getCause()        }        if (ex instanceof TransientException) {            String code = ((TransientException) ex).code()            return ! Neo.TransientError.Transaction.Terminated .equals(code) &&                ! Neo.TransientError.Transaction.LockClientStopped .equals(code)        } else {            return                 ex instanceof SessionExpiredException ||                 ex instanceof ServiceUnavailableException        }    }}As OGM happens to wrap exceptions it catches into CypherException we can unwrap those as well.To add this predicate to your Resilience4j config, add this to your configuration:resilience4j.retry.configs.default.retry-exception-predicate=\  your.package.RetrySDN6ExceptionPredicateNote: We will be adding a pre-build predicate to OGM that you can use for your convenience.Application using Spring Data Neo4j 6 inside Spring transactionsThe upcoming version 2.4 of Spring Boot will contain a completely revamped Spring Data Neo4j without Neo4j-OGM but still containing all the mapping features. The same application using a milestone of SDN 6 (formerly known as SDN/RX) is available as sdn6.michael-simons/neo4j-sdn-ogm-tipsA curated list of Neo4j SDN and OGM tips developed while answering questions on SO or for customers. …github.comThe predicate looks a bit different, but all the rest applies.Running the examplesThe examples require Java 11. I provided a simple client for the application. Built and run it like this:./mvnw clean compile./mvnw exec:java -Dexec.mainClass= org.neo4j.tips.cluster.client.Application It will keep on calling localhost:8080 and expects one of the services running.To run the pure driver based server or the SDN/OGM examples, use./mvnw spring-boot:run -Dspring-boot.run.arguments= --org.neo4j.driver.uri=neo4j://YOUR_DATABASE:7687 --org.neo4j.driver.authentication.password=YOURPASSWORD To run the SDN 6 example, the properties are a bit different./mvnw spring-boot:run -Dspring-boot.run.arguments= --spring.neo4j.uri=neo4j://YOUR_DATABASE:7687 --spring.neo4j.authentication.password=YOURPASSWORD To make the the SDN/OGM respectively the SDN 6 example use the Spring Data repositories, add --spring.profiles.active=use-sdn to the run arguments.All applications provide metrics for the driver (how many connections have been created) under http://localhost:8080/actuator/metrics/neo4j.driver.connections.created.The SDN/OGM and the SDN 6 application that use Resilience4j provide additional metrics about retries, such as:http://localhost:8080/actuator/metrics/resilience4j.retry.callshttp://localhost:8080/actuator/metrics/resilience4j.retry.calls?tag=kind:successful_without_retryhttp://localhost:8080/actuator/metrics/resilience4j.retry.calls?tag=kind:successful_with_retrySummaryThe Neo4j Java Driver and libraries such as Neo4j-OGM and Spring Data Neo4j works just fine against Neo4j clusters and cloud solutions like Aura. All three transaction modes (auto commit, managed and unmanaged transactions) can be used.A library using unmanaged transactions just works just fine against that dynamic environment.However, applications must plan and prepare for connection failures — regardless whether the database is deployed standalone or as a cluster. This is expected like with other databases. Connection failures can be mitigated by using built-in retry mechanisms of our drivers or using external solutions.In the Java world, you have two options to deal with this for Neo4j: Using the built-in mechanisms or a tool like Resilience4j.Resilience4j allows shaping those retries in a very fine grained way. We haven’t discussed what happens at the n-th retry: Either the thing fails completely or an alternative is called. Such a last resort would keep services available for the users with retries enabled later on.If you can not use either transactional functions or Resilience4j, make sure you keep the session object as short lived as possible. The Neo4j drivers are smart enough to validate the physical connection before handing out a session. When you open a fresh one, use and than close it immediately, chances are rather low that you suffer from an inflight loss of connectivity.Then you still have the option of handling the error from the calling client’s side which might have more context available or can just ask the user what to do.Happy Coding, MichaelMany thanks to Michael Hunger and Gerrit Meier for proofreading the post and fixing my errors.;Aug 24, 2020;[]
https://medium.com/neo4j/user-segmentation-based-on-node-roles-in-the-peer-to-peer-payment-network-1a766c60a4ee;Tomaz BratanicFollowJul 21, 2022·12 min readUser Segmentation Based on Node Roles in the Peer-to-Peer Payment NetworkUtilize the Neo4j Graph Data Science library to identify node roles and use them as features for the user segmentation processKnowing your users is vital to any business. When your users can interact with each other on a social media platform, content sharing platform, or even work-related platforms, you can construct a network between your users based on their interactions and extract graph-based features to segment your users. Of course, these same approaches can be applied to other platforms that are not user-centric.In this blog post, I will walk you through the user segmentation process of a peer-to-peer payment platform through network analysis.On a peer-to-peer platform, users can digitally transfer money online to anyone else on the platform. Such platforms have gained a lot of popularity in recent years. Examples of real-world peer-to-peer payment platforms are Venmo, PayPal, and Revolut.Peer-to-peer payment network. Image by the author.We can represent the transactions between users as a network where nodes represent users, and the relationships represent transactions. The connections have a weight or a property that contains the information about the total amount sent. If your dataset includes the timeline information, you could also add the dates of transactions to the relationships.In order to segment the users in the network, we need to come up with features that describe the role of users in the network. In general, we could take advantage of running unsupervised algorithms that auto-magically encode node positions and roles in the network as a vector per node.However, the problem with the unsupervised approach to feature engineering is that it’s hard to explain the results. For this reason, we will manually construct feature vectors for users.We can encode anything in the feature vectors we deem descriptive of user roles in the network. For example, in a peer-to-peer payment network, it makes sense to encode how many transactions and their values a user has but also look at which users connect to different communities and act as a bridge between them.In this example, we will combine some simple statistics like the average transaction value of a user with outputs of graph algorithms like the Betweenness Centrality to encode the user roles in the network.User segmentation process. Image by the author.As mentioned, we need to encode user roles and positions in the network as vectors. In this post, we will manually construct features describing nodes. These features are:Average transaction amountYears since first transactionWeighted in-degree (total amount received)Weighted out-degree (total amount sent)Betweenness centralityCloseness centralityWe will use these six features to encode node roles and positions in the network. The average transaction amount and the years since the first transaction features are used to capture how old they accounts are and how much on average they send. Next, we will use the weighted in and out-degrees to encode the total amount sent and received by the user. Lastly, we will use the Closeness and Betweenness centralities algorithms to encode the node position in the network.After the feature engineering process is completed, we will use the K-means algorithm to cluster the data points into groups or communities of users. These communities will represent various segments of users on the platform.Environment setupWe will be using Neo4j as the database to store the peer-to-peer network. Therefore, I suggest you download and install the Neo4j Desktop application if you want to follow along with the code examples.The dataset is available as a database dump. It is a variation of the database dump available on Neo4j’s product example GitHub to showcase fraud detection.fraud-neo4j-v44.dumpEdit descriptiondrive.google.comI wrote a post about restoring a database dump in Neo4j Desktop a while back if you need some help. After you have restored the database dump, you will also need to install the Graph Data Science and APOC libraries. Make sure you are using version 2.1.0 of the GDS library or later.The code with the examples in this post is available on GitHub.blogs/p2p-network-analysis.ipynb at master · tomasonjo/blogsJupyter notebooks that support my graph data science blog posts at https://bratanic-tomaz.medium.com/ …github.comYou will need to have the following three Python libraries installed:graphdatascience: Neo4j Graph Data Science Python clientseaborn: Visualization libraryscikit-learn: We will use t-SNE dimensionality reductionSetting up the connection to Neo4jNow we are ready to start coding. First, we need to define the connections to the Neo4j instance with the graphdatascience library.We need to fill in the credentials to define the connection to the Neo4j instance. Make sure to change the passwordvariable to the appropriate value. I like to print the gds.version() method to make sure that the connection is valid and the target database has the Graph Data Science library installed.We can quickly evaluate the populated graph schema with the apoc.meta.stats procedure.There are 33,732 users in the database and 102,832 P2P transactions. There could be multiple P2P transactions between a pair of users. Therefore, we are dealing with a directed weighted multigraph. We have some additional information about the users available, such as which IPs, credit cards, and devices they have used.Feature engineeringWe will now move on to calculating user features used for segmentation. The first two features we will extract are the account age, calculated by looking at the first transaction, and the average transaction amount. We will use a single Cypher statement to calculate them and store them as node properties in the database.The gdsobject has a run_cyphermethod that allows you to execute any Cypher statements.The remaining four attributes will be calculated by executing graph algorithms available in the Graph Data Science library. The graphdatascience library allows you to execute any GDS procedure and algorithm using pure Python code. I wrote an article on mapping GDS syntax from Cypher statements to pure Python code to help you with the transition.First, we need to project an in-memory graph. The projected graph schema will be relatively simple and will contain only User nodes and P2P relationships. In addition, we will include node properties accountYears and avgTransactionAmount and relationship property totalAmount in the projection.Since we want to use network features like the Betweenness and Closeness centrality for user segmentation, we will first evaluate how connected our network is using the Weakly Connected Components algorithm. The Weakly Connected Components algorithm is used to find disconnected parts or islands in the network and can help you evaluate how connected the network is overall.We will use the mutatemode of the Weakly Connected Components algorithm, which stores the results to the projected graph and returns high-level statistics of the algorithm result.There are 7743 disconnected components in our P2P network. That is a high and slightly unexpected amount of disconnected components, as there are only around 30,000 users in the network. The largest component has 11,311 members, about 30 percent of the population, while other components are tiny and contain only a few members. We notice that 99 percent of communities have 12 or fewer members.Since we want to segment our users based on their network features, we will focus our analysis on the largest component only and ignore the rest. This is because the network attributes are not very descriptive when dealing with small components that contain five members. More importantly, I want to show you how to filter the largest component in your projected graph. However, you could always run your analysis on the whole network if you wanted to, or if that was the requirement.The subgraph filter projection procedure allows us to filter projected graphs on mutated properties. In this example, we will select only nodes that are part of the largest component. Then, the subgraph filter projection creates a new graph based on the specified predicates.First, we needed to fetch the id of the largest component. We can retrieve mutated properties from the projected graph using the gds.graph.streamNodeProperty method. Next, we applied a simple aggregation and sorting to extract the particular component id, and then used it as an input to the subgraph filter projection method.Now that we have the projected graph containing only the largest component ready, we can extract the network features. We will begin by calculating the weighted in and out-degrees. Again, we will use the mutatemode of the algorithm to store the results back to the projected graph.The only two features we need to calculate are the Betweenness and Closeness centralities. The Betweenness centrality is used to find bridges in the network connecting different community nodes.Sample network where nodes are colored based on the Betweenness centrality from white (smaller score) to red (higher score). Image by the author.This example visualized a Marvel network, where relationships represent which characters appeared together in a comic. A prime example of Betweenness centrality is the character Beast, which connects the right-hand-side community with the other part of the network. If he was removed from the network, there would be no connection between the two communities. We can say that character Beast acts as a bridge between the two communities.Closeness centrality is a way to identify nodes close to all the other nodes in the network and can therefore spread the information to the network efficiently. The idea is that the information only spreads through the shortest paths between pairs of nodes.Sample network where nodes are colored based on the Closeness centrality from white (smaller score) to red (higher score). Image by the author.We can observe that nodes in the center of the network have the highest Closeness centrality score as they are able to reach all the other nodes the fastest.We calculate the Closeness and Betweenness centrality score with the following code:Feature explorationBefore we move on to the K-means clustering part, we will quickly evaluate the distributions of our features. We can fetch multiple mutated properties from the projected graph using the gds.graph.streamNodePropertiesmethod.The features_df dataframe has the following structure:Structure of the features_df dataframe. Image by the author.We can use the features_df dataframe to visualize the distributions of our features.Distributions of the features. Image by the author.Interestingly, the account age is equally distributed through the years. Most users have a small average transaction amount of only a few hundred. However, some outliers have an average transaction amount of more than 3000. We have a few outliers in the weighted out-degree that have sent values of over half a million. The dataset doesn’t contain the currency, so I can’t say if we are dealing with USD or not.The gds.graph.streamNodePropertieshas a special keyword separate_property_columns attribute that pivots the dataframe automatically for us. Let’s try it out.The pivot_features_dfdataframe has the following structure:Structure of the pivot_features_df dataframe. Image by the author.If you are not that visually oriented and want to get a table of distribution statistics, you can use the describe method of the dataframe.ResultsResults of the describe method. Image by the author.The pivot dataframe structure is also handy when we want to visualize the correlation matrix.ResultsCorrelation matrix. Image by the author.We can observe that the Betweenness centrality correlates with both the weighted in and out-degrees. Other than that, it seems that our features are not that correlated.K-means clusteringK-means clustering is a widely used technique to group or cluster data points. However, since there are no training labels to learn from, the K-means algorithm is regarded as an unsupervised machine learning algorithm. The algorithm starts with the first group of randomly selected centroids used as baseline points for every cluster. The number of clusters is a fixed number defined with the parameter k. The algorithm then assigns each data point to the nearest centroid and iteratively optimizes the position of centroids.There are plenty of great resources on the web about the K-means algorithm, so I won’t go into detail about how it works.But there’s one crucial component to understand how it differs from other community detection algorithms like the Louvain or Label Propagation algorithms. Instead of using a graph of nodes and relationships as an input, we need to input vectors (array of numbers) of features that describe each data point.Before running the K-means algorithm, we need to standardize our features, so that some features with high values will not skew the results. Therefore, we will use the standard score scaler, which is available in the Graph Data Science library.We have selected our six features to be scaled and mutated under the features node property in the projected graph. The features node property type is a vector or an array of numbers.The only thing left to do is execute the -means algorithm. We need to define how many clusters we want to identify by specifying the parameter k. For the purpose of blog presentability, I will use a smaller value of 6. However, you should probably use a larger number when you don’t need to sacrifice accuracy for blog presentability like me.Inspect cluster resultsWe will begin the cluster analysis by evaluating the size of clusters. First, we need to merge the kmeans_df dataframe with the pivot_features_df. Next, we use a simple group by aggregation to calculate the size of clusters.ResultsCluster size results. Image by the author.The cluster sizes vary from 217 members all the way to 4870 members. While we could calculate statistics of features for every community, it is tough to compress the results and make them presentable in a blog. Therefore, we will first evaluate the weighted out-degree (total amount sent to other users) per cluster.ResultsDistribution of weighted out-degree per cluster. Image by the author.Users in communities 3 and 5 have no outgoing transactions, as their total amount sent to other users is zero. Communities 1 and 2 represent users that sent a small amount to other users. On the other hand, the community with id 4 represents power users who sent vast amounts to other users.Let’s say we are interested in learning more about the power users community. We can use Pandas methods to filter and transform the data to an appropriate structure to be visualized with the Seaborn library.ResultsFeature statistics of power user community. Image by the author.It seems that power users are more or less the oldest accounts. Their average transaction amount usually ranges between 500 to 2500. So they sent a lot of currency to other users, but also received a lot of it. Not surprisingly, they have, on average, a high Betweenness score. Therefore, we can assume that they connect various communities of users and act as a bridge between them.Lastly, we will plot the clusters on a scatter plot. In order to achieve this, we need to use a dimensionality reduction algorithm like t-SNE to reduce the feature dimensionality to 2.ResultsScatter plot visualization of identified clusters. Image by the author.Maybe it is evident from this visualization that our cluster analysis accuracy would benefit from raising the number of clusters by increasing the K value. You may also be asking yourself why the split between clusters isn’t more distinct. For example, the red community is present on both the left and right sides of the visualization. I attribute this to the t-SNE dimensionality reduction algorithm. If we were to run the K-means algorithm on only two features, we should get a nice distinct split between clusters.ConclusionK-means algorithm was added to the Neo4j Graph Data Science library just recently and is a wonderful addition that can be used in your data science workflows. I also enjoy the new Neo4j Graph Data Science Python client that seamlessly integrates with other Python data science libraries.It gives me an excuse to brush up on my visualization skills to make excellent presentations or explore outputs visually. I will definitely be using it in my projects. I encourage you to try it out and learn about its benefits.As always, the code is available on GitHub.;Jul 21, 2022;[]
https://medium.com/neo4j/conversational-artificial-intelligence-with-neo4j-and-unreal-engine-part-2-c7bafe000b1f;Antonio OrigliaFollowDec 16, 2022·7 min readConversational Artificial Intelligence with Neo4j and Unreal Engine — Part 2After describing the role of humanities researchers in populating and analyzing knowledge graphs supporting the design of Embodied Conversational Agents, let’s see how to connect this knowledge in technological applications that have a certain degree of intentionality, as opposed to approaches based on machine learning only.Some contextIn the last few years, industry-grade game engines such as Unity and the Unreal Engine have been made available to the general public. Differently from previous experiences, which required strong programming skills to actually make use of the engines, the newest versions of these tools pay extreme attention to entry-level users, developing user-friendly interfaces for developers to access their most powerful technologies. As a matter of fact, the interest of game engine developers is now to make features such as environment navigation, animation logic, etc. more accessible and easy to integrate into complex workflows. While the industrial effort concentrates on interface quality, development speed and technological integration, researchers in the AI field are mainly interested in the models underlying the interactive processes. These aspects are less explored by the industry, making the main activity of AI and HMI researchers complementary to the industry activity. Integration between the two efforts has the potential to bring benefits to both realities. Specifically, the practical advantages of designing an approach centered on freely available, continuously evolving, technology lie in the possibility, for researchers, of concentrating on interaction management models, rather than on technicalities brought in either by the necessity to compete with the design quality standards set by the entertainment industry or by the need to integrate a new control device.Framework for Advanced Natural Tools and Applications with Social Interactive AgentsThe Framework for Advanced Natural Tools and Applications with Social Interactive Agents (FANTASIA¹) is a plugin for the Unreal Engine that concentrates on supporting the development of RTI3D applications based on natural interaction and social processes. A framework to integrate game engines with tools that are important for studies on natural interaction is relevant to support AI and HMI research and it integrates access to Neo4j from inside Unreal to build a coherent development experience through the Blueprint scripting language. The foundations of this effort can be summarized as:Build upon the interest of the gaming industries in providing advanced environments to develop interactive experiencesLet researchers concentrate on Artificial Intelligence models while avoiding being left behind with respect to industrial standards set on designThe prototype version of FANTASIA was presented in 2019 while the re-engineered version has been presented at the NODES conference in 2021 and at the ACM Multimedia Conference in 2022. You can find out more about it on the Github repository and on the Youtube channel. The following Figure shows the FANTASIA architecture.The FANTASIA architectureNeo4j for Embodied Conversational AgentsResults obtained through LORIEN can be used to implement ECAs using Neo4j and the Unreal Engine through FANTASIA. Our attempt to converge towards a generic dialogue management model with argumentation capabilities involves the harmonic use of different AI tools. Our ongoing work aims at formalizing methodological procedures to transfer LORIEN findings for use in a computational model we call Artificial Neural and Graphical Models for Argumentation Research (ANGMAR).In ANGMAR, neural approaches are mainly used to manage tasks like Automatic Speech Recognition, Intent Recognition, Entity Recognition and Speech Synthesis. These tasks are the ones that are closer, in the perception-action cycle, to the physical level. Deeper layers, dedicated to reasoning and decision making, are instead implemented using graphical models like graph databases and Bayesian Networks. The connection between the different levels happens in Neo4j, where utterances, recognized intents and entities are represented in form of graph and linked to the CCG in a very similar way to the one used in LORIEN to represent dialogue corpora. In ANGMAR, the graph database is aligned with the intent/entity recognizer so that utterances can be directly represented together with what they represent. When intents need specific parameters (slots) to be filled, these are also represented as nodes and linked to the named entities recognised in the utterance. To support reasoning, intents may also imply beliefs that the speaker intends for the counterpart to accept. The following figure summarizes the steps taken in ANGMAR to update dialog history and the consequent belief graph.The belief graph created by analysing the user utteranceIntentionality in graphsThrough graph structures, ANGMAR represents dialogue dynamics and can analyze these to decide what to say next. This process relies on communication models coming from the linguistics field to take into account what could happen from communication issues to intentional moves. After each user move, the system evaluates what to do depending on the context, intended as the graph configuration, dialog management priorities and final objectives. System moves are intended to alter the graph and take it towards a desired configuration, which is a graph structure containing a goal pattern. A desirable graph is a graph that exhibits this pattern and the system will not attempt to modify it: for the case of the movie recommendation task, a desirable pattern may represent a user accepting a recommended item. In general, a linguistic description of dialog management procedures provides a set of priorities, for the communication process, for possible moves to be performed. For example, solving communications problems takes priority over answering open questions and answering open questions takes priority over performing intentional moves (in this case recommending a movie or asking a profiling question).Task prioritization with Behaviour TreesIn ANGMAR, dialog strategies are organized in a Behavior Tree (BT) using the Unreal Engine AI editor to represent these priorities. Using FANTASIA, it is possible to add to the BT a node designed to submit queries to Neo4j, so that graph data can be used to make decisions. During dialogue management, there are a number of communication issues that may happen and have to be solved as soon as they are detected. Over these issues, there is a further hierarchy prioritizing them so that appropriate clarification requests can be generated depending on the detected problem³.For each kind of problem described in the linguistics background, a specific graph pattern can be searched to verify if the problem is present. If this is the case, the corresponding recovery strategy can be adopted. The following Figure shows the subtree dedicated to detecting and solving acoustic problems.BTs give priority to tasks on the leftmost branches so, if they succeed, rightmost tasks are not evaluated. In this case, the most important problem to solve is Acoustic Confidence. The ExecuteNeo4jQuery task allows BTs to run a Cypher query and save its result in a dedicated data structure provided by FANTASIA for Neo4j data. The following task checks if the error pattern was found and generates the appropriate reaction, ignoring all the rest of the possible moves. In the current implementation of ANGMAR, not all problems are actually managed but, from a theory representation point of view, they all have a clear position so that, as research proceeds, we know where to position further pattern checks and specific dialog repair strategies. The flexibility offered by BTs lets us update the computational model iteratively as the theoretical model evolves.Handling logical conflictsA particularly interesting communication problem concerns conflicts between previous beliefs from the system and incoming evidence from the user. Linguistics research highlights that specific question forms must be used to efficiently communicate the problem, and the Neo4j transaction system, made available by FANTASIA, allows to implement a reasoning mechanism that allows to verify that beliefs implied by the user do not conflict with existing beliefs. Specifically, if no higher priority problem is detected, the system can temporarily accept the beliefs implied by the last user utterance by opening a transaction and update the belief graph without committing the changes. Then, conflicting patterns can be searched for in the temporary graph: this includes checking that a belief and its negation do not exist at the same time, in the graph, concerning the same subject and predicate. If a conflicting pattern is found, the corresponding clarification request is generated and the transaction is rolled back. This effectively implements a hypothesizing mechanism that allows the system to reason about what would happen if it was to accept the belief implied by the user. A graph, representing the PCG, that exhibits this kind of pattern is defined as incoherent. The following Figure shows the subtree dedicated to the management of this situation.You may find out more about conflict management with clarification requests using FANTASIA from the Youtube interview given by Dr. Maria Di Maro, from our lab.In the last part of this series, I will concentrate on decision making based on Bayesian Networks, dynamically assembled by extracting sub-graph structures from Neo4j.References¹ Origlia, A., Cutugno, F., Rodà, A., Cosi, P., & Zmarich, C. (2019). FANTASIA: a framework for advanced natural tools and applications in social, interactive approaches. Multimedia Tools and Applications, 78(10), 13613–13648.² Origlia, A., Di Bratto, M., Di Maro, M., & Mennella, S. (2022). Developing Embodied Conversational Agents in the Unreal Engine: The FANTASIA Plugin. In Proceedings of the 30th ACM International Conference on Multimedia (pp. 6950–6951).³ Di Maro, M. (2021).  Shouldn’t I use a polar question?” Proper Question Forms Disentangling Inconsistencies in Dialogue Systems.;Dec 16, 2022;[]
https://medium.com/neo4j/neo4j-logging-monitoring-with-elastic-cloud-and-elk-stack-77a2565b3bf2;David AllenFollowMay 23, 2019·4 min readNeo4j Logging/Monitoring with Elastic Cloud and ELK StackElastic Cloud is frequently used as a monitoring and logging solution the ELK Stack” refers to ElasticSearch, LogStash, and Kibana, which are three key components of the Elastic platform. Together, they provide a lot of powerful capabilities for monitoring and dashboarding.This article will describe how to configure Neo4j logs and metrics to stream effortlessly to an Elastic instance, so you can take advantage of those capabilities. Our approach relies on Elastic’s Beats Platform, which are open source data shippers that you install as agents on your servers to send operational data to Elasticsearch.Neo4j Logging & Monitoring with Elasticsearch and KibanaPre-requisite: Have a Neo4j instance. If you don’t already have one, you can use these directions for GCP to start a new one on Google Cloud.Pre-requisite: Have an ELK stack instance, or preferably, launch an Elastic Cloud instance.Logging SetupNeo4j writes regular files to disk. In order to get the contents of those log files to Elastic on a regular basis, we’ll use a program called filebeat that monitors the files and sends updates to Elastic ongoing.We’re going to install and configure filebeat by executing the following commands on the Neo4j host.Install Filebeatcurl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.1.0-amd64.deb sudo dpkg -i filebeat-7.1.0-amd64.debConfigure FilebeatEdit the configuration file /etc/filebeat/filebeat.yml to point to your Elastic Cloud instance:cloud.id:   clustername:CLOUD_ID_HERE cloud.auth:  elastic:MYPASSWORD This configuration will be a little bit different if you’re pointing to a self-hosted Elastic install, but boy is this route easy and convenient. Make sure that the same configuration file checks for Neo4j logs. Also make sure enabled is set to true”, since it’s false by default in the file:- type: log  enabled: true  paths:    - /var/log/neo4j/*Set up Filebeat and Start the Servicesudo filebeat setup sudo service filebeat startAt this point, filebeat has created an index for itself, and is streaming log messages to that index. By using the Kibana dashboard, we can see the data coming through — in this example I’m filtering it down to the contents of just one file, security.log”.Neo4j’s security.log appearing in KibanaLogs are good, but what about things like CPU, memory, disk, and Neo4j metrics that we’d want to use to monitor the host?Host Metrics SetupFirst, we’ll need to install metricbeat, which is a collector application that picks up the information off of the host where Neo4j is running and sends it to Elastic Cloud. Really the same thing as filebeat, but for metrics.Install Metricbeatcurl -L -O https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.1.0-amd64.debsudo dpkg -i metricbeat-7.1.0-amd64.debConfigure Neo4j to Expose Prometheus MetricsWe’ll also want to grab Neo4j’s built-in metrics (which can be exported by Prometheus) so we will configure Neo4j to do that, by editing neo4j.conf or neo4j.template if you are running a Neo4j Cloud instance, and set these Neo4j settings:metrics.prometheus.enabled=truemetrics.prometheus.endpoint=localhost:2004Then, install the prometheus exporter with apt-get install prometheus and configure it by editing /etc/prometheus/prometheus.yml with the following configuration to export Neo4j’s metrics:scrape_configs:  - job_name: Neo4j-prometheus    scrape_interval: 5s    scrape_timeout: 10s  static_configs:      - targets: [localhost:2004]Notice how localhost:2004 in the prometheus config matches where Neo4j is publishing metrics. Neo4j exposes an endpoint where a collector” can scrape metrics, but doesn’t do the scraping itself. That’s why you need to install prometheus server to collect the data, and provide the endpoint that metricbeat needs.Enable System and Prometheus Modules in MetricbeatEdit the configuration /etc/metricbeat/metricbeat.yml adding the same parameters as above for filebeat, to tell it how to connect to Elastic. We will then enable the right modules:sudo metricbeat modules enable systemsudo metricbeat modules enable prometheusYou’ll need to tweak the prometheus module config so that it knows the port that Neo4j prometheus metrics are exposed on. On my system that’s /etc/metricbeat/modules.d/prometheus.yml, where my config looks like this:- module: prometheus    period: 10s    hosts: [ localhost:9100 ]  metrics_path: /metricsNotice here that the metricbeat plugin is talking to the prometheus exporter, not to the Neo4j prometheus endpoint.Start the Metricbeat Servicesudo service metricbeat startWithin a few minutes, we should start to see metrics flowing through these can be visualized with default Kibana dashboards such as the one shown below, by using the metricbeat-* index in Elastic.Live OS Metrics being sent by Metricbeat to KibanaWhat’s NextWith prometheus metrics in play, you can use Kibana to define custom visualizations of those metrics, such as the example below. These individual visualizations then go into the Kibana catalog, where you can start to stack them up to build custom monitoring dashboards.Kibana Visualization of Neo4j Page Cache Hit Ratio (Prometheus Metric)Further ResourcesHow to configure Prometheus with Neo4jBuilding a Dashboard with KibanaMetricbeat documentationFilebeat documentation;May 23, 2019;[]
https://medium.com/neo4j/baryon-the-neo4j-react-ui-component-e8484dbae38f;Satoshi (euonymus)FollowOct 21, 2019·6 min readBaryon the Neo4j React UI ComponentWhat is Baryon?Baryon is a react component to compose organized UI view for your Neo4j graph that allows you to inspect simply and intuitivelyYou can see more about BaryonGithub: Baryon repository hereNeo4j community: BaryonDevpost: Baryon on Global GraphHackWhy Baryon?The reason why I created Baryon is that I wanted to show the power of graph to ordinary people. I thought Graph database like Neo4j is been used in rocket science for years among researchers and scientists, but the foundation of graph lays everywhere in life. Baryon can visualize any kinds of relationships in a media like UI view, so people can easily dig deeper to find surprising connections among things.What it doesBaryon provides you a simple UI to inspect Neo4j graph dataSubject node appears in the main areaRelationships appear in a listEach relationship is expressed in sentence powered by RDF tripleEach node or relationship can have start-date and end-dateRelationships are sorted by start-date of relationships, then start-date of nodesSecondary relationships on each connected node is listed below the nodeSecondary relationship list can be configured to be On / OffEach node-label has their label-properties(*1)Relations are categorized depending on relationship-typesEach label-property is a collection of relationship-types(*2)URL path to Baryon UI on your app can be configured(*1): label-properties are not the properties in Neo4j terms. label-properties are semantic types belong to labels, for instance Person has Parents, Children and so on.(*2): Both BROTHER_OF and SISTER_OF go into siblings label-propertyThe key of Baryon is Data ModelingIn order to organize data in a consistent structure, you are required to keep some small rules.There are two aspects in data modeling on BaryonCriteria you need to keepData Models those Baryon takes careCriteria you need to keepWhen you create nodes and relationships on Baryon, properties are the key.Actionable node propertiesActionable relationship propertiesActionable properties are the properties those give some visual effects on Baryon Component. Those are all optional except name property. So you don’t need to worry about filling all the properties at once. It’s just better to have.Actionable properties on BaryonYou can see what is actionable properties at github repositoryBaryon cares about life time of nodes and relationships.Nodes are the representative of Things in Universe. And things always have birth moment and death moment. Even though sometimes those are vague. So Baryon asks nodes to have start and end property. start represent the birth moment of the Node and end does the death moment. Some nodes happens momentary like a spark, such as Events. In this case you don’t have end property, but set is_momentary True.Like wise, Relationships have start and end properties. As well, relationship always starts at some point, and ends eventuallyInterestingly, both nodes and relationships live in similar time period have stronger relations each other. People live in the same era may know each other. Two people worked at the same company at the same period surely knows each other. So sorting and filtering by the life time does have a lot of meanings. Relationships in Baryon are always sorted by the start property. Unfortunately filtering feature is not yet implemented. It will be in the future.Data Models those Baryon takes careThe reason why Baryon can organize categories neatly is because Baryon have its own data model inside. Baryon is built with following structuresBaryon’s abstract Data StructureSubject Node (on Neo4j)Label (on Neo4j)Template (on Baryon)Label Property (on Baryon)Type List (on Baryon)Type (on Neo4j)Relationships (on Neo4j)Object Node (on Neo4j)Node and LabelThese are Neo4j Resources. As you know, node is a thing like John”, Neo Technology” or anything. Label of course is a role of nodes, such as Person, Company, MusicSong, or stuff.Template, Label Property and Type ListIn the Neo4j world, Types of Relationships are like, IS_SON_OF, IS_DAUGHTER_OF, WORKED_AT, IS_GRADUATED_FROM and STUDIED_AT. Each of them contains individual contexts. You can group relationships by IS_SON_OF type easily, but if you need to group IS_SON_OF and IS_DAUGHTER_OF into the same category, it’s not so simple.On the other hand, Each Label has its own characteristics. For instance, Person has Spouses, Parents, Children, Siblings, Jobs, Alma maters, Works. Company has Parent Organizations, Sub Organizations, Founders, Employees, Products, Services and so on. Come to think of it. These are not directly the Type of Relationships. So I call them Label Property. Templates represent these characteristics by grouping these Label Properties.TemplateTemplate is a group of Label Properties on each Label of Node. Every Label has its Template.Examples of TemplateLabel PropertyLabel Properties are properties on each Label. As an example, Person almost always has Parents, Jobs. whereas MusicGroup has Members and Discography.Person Template with Label PropertiesType ListLabel Property also is a group of Types. For example Parents Label Property may have Types like IS_SON_OF, IS_DAUGHTER_OF, IS_CHILD_OF. Because I want to keep the model simple, I always add directed relationship from New generation to Older one. So there is no IS_PARENT_OF Types. Now, all these different Types have to be under Parents or Children Label Property. You may notice that direction of the relationships matter, and that’s what Baryon takes care of.Example Data Structure in BaryonExample Data Structure in BaryonHow to UseTo use Baryon, please take a look at Github repositoryFuture worksI started Baryon when Neo4j announced Global GraphHack. Because this is in the part of a hackathon, I didn’t have time to penetrate many things. I still have a lot of ideas to improve this.Life time filteringActivate GraphQL version, so that you can publish your project without exposing your Neo4j ID/passwordLogin feature to achieve your private graphCreate a new style media on Baryon with universal Graph dataBaryon is a derived project from gluonsgluons is my original project before Baryon is started.gluons” is a new style media which introduces all the relationships in universe with graph structured data. Current gluons does not use Graph Database, but data structure is exactly same as graph. Baryon is a derived project from gluons, which only represent UI view part of it.In other words, gluons is a media having its own dataset, and Baryon is a UI view that gluons will also use in the near future.gluonsI believe gluons can replace Wikipedia.gluons in Englishgluons in JapaneseNOTE1: Because I started Japanese version first, Japanese gluons is much more substantial than English one. But you can still get some feelings from English version hopefully.NOTE2: gluons is extremely slow, because it does not use graph database behind. I’m planning to migrate to Neo4j soon.;Oct 21, 2019;[]
https://medium.com/neo4j/role-based-access-control-in-neo4j-4-1-3e65d5b3f45;Adam CowleyFollowJun 23, 2020·7 min readRole Based Access Control in Neo4j 4.1The GA of Neo4j 4.1 is officially out this week. Head over to neo4j.com/download or create a new local graph in Neo4j Desktop to give it a try now.In celebration, I thought I’d write a quick tutorial on the new Role Based Access Control features.A brief historyRole Based Access Control has been around since Neo4j version 3.1 with built in reader, publisher, architect and admin roles. A new set of DBMS procedures allowed you to create users and assign them to roles.Along with multiple databases, Version 4.0 introduced a new syntax for defining roles. Instead of calling procedures, these commands are now run against the system database. For example, you can use the CREATE USER to create a User before running GRANT ROLE to assign the user to a role.:use systemCREATE USER adam SET PASSWORD $passwordGRANT ROLE reader to adamNote: Neo4j 4.1 is now clever enough to recognise system commands, so the :use system command that was required in 4.0 is no longer necessary.A worked exampleThe official documentation provides an example based around healthcare but I thought I’d try to put a different spin on it.Instead, I’ll take a look at how these features can be applied to the Northwind dataset. If you’re not familiar, Northwind is an example dataset containing a Product Inventory including Products, Categories and Suppliers Customers and their Orders and Employee information and their territories.For more information or to import the data yourself, either check out the Relational Data to Neo4j developer guide or run the Northwind Browser Guide in Neo4j Browser::play northwindThere is the how the data model looks once the data is in Neo4j:The Northwind Data Model in Neo4jA :Customer :PURCHASED an :Order which contains one or more :Products. Those :Products are a part of a :Category and supplied by a :Supplier .Let’s assume that the data has all been loaded into a database called northwind.Sensitive InformationIf a Customer’s information, or their order history fell into the wrong hands, this would be a GDPR nightmare. For this reason, we should create some constraints up front for the different kinds of users that will be accessing the data.In a real world scenario, there may be many types of users that would need to access this data. For the sake of brevity, let’s settle on the following users and their purpose for accessing the data:The website should be able to read all data from the website, create new orders and customers, but have readonly access to Products, Categories and Suppliers.Suppliers should be able to amend Products and their Category listings.The Data Science team should be able to view Product and Order information without accessing the Customer details.Let’s start with a user that will be used by the website’s application servers. For this, I’ll open up cypher-shell, switch to the system database, and run the CREATE USER command.neo4j@neo4j> CREATE USER website SET PASSWORD letmein CHANGE NOT REQUIREDIf I run theSHOW USER command, I can see that the user has been automatically granted access to the DEFAULT graph (neo4j unless you set dbms.default_database in neo4j.conf) with this role of PUBLIC.neo4j@neo4j> SHOW USER website PRIVILEGES+-----------------------------------------------------------------------------------+| access    | action   | resource   | graph     | segment    | role     | user      |+-----------------------------------------------------------------------------------+|  GRANTED  |  access  |  database  |  DEFAULT  |  database  |  PUBLIC  |  website  |+-----------------------------------------------------------------------------------+The new PUBLIC RoleThe PUBLIC role is a new feature of 4.1, and allows you to set default the default permissions for all users by default. This role gets assigned to all users by default.Important: This role is inherited by every database by default. It can be modified but can’t be removed.So, if the information is stored in the northwind database, we can use this role to grant all users access to the database.GRANT ACCESS ON DATABASE northwind TO PUBLICAlthough this grants access to the database, it doesn’t currently give the user the privileges to read any of the data that it contains. For that, we will need to be explicit. All roles should be able to read Products and Categories, so we can grant the ability to runMATCH queries on the database.GRANT MATCH {*} ON GRAPH northwind NODES Product, Category TO PUBLICLet’s take a second to unpack that:GRANT MATCH {*} — Allow the user access to all propertiesON GRAPH northwind — To the database ‘northwind’NODES Product, Category — To nodes with the labels Product or CategoryTO PUBLIC — To the role ‘PUBLIC’Now all users will be able to run a MATCH query on Product or Category nodes but no relationships will be visible to the User. For this, we have to GRANT TRAVERSE privileges.GRANT TRAVERSE ON GRAPH northwind RELATIONSHIPS PART_OF to PUBLICNow all users will be able to read the product catalogue.Restricting WritesAnother feature introduced in 4.1 is the ability to create finer-grained control for write queries. Now it is also possible to restrict what a user can write to the database.In our case, the website user that we’ve created should be able to create new Customer accounts and Orders but shouldn’t be able to modify the product catalogue. By default, the PUBLIC user doesn’t the ability to create nodes, so if they try to create something they will see the following error:Neo.ClientError.Security.ForbiddenCreate node with labels Category is not allowed for user website with roles [PUBLIC].Instead of assigning this to the PUBLIC role, we should instead create a new role and then GRANT the role to the user.CREATE ROLE websiteUserGRANT ROLE websiteUser TO websiteNow we can be specific about what the user can do. In this case, we want to allow them to view all properties on the Customer and Order nodes, and also traverse the PURCHASED and ORDERS relationships.GRANT MATCH {*} ON GRAPH northwind NODES Customer, OrderTO websiteUserGRANT TRAVERSE ON GRAPH northwind RELATIONSHIPS PURCHASED, ORDERS TO websiteUserA quick look at the user’s privileges by running SHOW USER website PRIVILEGES should show the privileges that have been inherited from both the PUBLIC and websiteUser roles.A list of the website user’s roles inherited from PUBLIC and websiteUser roles.Now the website user should be able to see the all of the information held about the order.Now that the user can read the data, we want to make sure that they can write data as well.There are a number of properties against the node that the user should be able to write, but also a few that shouldn’t be available. Let’s say for arguments sake, that a back-office process runs which assigns an employee, then another system that sets the shippedDate once the order is dispatched.{     shipCity :  Aachen ,     orderID :  11067 ,     freight :  7.98 ,     requiredDate :  1998-05-18 00:00:00.000 ,     employeeID :  1 ,     shipPostalCode :  52066 ,     shipName :  Drachenblut Delikatessen ,     shipCountry :  Germany ,     shipAddress :  Walserweg 21 ,     shipVia :  2 ,     customerID :  DRACD ,     shippedDate :  1998-05-06 00:00:00.000 ,     orderDate :  1998-05-04 00:00:00.000 ,     shipRegion :  NULL   }We could be explicit and define the properties that the role should be able to apply by providing a comma separated list. This may cause problems if a new feature is added to the website. Instead, we can use the wildcard * character to allow the user to set all properties, then be descriptive about what we want to disallow.First, let’s give the role permission to CREATE a node:GRANT CREATE ON GRAPH northwind NODES Order TO websiteUserNext, allow the role permission to SET all properties using the wildcard:GRANT SET PROPERTY {*} ON GRAPH northwind NODES Order TO websiteUserThen, deny permission to set the employeeId and shippedDate roles:DENY SET PROPERTY {employeeID, shippedDate} ON GRAPH northwindNODES OrderTO websiteUserOnce this has been applied, any user with this role will receive an error:website@northwind> CREATE (:Order {employeeID: 1})Neo.ClientError.Security.ForbiddenSet property for property employeeID is not allowed for user website with roles [PUBLIC, websiteUser].Hopefully by now you get the idea. The combination of GRANT and DENY commands allow you to be very specific or generic when assigning roles.What else is new in 4.1Along with new writer privileges, Neo4j 4.1 now also offers commands for granting or denying privileges held by the built in roles that were introduced in previous 3.x versions.Reader — Read-only access to the databaseGRANT ACCESS    ON DATABASE * TO readerGRANT MATCH {*} ON GRAPH    * TO readerEditor — Read access plus limited ability to write to the graph using the existing labels, relationship types, and property keys.GRANT ACCESS               ON DATABASE * TO editorGRANT ALL GRAPH PRIVILEGES ON GRAPH    * TO editorPublisher — Read and write access to the graph with the ability to create new labels, relationship types, and property keys.GRANT ACCESS               ON DATABASE * TO publisherGRANT NAME MANAGEMENT      ON DATABASE * TO publisherGRANT ALL GRAPH PRIVILEGES ON GRAPH    * TO publisherArchitect —As above but with the added ability to create indexes and constraints.GRANT ACCESS                ON DATABASE * TO architectGRANT NAME       MANAGEMENT ON DATABASE * TO architectGRANT INDEX      MANAGEMENT ON DATABASE * TO architectGRANT CONSTRAINT MANAGEMENT ON DATABASE * TO architectGRANT ALL GRAPH PRIVILEGES  ON GRAPH    * TO architectAdmin — Full privileges to the database: reading, writing, creating indexes and constraints, creating databases and users, and managing the transactions of others.GRANT ALL DBMS     PRIVILEGES  ON DBMS     * TO adminGRANT ALL DATABASE PRIVILEGES  ON DATABASE * TO adminGRANT ALL GRAPH    PRIVILEGES  ON GRAPH    * TO adminGRANT TRANSACTION  MANAGEMENT  ON DATABASE * TO adminFurther ReadingThe Cypher Manual entry on Subgraph SecurityAuthorisation and Access Control from the Operations Manual;Jun 23, 2020;[]
https://medium.com/neo4j/discover-auradb-free-week-16-the-csv-data-importer-tool-29f51fa638a2;Michael HungerFollowJan 24, 2022·10 min readDiscover AuraDB Free Week 16 — The CSV Data Importer ToolThis week we’re not focusing on a particular dataset but on an upcoming tool that will make getting CSV data into Neo4j, including AuraDB Free, much easier.If you are looking for a guided experience for CSV data import, check out our brand new GraphAcademy Course.The Neo4j Data Importer is a client-side Browser tool, that allows you toLoad CSV filesCreate a graph model of labeled nodes and typed relationshipsMap CSV files to the elements of the graph modelRun the import against local and remoteImport and export the model (optionally including the CSV files) into shareable datasetsUpcoming Data Importer ToolYou can find the tool for the time being here.It is also available for non-encrypted server connections (e.g. localhost) hosted on an http URL.If you rather watch the recording of this week’s live-stream you can do so here:Neo4j AuraDB FreeNeo4j AuraDB Free is the free tier of our hosted cloud service for Neo4j.Go to neo4j.com/cloud/aura to log in and create a new empty AuraDB Free Database.Neo4j AuraDB Create Free DatabaseCopy the password, you need it later, your database should be ready to be used in 2–3 minutes.The other thing you need is the connection URL that can be found with the running database.Neo4j AuraDB Connect ScreenData ImporterThe data importer is a pure browser-based tools, your data will not leave your computer, except to be imported into your Neo4j instance.It is meant for initial data imports of medium complexity (10 node- and relationship-types) and medium size (100k to 1M elements).Complex data transformations and id-mappings are currently out of scope, as well as production like ETL pipelines or recurring import runs.Today we want to look at 3 examples:Small size movie graphMedium size movie graph with a single denormalized fileMore complex northwind datasetMovie DatasetThe movie dataset is our often-used dataset of people acting in and directing movies.Load CSV FilesYou can find the CSV files in https://data.neo4j.com/importing/movieData.zipUnzip the file and either drag and drop or Browse and select all the files in the left side of the app.Movies Load CSV FilesEach of the files shows the headers and the first row of data, which is useful for determining the datatype.In the left pane you can also fold the files and remove them individually.The individual columns get a green marker when they are mapped so you see your progress.Create ModelIn the Discover AuraDB Free live stream we’ve used the arrows.app frequently to model our data. The data importer currently embeds the same drawing UI, so you should be probably familiar.You can add nodes with the icon, or drag out relationships to existing or new nodes from the halo of a node.Labels and relationship types can be edited directly in the visualization or on the right in the mapping panel.In the mapping panel you can also reverse the direction of relationships.With the delete or backspace key on your keyboard you can delete selected nodes and relationships. You can also multi-select nodes and rels for moving or deletion.Our movies model should look like this:Movies Data ModelMap DataIn the next step you can map the data from CSV to the graph elements.The fastest way to map a node is toSelect a file to mapAdd from FileSelect all checkboxes via the All link on topOptionally deselect the ones you don’t wantChange property name as needed with the edit iconChange the datatype (string, float, int, boolean) where necessarySelect the id field (Don’t forget this part)After you’ve done these things for a node it gets a blue checkmark in the visualization.Movies Data MappingYou can also create properties manually and map them individually in the Mapping tab.For relationships you pick the the file and then select for the keys for each of the nodes the corresponding columns from the file — those show then already checked with the green box.In our case that’s first from the acted_in.csv file the person_tmdbid column mapped to the :Person(tmbdId) key property and then second the movieId column to the :Movie(movieId) key.Additionally you can also select further fields to be stored on the relationship (in our case role).Map Acted-In RelationshipAs you map the CSV files on the left side get green boxes for their columns you can see what’s left to do and fold files that are finished”.Export ModelThe blue Model button contains some useful functionality.One nice feature is the ability to export both the model/mapping JSON file that can be shared with others.Even better is the ability to export both the mapping and the CSV files as a compact zip archive that contains a ready-to-import” dataset.You can even use the ?model=url query parameter to share the data importer with a model-json and/or zip file to be used (see below with Northwind).Data importer also stores the current mapping data in the browser storage, so if you open the app again later, the mapping is still there, you just need to reload the data files.Run ImportAfter all the mapping is done and all nodes and relationships have their blue checkmarks, we can import the data.Also check that the CSV files have mostly green boxes only leaving the fields that you don’t want to import.Database Connection Details for Data ImportHitting the Run Import” button gives you the ability to provide the database connection URL (from the database information), username (usually neo4j) and password that you hopefully saved from your AuraDB Free credentials popup.Make sure to use the right version of data importer, for secure database connections like Neo4j AuraDB use the https hosted version for other database connections that don’t use SSL certificates use the http hosted version.The import progress is shown in the little circle left of the Run Import button, that turns into a Book” icon to show the results.In the results popup you see:Total runtimeRuntime per statementData volume processed per statementNodes / relationships / properties created per statementOption to show the constraint and create Cypher statement for each elementYou can also download the results as json file, for safekeeping or debugging purposesThe import processes batches of data from the CSV files streaming them through to Neo4j with the JavaScript bolt driver (via Websocket connection), so no large volume of data is kept in your browser or pushed to the server at once.Each chunk is handled individually. Currently after the first failing statement a graph element is not further imported, control of that behavior will come in the future.For idempotent and concurrently safe operation MERGE (on the id-property) is used for nodes and relationships. Currently relationships are only merged on the relationship-type for performance reasons, not on key” rel-properties.Post ProcessingSome aspects that the data importer doesn’t cover can be handled with post-processing.E.g. in our case the genres attribute of movies contains a pipe | separated list of genre names.So we can open Neo4j Browser and run the following statement to extract Genre nodes and connect the movie to its genres.// find all movies with genres propertyMATCH (m:Movie) WHERE NOT m.genres IS NULL// split string on pipe symbolWITH m, split(m.genres, |) as names// remove unneeded propertyREMOVE m.genresWITH *// turn list of names into rowsUNWIND names as genre// uniquely create Genre nodeMERGE (g:Genre {name:genre})// uniquely create relationship between movie and genreMERGE (m)-[:GENRE]->(g)// Added 17 labels, created 17 nodes, set 110 properties,// created 212 relationships, completed after 41 ms.Now we can see the Movies and their Genres in full beauty in Browser or BloomMovies and Genres in Neo4j BloomSimilarly if we forgot to transform a data-type during import we can still do it after the fact.MATCH (m:Movie)SET m.budget = toFloat(m.imdbRating)SET m.budget = toInteger(m.budget)SET m.year = toInteger(m.year)Denormalized Movie DatasetThe denormalized dataset, contains all data in a single file.Usually you get denormalized CSV files when a data export from a relational database joined all tables together in one file or a data scientist exported a single data-frame that joined all data together.The CSV file can be found at this URL. It is the same model as before, just all data exported into one file and more data volume (20MB, 9486 people, 5549 movies, 22057 relationships).As the file contains both ACTED_IN and DIRECTED relationships and our import tool doesn’t pre-process the data, we can filter out the CSV file on the command line with csvkit or xsv into two separate files.xsv search -s work ACTED_IN 2-movieData.csv > 2-movieData-acted.csvWe need to map the same file repeatedly to our nodes (Movie, Person) and relationship (ACTED_IN).Here is what the mapping looks like after we already mapped the Movie node and are about to map the Person node.Mapping of a denormalized CSV fileIf you want to have a look, here is the mapping file.Importing the data takes a bit longer (15–20 seconds) as we have much more data to send to the server, and also have to process the same file three times.NorthwindThe Northwind data model is like the movies dataset a normalized dataset.Mapped Northwind Data ModelYou can find the CSV files in our Neo4j Graph Examples Repositories in the import folder of the northwind repositoryhttps://github.com/neo4j-graph-examples/northwind/tree/main/importThere you can also find an already mapped model+csv zip file to be loaded directly into the tool, that you can import directly into your graph database.If you directly want to point data importer to that zip file you can use the ?model=url query parameter, just make sure there are no redirects.For GitHub that means to use the raw”-urls for binary data, e.g. https://raw.githubusercontent.com/neo4j-graph-examples/northwind/main/import/northwind-data-importer-mode-data.zipOtherwise the Northwind model is pretty straightforward, it took me roughly 10 minutes to map all 11 files to nodes and relationships while talking, explaining and discussing.Some details that I want to call out.Incremental ModelingThis time we didn’t model the graph upfront but went through the files identified, what belongs into a node, what is the key and what are relationships columns (1:n) pointing to other nodes.Those we didn’t store on the current node but re-used the file to create the relationships.We also had some join-table” files, like employee_territories or order_details that we used directly on the relationships.So step by step we evolved our graph model, discussed choices and checked off more and more of the CSV files.Some modeling choices that could be different:pull out address information from order and employee, but there was no good key identifiercreate a line-item node for the oder-detailsData TypesYou can convert units likeunitPrice to floatquantity to integerdiscontinued to booleanby editing the data-type for the properties in the lower right panel.Data on Rich-RelationshipsWe could have modeled the order-details.csv as a LineItem node, but this time we wanted to store these attributes on the relationship between Order and Product, so we had more attributes on that ORDERS relationship that usually.We also converted them as shown above from string to numeric datatypes.One to many relationshipsIf you want to map a CSV column to another node (e.g. categoryID) in Product, don’t store it on the product node but use that same file for the relationships between product and categorySelf RelationshipsSelf relationships are also possible, just drag out the node and return it to the starting node.In this example an employee reportsTo another employee so you would create an REPORTS_TO self-relationship.Run ImportRunning the import for Northwind takes just 8 seconds as the data volume is not high here, just the model complexity.Import Results NorthwindIn the end we had a nice graph of retail data, that we could query with Neo4j Browser or visualize with Bloom, here the Customers that had ordered Gorgonzola” in the Dairy Products” Category.Bloom Data Import NorthwindFeedbackWe would love to get feedback for the data importer, please share it here: https://neo4j-data-importer.canny.io/feature-requestsor ask in our Discord or Forums.Please remember that some of the missing” features are intentionally for the initial MVP.Learn More in GraphAcademyThe dedicated course Importing CSV Data into Neo4j” offers not just tips and tricks around manually importing CSV files but also has a detailed section with exercises on the data importer.https://graphacademy.neo4j.com/courses/importing-dataNeo4j GraphAcademyWe can also recommend the newly released Building Neo4j Applications with Node.js” course.Happy learning, importing & exploring.;Jan 24, 2022;[]
https://medium.com/neo4j/overview-of-the-neo4j-graph-data-platform-df382daaf280;William LyonFollowMay 12, 2021·16 min readOverview of the Neo4j Graph Data PlatformGraphStuff.FM Episode #3The Neo4j Graph Data PlatformIn this episode of the GraphStuff.FM podcast, Lju Lazarevic and Will Lyon break down how the different pieces of the Neo4j Graph Data Platform fit together.They discuss the individual components of the graph platform, how developers and data scientists can get started, and why the value delivered by the platform is greater than just the sum of the individual components.IntroductionIn this episode, we are discussing the Neo4j graph data platform. To provide some context, we’re going to be covering:The backstory of Neo4jNeo4j use casesHow the platform has evolved over time and the amazing tools that are available todayHistory of Neo4j & the Evolution from Graph Database to Graph PlatformNeo4j is fundamentally a graph database. Let’s start off by talking about why Neo4j was created, and how it’s evolved from those early days.Neo4j was first created as an embedded Java database this is where that 4j” in the name comes from. It has since evolved beyond that, so the 4j” Java aspect is no longer really relevant today, but is a nod to its history.Neo4j was created to address some problems that the founders were having in building a Content Management System (CMS), specifically for some of the rights and metadata around the usage of photos. They found it very difficult to represent in a relational database because of all of the different connections and relationships, and the richness of the data. So that’s why Neo4j was first created.The founders quickly realized that there were lots of other interesting use cases beyond just this embedded database in the CMS application. So Neo4j quickly evolved into a more generalized system. There was rapid uptake of Neo4j, where it was being used for generating personalized recommendations, as well as handling logistics and routing. It was also popular for dealing with the complex access patterns, inherent in things like identity and access management and fraud detection, where you’re interested in the connections between actors and a payment network.Another example is Customer 360, where an enterprise wants to understand the different components and interactions that they’ve had with a customer throughout their journey with the organization. Also use cases like monitoring network operations — if you’re responsible for a data center, being able to understand all the connections from the hardware — what rack is a server deployed on, to the different interfaces, right down to the software applications deployed, and all associated dependencies.These are some of the first core use cases where Neo4j really started to shine. Being able to understand the connections in your data, and why those connections are important to you, highlighted this idea of a graph-shaped problem’’ (previous webinar video: Are you looking at a graph shaped problem?).The real power of Neo4j is that not only can you examine those relationships, but you can do this on transactions, and apply it to real-time events as they’re happening. For example, somebody requests a new financial product. You want to be able to identify any potential fraud associated with the request. Being able to apply graph analytics on your existing data whilst incorporating new data coming in enables you to understand in real-time whether to approve or reject the request.There are many different types of users working with the database: developers, administrators, data scientists, analysts, and so forth. When the ICIJ was looking at the Panama Papers, we saw this idea of many different users being involved — the idea of the citizen scientist going off and looking at the data, the technologists building the solution, and the journalists getting involved using their domain knowledge to research people of interest.Another fun thing about working with Neo4j and exploring some data is the flexibility you have with the data model. You still need to think about how your data elements are related to each other, but it doesn’t have to be perfect. You don’t have to declare your schema intentions ahead of loading data, which allows you to get your data in as quickly as possible and then test that hypothesis straight away.So from a proof of concept perspective, the graph database model allows us to test ideas out rapidly and iterate at pace.It’s also important to understand the context of larger trends going on in the developer and data science ecosystem as well. For example, realizing that we live now in a cloud-first world. Developers have expectations around tooling and ease of use and what they can accomplish within a certain amount of time.As a developer, when I’m using a new tool, I have greater expectations for what I can accomplish in that first hour, and that first week, by using the tool. Nowadays, I expect to be able to build and deploy an application within just a couple of days of the first contact with a new technology. So we want to be able to address those emerging trends and really enable developers and data scientists to be more productive.Components of the Neo4j Graph Data Platform, Neo4j Database, & Graph NativeSo let’s look at the graph database platform. At the core is the Neo4j graph database, and surrounding it are some of the tooling, use cases, and audiences.First, let’s have a look at how graph databases, and specifically Neo4j, are different from other databases such as relational, document, and so forth. One of the core differences, and advantages, of Neo4j is the property graph data model. All of the contact points are graph-based: the thinking, the modeling, the querying, and the storying. A graph database consists of entities called nodes, and the connections that join them together are called relationships. Importantly, the relationships are first-class citizen of the data model, and this is represented in how we store and query them in the database.So speaking of storing and querying, we use a query language called Cypher to work with Neo4j. If you’re familiar with relational databases and SQL, you can think of Cypher as SQL for graphs, focused on pattern matching. It is a declarative query language that features ASCII art patterns. As Neo4j is optimized for traversing the graph, the query reflects this traveling from node to node by following relationships. You’ll often see queries describing patterns traversing many relationships, maybe 10–12 or even unspecified variable-length patterns. Neo4j does this through a concept called index-free adjacency. When moving from one node to the next, rather than doing an index lookup operation to do that traversal, the database will look up the pointer for the next hop, which computers are very good at.The implication of this is that performance of traversals are not dependent on the overall size of the data. A local graph traversal going from one node following a chain of relationships is not dependent on the overall size of the graph, be it small or very large. Now, Compare this concept to a relational database, where the equivalent would be table joins. Indexes are used to see where two tables overlap to join them. This index-backed operation performance is based on the size of these two tables. As the tables grow, the performance is going to be impacted.You’ll often hear the term graph native,” and this is fundamentally what it means. It’s about optimizing all the way through the stack from the data model to the query language, and then exploring how that data is stored on disk and how the database processes it is optimized for graph and graph workload. So that’s what graph native means when you hear that term. The Neo4j database is also very extensible. You have the ability to write user-defined procedures, functions, and plugins to define a custom logic beyond Cypher. This also forms the basis for other interesting components within the graph platform, which we will cover later.Neo4j Desktop, Using Neo4j in the Cloud, & On-PremWe’ve briefly covered the Neo4j database, how it works, and the differences between it and more traditional databases. The next question is: how do we get the database up and running? How do we interact with it?For many of you, your first touchpoint with Neo4j has probably came from a google search, and most likely you will have encountered the download Neo4j Desktop button. So what is Neo4j Desktop? It is mission control for your Neo4j projects — a developer tool that allows you to manage your Neo4j instances in a nice way. It allows you to manage remote and local database instances, as well as run graph apps. If you’re trying out Neo4j, you’re pulling together a proof of concept or just learning and having a play, then it also allows you to very easily get started without worrying too much about configuration, setting up Java, and so forth.You can also set up the database up through a console window or as a service, with tools such as Neo4j Admin and other utilities to manage the product. You also have ways of getting different formats of your database, so if you’d like to use Docker, you can go and get a Docker container of Neo4j. We have got a Helm Chart as well, if you are looking at the Kubernetes route. You can also get Neo4j in various flavors and other packages, for example Linux and Windows. There are also monitoring tools such as Halin, which is one of our Neo4j Labs applications, which we’ll cover later.Now, if that sounds like too much of a headache, we do have a cloud option available to you as well. Neo4j Aura is a scalable, resilient database as a service with three different flavors available. The professional tier is your more traditional cloud service — you sign up with a credit card, spin up a database, and away you go. If you need something that is a bit more dedicated to your enterprise, e.g. you want SLAs and other guarantees, there is an enterprise offering as well. Last but not least, Neo4j Aura Free tier is coming shortly, targeted at developers who are looking to learn, or have small projects they’d like to run in the cloud. Do sign up on the wait list!Neo4j Browser & Low-Code Graph AppsRegardless of how you have deployed a Neo4j, one of the first tools you’ll use to interact with Neo4j is called Neo4j Browser. Neo4j Browser runs in your web browser, and can be thought of as a query workbench for Neo4j. It allows you to execute Cypher queries against the database and then visualize and interprets the results. There are also various administration commands that can be executed within the Browser, but you can think of it as the starting point for writing CSV scripts for importing data and getting started.Neo4j Browser is one of the tools that we call a graph app. Graph apps are single page applications that run in Neo4j Desktop. They have an additional API injected into the application that gives them access to the database currently running in Neo4j Desktop. Anyone can build a graph app to do similar operations against the database, and from this it is interesting to see what’s possible from a low-code tooling perspective. There are options to run these graph apps independently from Neo4j Desktop in a web browser, and still be able to connect to a database.A great example of a low-code graph app is NEuler, the graph algorithms playground. NEuler gives you a no-code environment to explore, compose, and execute graph data science and graph algorithms on Neo4j. Using the Graph Data Science library, it allows you to try out the algorithms without being overwhelmed by pulling together the specific queries and necessary configuration.Another graph app is GraphQL Architect. GraphQL Architect allows you to construct, develop, run, and query a GraphQL API locally or within the context of Neo4j Desktop. It allows you to export work once you’ve built out and tested your GraphQL API.The Charts graph app allows you to do exactly as the name implies — create charts. Often the answer to our question, even when working with graph data, is a tabular result, and charts help us with the visual interpretation of that.These are some of the graph apps that have been built by the Neo4j team, mostly from the Neo4j Labs program, which will be covered in more detail shortly. There are also great community-built graph apps, which you can check out from the graph apps gallery. Let’s highlight two of them.GraphXR allows you to render 3D data visualizations, which allows you to explore your data in an almost VR-type environment. Neomaps allows you to visualize spatial data stored in Neo4j. For example, you may have spatial and geometric data in your database, and you can use Neomaps to represent them as points, line segments, or a polygon. You also have the option to build layers, where you can define what geometrics to represent on the map.Neo4j Language Drivers & Building APIsSo we’ve reached the stage where we’ve got the database downloaded, installed, and running, and now we’re able to explore our data using either the Neo4j browser or one of the graph apps. Now it’s time to think about other ways to connect to and interact with the database. There are a number of ways to achieve this.Neo4j DriversThese are language drivers that allow you to communicate with the database. The official Neo4j drivers are available for JavaScript, Java, .NET, Python, and Go. They all work on the idea of a common abstraction — all of the drivers have similar phrasing around how to connect to the database. They all follow very similar patterns across all of them. The syntax you would use for the JavaScript driver would look very similar to the Java one, and so forth. They also operate on the idea of sessions — you create a session, run all the transactions you need against the database, and then close that session off.There are also a large number of drivers that have been created by the wonderful Neo4j community. They are available in other languages such as R, PHP, Ruby, Julia, and you’ll also find community drivers for some of the officially supported languages as well, such as .NET. Most of these drivers run under the ‘Bolt Protocol’ — a system that manages how you connect to the database, and takes away the concern of figuring out which database to connect to if, for example, you’re running a cluster. For instance, the Bolt Protocol will deal with where to send your query if it’s a write query over a read query. There are also object graph mappers for Java, as well as Spring Data Neo4j, which is hugely popular and used in a lot of large companies.GraphQL APIAnother key option available to query the database is via the GraphQL API. This is an option that allows us to rapidly build an API layer between the database and a client. GraphQL is very interesting in the context of graph databases, because there’s a lot of symbiosis between it and the property graph database. One is that GraphQL makes the observation that your application data is a graph. So you can think of a GraphQL query as a traversal through this data graph, and specifying exactly the data from that traversal that should be returned. Also with GraphQL, we often build these very nested queries that are again a traversal through the data graph. Being able to have a graph database back-end like Neo4j that’s actually responsible for executing those traversals on a property graph means that those queries are going to be very efficient and optimized by the database engine.Graph Data Science, Analytics, & VisualizationWe’ve already talked about building applications, where as developers we need to build that API layer. But another key part of the graph data platform is more of the analytics use cases, where the users are more likely to be data scientists, analysts, or even maybe business users. Let’s explore the tooling in that space of the Neo4j platform.One of the most interesting pieces of the platform is the Graph Data Science Library. The Graph Data Science Library allows you to run graph algorithms in Neo4j, such as centrality, page rank, and community detection algorithms to find communities or clusters in the group. And these also form the basis for more advanced techniques that feed into machine learning in our artificial intelligence pipelines.Another really exciting element that we have sitting in this quadrant is Neo4j Bloom. Neo4j Bloom is another graph app which allows you to visualize and explore your data using near-natural language. What does this mean? If you have a good understanding of your data model and your domain, you don’t really need to know any Cypher to be able to go off and investigate your data. Bloom will investigate the data in your database, do some sampling, and build out a view based on elements such as labels and relationship types.It has lots of flexibility around how it is interpreted — you can enter an approximation of the label or property that you’re looking for, and Bloom will find it for you, along with patterns, allowing you to visually explore your data. You can further customize your Bloom environment with icons, colors, and sizing. With common phrases, you can turn these into search phrases,” which enables you to use the full power of Cypher for repeatable searches, with a friendly, parameterized command. All of these features together allow you to share the same perspective within a team or department, making it easy to discuss and explore discoveries and so forth.What makes Bloom even more powerful is that it weds really well with graph data science, and you have the option to further tailor your perspective. So you can do things such as page rank, where you’re looking for influential nodes, and then dynamically size them based on their range of influence. You can display hierarchies — all these factors make Bloom a fantastic visual aid for the data scientist.Neo4j Connectors & Neo4j LabsNeo4j is part of a larger architecture that enables it to work with other technologies. This is where the Neo4j Connectors can be really helpful and come into play here.The Neo4j Connector for Business Intelligence. This feature is designed to enable Neo4j to be used with business intelligence tooling, such as Tableau, Looker, etc. It works by generating SQL statements. So in Tableau you can go and configure a bar or line chart, and Tableau will generate SQL statements to fetch the data, which the connector then handles, rendering a chart.The Neo4j Connector for Apache Spark. Apache Spark is the framework for distributed data processing. The Neo4j connector for Apache Spark allows us to read and write data to Neo4j from Spark jobs. This is especially useful where we have very, very large data sets that we’re performing some processing or some ETL process on, and we can do that in a distributed fashion to transform and then load that data into Neo4j.The Neo4j Connector for Apache Kafka. Apache Kafka is a distributed event streaming platform. It has publishers that send events into channels, called topics. Systems looking to receive these events, called consumers, will subscribe to topics, and consume the events as they are published, performing some action as required — a common real-time based architecture pattern. The Neo4j connector for Apache Kafka allows the database to subscribe to topics and consume events, as well as publish events to a topic when changes happen to the database.Let’s look at some Apache Kafka use cases. It can be really powerful when you combine graph data science with Kafka. Take fraud detection, for example. As data is coming into the system, an algorithm can inspect the data and flag suspicious transactions as fraudulent. You can then publish an event that goes out into Kafka, alerting the analytics team that can then dive into more detail. Real-time root cause analysis is also interesting, as well as real-time recommendations based on buying behaviors, current stock, etc. So any sort of streaming real-time process can take advantage of the Neo4j database + Apache Kafka combination.Kafka has this concept of the stream table duality, which is a way of thinking about how streaming data and databases work together. Essentially any stream can be transformed into a table, and vice-versa. What the Neo4j connector for Apache Kafka does is extend this stream table duality into a trinity where graphs come into play. You can now also compose that stream as a graph, right? This helps thinking about streams as graphs as well as tables.As well as connectors, there are other ways to interact with the database. All these components being discussed are sourced from Neo4j Labs. Neo4j Labs is where we explore experimental integrations, based on the latest and upcoming trends, what’s new in technology, and what is the associated graph story. Let’s have a look at some of the Neo4j Labs projects.The APOC library (Awesome Procedures On Cypher) is the most popularly and widely used project in Labs. It has 400+ functions and procedures and does everything from data import, manipulation, text cleaning, and it even allows you to build your own custom Cypher procedures and functions.Another very popular Labs project is the Neosemantics library. This allows you to convert all RDF (Resource Descriptive Framework) into property graphs and back again. RDFs are extraordinarily popular for ontologies. If you’re looking to validate your data against a model, or serialize your data, it’s a very powerful mechanism. There will be reasons why you want to keep it in that format, as well as reasons for wanting that data as a property graph. Neosemantics allows you to transfer back and forth as required.The final Labs project we’ll cover is GRANDstack, which consists of GraphQL, React, Apollo, and the Neo4j database. It is for full stack developers who have a lot of different technologies to consider as they’re building their application. GRANDstack is a series of technologies that work well together. It’s helpful for full stack developers to understand how the pieces fit together and then also leverage some of the integrations between technologies into a powerful, full stack starting kit.GRANDstack is for building applications leveraging the Neo4j GraphQL integration. React has some great tooling for using GraphQL as a data source. It made a lot of sense that Apollo is tooling both on the server and the client for building graphical APIs, as well as working with GraphQL data on the client. GRANDstack is the best in show of combination of these technologies with an opinionated way of how to get started with them together. The easiest way to get started with GRANDstack is to use the create-grandstack-app command line tool, which will generate a skeleton of a full stack application using all of these components together.Closing RemarksThis has been a whirlwind tour of the history of Neo4j, as well as the platform itself. Hopefully it wasn’t too overwhelming, and you found this technology relevant or interesting to you.If you want to get started in a gentle, guided environment, the Neo4j Sandbox is a great way to dip your toes in the water of the larger Neo4j Graph Data Platform. Sandbox allows you to spin up a Neo4j instance, preloaded with some data and guided Cypher queries to start working with and exploring data. There are different use cases to choose from, and you can also bring in your own personalized data, such as your Twitter network. Sandboxes also includes libraries like the Graph Data Science Library and some of those graph apps that we mentioned earlier as well.If you get stuck during your learning journey, do check out our community forum. We have an amazing community of members who come and talk about their experiences, and you’ll find many of them eager to guide you as well. It’s also a great place to let us know if you’re working on something fun — we want to hear your graph story. What projects are you working on, what are you experimenting on, what fun thing have you learned? Please post it there, we’d love to hear about it!Be sure to subscribe to GraphStuff.FM in your podcast app and give us a review on Apple Podcasts!;May 12, 2021;[]
https://medium.com/neo4j/creating-the-magic-remote-control-dd2b69e10ceb;Chris HeiszFollowJan 25·7 min readCreating the Magic Remote ControlOr the boring version: How we saved the business hundreds of hours of work a year by implementing automationImage by pexels.comMy Eastern European grandfather had a very old TV in his room that was created back in the old Soviet era. On occasions, the telly went blurry, sometimes the sounds got distorted, and rarely it looked like it was tuning out of the channel, halfway through the movie.The classic solution of my grandpa, to any of these problems, was the same he gave an actual, physical slap to the side of the telly, and eventually, the quality of the service got restored.Many decades later I find myself looking at Kubernetes pods, that run certain microservices, and when one of them starts misbehaving, sometimes I give them a virtual kick by restarting them, and weirdly, the quality of the service just restores.I’d like to think that I am not just a weird repetition of history, carrying some special genes in my cells that I inherited from my grandad. It is rather a phenomenon that plagues thousands of engineers’ everyday life no wonder, some people think that SRE stands for Service Restarting Experts.Is that really the best thing we can do as engineers to mitigate certain, hard-to-catch misbehaviors? That’s the question we asked ourselves in the Neo4j Aura Team, and we found a very satisfying alternative, which ended up saving the business from a LOT of toils, worthy of writing an article about it.At Neo4j Aura, we are hosting thousands of scalable, reliable Neo4j graph databases that our users can use as they like, scale as they like, or pause as they like.For us, databases are pretty much like cars in a factory. They come in different models, shapes, and sizes, they go out on a conveyor belt to their rightful owners, and we keep a close eye on their metrics while the drivers are putting thousands of miles (nodes and relationships) into them.Image by pexels.comSometimes some of the owners are so excited about their new toy, and the lack of maintenance needed from them, that they decide to take their precious car to a drifting competition.In the old times, their enjoyment of the heavy use of resources could turn the squeaky noise of the wearing tire into a painful ringing sound, representing a Pager from Opsgenie going to my phone, on a Saturday evening.As easy as it can be to set up a database as a user, maintaining them used to be a pain as an engineer. To achieve high resilience, we run the graph databases in a redundant way on Kubernetes.If you are an SRE reading this, you may think the automation is easy, just make sure that the pod automatically restarts every time we notice an error of any kind, and I know that many businesses do exactly that.But you cannot restart your way out of all of the problems that can happen, because you can’t predict the nature of every problem that may arise. Also, just imagine what would happen if an error would restart an entire stateful database cluster.It would cause unavailability which is the worst scenario that can happen to the business and the user. And finally, a simple pod restart does not always guarantee that the service will run well when new pods come up.The thing is, we are not just a company hosting software in a resilient way. We are also the company that writes the software that we are hosting.And having all this data about potential issues and scenarios that can go wrong, is by far the most useful tool that we can utilise to make our graph database even better.So as tempting as it may have been, running a restart.sh script on a CRON schedule was no option.In order to gather the most information about any issues that can occur, related to Neo4j itself, or its configuration, we started creating an internal portal which we humbly named The SRE Portal. (Because not all heroes wear capes.)You can imagine it as the Tinder of Neo4j Aura databases. Thousands of profiles, showing all intricate details of each and every database that is living in hundreds of different locations (Kubernetes clusters). And as it is with most Tinder profiles, most of them will never be checked during their lifetimes. (Sorry, not sorry)A Tinder ahem Database Profile of an AuraDB ClusterHowever, if we notice an issue with any one of them, we can immediately check their stats, looking for discrepancies, and act on it, by raising an incident directly from the SRE portal.That’s great engineers now don’t have to go to the length of connecting to a Kubernetes cluster, checking the Custom Resources, the Statefulsets, or describing a pod’s status, all of the relevant information is presented on the page itself! 🎉However, we quickly realized that our role as SREs within the business should be not about firefighting, but giving the tools to all engineers to analyze the issues themselves and have an appropriate response to them, instead of a hasty reaction.And for that reason, we decided to put shiny, colorful HTML buttons on the portal, each representing a complex series of actions.We named these buttons collectively: The Aura OperationsDo you want to create a backup, based on a certain Neo4j Cluster member? Just click the button!Would you like to revert the database to an older state? No worries, do a quick click… you guessed it… on the button!Would you like a Kubernetes job to spawn into the cluster, triggering a Java Heap Dump for analysis? You don’t even know what it means? Don’t worry about it, JUST CLICK THAT SHINY BUTTON WE MADE FOR YOU.Jokes aside, we of course made sure that certain buttons can only be used by certain engineers within the business.The idea is to take frequently needed processes, and automating them away, abstracting and grouping them all behind these buttons.We have built a Go CLI App with the help of Cobra and Viper, and every time someone clicks on a button, our Go App gets pulled down from the Container Registry to the relevant Kubernetes cluster with the appropriate parameters, to deal with the required process in the form of a Kubernetes Job.Process of the Button ExecutionDown to the TechnicalitiesViper and Cobra are such a nice duo when one creates a CLI App. It is to Go, what Bud Spencer and Terrence Hill were to comedic television. A real pleasure to have around.One of the amazing things about Viper is how it allows you to take a set of Environment Variables and turn them into a Go struct with a one-liner.type Config struct {   Environment              string `mapstructure: environment `   SlackToken               string `mapstructure: slack_token `   SlackChannel             string `mapstructure: slack_channel `   OperationCaller          string `mapstructure: operation_caller `   StartedTimestamp         string `mapstructure: started_timestamp `} var (    config Config // config holds the Apps global config in one place) func initConfig() {   viper.Unmarshal(&config) // storing the whole config in a single struct}You can also easily validate their content, and come back with an error if anything seems amiss.At the same time, Cobra allows you to define subcommands, so you can easily call the App as:./aura-operations-utility gather --dbid abcdef12 --core 1,2,3 --full-diagnosticsThat will run the function that you related to this sub-command.gather = &cobra.Command{   Use:    gather ,   Short:  gathers diagnostics from a cores member ,   Long:     Grabs logs, and a heapDump and uploads them into a bucket. The URL to the bucket is              printed to stdout.    Example:  aura-operations-utility gather --dbid abcdef12 --core 1,2,3 --full-diagnostics ,   Run: func(cmd *cobra.Command, args []string) {      // Implementation of the analysis gathering...   },}This framework generates helpers for each subcommand automatically, so you can really focus on the business logic itself. This structure above also makes it incredibly easy to scale a CLI Application, while having a nice, separated structure that contains the implementation of every subcommand separate from each other.So at the end of the day, instead of finding the appropriate Cluster on our Terminal, then getting permissions sorted, then connecting to the Cluster, then looking up and editing their Kubernetes resources in place… all of these can be concerns of a dystopian era of the past.Image by pexels.comAnd what remains is nothing more but a Magic Remote Control that I wished my grandfather had A button click for every scenario that can happen one that allows our business not just to react, but to respond, gather information, and analyze issues. That is, so we can learn from them, without having to go through the same, complex processes over and over again.If you’re curious about what these databases look like from a user’s point of view, check out Neo4j AuraDB.;Jan 25, 2023;[]
https://medium.com/neo4j/py2neo-v4-2bedc8afef2;Nigel SmallFollowJun 7, 2018·7 min readPy2neo v4: The Next GenerationAs both a busy dad and the team lead of the Neo4j Drivers Team, the amount of spare time I’ve had to work on personal projects over the past couple of years has been somewhat reduced. But with all the interest by Python users in keeping Py2neo going, I couldn’t resist starting work on it again. So I’m delighted to finally be able to deliver a brand new major release with a number of shiny new features and improvements. Please welcome Py2neo v4!As usual you can find the library at the Python Package Index and can install it with pip (pip install py2neo). Take a look at the comprehensive documentation and if you encounter any problems, shout out in the #neo4j-python channel in the neo4j-users Slack.Py2neo now wraps the 1.6 release of the official Python driver, which takes care of all the low-level, nitty-gritty, binary-winery(?) things a database driver needs to handle. This allows Py2neo to focus on higher-level features and proper pythonic API and integrations.Grabbing a GraphAs with every previous version of Py2neo, the main way into the library is through the Graph. The constructor can accept a range of settings and the code behind this has now been completely overhauled to fix several former issues, such as a failure to recognise custom port numbers. The default protocol is also now Bolt, rather than HTTP, and the hard requirement on HTTP has been completely removed.So, to get connected, simply create a Graph object:>>> from py2neo import Graph>>> graph = Graph( bolt://myserver:7687 , auth=( neo4j ,  psswrd ))Along with a regular connection URI, the full set of settings accepted by the Graph constructor is as follows:auth - a 2-tuple of (user, password)userpasswordsecure (boolean flag)schemehostportuser_agentAPI OverviewPy2neo exposes several logical layers of API on top of the official Python driver. The lowest level Cypher API provides Cypher execution facilities very similar to those in the driver, but with a few extras such as coercion to a Table object:>>> graph.run( MATCH (a:Person)                RETURN a.name, a.born LIMIT 3 ).to_table() a.name             | a.born --------------------|-------- Laurence Fishburne |   1961  Hugo Weaving       |   1960  Lilly Wachowski    |   1967>>> graph.evaluate( MATCH (a:Person) RETURN count(a) )142The next level up, the Entity API, wraps Cypher in convenience functions that provide a full set of CRUD operations on Node and Relationship objects. This can make for clearer application code at the expense of fine-grained control. The NodeMatcher, for example, constructs and executes a Cypher MATCH statement and returns Node objects:>>> [(a[ name ], a[ born ])     for a in graph.nodes.match( Person ).limit(3)][(Laurence Fishburne, 1961), (Hugo Weaving, 1960), (Lilly Wachowski, 1967)]Other Entity API methods include Graph.create, Graph.delete and Graph.merge (as well as similar transactional variants). Note that Graph.merge has now been completely rewritten to use Cypher’s UNWIND clause internally. This addresses some previous performance issues for the method when used at scale.The topmost level of API is Py2neo’s OGM API. This allows creation of GraphObjects that wrap nodes in native classes and provide attributes to model their relationships and properties.>>> from py2neo.ogm import GraphObject, Property>>> class Person(GraphObject):...     name = Property()...     born = Property()...>>> [(a.name, a.born) for a in Person.match(graph).limit(3)][(Laurence Fishburne, 1961), (Hugo Weaving, 1960), (Lilly Wachowski, 1967)]More about MatchersThe old py2neo.selection module has been renamed to py2neo.matching and the NodeSelector is now called NodeMatcher. There’s also a new RelationshipMatcher, which is an evolution of the old Graph.match method implementation.A NodeMatcher offers a DSL that can be used to locate nodes which fulfil a specific set of criteria. Typically, a single node can be identified passing a specific label and property key-value pair. However, any number of labels and any condition supported by the Cypher WHERE clause is allowed.For a simple match by label and property use the match method:>>> graph.nodes.match( Person , name= Keanu Reeves ).first()(_224:Person {born:1964,name: Keanu Reeves })For a more comprehensive match using Cypher expressions, the where method can be used to further refine the selection. Here, the underscore character can be used to refer to the nodes being filtered:>>> list(matcher.match( Person ).where( _.name =~ K.* ))[(_57:Person {born: 1957, name: Kelly McGillis}), (_80:Person {born: 1958, name: Kevin Bacon}), (_83:Person {born: 1962, name: Kelly Preston}), (_224:Person {born: 1964, name: Keanu Reeves}), (_226:Person {born: 1966, name: Kiefer Sutherland}), (_243:Person {born: 1957, name: Kevin Pollak})]Orders and limits can also be applied:>>> list(matcher.match( Person ).where( _.name =~ K.* )    .order_by( _.name ).limit(3))[(_224:Person {born: 1964, name: Keanu Reeves}), (_57:Person {born: 1957, name: Kelly McGillis}), (_83:Person {born: 1962, name: Kelly Preston})]And if only a count of matched entities is required, the length of a match can be evaluated:>>> len(matcher.match( Person ).where( _.name =~ K.* ))6The underlying query is only evaluated when the selection undergoes iteration or when a specific evaluation method is called (such as with first). This means that a NodeMatch instance may be reused before and after data changes for different results.Relationship matching is similar:>>> keanu = graph.nodes.match( Person , name= Keanu Reeves ).first()>>> list(graph.relationships.match((keanu, None),  ACTED_IN )               .limit(3))[(Keanu Reeves)-[:ACTED_IN {roles: [Neo]}]->(_6), (Keanu Reeves)-[:ACTED_IN {roles: [Neo]}]->(_158), (Keanu Reeves)-[:ACTED_IN {roles: [Julian Mercer]}]->(_151)]And lastly, the Node and Relationship objects received can be reused, along with new instances, for further operations. Note that Relationship objects are now always dynamic subclasses of the Relationship base class and can be created via those subclasses:>>> mary_poppins = Node( Movie , title= Mary Poppins )>>> ACTED_IN = Relationship.type( ACTED_IN )>>> graph.create(ACTED_IN(keanu, mary_poppins))>>> graph.match((keanu, mary_poppins)).first()(Keanu Reeves)-[:ACTED_IN {}]->(_189)Reporting, Analytics and Data Science IntegrationsVersion 4 brings some new opportunities for reporting and data analysis as well as integration with several popular data science libraries.The new Table class provides methods for multiple styles of output, including Github-flavored markdown, HTML, CSV and TSV. It also has a _repr_html_ method attached, which allows results to be rendered elegantly in Jupyter.Oh, and keep an eye out for more Jupyter support. Exciting things are in the pipeline!The Cursor object now has methods to export to numpy, pandas and sympy objects. For example, the numpy.ndarray and the pandas.DataFrame:>>> graph.run( MATCH (a:Person) RETURN a.name, a.born LIMIT 3 )    .to_ndarray()array([[Laurence Fishburne, 1961],       [Hugo Weaving, 1960],       [Lilly Wachowski, 1967]], dtype=<U18)>>> graph.run( MATCH (a:Person) RETURN a.name, a.born LIMIT 3 )    .to_data_frame()   a.born              a.name0    1961  Laurence Fishburne1    1960        Hugo Weaving2    1967     Lilly WachowskiThe Cypher LexerFor good measure I’ve also added a Cypher lexer for Pygments. This can be used to tokenise Cypher statements or split multiple statements that are separated by semicolons.>>> from pygments.lexers import get_lexer_by_name>>> lexer = get_lexer_by_name( py2neo.cypher )>>> list(lexer.get_tokens( MATCH (a) RETURN a ))[(Token.Keyword, MATCH), (Token.Text.Whitespace,  ), (Token.Punctuation, (), (Token.Name.Variable, a), (Token.Punctuation, )), (Token.Text.Whitespace,  ), (Token.Keyword, RETURN), (Token.Text.Whitespace,  ), (Token.Name.Variable, a), (Token.Text.Whitespace, \n)]The lexer is used in the new interactive console, which I cover in a second.Managing Neo4j InstancesThe old neokit module and versions.txt have been rolled into py2neo.admin.dist and py2neo.admin.install — these modules contain information about Neo4j distributions as well as download and install facilities. They are also used by the command line py2neo get subcommand (more on that later).The py2neo.admin.dist module contains details of Neo4j server versions as well as a Distribution class with a download method.The py2neo.admin.install module then brings facilities for installing, starting, stopping and configuring Neo4j instances, as well as classes for manipulating auth files.The majority of the functionality in these modules is exposed through the py2neo command line tool.Command Line UsageFinally, and excitingly, there is a new command like tool called (somewhat unimaginatively) py2neo. This comes with multiple subcommands and, is currently only tested to work on Linux. If you try it on OSX, BSD or Windows and it works there, please add a comment.py2neo getThe get subcommand can be used to download Neo4j tarballs. When used without arguments, the latest version is downloaded alternatively, a version can be specified.py2neo authThe auth subcommand hides a set of sub-subcommands all related to auth file management. These can be used to list, update and remove users in a Neo4j auth file.$ py2neo auth update data/dbms/auth alicePassword:  *******Repeat for confirmation:  *******$ py2neo auth list data/dbms/authneo4jalicepy2neo runThe run subcommand gives a simple way to run Cypher at the command line, sending the results to stdout. Connection parameters are managed through environment variables, such as NEO4J_URI=bolt://myserver:7687 and NEO4J_PASSWORD=P4ssw0rd.py2neo consoleThe console subcommand kicks off an interactive Cypher console with full syntax highlighting. This rolls my separate (and now obsolete) console project, n4, back into Py2neo.In a similar way to the run subcommand, environment variables can be used to manage the connection details.Here’s a demonstration of the console in action…Py2neo v4 console demonstrationRecorded by technigeasciinema.orgIn conclusion…So there are the new features! I hope that Py2neo v4 is useful for you and I look forward to hearing how it’s being used.Finally, if you’d like to chip in with the project, I could do with help on improving the docs, building example projects and providing more third party integrations. Oh, and helping to make it all work better in Windows!;Jun 7, 2018;[]
https://medium.com/neo4j/neo4j-ogm-and-spring-data-neo4j-a55a866df68c;Michael SimonsFollowMay 8, 2019·7 min readNeo4j-OGM and Spring Data Neo4jHow to choose an unique identifier for your database entitiesI work for Neo4j, where I’m mainly responsible for maintaining Neo4j-OGM, our object graph mapper, and Spring Data Neo4j, our implementation of the Spring Data repository abstraction. While this current work is mostly graph related, I also worked and I am still working with relational databases, too.During the last months I noticed that some issues that I encountered in the context of object relational mapping, keeps reappearing in object graph mapping, most of the time with similar consequences.Today I’m focus on one of those things: Identifiers.I’ll use the wording database entities when I speak about things that have an identity inside the database. I’ll use application entities when I speak about identifiable objects inside your application. When using Neo4j-OGM, those are Neo4j-OGM @NodeEntity, when using JPA or Hibernate with a relational database, that will be @Entity. Usually those corresponds to each other. Both entities are not entities in the pure sense of Domain Driven Design.All database entities stored in Neo4j, Nodes as well as Relationships, need an identifier that allows them to be retrieved. Neo4j creates or reuses internally unique values for those, representing record-offsets in the datastore. The internal id of a database entity can be retrieved via calling id() with the Node or Relationship in question. This internal id is very similar to the row-id of a relational database.Most of the time your business should not be concerned with the physical ids of things. However, your application will want to identify things as well. If you’re lucky, you’ll find a business key that is unique and won’t change (Hint: People’s names are not good candidates, as are most other things that you’d think of first!).Congratulations, you found a natural key. If this is not possible, then you need surrogate key.The Neo4j internal id could be a surrogate key, as it can be easily extracted and used. However, it might turn out that it is problematic to share with other systems for long time storage or give long-time guarantees (i.e. that the same resource will be reachable under the given id): The graph database might reuse the record and with it the id, it when a database entity has been deleted.So what are your options?Use the internal database entity id as id for your application entityYou’re mapping code would look like this (except for a proper name, which is here used to differentiate the different approaches):@NodeEntitypublic class PersonWithInternalSurrogateKey {   @Id @GeneratedValue   private Long id   private String name   public Long getId() {      return id   }   public String getName() {      return name   }   public void setName(String name) {      this.name = name   }}Notice that you don’t need a setter for the id as it is immutable. You also can add a pair of equals/hashCode method here based on the id. Although I personally am not a fan of that (as the hash code will return different values before and after persisting the entity as the attribute id will be set by Neo4j-OGM), it is according to Java’s Object#equals and #hashCode contracts: Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCodemethod must consistently return the same integer, provided no information used in equals comparisons on the object is modified. Your Spring Data repository will then look like this:import org.springframework.data.repository.CrudRepositorypublic interface PersonWithInternalSurrogateKeyRepository    extends CrudRepository<PersonWithInternalSurrogateKey, Long> {}The ID used in the repository refers to the attribute annotated with @Id. The flow of using it is roughly like this:@Testpublic void flowWithInternalSurrogateKeys() {   PersonWithInternalSurrogateKey newPerson      = new PersonWithInternalSurrogateKey()   newPerson.setName( Homer )   PersonWithInternalSurrogateKey savedPerson      = surrogateKeyRepository.save(newPerson)   assertThat(savedPerson.getId()).isNotNull()   Optional<PersonWithInternalSurrogateKey> loadedPerson      = surrogateKeyRepository.findById(savedPerson.getId())   assertThat(loadedPerson).isPresent()}Advantages: It is pretty clear that the id attribute is the surrogate business key, it takes no further effort or configuration to use it. Disadvantage: It is tied to Neo4js internal database id, which is not unique to our application entity only over a database lifetime.Use externally provided surrogate keysThe @GeneratedValue annotation can take a class implementing org.neo4j.ogm.id.IdStrategy as parameter. Neo4j-OGM provides InternalIdStrategy (the default) and UuidStrategy out of the box. The later generates new UUIDs for each entity and returns them as java.util.UUID. An application entity using that would look like this (Getters and setters remain the same, apart from the result type):public class PersonWithExternalSurrogateKey {   @Id @GeneratedValue(strategy = UuidStrategy.class)   private UUID id   private String name}The repository doesn’t change much either:import java.util.UUIDimport org.springframework.data.repository.CrudRepositorypublic interface PersonWithExternalSurrogateKeyRepository   extends CrudRepository<PersonWithExternalSurrogateKey, UUID> {}Nor does the flow:@Testpublic void flowWithExternalSurrogateKeys() {   PersonWithExternalSurrogateKey newPerson     = new PersonWithExternalSurrogateKey()   newPerson.setName( Homer )   PersonWithExternalSurrogateKey savedPerson    = externalSurrogateKeyRepository.save(newPerson)   assertThat(savedPerson.getId()).isNotNull()   Optional<PersonWithExternalSurrogateKey> loadedPerson    = externalSurrogateKeyRepository.findById(savedPerson.getId())   assertThat(loadedPerson).isPresent()} We have to discuss two separate things regarding advantages and disadvantages. The assignment itself and the UUID-Strategy. A universally unique identifier is meant to be unique for practical purposes. To quote Wikipedia: Thus, anyone can create a UUID and use it to identify something with near certainty that the identifier does not duplicate one that has already been, or will be, created to identify something else.”Our strategy uses Java internal UUID mechanism, employing a cryptographically strong pseudo random number generator. In most cases that should work fine, but your milage might vary.That leaves the assignment itself:Advantage: The application is in full control and can generate a unique key that is just unique enough for the purpose of the application. The generated value will be stable and there won’t be a need to change it later on.Disadvantage: The generated strategy is applied on the application side of things. In those days most applications will be deployed in more than one instance to scale nicely. If your strategy is prone to generate duplicates than inserts will fail as uniques of the primary key will be violated. So while you don’t have to think about a unique business key in this scenario, you have to think more what to generate. A solution could be a strategy that uses an hilo-algorithm similar that Hibernate uses or solutions like Snowflake applied by twitter or Sonyflake.Using business or natural keys as primary keysIn my book, this is the optimal case, but very rare and hard to decide. Things like names tend to change, even social numbers. German tax payers get assigned a unique id right after birth, given they have a permanent address. This might be a candidate:@NodeEntitypublic class TaxPayer {   @Id   private String taxId   private String name   public TaxPayer(String taxId, String name) {      this.taxId = taxId      this.name = name   }   public String getTaxId() {      return taxId   }   public String getName() {      return name   }   public void setName(String name) {      this.name = name   }   @Override   public boolean equals(Object o) {      if (this == o)         return true      if (!(o instanceof TaxPayer))         return false      TaxPayer taxPayer = (TaxPayer) o      return Objects.equals(taxId, taxPayer.taxId)   }   @Override   public int hashCode() {      return Objects.hash(taxId)   }}You’ll notice that the whole entity makes much more sense from an application point of view. The tax id doesn’t change, so I can provide a sane equals/hashCode pair. Due to restrictions in Neo4j-OGM, the field must not be final, though.While the repository change only in regard with the type of identifiable used, the flow can now different:@Testpublic void flowWithBusinessKeys() {   TaxPayer taxPayer = new TaxPayer( 4711 ,  Michael )   TaxPayer savedTaxPayer = taxPayerRepository.save(taxPayer)   assertThat(savedTaxPayer).isEqualTo(taxPayer)   Optional<TaxPayer> loadedPayer      = taxPayerRepository.findById( 4711 )   assertThat(loadedPayer).isPresent()}With the assigned business key it is not necessary to use the returned object instead of the one initially created. I would stick to the flow using the returned object nevertheless, as you might use our auditing support, that allows attributes like updatedAt or updatedByto be filled with the current timestamp or user.Advantages: Using a business or natural key as primary key is natural. The entity in question is clearly identified and it feels most of the time just right in the further modelling of your domain.Disadvantages: Business keys as primary keys will be hard to update once you realise that the key your found is not as stable as you thought. Often it turns out that it can change, even when promised otherwise. Apart from that, finding identifier that are truly unique for a thing is hard.Don’tsWhile some other combinations are possible, I do not recommend them. Look at this bad person together with their repository:public class BadPerson {   @Id @GeneratedValue   private Long internalId   private Long id   public Long getInternalId() {      return internalId   }   public Long getId() {      return id   }   public void setId(Long id) {      this.id = id   }}public interface BadPersonRepository extends CrudRepository<BadPerson, Long> {}Which id would the findById method take? Well, I had to try it out for myself:@Testpublic void flowWithBadPersons() {   BadPerson person1 =  new BadPerson()   person1.setId(42L)   BadPerson savedPerson = badPersonRepository.save(person1)   assertThat(     badPersonRepository.findById(savedPerson.getInternalId())   ).isPresent()   assertThat(badPersonRepository.findById(42L)).isEmpty()}So: The internal id (internalId) it is , because the method findById comes from the repository abstraction. You would need to overwrite with a custom query to fulfil your implicit expectation. So please just avoid such setups, they are confusing and error prone.ConclusionNeo4j-OGM can deal with internal and external surrogate keys. From an application perspective my preferred solution would be finding natural business keys. As this is sometimes really hard, there are easy ways to generate surrogate keys. Be aware that those need to be unique as well. That might be even difficult for some UUID implementations.If you don’t represent nodes as external resources in your application and therefore you’re not relying on longterm guarantees about the ids, internal keys should work for you fine.Note: All examples used are available here:michael-simons/neo4j-sdn-ogm-tipsA curated list of Neo4j SDN and OGM tips developed while answering questions on SO or for customers. …github.comPhoto by Aaron Burden on Unsplash.;May 8, 2019;[]
https://medium.com/neo4j/language-buffet-using-neo4j-with-graalvm-abf824b504fd;Jennifer ReifFollowFeb 4, 2021·10 min readLanguage Buffet: Using Neo4j with GraalVMIn this first installment we look into how to connect to Neo4j from a variety of languages, all by using the Java driver via GraalVMs polyglot support.Part 2 explains to use GraalVM’s polyglot capabilities to run different languages dynamically from a user defined procedure in Neo4j.Part of the goal for Neo4j Labs is to provide a broader spectrum of useful integrations with Neo4j and other technologies. Technologies that have the opportunity to complement or expand upon the abilities of Neo4j are explored, and potential integrations are built and passed to you (the community) for feedback and use. GraalVM is one of these technologies that can broaden the accessibility of Neo4j.GraalVM is a relatively new technology that aims to optimize performance of application startup, reduce the footprint of applications, and provide a polyglot environment for accessing abilities in certain languages from other languages. At its core, it is a virtual machine (VM) with a compiler and polyglot API, though it also includes a native image compiler and built-in toolchains for various languages.The possibilities of using GraalVM and Neo4j actually vary quite a bit, but some of the larger strategies for integrations with Neo4j are listed below.Polyglot clients — access Neo4j from languages using an official driver (like the Java driver). This allows you to use libraries in the source language for connecting to Neo4j from target languages like Python, Ruby, R, Javascript, LLVM, as well as other GraalVM language implementations.Library-sharing — access non-Java libraries to use within programs in other languages. For instance, you could pull in Python’s ratelimit library or Javascript’s colors into a Java program that interacts with Neo4j.Polyglot procedures — extend Neo4j and the Cypher query language by writing procedures and functions in any language and packaging them as a Neo4j database plugin. Typically, Neo4j only allows you to write extensions in JVM languages (Java, Kotlin, Scala, Groovy, etc), but this changes that. It also means you can execute language-specific code within a Cypher procedure (i.e. run Python inside a Cypher statement).Polyglot Cypher — use Cypher as the query language in various programs (by implementing Cypher in GraalVM’s Truffle language framework). This would allow you to embed Cypher code in your Python or Javascript program for executing against Neo4j.The last option, especially, is beyond the scope of our efforts, as it would involve quite a large effort to implement the entire Cypher language into Truffle for GraalVM. However, for the integration at this point in time, we are focusing on providing the first and the third options — using an official driver in one language to access Neo4j from other languages and creating a Neo4j extension to call language-specific code within Cypher.With those two goals in mind, let’s take a closer look at the first one in this post. The third option will be explained in detail in another post.Access Neo4j from languages using an official driverWe’ll start with the ability to connect to a Neo4j database from various languages using an official driver. Our example will use the Java driver, but you could use other drivers for Neo4j, as well. We will import the Java driver to programs in Javascript, Python, Ruby, and R, allowing us to connect to the database from those languages.Note: Python and Javascript have their own official drivers, so you can use the driver for the related language. For example purposes, we chose a consistent driver (in this case, the Java driver) to connect from different languages.SetupFirst, if you don’t already have some version of it, we will need Neo4j. Since I already have Neo4j Desktop installed, I will use that, but Neo4j Server or Sandbox will work, as well. In Neo4j Desktop, I create a graph, but I’m going to wait to start it until I get the rest of my environment set up.Next, I need GraalVM. This is another JDK (Java Development Kit) install, which is a bundle of tools for developing Java applications. For those already familiar with this, feel free to use whatever way you are most comfortable with handling java versions and JDKs. If you’re new to JDKs, an article I found explains the components of the Java environment. For managing all the options on my machine, I really like using SDKMAN!. It automatically syncs classpaths and seamlessly allows me to change versions and providers with a command or two. The commands to install the GraalVM JDK with SDKMAN! are listed below.#List available Java vendors and versions in SDKMAN!% sdk list java#Install one for GraalVM (my current version)% sdk install java 20.3.0.r11-grl#Switch Java versions% sdk use java 20.3.0.r11-grl#(optional) Set it as the default JDK for your system% sdk default java 20.3.0.r11-grl#Verify Java version on your system (and results for my environment)% java -versionopenjdk version 11.0.9” 2020–10–20OpenJDK Runtime Environment GraalVM CE 20.3.0 (build 11.0.9+10-jvmci-20.3-b06)OpenJDK 64-Bit Server VM GraalVM CE 20.3.0 (build 11.0.9+10-jvmci-20.3-b06, mixed mode, sharing)Note: when you install a version of Java, it may prompt you to set it as default in the install. However, if it doesn’t or you choose to set it as default later, I included the command to do that.Ok, those are the base requirements to install — GraalVM and Neo4j. There are a couple of other setup needs to run various languages with that. Though you can use standard language environments, I’ve opted for the built-in GraalVM languages, as I assume those have less setup overhead. To install each of the GraalVM-supported languages, we can use the GraalVM Updater (gu) tool. Commands for using gu to install each language are shown below.#See what’s there alreadygu list#Pythongu install python#Javascript (included)#Rgu install r#Rubygu install rubyNote: gu is included in the base install of GraalVM. And, if you haven’t installed any other languages before you run the gu list command shown first in the code block above, you may notice that a couple of things are already there. That’s because these are built into the GraalVM general install.For the R install, there are a couple other dependencies listed in the documentation that are needed. My Mac already had these installed on my system, but depending on your operating system and version, you might want to verify them.With Ruby, there are a couple of extra dependencies that need to be installed, as well. Most of these were already installed on my Mac, but you can verify these for your operating system and version. After those are complete, the first command in the code block below runs a script to connect openssl and libssl.I also had some issues with the recommendation to use a Ruby manager. It moved the path around where I couldn’t execute Ruby. I ended up uninstalling my Ruby manager and remapping TruffleRuby. In the end, the two commands below should help you see if your environment looks similar to mine. Note that SDKMAN! is in my path for TruffleRuby.#After installing deps, make the Ruby openssl C extension work with your system libssl)<path to your GraalVM JDK>/languages/ruby/lib/truffle/post_install_hook.sh% truffleruby -vtruffleruby 20.3.0, like ruby 2.6.6, GraalVM CE Native [x86_64-darwin]% which truffleruby/Users/jenniferreif/.sdkman/candidates/java/current/bin/trufflerubyYou can check that all the desired languages are installed by running the gu list command again to see all the languages you now have.Finally, we can go back to our Neo4j Desktop and start the database. Next, we will get our project ready to run!Connecting to Neo4j from languagesWe will need the language driver of our choice for connecting to Neo4j — in this example, with the Neo4j Java driver. I’ve chosen a Maven project, as you can see on the Github project with the Java driver dependency in the pom.xml file. Maven will also pull down the reactive streams dependency with that. For a Gradle version, see Michael Simons’s project.However, if you are not using something like Maven or Gradle for dependencies, you can download each required jar directly from Maven (Java driver v4.0.3 jar, Reactive streams v1.0.3 jar).Let’s look at the pom.xml and review any other items there. Besides for the GraalVM SDK and Java driver dependencies, there is a build configuration that we’ll walk through below.<build>  <plugins>    <plugin>      <artifactId>maven-dependency-plugin</artifactId>      <executions>        <execution>          <phase>prepare-package</phase>          <goals>            <goal>copy-dependencies</goal>          </goals>          <configuration>   <outputDirectory>${project.build.directory}/lib</outputDirectory>          </configuration>        </execution>      </executions>    </plugin>  </plugins></build>This includes the Maven dependency plugin that copies dependencies into an output directory. For more information, I found these articles helpful for understanding the plugin goals, phases, and directory settings. Notice that I’ve tweaked the output directory a bit to drop my three main dependencies (GraalVM SDK, Java driver, and reactive streams jar) into a /lib folder.You may need to build and/or run a mvn clean package to pull down dependencies before running. This will ensure that the JARs get dropped into the lib folder.Language programsNow we get to the true core of connecting to Neo4j with different languages! In the src/main/java folder of the project, we have 4 programs in 4 different languages. Each of them uses the Neo4j Java driver to connect to our running Neo4j database, execute a query, and return the results.The only differences between the programs is the language syntax, so you can choose whichever one you are most comfortable with to review. I will walk through the Python one in this post.import java#Add Java libraries to GraalVM from Pythonjava.add_to_classpath(../../../target/lib/reactive-streams-1.0.3.jar”)java.add_to_classpath(../../../target/lib/neo4j-java-driver-4.0.3.jar”)The first few lines (shown above) import Java so that we can access Java libraries and then add the Java driver and reactive streams dependency jars to our classpath. As an alternative, we could instead specify the classpath jars on the command line when we execute the program, but I prefer concise and clean commands, so I would rather place the jars inside my program code. The path to each of the jars is the output directory I specified in the pom.xml for the Maven build configurations.You may notice that, if you’re in any of the other programs, they don’t require a general import of Java. Also, for Ruby, you may notice that there aren’t statements to add the dependency jars to the classpath. This is because TruffleRuby doesn’t currently support this syntax (hopefully yet), so we must add the jars to the classpath on the command line when we execute. We will see this in a bit.Next, we bring in the required Java classes for connecting to Neo4j and set up our connection details in a driver object.# This brings in the required classesgraphDatabase = java.type(‘org.neo4j.driver.GraphDatabase’)authTokens = java.type(‘org.neo4j.driver.AuthTokens’)config = java.type(‘org.neo4j.driver.Config’)sessionConfig = java.type(‘org.neo4j.driver.SessionConfig’)# This is a call to the static factory method named `driver`driver = graphDatabase.driver(    ‘bolt://localhost:7687’,    authTokens.basic(‘neo4j’, ‘Testing123’),    config.builder()      .withMaxConnectionPoolSize(1) # Don’t need a bigger pool size for a script    # .withEncryption() # Uncomment this if you want to connect against https://neo4j.com/aura/      .build())In the first few lines above, we bring in our database, authentication, and configuration classes for connecting to the database with Java. With each of the Neo4j language drivers, there is a consistent pattern designed for running operations on the database — driver, session, transaction, and query/results. The driver manual discusses these in a bit more detail, but no matter which language program you are looking at, you will notice the same structure across all of them.The next few lines set up a communication channel with the database by stating the connection string, authentication, and configuration required. Note that you will need to modify your password to be the one you used when you created your Neo4j database. If you are using a Neo4j Sandbox or another remote database instance (remote server installation, Aura, or another cloud deployment), you will also need to modify the connection string (right above the auth).With our database connection done, we can dive into the work of running operations against the database.# Python dicts are not (yet?) automatically converted to Java maps, so we need to use Neo4j’s Values for building parametersvalues = java.type(‘org.neo4j.driver.Values’)def findConnections(driver):  query = ””    MATCH (:Person {name:$name})-[:ACTED_IN]->(m)<-[:ACTED_IN]-(coActor)    RETURN DISTINCT coActor  ””session = driver.session(sessionConfig.forDatabase(neo4j”))records = session.run(query, values.parameters(name”, Tom Hanks”)).list()coActors = [r.get(‘coActor’).get(‘name’).asString() for r in records]session.close()return coActorsThe first line in the code block above converts the Python dict type to the Neo4j value types — basically translating between data types in the Python language and Neo4j data types. This is Python-specific, so if you look at any of the other language programs, that translation line is not there. The next block of code is a Python method that executes a query and returns the results. It starts by defining our query, looking for actors who acted in the same movie as a specific person (coactors). We want to let users pick who they want to search for, so we put a query parameter of $name in the property of the first Person in the query. Then, we create a session to execute transactions and run the query, passing in a parameter for name with a value of ’Tom Hanks’. For each record in the result list, the next line loops and gets each coactor’s name and ensures it is a string. Since this is our only operation, the session gets closed, and our formatted results (a list of name strings) get returned.Our program contains just a few more lines that wrap up our results and show them to us.results = findConnections(driver)for name in results:  print(name)driver.close()The first line above calls the findConnections method we defined in the last segment and stores the output of that in a variable called results. Now we want to check that we got the results the way we wanted and to see the list of coactors for Tom Hanks. The next line loops through our results and prints each name to the console. Finally, we close our connection to the database entirely, since our work is complete.Wrapping up!No matter which language example you chose to follow in the Github project, the walkthrough of the syntax should look remarkably similar, with only language-specific syntax differing. This is an introductory example of a query and results, but hopefully, this gives enough of a foundation for you and others to expand upon to fill your needs! Feel free to try out other drivers and various language combinations, as well.With this project, we are certainly looking for feedback to understand what users need and are looking for in this space. We’d be happy to hear from you either via Github (liking the project or creating issues/feature requests) or via the Neo4j Community Site (getting help or letting us know what you like/dislike). Happy coding!ResourcesGithub projectGraalVM documentationLearn Neo4j: developer guidesNeo4j Java driver manualAsk questions: Neo4j Community SiteMichael Simons GraalVM/Neo4j blog post;Feb 4, 2021;[]
https://medium.com/neo4j/neo4j-python-4-0-driver-the-latest-driver-for-the-next-gen-database-a5be6ecd481f;Ljubica LazarevicFollowJun 24, 2020·3 min readNeo4j Python 4.0 driver— the latest driver for the next gen databaseIt’s here! The official Neo4j 4.0 Python driver is now available.Sleek Street by Lucas Ludwig on UnsplashThe latest Python driver is now available, joining the 4.0 drivers for Java, JavaScript and .NET.As was covered previously, the naming convention has now changed to better align with database versions (so 4.0), rather than the previous convention which would have been 2.0 — the next version up from 1.7.New featuresThe biggest driving feature for the latest version of the drivers is support for Neo4j 4.x.You can use the new drivers with a 3.5 Neo4j database. Do be aware you won’t be able to use the new features such as multi-database, you’ll need Neo4j 4.0 for that!Breaking changesAs with all major releases, there can be breaking changes, and with the latest version of the python driver, the following outline some of the changes that have been made:Python 2.7 support has been dropped. The previous driver version (1.7) supports older versions of python, and the Neo4j 4.0 database will work in fallback mode with that driverDefault configuration for encrypted is false. If you enable encryption, the default trust mode is determined by the operating system (e.g. self-signed certificates will fail on certificate validation by default).The namespace has changed. import neo4j.v1 has now changed to import neo4jYou no longer need to use the protocol bolt:// or bolt+routing:// in connection strings when working with clustered deployments. This is replaced by neo4j://. You can still use bolt:// for a single-instance, or with a single member in a cluster.There have been a series of API changes: Result.summary() has been replaced with Result.consume().These APIs have been removed:Result.data() (use Record.data()), Transaction.sync() (use Result.consume()), Transaction.success(), Transaction.close(), Session.sync()(use Result.consume()), Session.detach()(use Result.consume()), Session.next_bookmarks(), Session.has_transaction(), and Session.close().A Bookmark type has been introduced: neo4j.Bookmark.The bolt+routing:// protocol-scheme has been renamed to neo4j://.There have been some class renaming changes:BoltStatementResult to Result, StatementResultSummary is now ResultSummary, Statement is now Query.Similarly, the argument renaming changes: statement/cypher are now query, StatementResultSummary.statement is now ResultSummary.query, StatementResultSummary.statement_type is now ResultSummary.query_type StatementResultSummary.protocol_version is now ResultSummary.server.protocol_version.Dependencies toneobolt and neotime have been removed, and pytz is now a dependency.A quick example with multi-database and neo4j://from neo4j import GraphDatabaseuri =  neo4j://localhost:7687 driver = GraphDatabase.driver(uri, auth=( neo4j ,  password ))def get_friends_of(tx, name):    query =    MATCH (a:Person)-[:KNOWS]->(f)               WHERE a.name = $name               RETURN f.name AS friend       result = tx.run(query, name=name):    return [r[ friend ] for r in result]with driver.session(database= people ) as session:    friends = session.read_transaction(get_friends_of,  Alice )    for friend in friends:        print(friend)driver.close()When installing the Neo4j python driver with pip3 install neo4j-driver, you should get the 4.0 or newer version. Remember that it only supports Python 3.x!You can save this file as test.py and then run python3 test.py or use the python3 interpreter to execute it interactively.Racing to the resourcesYou can get the latest driver documentation from here, including more example code snippets.;Jun 24, 2020;[]
https://medium.com/neo4j/exploring-the-u-s-66a46bdf1c0;Michael McKenzieFollowSep 6, 2020·9 min readAditya Vyas on UnsplashExploring the U.S. National Bridge Inventory with Neo4j — Part 2: Importing and Storing Raw Data in Neo4jMoving forward, this series assumes a basic understanding of graph databases and Neo4j. In a nutshell, Neo4j is a labeled property graph database that consists of Nodes, Labels, Directional Relationships, and Properties. You can learn more about graph databases and Neo4j here.Read the introduction (Part 1).NBI FilesThe National Bridge Inventory (NBI) is a database of all bridges in the United States, including the District of Columbia and Puerto Rico. There are two versions of files available: delimited and no delimiter. Our exploration will start with the delimited comma-separated values (CSV) file as it will make the importing much easier. Additionally, rather than download and store every file locally, I created a Google Sheet to manage all of the file URLs.The decision of importing and storing the raw data in the graph itself has several pros and cons:Pros:Simplicity: Storing and managing the file URLs in a single document means managing a single file, rather than all the files for each year. Additionally, using the LOAD CSV Neo4j functionality allows us to import and create the :File nodes from that sheet. If a file is updated, then we need only to re-import it directly from the URL.Speed: Importing and storing the raw data in the graph means only having to worry about importing CSV data once. Granted, the initial importation can be a bit slow depending on the throughput of your connection and the power of the machine you have running the import (For reference, I am running Neo4j Desktop on a 2019 MacBook Pro with 16GB RAM and 500 GB storage. I have configured 4GB heap and 4GB page cache for this version 4.1.1 database instance). It is faster to work and manipulate the data within the graph rather than relying on importing data each time new raw data needs to be read and added to the graph structure. Additionally, if at any point a large scale restructuring of the schema is needed, all of the data are readily available.Traceability: As we will see later in this post, we can connect the files and rows to more easily track how data have changed with each annual report.Cons:Storage: By far the biggest and only real drawback I have encountered thus far, is storing the raw data in the graph as it takes up A LOT of space. As of this post, I have imported the raw data for all delimited files for every state from 1992 to 2019.So now we decide on a schema and import!Importing Raw DataDeciding on the graph structure for the raw data is pretty straight forward. Following the Neo4j principles we simply take the statement Each file contains rows of data, where each row represents a single bridge.” Converting that to cypher we get (:File)-[:CONTAINS]->(:Row). We set additional properties on the :File and :Row nodes that make them unique.We provide additional context for each node by assigning properties. For the :File nodes we’ll assign the properties name (the name of the file), url (the URL where the file is stored on the NBI website), folder (this property, in addition to the label, designates this file node as containing delimited data), year (the year of the data reported), and createdOn (to signify when the data row was added to the graph). Similarly, since we are using CSV files that contain headers, we utilize the mapping functionality to assign the properties from the files.In addition to the properties mapped from the CSV file, we add two additional properties: file (which we set to be equal to the name property on the :File node, and will be how we will connect the :Row to its :File), and createdOn (to signify when the data was added to the graph).Given the large number of rows to import we break the import, node creation, and relationship connections into three steps. This helps us address eager queries:Step1: File Node Importing// Create nodes for each file URL (Delimited Files)// Data loaded from https://www.fhwa.dot.gov/bridge/nbi/ascii.cfm via define URLs stored in Google SheetLOAD CSV WITH HEADERS FROM  https://docs.google.com/spreadsheets/d/1sFcY7LFBCGXSFG336UPoOf72BBv3bmv_AVaYLxwiV4A/export?format=csv&id=1sFcY7LFBCGXSFG336UPoOf72BBv3bmv_AVaYLxwiV4A&gid=1318941318  AS rowWITH rowWHERE NOT row.Year IS NULLWITH row, row.URL AS fileUrlMERGE (file:File:DelimitedFile {url: fileUrl})ON CREATE SET file.url = fileUrl,              file.folder = row.Folder,              file.name = row.File,              file.year = toInteger(row.Year),              file.createdOn = datetime(),               file:ImportFileRowsStep 2: Row Node ImportingFor this step, we are using APOC for apoc.periodic.iterate:// Iterate through files and import rows (Delimited)MATCH (file:ImportFileRows)REMOVE file:ImportFileRowsWITH collect([file.url,file.name]) AS filesUNWIND files AS fileCALL apoc.periodic.iterate(CALL apoc.load.csv($url,{header:true, quoteChar: \u0000 }) YIELD map AS rowRETURN row,CREATE (fileRow:Row:DelimitedRow)SET fileRow += row,    fileRow.file = $name,    fileRow.createdOn = datetime(),    fileRow:ConnectRowToFile,{batchSize:10000, parallel:false, params:{url: file[0], name: file[1]}}) YIELD batches, total, timeTaken, committedOperations, failedOperations, failedBatches, retries, errorMessages, batch, operations, wasTerminated, failedParamsRETURN batches, total, timeTaken, committedOperations, failedOperations, failedBatches, retries, errorMessages, batch, operations, wasTerminated, failedParamsStep 3: Connecting File Node to Row Node// Connect rows to files (Delimited)CALL apoc.periodic.iterate(MATCH (row:ConnectRowToFile)RETURN row,REMOVE row:ConnectRowToFileWITH rowMATCH (file:DelimitedFile)WHERE file.name = row.fileMERGE (file)-[:CONTAINS]->(row),{batchSize:10000,parallel:false}) YIELD batches, totalRETURN batches, totalNotes:There are some additional ETL tools available that make importing all of this data a bit quicker when starting with a fresh database. I chose to focus on writing the Cypher intended to function as a way of processing new files and data as they are released. That way the Cypher can easily be reused as needed.The base nodes that will be referenced are :File and :Row. However, in the three code snippets each :File and :Row nodes are also assigned additional labels: :DelimitedFile and :DelimitedRow, respectively. These come into play in a later post. For now we’ll use :File in lieu of :DelimitedFile, and :Row in lieu of :DelimitedRow.Two other temporary” labels can also be spotted in the code snippets above: :ImportFileRows and :ConnectRowToFile. These labels are used to help process the data in steps (briefly discussed in the next section).Breaking the importation into multiple steps stems from some lessons I have learned in my career using Neo4j with large datasets that are added to on a regular basis:Breaking Cypher into smaller chunks allows for more focused tuning where needed. Big, long Cypher statements are quite beautiful to look at, but can be challenging to improve over time. By dividing the processing of data into smaller Cypher statements, it is possible to more easily manage importation and manipulation of data.It is possible to create a pattern of importing and manipulation that is easy to replicate. In our exploration of the bridge data, new data are added annually. Similarly, I have worked with new data that must be added on a more regular basis. In situations where data are constantly being added to the graph, using a series of steps to process data allows for more fine tuned control of data processing.Using temporary” labels to process the data makes it easier to track what data are being processed. This can be valuable in tracking metric performance of queries as you build.Now that we have run these three queries we have a graph structure that looks like:The graph currently contains the raw data from all delimited files from 1992 to 2019. We have 1,456 :File nodes, and 18,579,580 :Row nodes and :CONTAINS relationships each. That is a total of 18,581,36 nodes and 18,579,580 relationships equating to approximately 122 GB of data!!!The next two steps we’ll take in adding to our graph model come down to preference. Due to the flexibility of Neo4j and usability of Cypher, we can query and return :File and :Row nodes according to their properties, sorting and filtering as needed. However, for this exploration, explicitly adding in relationships between each related file and each associated row provides a good visual of how bridge conditions progress over time. Additionally, given the number of :Row nodes connected to each :File node, it becomes easier to find all connected :Row nodes. The only drawback to consider is that these relationships take up more storage. For me, that is a price I am willing to pay.ONWARD!Connecting Successive FilesTo model the files in successive order we want to establish what the NEXT FILE will be from any other file. We need to utilize two of the five properties on the :File node, year and name. It is important to only connect files that represent data from the same state. For that we’ll use the first two letters from the name property, since the name properties all follow the same naming convention. For example AZ18.txt” and AZ19.txt” are both files for Arizona, whose state abbreviation is AZ. Therefore, we can find all associated files by the first two letters for the property name. Therefore using name and year we connect :File nodes using the following cypher:// Create (:File)-[:NEXT_FILE]->(:File) for each state in ascending orderMATCH (file:File)WITH DISTINCT left(file.name,2) AS stateAbbrWITH stateAbbrORDER BY stateAbbrMATCH (df:DelimitedFile)WHERE left(df.name,2) = stateAbbrWITH stateAbbr, dfORDER BY df.year ASCWITH stateAbbr, collect(df) AS fileArrayUNWIND range(0,size(fileArray)-2,1) AS indexWITH fileArray[index] AS start, fileArray[index+1] AS endMERGE (start)-[:NEXT_FILE]->(end)Now we have a chain connecting :File nodes for each state in ascending order by year:Encoding DocumentBefore we connect rows, let’s pause and take a quick look at the Recording and Coding Guide for the Structure Inventory and Appraisal of the Nations Bridges. This document is the key to decoding and understanding the data stored on each :Row node. The document provides an explanation for the values stored on each property. It also provides the decoding information to convert the data to more usable information. For example, condition ratings are reduced down to a single alphanumeric character in the files. The corresponding description of this single alphanumeric character provides additional context that helps guide our analysis. We’ll dive into this more in successive posts.Connect Successive RowsIn order to connect rows we’ll just focus on one of the properties stored on the :Row node, STRUCTURE_NUMBER_008, as it is unique for every bridge within a particular state. Since we have already connected the :File nodes in order, we’ll use those relationships to connect the related rows in the same order as well. Because we are learning as we go, in hindsight, this could be a good place to consider a temporary” label to help process the data. For the time being, we’ll use what we have and run the cypher to connect the rows:// Create (:Row)-[:NEXT_ROW]->(:Row) by state for each record in ascending order by file yearMATCH (file:File)WITH DISTINCT left(file.name,2) AS stateAbbrWITH stateAbbrORDER BY stateAbbrMATCH (df:DelimitedFile)WHERE left(df.name,2) = stateAbbrWITH stateAbbr, dfORDER BY df.year ASCWITH stateAbbr, collect(df) AS fileArrayUNWIND range(0,size(fileArray)-2,1) AS indexWITH fileArray[index].name AS startFile,      fileArray[index+1].name AS endFileCALL apoc.periodic.iterate(MATCH (:DelimitedFile {name: $startFile})-[:CONTAINS]->(startRow:ConnectRowToNextRow)MATCH (:DelimitedFile {name: $endFile})-[:CONTAINS]->(endRow:ConnectRowToNextRow)WHERE endRow.STRUCTURE_NUMBER_008 = startRow.STRUCTURE_NUMBER_008RETURN startRow, endRow,MERGE (startRow)-[:NEXT_ROW]->(endRow),{batches:1000, parallel:false, params:{startFile: startFile, endFile: endFile}}) YIELD batches, total, timeTaken, committedOperations, failedOperations, failedBatches, retries, errorMessages, batch, operations, wasTerminated, failedParamsRETURN batches, total, timeTaken, committedOperations, failedOperations, failedBatches, retries, errorMessages, batch, operations, wasTerminated, failedParamsNow that the :File nodes and :Row nodes are connected we have an updated schema that looks like:Up next…. (:State)-->(:County)-->(:Place)-->(:Bridge)!← Part1: BackgroundPart 3: Connecting Bridges to States →;Sep 6, 2020;[]
https://medium.com/neo4j/new-neo4j-community-site-forum-79bbaf2a062d;Michael HungerFollowAug 24, 2018·2 min readNew Neo4j Community Site & ForumWe are very excited to announce our brand-new Neo4j Community. You can ask and answer technical questions, post projects and articles you published, and collaborate on meeting each other in local communities.Technical Help ForumBefore you continue reading, please go to the site and create an account with your social logins. It would be great if you could introduce yourself and start looking around.Neo4j Community Forum Intro VideoIf you ask questions please post in the correct category, but search first if your question has already been answered. It is also helpful if you add additional tags, please share all details you have for others to understand and reproduce your question. Each category also an introductory message, with a number of helpful links.Community EngagementCheck out the local groups and if you have any, share your projects and published blog posts. Going forward we will also award badges for engagement.Why did we move?As our user slack grew to 8750 members, it became more and more difficult to answer questions properly, and many discussions descended into private messages. Also, the 10.000 message limit made it impossible to find out if a question was already answered, and required the helpful folks to repeat themselves time and again. Others that encountered the same issues, came to the same conclusion and moved to a dedicated forum, several of those also to Discourse as we did. Going forward, we will reduce the number of slack channels and reduce our interactions to some casual chat.Looking forward to see you on our new Community Site!If you have any questions please ask in the Feedback category or email us at devrel@neo4j.com;Aug 24, 2018;[]
https://medium.com/neo4j/bloom-scene-actions-different-way-to-interact-with-your-graph-b7e2405a99b4;Claudio GallinaFollowApr 21, 2022·8 min readBloom Scene Actions: A Different Way to Interact With Your GraphNeo4j Bloom helps people connect and interact with their graph data.Exploring the meaning of data with an appropriate visualization is extremely important for us, and for the users who want to explore the relationships and properties stored in their database.Bloom lets us easily expand and follow the different paths in our graph, which is great for exploration and analysis, but we can enhance the capabilities of Bloom even further by leveraging the power of Neo4j Cypher.Cypher is a great query language that allows us to query our data and perform write operations, like updating properties or adding new relationships. Bloom already supports Cypher queries in Search Phrases — using the search box the user can add a specific command, the Cypher code to execute along with any parameters — and recall it easily from the UI.The new Bloom 2.2 expands this capability by adding a new tool in our toolbox: Scene Actions.”What’s a Scene Action?A Scene Action lets us execute a Cypher query contextually on a specific user selection of nodes and relationships in the scene.Once the Scene Action is configured, the user will be able to select nodes or relationships and run the custom Cypher query on those nodes/relationships.Adding a Scene Action is straightforward!Open the Perspective Designer panel on the left side, select Saved Cypher” and then Scene actions.”The Action name will be shown when we open the right click context menu.Cypher query contains the code to execute. To reference the user’s selection, use the implicit parameters $nodes and $relationships, and Bloom will take care to pass the values of the parameters as Lists of ids.The expression id(n) in $nodes will let us reference the nodes based on the user selection.Same with id(r) in $relationships, but for relationships.Action availability allows us to define whether we should show the Scene Action in the context menu or not, based on the types of nodes and relationships in the user’s selection. By default, all nodes and relationships are included so the scene action will appear in the context menu regardless of what the user selected before right-clicking.Let’s Dive In!The following examples are created using the commonly available dataset Northwind.” We suggest starting from your own database, and you can use Aura DB Free to set up your own instance.Add the data following this tutorial about setting it up or simply start Neo4j Browser and execute the command :play northwind-graph, then follow the easy step-by-step tutorial. You will get all the data you need!data model and Order’s propertiesHere’s a brief explanation of the data model we’ll use (as pictured):( Customer )-[ PURCHASED ]->( Order )( Order)-[ ORDERS ]->( Product )( Product )-[ PART_OF ]->( Category )Order” contains the basic information of each orderThe relationship ORDERS” contains the order item details (quantity and price of a product in the Order)We will use different examples to show some applications of Scene Actions.Scene Action 1: Get the Products Purchased by a CustomerStarting from a set of Customers we want to retrieve all the Products that the Customers have purchased over time.If we want to get this information using existing Bloom features, we would need to use the Expand” action twice — once to see a customer’s Orders and again to see an Order’s Products.We can create a Scene Action and limit the availability to user selections that include the Customer” node label.MATCH p = (c:Customer)--(:Order)--(:Product) WHERE id(c) in $nodes RETURN pThis example highlights the following advantages:We can do any read operation using the selected nodes/relationships as input values.We can transform repetitive operations into a Scene Action.We can simplify an analyst’s exploration using a command name that is meaningful in the context of the data model.Scene Action 2: Get the Latest OrderWe can also expand and select paths based on property values — in this case, we want to retrieve the most recent order.This Scene Action can be applied to selections having either Customer or Product node labels, and we can get the most recent Order purchased by a Customer, or the most recent Order containing a specific Product.Here’s the Cypher we can use to create a Scene Action for Customer/Product.MATCH (c)WHERE id(c) in $nodesCALL {   WITH c   MATCH (c)--(m:Order)   RETURN max(m.orderDate) as maxOrderDate}WITH c, maxOrderDateMATCH p = (c)--(o:Order)WHERE o.orderDate = maxOrderDateRETURN pIn this example, we see some additional advantages:We can explore paths based on properties, aggregate functions, subqueries, and procedures (including calls to APOC and Graph Data Science).We can reuse the Scene Action for several Categories and hide it for others.Scene Action 3: Get Orders That Contain More Than One of the Products SelectedGiven a set of Products selected, we want to retrieve all orders that contain more than one product in the selection.In this case, the query will search for some shared data within the selection.This Scene Action is only for Product category:MATCH p = (w:Product)—-(:Order)--(q:Product)WHERE id(w) in $nodes and id(q) in $nodesRETURN pHere we see more advantages:We can retrieve data based on shared property values or shared relationships within the selection.We can reuse the selection ($node) inside the same query more than once.Scene Action 4: Get Cheap and Expensive ProductsThis query is very similar to the previous one, but in this case, we use a UNION of two disjoint queries. As before, we can apply the selection more than once and retrieve the information we need.This Scene action is for Category nodes:MATCH (c:Category)-[r]-(p:Product)WHERE id(c) in $nodes and p.unitPrice < 10RETURN *UNION ALLMATCH (c:Category)-[r]-(p:Product)WHERE id(c) in $nodes and p.unitPrice > 50RETURN *Advanced Scene ActionsThe following examples will update/create data in the database. The user is required to have write privileges in the database and the setting Allow write transaction” enabled in the Experimental Features” section in Bloom. The Experimental Features” can be enabled in the Settings panel.Scene Action 5: Increase Ordered Quantity by 10Until now we have just retrieved data from the underlying graph. Sometimes we want to change the data and trigger a more complex series of operations.In this example, we want to enable the user to increase the quantity of a specific Order item, and the following steps are needed to perform this business operation:Select the Order item, which is stored in the relationship ORDERS between the Order and the Product.Increase the quantity” value by 10.Select the Product node and increase the value of unitsOnOrder” by 10.In the Product node, decrease the unitsInStock” by 10.If there are not enough unitsInStock” of the product we should not perform the operation.The following Scene Action can be applied to relationships of type ORDERS:MATCH ()-[r:ORDERS]-(p:Product)WHERE id(r) in $relationships and p.unitsInStock > 10WITH r, pSET r.quantity = r.quantity + 10SET p.unitsOnOrder = p.unitsOnOrder + 10SET p.unitsInStock = p.unitsInStock — 10RETURN *Bloom will continue to respect Neo4j’s permissions for users attempting to write to the database, so the user would need to have appropriate authorizations on their database role.The following advantages can be seen in this example:We can update relationships properties (write operation) by changing the underlying data and update the visualization in Bloom. (We can change nodes properties as well.)We can also apply Scene Action to relationships and we can limit the scope of applicability of the Scene Action to a single relationship type.Because we can easily perform several operations involving updates in nodes and relationships, a Scene Action can implement business logic procedures, making them easy to execute.Using Scene Action and Style RulesTry to add a style rule for Products based on unitsOnOrder” — you’ll see an immediate change when applying this Scene Action in the scene!Scene Action 6: Create a New Order From a SelectionUntil now we have retrieved data and updated properties, but we can do much more, like adding new data or facilitating data insertion.Given a selection of products and customers we want to be able to create new orders with the following requirements:Every selected customer will have a new Order connected with all the selected Products.Prefill some values in the new Order, like customerID,” orderDate,” and orderID.”Connect the new Order with the Products with an ORDERS relationship.Prefill some values in the relationship, like the orderID” from the order and the unitPrice” from the Product. Other properties can have a default value.The following Scene Action will help us automate this logic, in the exact order of the requirements above.MATCH (c:Customer)WHERE id(c) in $nodesWITH cCREATE (c)-[r1:PURCHASED]->(o:Order)SET o.customerID = c.customerIDSET o.orderDate = toString(localdatetime())SET o.orderID = toString(id(o))WITH c, o, r1MATCH (p:Product)WHERE id(p) in $nodesCREATE (o)-[r:ORDERS]->(p)SET r.discount = 0SET r.orderID = o.orderIDSET r.productID = p.productIDSET r.quantity = 1SET r.unitPrice = p.unitPriceRETURN *Select the Products and Customers first:The Scene Action will create new nodes and relationships:With this final example, we see even more advantages of Bloom’s Scene Actions:We can easily create new nodes and relationships in one step.The selection of the Customers and Products is done visually.Customize the insertion of new data based on specific business logic.Add multiple nodes and relationships.Set new property values from the current graph data and define some default values for other properties.The action is very easy to use — the user just needs to select the products and the customers. After that, the user can change the properties directly in Bloom.If we select more customers, we will create different orders.The insertion is done in the same transaction, keeping this operation secure from a consistency point of view.That’s All ?The applications of Scene Actions are limited only by the power of Neo4j Cypher.Even with this small dataset we demonstrated that we can show subsets of the graph, alter data in the database, and create entirely new data.We can automate repetitive tasks and implement complex operations in a safe and controlled manner, which empowers users to efficiently manipulate the graph data and perform business operations.So what would you use this feature for?Give us feedback or suggestions here. If you need more technical information about Bloom, see the documentation, or for more use cases take a look at our website.;Apr 21, 2022;[]
https://medium.com/neo4j/writing-kettle-plugins-splunk-4b51ac426751;Matt CastersFollowOct 30, 2019·4 min readWriting Kettle Plugins : SplunkDear Neo4j and Kettle friends,Once in a while you come across a data source for which your favorite data orchestration platform doesn’t have a connector. It happens in all tools and platforms because technology has a tendency to advance. That is why, in the early days of Kettle, I decided to implement various ways to allow users to create plugins. These Kettle plugin systems make it possible to create steps, job entries, encryption, compression methods, database connection types, and even data types. Right now there are 25 different plugin types and you can explore them all in the plugin browser. You can find that browser by finding the Show plugin information” entry in the Tools menu in Spoon.The way that Kettle plugins are implemented is always the same:Write a Java class that implements a plugin interface.Package the class with all other classes, images and resources you need in a jar file.Put the plugin jar file in a folder under plugins/ in your Kettle data-integration/ installation.So a few weeks ago, one of our Neo4j customers asked us if it was possible to read some data from their Splunk server so that they could load it into Neo4j using Kettle.Splunk Input icon and dialog screen shotsThe pluginThe first thing to do when writing a plugin for Kettle is to figure out how we access the data. Sometimes the issue is dealing with specific file formats. In other instances, we need to access particular web services. In most cases though, major technologies provide Java libraries. For Splunk, we can use a standard Java library conveniently available through a Maven dependency which we can add to our project:<dependency>  <groupId>com.splunk</groupId>  <artifactId>splunk</artifactId>  <version>1.6.5.0</version></dependency>The interface to implement to create a Kettle step plugin is org.pentaho.di.trans.step.StepMetaInterface and we annotate the class with a Step annotation:@Step(  id =  KettleSplunkInput ,  name =  Splunk Input ,  description =  Read data from Splunk ,  image =  splunk.svg ,  categoryDescription =  Input )@InjectionSupported( localizationPrefix =  Splunk.Injection. , groups = {  PARAMETERS ,  RETURNS  } )public class SplunkInputMeta extends BaseStepMeta implements StepMetaInterface {You can see the whole class here.This class, recognized as a plugin through the @Step annotation, is the starting point of all the plugin components. The Meta class takes care of serializing metadata (XML and repository) and for telling the outside world which classes are responsible for handling the workload and the dialog for editing the metadata. We’re using a convenient BaseStepMeta class to make sure we’re not wasting time implementing stuff which is generic.In the worker class, in our case SplunkInput.java, we take care of the actual work in reading from Splunk after connecting and executing a query. There are 3 main methods to look at: init(), processRow() and dispose(). The code is really simple and straightforward even though it took us a while to figure out how to make Splunk give us all the rows of a query and not just the first couple of hundred. (pro tip: you use JobResultsArgs.setCount(0))The way we got over this hurdle is by working on-site with a test Splunk instance. We did 5 iterations to fix small user interface problems and the query results limitation. All in all that took a few hours showing how easy it is to get quick results with an agile approach.MetaStoreTo handle the Splunk connection information we want to have a re-usable object. This will prevent us from having to type in information in every Splunk Input (and maybe later Output?) step. So we need to have a dialog like this:A parameterised Splunk connectionThe way Metastore objects are handled is by creating a simple Java Bean (POJO) called SplunkConnection:@MetaStoreElementType(  name =  Splunk Connection ,  description =  This element describes how you can connect to Splunk )public class SplunkConnection extends Variables {  private String name  @MetaStoreAttribute  private String hostname  @MetaStoreAttribute  private String port  @MetaStoreAttribute  private String username  @MetaStoreAttribute(password = true)  private String password...The annotations define the top level MetaStore object (SplunkConnection) and the attributes in it. The password option takes care of automatic encryption or obfuscation of the password when it’s being serialized or expressed in any form (XML, JSON, database, …).Once we have this class we can easily load or save instances using a MetaStoreFactory:factory = new MetaStoreFactory<SplunkConnection>( SplunkConnection.class, metaStore, PentahoDefaults.NAMESPACE )// Load a connection...//SplunkConnection connection = factory.loadElement( Splunk )// Save a connection//factory.save(connection)As you can see this is very simple and convenient code which takes the pain out of the whole notion of having centrally-defined metadata.ProjectThe complete kettle-splunk project is put up on my github account and I’m very much welcoming feedback and contributions in any form. Perhaps we can add features to convert the String data we get from Splunk into Kettle data types?Give the current release a try and let us know how it works for you!Cheers,Matt;Oct 30, 2019;[]
https://medium.com/neo4j/running-neo4j-on-google-cloud-6592c1b4e4e5;David AllenFollowApr 15, 2018·4 min readRunning Neo4j on Google CloudRecently, Neo4j made available VM-based images for running both community and enterprise in Google Cloud. Before this, plenty of people were already doing it, but generally had to roll their own and figure out how to do the cloud-specific configuration bits themselves. With these new GCP images though, the process just got a lot easier.This article shows just how easy this has become. We’ll set up a neo4j instance on GCP, configure firewall rules to allow us to access it, and connect Neo4j Desktop to show how to use a cloud instance with other tooling. In this article we will be working with single-node neo4j deploys. If you’re looking for multi-node clusters, neo4j has separate instructions for those.Launching a VMAll we need is two commands. The fact that it’s so easy to automate this means you can include it in scripts or CI/CD pipelines, as needed.gcloud compute firewall-rules create allow-neo4j-bolt-https \   --allow tcp:7473,tcp:7687 \   --source-ranges 0.0.0.0/0 \   --target-tags neo4jgcloud compute instances create my-neo4j \   --scopes https://www.googleapis.com/auth/cloud-platform \   --image-project launcher-public --tags neo4j \   --image=neo4j-community-1-3-3-5The first command configures firewall rules according to the instructions on neo4j’s site. Port 7473 is for HTTPS access to Neo4j Browser (the web-based shell we use to interact with the database). Port 7687 is for the binary Bolt protocol, to send queries to the database. The source ranges” allows all traffic from the entire internet, and the target tags” simply indicates that this rule applies to any VM in your google project that is tagged neo4j”.The second command creates a neo4j instance from the image named neo4j-community-1–3–3–5” in the launcher-public” project, and tags that image with neo4j”. Because we’re tagging the image neo4j, the firewall rules will apply, and we’ll be able to access it via those two ports. This will use an n1-standard VM, if you’d like more or less hardware you can consult the gcloud documentation.If all goes well, you’ll see output like this:Created [https://www.googleapis.com/compute/v1/projects/my-project/zones/us-east1-b/instances/my-neo4j].NAME      ZONE        MACHINE_TYPE   PREEMPTIBLE  INTERNAL_IP  EXTERNAL_IP     STATUSmy-neo4j  us-east1-b  n1-standard-1               10.142.0.3   35.196.123.210  RUNNINGAfter waiting a minute or two to allow the system service on the VM to come up, we can go to https://YOUR_VM_IP:7473 and log into neo4j using the default username and password neo4j/neo4j. Make sure to change the password immediately. Don’t worry if your browser gives you an SSL warning, this is expected since we haven’t configured an SSL certificate.If you prefer, you can use all of the existing tools like cypher-shell and others to work with this database directly by its public IP and 7867 bolt port.Our fresh new (blank) databaseCommunity, or Enterprise? Either works.In the commands above, we launched neo4j-community-1–3–3–5. If we wanted enterprise, we could simply instead launch neo4j-enterprise-1–3–3-5. Both are work fine. If you have an evaluation or commercial license, or you’re in the Neo4j startup program, you can use enterprise.Connecting your GCP Instance with Neo4j DesktopAfter starting Neo4j Desktop, click on New Project” in the left pane. Edit the project name like below, and click on Connect to Remote Graph”.Creating a new remote connectionYou can then enter in the details of how to connect to your instance:Following this screen, you’ll be asked for your username and password. Because we changed it from the default in the last step, make sure to use the one you changed it to here.When finished with the configuration dialog, hit the activate” button.Once activated, this database in neo4j can be treated as if it was local.Deleting your GCP InstanceUsing the gcloud utility, it’s just one last command:$ gcloud compute instances delete my-neo4jThe following instances will be deleted. Any attached disks configuredto be auto-deleted will be deleted unless they are attached to anyother instances or the `--keep-disks` flag is given and specifies themfor keeping. Deleting a disk is irreversible and any data on the diskwill be lost.- [my-neo4j] in [us-east1-b]Do you want to continue (Y/n)?  y;Apr 15, 2018;[]
https://medium.com/neo4j/creating-super-charged-neo4j-dashboards-with-neodash-2-1-30ff649057af;Aleksandar SimeunovicFollowSep 23, 2022·5 min readCreating Super-Charged Neo4j Dashboards with NeoDash 2.1As graph databases and graph data is getting more traction in the world the need for specialized graph powered visualization tools to provide feedback to end users is needed.A variety of tools are already available to visualize force-directed graphs but as it turns out these are only a small part of the puzzle. In practice, many more visualizations are needed. This is where NeoDash 2.1 comes in.NeoDash is an open-source, low-code Dashboard Builder for Neo4j. It lets you build an interactive dashboard with tables, graphs, bar charts, line charts, maps and more, in minutes.The latest NeoDash release comes packed with a lot of new features, for instance:A dashboard gallery has been added with sample dashboards on public datasets.New visualizations for your reports like sankey charts, choropleth map and some hierarchical charts.Finally, an improved user experience and new ways to export data out of dashboards.To try out NeoDash, try it in your browser, or install it into Neo4j Desktop. Check out the project’s Github repository for details on other types of deployments, as well as pointers for extending the application. Enjoy!NeoDash Dashboard GalleryIf you’re looking for inspiration on building a Neo4j dashboard, check out the brand new example gallery. With a growing list of dashboards to try out, there’s one available for a variety of use cases. Try it online here:https://neodash-gallery.graphapp.ioHave a public dataset & dashboard to share? Reach out to us and we’ll add your dashboard to the gallery.Sankey ChartThe sankey chart is used to depict flow from one set of values to another. In a sankey chart, there’s usually a many-to-many relationship and you want to get an overview of the dominant and less dominant flows between these entities by looking at the width of the connections.The sankey chart in NeoDash supports drawing nodes and edges directly as a flow diagram. In the image below we have a sankey chart where different persons have rated some movies. By looking at the picture we can see that Jim rated ‘The Matrix — Revolutions’ higher than ‘The Matrix — Reloaded’. With interactivity enabled, users can hover on the connection to get the exact rating value. Try out the sankey chart on a public movie database here.Choropleth MapA choropleth map is used to overlay geographical areas with numerical data. In most cases, this means applying a color to a country based on a value scale. With NeoDash, you can assign colors to countries based on the ISO 3-letter country codes. Creating the visualization below is as simple as two lines of Cypher:MATCH (c:Country)RETURN c.code, c.valueHierarchical VisualizationsA force-directed graph is a great way to display graph data. However, in many scenarios, users would like to see their data as a hierarchy. There are three new ways of displaying hierarchy-based visualizations in the new version of NeoDash:Sunburst: render a tree as a collection of concentric circles growing outwards.Treemap: displays multi-level hierarchical data using nested rectangles.Circle Packing: A variation of the treemap, but using nested circles.Depending on the depth of your tree, you might want to choose from these three visualizations. Check out the documentation on how to create a hierarchical visualization on your own graph.Dashboard FeaturesTo make it even easier to build a dashboard, NeoDash now supports a drag and drop layout. To move or resize a report, grab the handles on the top-left and bottom-right and update the layout:Other user experience changes include:Cloning reports in a page.The option to enable image downloads for each report.Downloading a CSV file from table reports.Details on all of these features are available in the brand-new documentation portal.Wrapping UpEager to learn more? This year, NeoDash will be a part of the NODES 2022 Online Developer Conference on Nov 16–17! NODES 2022 is a virtual conference of technical presentations by developers and data scientists solving problems with graphs. Best of all, it is 100% free to join — and a great place to meet other graph visualization enthusiasts.Register for NODES 2022!NODES 2022 - Neo4j Online Developer Education Summit - Nov 16-17neo4j.comIf you want to know how a presentation will look like at NODES 2022, have a look at the previous presentation from GraphConnect 2022:More cool features are on the way! Stay tuned. If you’d like to be up to date on the latest around NeoDash, subscribe to the Neo4j Developer Blog.;Sep 23, 2022;[]
https://medium.com/neo4j/getting-to-know-you-getting-to-know-all-about-you-1cd0d2aacc6b;Ljubica LazarevicFollowMar 18, 2021·5 min readBrooke Cagle on UnsplashGetting to Know You, Getting to Know All about You!Want to dabble in graphs with family and friends? Not quite sure where to start? How about a quick and dirty trick using Google Forms, Google Sheets and a completely free database on Neo4j Aura?Where Do I Start?So you’ve heard about graphs, they sound like fun! Now, you’re wondering where to start. Maybe you’ve had a go with one use cases in the Neo4j Sandbox, but now you want to create some data. Perhaps you’re not feeling too confident about generating or importing large data sets just yet. Or are you seeking that graph-based ice breaker to show off your new skills?I have a treat for you, my friend. I’m going to show you how to put together a super simple yet fun way to create some data, and find out fun and interesting things about your friends and family. All you’re going to need is a blank Neo4j Aura Free instance, and a Google account.Let’s get started!Riddle Me ThisWe are going to be using a Google Form as our data capturing device. For those of you who have not come across Google Forms before, they’re a quick and easy way to create a questionnaire. You can easily view the results afterwards in a Google Sheet. This is a neat way to easily generate data for your graph. If you’re completely new to Google Forms, check out this guide.Some recommendations:As we’ll not be handling nulls, do ensure all question answers are required.Try to limit free-entry boxes and stick to drop-down lists, like that you won’t need to worry about any post-data processing.In this example, I’m putting together a simple form to help that ‘getting to know you’ feeling, and I’m going to ask about favorite colors and activities.Here’s what my form looks like:And some of those activities!And this is what it’s going to look like when you share your form (you can either share the preview link, or hit that share button):Ok, now, share the form to your nearest and dearest. Do make sure you don’t have any sensitive data in there (such as email addresses, phone numbers, etc.)! We will be making the data from the form available publicly so that we can load it into Neo4j, so don’t have anything in there you wouldn’t want the world to see.If you’re doing the same questions as above, and you like more than one color and/or activity, you can fill out the form more than once.The Results Are In!Ok, so we now have lots of results from our friends and family, we can have a quick look at them inside Google Sheets. To do this you will need to navigate to the Responses tab, and then press the Sheets icon (both circled in red below):Click on create new spreadsheet and you’ll have the responses in a brand new sheet.Spin Up an Aura Free InstanceBefore we load some data, let’s get a Neo4j Aura instance set up.The Aura Free instance is a great way to get some hands on experience with Neo4j. It is a completely free, no-download version of the database. It may look similar to the Sandbox, however the key difference is whereas the Sandbox expires after a maximum of ten days, your Aura Free instance is there forever. Don’t forget to store your password in a safe place, if you lose it, you’ll have to remove your existing instance and create a brand new database!Setting up an Aura Free instanceOnce you’ve created a free instance, hit the Open With” button to start up the Neo4j Browser.Loading the DataNow we have database set up, we need to do one more change to our Google Sheet. We need to make it available to anyone with a link (viewer only is fine). Click on the Share button and make the adjustments, clicking Done when you’re finished.Once you’ve done that, use the follow Cypher query to load up the data. You will need to replace the <link to google sheet> bit of the URL to the one of your sheet, making sure the link is appended with /export?format=csvLOAD CSV FROM  <link to google sheet>/export?format=csv  AS rowWITH row SKIP 1 //skip the headersMERGE (f:Friend {name:row[1]})MERGE (c:Colour {value:row[2]})MERGE (a:Activity {value:row[3]})MERGE (f)-[:LIKES_COLOUR]->(c)MERGE (f)-[:ENJOYS]->(a)Your data model will look like this:Knowing Me, Knowing YouWith all that data loaded in, now it’s time to find out more about our nearest and dearest!Here are some example queries to explore your friends data. Don’t forget to share your sandbox details so that they can join in too! You’ll need to replace friend names as appropriate!Who likes the same color as me?MATCH (:Friend {name:Lju})-[:LIKES_COLOUR]->(c)<-[:LIKES_COLOUR]-(friends)RETURN friends.nameMost popular activities:MATCH (a:Activity)RETURN a.value, count(a) ORDER BY count(a) DESCShortest path between two friends:MATCH path= shortestPath((f1:Friend {name:Lju})-[*]-(f2:Friend {name:Jane}))RETURN pathAlso, don’t forget to explore your data in Neo4j Bloom, also available on your Aura free instance. What exciting discoveries do you find?If you want to ask a question and don’t know how, leave a comment on this blog post.;Mar 18, 2021;[]
https://medium.com/neo4j/graph-visualization-with-neo4j-using-neovis-js-a2ecaaa7c379;William LyonFollowApr 16, 2018·5 min readGraph Visualization With Neo4j Using Neovis.jsLeveraging Graph Algorithms For Data VisualizationUpdate: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comIn this post we’ll use a Neo4j Sandbox instance to start with a Twitter dataset, run PageRank and community detection on the data. Then show how to embed a graph visualization in a web app using Neovis.js.In this post we explore how to create graph data visualizations that use the results of graph algorithms like PageRank and community detection. After running some graph algorithms using the neo4j-graph-algorithms library we’ll use the JavaScript graph visualization library Neovis.js to create visualizations that can be embedded in a web app, fetching data directly from Neo4j.Goals Of Graph VisualizationThere are different motivations and tools for creating graph visualizations. This includes tools for exploring the graph — the type of interactive visualizations you might see in Neo4j Browser. Or visualizations for showing the results of some analysis. These can be interactive (something to be embedded in a web app or even a standalone application), or static, meant to convey specific meaning that might be used in print or a blog post.This post will focus on one tool that addresses some specific goals of graph visualization. This tool is Neovis.js and is used for creating JavaScript based graph visualizations that are embedded in a web app. It uses the JavaScript Neo4j driver to connect to and fetch data from Neo4j and a JavaScript library for visualization called vis.js for rendering graph visualizations. Neovis.js can also leverage the results of graph algorithms like PageRank and community detection for styling the visualization by binding property values to visual components.This screencast shows how to use the Neovis.js library to create graph data visualizations styled to the results of graph algorithms with data from Neo4j.Graph Visualization + Graph AlgorithmsThere are three common ways that graph visualizations can be enhanced with graph algorithms. Specifically this involves styling visual components proportionally to the results of these algorithms:Binding node size to a centrality algorithm, such as degree, PageRank, or betweenness centrality. This allows us to see at a glance the most important nodes in the network.Visually grouping communities or clusters in the graph is done through the use of color, so that we can quickly identify these distinct groupings.Styling relationship thickness proportionally to an edge weight, in social network data this might be the number of interactions between two characters, in logistics and routing data it might be the distance between two distribution centers and is useful for pathfinding algorithms (such as A* or Dijkstra’s).Getting Started With The DatasetWe’re going to use the Russian Twitter Trolls sandbox as our dataset. This dataset contains tweets from known Russian Troll accounts, as released publicly by NBC News. You can create your own Neo4j Sandbox instance here.The data model for the Twitter Trolls dataset includes Tweets, Users, Hashtags, URLs shared in Tweets and mentions and retweets.In graph data, often some of the most interesting relationships are inferred, and not directly modeled in the data. The User-User retweet graph is an example of this. Which users are retweeting which other users? Who is the most important User in this retweet graph? Are there groups of users that frequently retweet each other?Here we have a tweet that was posted by a user, that another user retweeted. This implies a user-user retweets relationship. Our first step will be to find those inferred retweets relationships and run our graph algorithms.To find the most important users and communities using this retweet network we will first find all Troll users and create a RETWEETS relationship connecting directly the users in the graph. We store a count property on the relationship representing the number of times that the user has retweeted the other:Once we’ve created these RETWEETS relationships we can run PageRank over this part of the graph (we could also use a Cypher query to run PageRank over the projected graph without explicitly created the relationships):Since we specify write: true above this will not only run PageRank but add a pagerank property to the nodes contains their PageRank score. We can then query using that property value to find the top ten Troll accounts by PageRank score:And finally we can run a community detection algorithm on the retweet network, in this case label propagation:This will add a community property to the nodes, indicating which community the algorithm has determined the node belongs to.So we’ve now run two graph algorithms (PageRank and label propagation), but how do we make sense of the results? A visualization can help us find insights in the data.Creating A Graph Visualization With Neovis.jsIn order to create a visualization with Neovis.js we first need to connect to Neo4j. In the details tab of our Sandbox instance we can find the connection details for our Neo4j instance:The server connection string, username, and password will be included in a config object that we’ll pass to the constructor for Neovis. We’ll also need to specify what node labels we want to visualize and how they should be styled (which properties determine node size and color).Neovis.js works by populating a <div> element with the visualization, so we’ll need to specify the id of that element in the config object, as well as how to connect to our Neo4j instance, and which properties to use for determining node size, color, and relationship thickness. Here’s the code to generate a graph visualization of our retweet network from our Neo4j Sandbox instance, using the pagerank property to determine node size, community for color, and the count relationship property for relationship thickness:And here’s how our visualization looks:There are a few more configuration options which you can read about in the project documentation.ResourcesNeo4j Sandbox: neo4jsandbox.com/Neovis.js GitHub page: github.com/neo4j-contrib/neovis.jsNeo4j Graph Visualization Developer Page: neo4j.com/developer/guide-data-visualization/Neo4j Graph Algorithms: neo4j.com/developer/graph-algorithms/Free download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Apr 16, 2018;[]
https://medium.com/neo4j/using-neo4j-to-find-the-most-powerful-package-a3b620dd8781;Tom NijhofFollowJul 4, 2022·2 min readUsing Neo4j to Find the Most Powerful Package2:30 PM (Pacific Time) March 22, 2016. NPM packages broke for a few hours because Left-pad was unpublished. This showed us, as developers, that sometimes one person holds the power to break a lot. All because Left-pad was popular, but Left-pad was also used in other popular packages that were used in yet other packages. This is an example of a powerful package I want to find in Python.I made a graph database with Neo4j for another project, but could use this same database to answer this question. Within the 5,317 packages, I will look at how many packages depend directly or indirectly on each package, and then list those.MATCH (n:Package)<-[:DEPENDS_ON*0..]-(m)WITH n as n, count(DISTINCT m) as num_depsRETURN n.name as Name, num_deps ORDER BY num_deps DESC LIMIT 25List of the first 25 packages| Name                │ num_deps │╞═════════════════════╪══════════╡│ typing-extensions   │1518      │├─────────────────────┼──────────┤│ six                 │1310      │├─────────────────────┼──────────┤│ zipp                │1004      │├─────────────────────┼──────────┤│ idna                │1002      │├─────────────────────┼──────────┤│ urllib3             │983       │├─────────────────────┼──────────┤│ charset-normalizer  │960       │├─────────────────────┼──────────┤│ certifi             │954       │├─────────────────────┼──────────┤│ requests            │902       │├─────────────────────┼──────────┤│ importlib-metadata  │822       │├─────────────────────┼──────────┤│ python-dateutil     │701       │├─────────────────────┼──────────┤│ pyparsing           │627       │├─────────────────────┼──────────┤│ colorama            │624       │├─────────────────────┼──────────┤│ attrs               │596       │├─────────────────────┼──────────┤│ packaging           │569       │├─────────────────────┼──────────┤│ importlib-resources │474       │├─────────────────────┼──────────┤│ pytz                │472       │├─────────────────────┼──────────┤│ numpy               │463       │├─────────────────────┼──────────┤│ setuptools          │428       │├─────────────────────┼──────────┤│ pyyaml              │380       │├─────────────────────┼──────────┤│ click               │353       │├─────────────────────┼──────────┤│ markupsafe          │336       │├─────────────────────┼──────────┤│ jinja2              │328       │├─────────────────────┼──────────┤│ dataclasses         │301       │├─────────────────────┼──────────┤│ typing              │278       │├─────────────────────┼──────────┤│ wrapt               │252       │└─────────────────────┴──────────┘Six and 100 of its 327 direct first dependenciesThis gives us the result. Typing-extensions is the highest package, but it is maintained by Python itself. So if Guido van Rossum ever goes crazy, 100% of Python is unsafe.But after that comes Six. Six is a Python 2 and 3 compatibility library used directly by 327/5317 packages (one relationship hop away), and indirectly by 1310/5317 packages (more than one hop). Looking at the insights of Six’s GitHub page, we see only 1 person with a significant influence. Let’s hope Benjamin Peterson has mercy upon us all!So it will be time to stop asking Why is Six afraid of Seven?” and start asking Why is Python afraid of Six?.”;Jul 4, 2022;[]
https://medium.com/neo4j/how-to-backup-neo4j-running-in-kubernetes-3697761f229a;David AllenFollowOct 11, 2018·3 min readHow to backup Neo4j Running in KubernetesUPDATE June 2020: This article has been updated and adapted to work with the new Neo4j Helm repo, and with Neo4j 4.0, and 4.1Once you’ve deployed either a single instance of Neo4j or a cluster in kubernetes, maintenance and administration needs are going to require that you take regular backups. This article will step through how you can do that.Most of the examples and code in this article are taken from Neo4j’s Helm repository. The backup container code and documentation you can find in the tools directory of that repo.The approach should be suitably generic to work with most any Kubernetes distribution, but in the examples, we’ll be using GKE.ApproachTo back up a running cluster, first — it doesn’t really matter which machine you connect to. It can be the leader, a follower, or a read replica. As long as it’s got the data, it can be a backup target.The way we’ll go about this is to use a specialized Docker container that has neo4j installed (thus giving us the neo4j-admin tool). This will be the backup container. Its job is to run the backup tool, compress the result, and upload the backup set to cloud storage (in our case, Google’s Cloud Storage).The Backup Helm ChartThe tools directory provides a mini-helm chart, with a specialized container and a shell script. To install the backup chart (and take a backup) a few parameters are needed:neo4jAddr — the address of your Neo4j instance. This can be any internal to Kubernetes DNS name that will get a connectionbucket — the name of the google storage bucket where you want to save the backup, such as gs://my-backups.databases — a comma-separate string of databases to backup to, for example neo4j,systemOptional parameters include pageCache, heapSize, and others. These let you control how much memory is allocated to the backup, and the name of the resulting file.In order to upload your backups to google cloud storage, you also need to create a kubernetes secret with your service account key.Installing the Backup ChartIt works like this:helm install my-backup-deployment . \   --set neo4jaddr=my-neo4j.default.svc.cluster.local:6362 \   --set bucket=gs://my-bucket/ \   --set database= neo4j\,system  \   --set secretName=neo4j-service-keyWhat’s happening is:We’re deploying a single pod that never restartsThat pod is linked to a certain container image which you’ll want to change if you build this container separately.The helm set variables get passed as environment variables to the container.In the volumes section, we will mount the kubernetes secret named neo4j-service-key to the /auth path. This lets the container authenticate against google storage.It’s just another pod, so all of your normal logging and monitoring approaches apply. If all goes well, a backupset magically appears in google storage.What’s Next?You’ll probably want to schedule execution of a container like this on a regular basis (for example at least weekly)As your data grows in size, the most likely thing you’ll need to tweak is your HEAP_SIZE and PAGE_CACHE to make sure you have enough RAM to take the backup. Keep this in mind too when planning the YAML for the resource, your kubernetes cluster has to be able to request and allocate enough RAM for the inside of the container to allocate your request.Points of AdaptationLet’s say you need to backup to S3 — in this case the entire structure can be used, but you need to change the script to pass the credentials in differently, and change the container to install the AWS CLI tools. But the overall approach is the same.Other things you might want to consider is whether or not to zip/archive your backups. As raw directories, they’re larger, but it saves CPU time and gets your backup to cloud storage sooner if you just recursively copy the directory. In this implementation we’ve opted for compression, but some go without, as the trade isn’t worth it for them.ReferencesOperations Manual: Backing up Neo4jQuestions? Head on over to the Neo4j Community Site and ask them in the Cloud topic! If you figure a way to adapt this approach to a new cloud storage type or deployment setup, please let us know.;Oct 11, 2018;[]
https://medium.com/neo4j/week-38-exploring-chatgpt-for-learning-code-data-nlp-and-fun-b7116b7e25e5;Michael HungerFollowDec 6, 2022·8 min readWeek 38 — Exploring ChatGPT for Learning, Code, Data, NLP, and FunLast week (Nov. 30) ChatGPT was launched by open.ai and I’ve spent the days since then exploring different areas of its application (from world knowledge to software development). In today’s stream, we want to see if we can apply it to Learning Graph Databases.If you missed the stream, you can watch the recording here — we had a lot of fun!Learn Graphs with ChatGPT3Also check out our upcoming livestreams this week with Going Meta 11 (Graph Expectations), and Learn with Neo4j with Jason Lengstorf on Building and Deploying Graph apps with Netlify.But back to ChatGPT. It was announced on November 30 and I tried it out the same day. I started with normal questions, then went to world knowledge, predictions of the future, and then to code.ChatGPT: Optimizing Language Models for DialogueWeve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for…openai.comWorld Knowledge QuestionsIt was also able to explain What if”-like questions, like How many LEGO blocks would it take to get to the moon,” but I haven’t checked the actual numbers.Legos to the MoonOne word of caution!Please don’t take things that the language model tells you as truth, it very much tries to please the human and come up with answers instead of saying I don’t know.” There were several occasions when it was plainly wrong. So always check the output, and rather use its responses as inspiration, quick-typing help, or as a starting point. But never rely on the correctness for critical applications.It even provides its own disclaimer after producing an incorrect Cypher statement with wrong comments / explanations.Incorrect Cypher Statement and ExplanationsDisclaimer, that it doesn’t know anything.Some useful tips:Try to be specific with your prompt — the clearer you are the better the answers.You can make it do things if you tell it to imagine” a situation, and then play a role or act as the imagined character.If it stops in between you can just say continue,” as it’s limited to a 4096 token output.Sometimes if it rejects your question and doesn’t want to do something it helps to hit try-again a few times — having it translate to other languages took a few tries before working fluently. (It even translated the US anthem to Klingon.)For fun, you can have it answer in different literary styles.You can trick it into circumventing its default prompt and limitation (like rendering images or enabling browsing). Some people on Twitter have been successful doing that, but I couldn’t.In general, it’s definitely a more cohesive answering machine than Google or StackOverflow, as it produces a single answer with explanations (but no source attribution!) And again — be aware that its answer is not necessarily correct!Explanations Graphs and Neo4jIt is also impressive in applying general concepts like graph modeling to a specific domain (like here, to a supply chain):Supply Chain with GraphsGenerating DataWhat it is quite good at is generating information. You can tell the model to be a data generator and then have it generate random data based on that.Generate Fictional Company CSVGenerate Friend-NetworkAlso, getting existing information from its model and transferring formats works well, like here with the Royal Family as Graphviz Dot file, or turning JSON into CSV (from d3 les miserables).Generate DataDocumentation Lookups / SearchesIf you’re looking for something in a documentation that’s not well explained, ChatGPT can do a better job of it. It also kept the context of the previous discussion about the movie recommendation graph.NOTE: The training cutoff was some time in September 2021, so newer information like here for the graph-data-science library v2 is not included.Documentation Lookup (but outdated)Generating CypherGiving a textual representation of a graph, it can generate Cypher queries for the dataset. While that worked well yesterday, today its queries didn’t make sense.Generating Cypher (right one is wrong)Comparing Cypher and SQL or Relational with GraphFor comparing databases or query languages it does a reasonably good job, and even provides code examples.Comparing SQL-Cypher and Relational-GraphTranslating between Query LanguagesFor translating between the query languages, you really need to understand the answers and be able to verify the results. While a regular Common Table Expression (CTE) to Cypher worked, the recursive CTE came up with made up stuff each time. Only in the inverse direction from var-length path query to recursive CTE did it work.Translating between Query LanguagesAnswering Stackoverflow QuestionsIt can also answer stackoverflow questions (or advent of code and other code-challenges), like Tomaz Bratanic demonstrated here:Get multiple level of one to many relationship resultsI have a graph that looks like this A1 A2 A3 A4 A5 \ / \ | / S1 S2 \ / E1 There can be many E nodes. But the essence…stackoverflow.comAnswering Stackoverflow QuestionsBut in doing so it still produces a large percentage of seemingly correct and well explained (longish) answers that are still incorrect! And who except for subject matter experts could check them? And where should they take the time from doing that, while it costs seconds to produce them?That’s why ChatGPT outputs have been temporarily banned from Stackoverflow.Temporary policy: ChatGPT is bannedThis is a temporary policy intended to slow down the influx of answers and other content created with ChatGPT. What the…meta.stackoverflow.comNLP: Natural Language to CypherAn interesting application is to translate phrases from natural language to Cypher — you can even give it an explanation of the graph model to use.Natural Language to CypherThis is also what Sixing Huang used in doctor.ai as described in this article and explained on our livestream.Relationship Extraction with GPT-3Accelerate knowledge graph construction with GPT-3medium.comMaking Stuff UpIf you try to use it for NLP on text you have to be careful though, especially if you want cross-references with other sources like dbpedia.So while this works:Entity ExtractionAsking it to produce wikidata IDs makes up random references!GPT NLP on Text with *incorrect* correlation to wikidata idsWhat is quite useful that it keeps the context of the conversation, so we can continue to ask it repeatedly different questions about the same topic without specifying it time and again.New Knowledge vs. Drawing InsightsWe got a question in the chat about ML models not being able to generate new knowledge. That’s true, but what is interesting— similar to knowledge graphs — is they are able to connect disparate parts of information that a human might not have done yet. I think that’s where most ideas come from — not truly novel” thoughts but derivations and combinations of existing knowledge (see this TED talk by Matt Ridley).Here is an example where I asked it to create a card games for an tv series (The Expanse), including showing some cards.Card Game for The ExpansePoem About Graphs Are EverywhereIn closing, we had it write a poem:generate a poem about graph are everywhere in the style of Neil Gaiman:Graphs are everywhere,In every shape and size,From the simple line graphTo the complex network.They show us patterns and trends,Reveal connections and insights,Help us make predictions and decisions,And bring order to the chaos of data.In the world of finance and science,In the realms of art and politics,Graphs are a powerful toolFor understanding and explaining.They are a window into the world,A way to see beyond the surface,To discover the hidden truthsAnd to tell the stories of our time.So let us embrace the power of graphs,And let them guide us on our journey,As we explore the wonders of the worldAnd seek to understand the universe.ConclusionChatGPT is impressive and helpful— but also scary and unhelpful. I guess like everything, if you already know what you’re doing it’s a powerful tool, but if not you can easily be led astray if you trust it too much. Things that definitely need to be added is validation of its own answers and providing sources. Speaking of sources — as it draws its information from publicly available information, if it’s commercialized, then open.ai, the company should give back to creators, wikipedia, researchers, developers, and many more, but I doubt they will.OpenAI as a for-profit companyHappy Holidays;Dec 6, 2022;[]
https://medium.com/neo4j/slice-and-dice-your-graph-data-with-neo4j-bloom-f8cc0bb62908;Angeliki KomianouFollowMar 16·8 min readSlice and Dice Your Graph Data With Neo4j BloomNeo4j Bloom is an advanced graph exploration tool. You can display different Neo4j graphs and explore graph data using the search feature, expanding it visually or by using custom Cypher queries.Photo by Shane Scarbrough on UnsplashBut now we’ve added a new feature to interactively explore your graph changing by values on your nodes and relationships — The Slicer!In Neo4j Aura you might press shift+reload in Neo4j Workspace to get the latest version with this feature.Each node or relationship in a Neo4j database may contain different properties. For example, a node with the label Movie can have different properties like title, year, rating, etc. The values and types of the properties can vary and the distribution of the distinct values for each property is different among nodes or relationships.What if you are interested in individual properties in a specific range of values?Bloom’s new feature, the Slicer, displays all the existing distinct values of a property of a node or relationship in the scene, in ascending order, like a timeline or histogram — you can easily slice the visual Bloom scene based on those values.This gives you the ability to interactively display specific subgroups of nodes and/or relationships which contain a selected range of property values and allows to see the changes over time (for date/time properties) or otherwise highlight nodes with varying property values (for numerical properties). This enables Bloom users to tell a story about their data.Sliced Bloom scene based on a specific range of dates using the Slicer. The scene represents all the relationships between Customer — Order and the orders are sliced according to their orderDate property. Customers are placed according to their coordinates using the new Coordinate Layout featureYou can select a specific slice of the set of the total distinct values of a property using the Slicer chart bar. Each bar of the chart represents the count of the nodes and/or relationships that contain a distinct property value (or a subgroup of values in the case of large datasets).Open SlicerFor example, you can focus only on the nodes which have the value 1998 for the property born. Selecting two continuous bars, 1998 and 1999, only the nodes, which contain the property born, and their value is inside of the selected range [1998, 1999] will be shown in the Bloom scene.How to create rangesInitially, you can select from a list of options that contain the full range of properties. Each option represents a different set of nodes and/or relationships and is a combination of:the group(s) the property belongs to category name (for nodes) or relationship type (for relationships)the property type (supported types of values are integer, float, date/time, or duration)the property name (for example, year)List of options for new rangesIn case of universal properties (with same name and type) for all graph elements, an additional option is generated and gets tagged as all.Choosing the all” option, the selection of the visible nodes or relationships depends on all the nodes and/or relationships where the property exists and the property values type is the same as the range property type.Values generated by the Graph Data Science integration in Bloom are considered properties and are included in the list. You can choose them for creating a new range in the Slicer based on their values.Selection of visible nodes and relationshipsIn the basic operation, you just select a property and then can see the distribution in the histogram and select the slice that you want to be visible.Like shown below for Product(unitsInStock) or Order(orderDate).Product(unitsInStock)Order(orderDate)Playback modeThere are two types of range selection:(a) manual, by resizing/dragging the range window on the histogram(b) playback mode, which automatically changes the window in a specific period of time the bloom visual scene gets sliced automatically, as well, according to the temporary window position.Also, the playback mode can be customized. You can select the mode and the speed of the animation.There are three types of playback modes:Slide range to end: which slides the slice from left to right keeping the range window width fixed over timeStart of range to end: which enlarges the specified range from the end of the range to the end of the histogramWithin range: which reduces the range from the start of the range to the endSelection of the playback speed and mode from the Slicer settingsNotes:The ranges configuration is stored locally in your Browser or in the database (for enterprise users) in order to be shareable and reusable for future sessions.Any change in the scene removes automatically the existing ranges.Slicer is now available for exploration in the new 2.7.0 Bloom release and in the Neo4j workspace.Give us feedback or suggestions here. If you need more technical information about Bloom, see the documentation, and for more use cases take a look at our website.The following section goes more into the nitty gritty details of compound ranges but is not necessary to understand how the slicer works and to get value out of it.Here be Math — Parallel compound rangesYou can create different ranges in the Slicer (up to five), adjusting the Slicer window and focusing, in parallel, on different properties and/or different groups. You need to click on Add Range” to achieve that.A range is defined by the below parameters:A node or relationship (s) is defined by the parameters:And for each node or relationship property:A graph, G = (V, E), in the Bloom scene, is a combination of nodes (V) and relationships (E).Initially, we check if a node category or relationship type is included in the range groups and contains a property that matches the range property name and type.The set that satisfies the above criteria — where i is the number of each range and k is the number of a property of a node or relationship — is:The function that returns the value of each property of a node or relationship (in this context s), which belongs to the above set, is:In order to calculate the final visible nodes and relationships we take into consideration the following sets:Combining (1) and (3) we get the final visible nodes by:The same logic is applied for the selection of visible relationships, (2) and (4), but for the calculation of the final visible relationships there is an additional layer of filtering.For example, you can have two ranges selected for the properties:(a) Product integer unitsInStock(b) Product integer reorderLevelInitially, all the possible values of (a) are between [0, 125] and all the possible values of (b) are between [0, 30] which cover the full range of the nodes in the scene. Then, using the Slicer, you can change the ranges, for example, to [8, 9] and [10, 20] respectively.In that case, according to (5), we focus on the nodes that belong to the Product category and include the properties unitsInStock with type integer and/or reorderLevel with type integer, as well. Any node that does not match any of the above criteria is visible. In case a node matches the range criteria, the value of the node properties unitsStock and/or reorderLevel should be inside the selected ranges, between [8, 9] for (a) and [10, 20] for (b), in order to be visible.In case of a node that contains both unitsInStock and reorderLevel (only for integer type values) then both properties should satisfy the selected range of values in order to be visible.Also, relationships which are linked to hidden nodes are not visible as well (7).If a node contains both properties where at least one is out of their range then it’s not visible and is considered hidden in the chart. The number of the hidden nodes is displayed with lighter color in the chart bars.Slicing nodes of the same categoryThe same rules are applied in case of relationship properties. For example, you can add two new ranges:(a) ORDERS integer quantity, where ORDERS is a relationship type(b) Order datetime orderDate, where Order is a categorySlicing nodes and relationships in parallerIn the above example, (a) hides all the relationships with quantity value less than 7 and (b) hides the nodes with orderDate earlier than a specific date.In the first image you can see that inside a valid range, [7, 130], some of the relationships are hidden (displayed with lighter color). That happens because some of the relationships found in the selected (a) range are associated with hidden nodes which contain the property orderDate and are out of the range (b).A node or relationship, with matching range criteria, has to satisfy all the ranges in order to be visible, as explained in the previous example as well.Congratulations! You made it all the way to the end, so now go and try out the cool new feature and let us know how you like it.;Mar 16, 2023;[]
https://medium.com/neo4j/announcing-10-000-leonhard-euler-idea-contest-winners-67488e5c2c66;Tara Shankar JanaFollowJun 23, 2021·5 min readAnnouncing $10,000 Leonhard Euler Idea Contest WinnersThanking all participants for their inspiring submissions and congratulations to the winners!The results are in: The Neo4j Leonhard Euler Idea Contest, with US $10,000 in prizes, has drawn to a close and the winners were announced at NODES 2021 by Emil Eifrem, CEO and Co-Founder of Neo4j.A record number of innovative, thought-provoking, and inspirational ideas were submitted from around the world, and I wish to thank each one of you wholeheartedly for taking the time to submit your ideas.The primary goal of the idea contest was to excite and inspire developers, students, technology enthusiasts, and hackers to see the innovations, applicability, and feasibility possible — with graph database and GraphQL Library across different use cases — and present their work to a thriving global community of graph enthusiasts and developers.The contestants also got early access to our free cloud offering Neo4j Aura to build their GraphQL solutions, which is now available to everyone!Neo4j Aura - Fully Managed Cloud SolutionMenu Neo4j Aura™ is a fast, reliable, scalable and completely automated graph database as a cloud service. Aura enables…neo4j.comThat goal was reached, and we couldn’t be more excited and impressed. Thanks for making the idea contest a roaring success.Neo4j GraphQL Library- MechanicsThe Contest by the NumbersWe launched the idea contest on Apr 27, 2021, with the general availability (GA) of Neo4j GraphQL Library, and closed submissions on June 7, 2021. During those 41 days, more than 300 developers, data scientists, and professionals from 58 countries registered and submitted their ideas for the Leonhard Euler idea contest. We received 28 qualified, stunning idea submissions from which three prize winners and five most creative ideas were selected.If I have a thousand ideas and only one turns out to be good, I am satisfied.” — Alfred Nobel, of the Nobel Prize [and probable graph enthusiast]JudgesThe Leonhard Euler Idea Contest winners were selected by a stellar panel of judges from across the Neo4j Graph Database industry. They evaluated each submission on originality of idea, complexity of implementation, feasibility of solution, and whether the idea makes developers’ and people across the globe’s lives easier. Here’s our panel of brilliant judges:Amy Hodler (Graph Analytics and AI Program Director, Neo4j)Cristina Escalante (COO, Silver Logic)Jennifer Reif (DevRel, Neo4j)Ljubica Lazarevic (DevRel, Neo4j)William Lyon (DevRel, Neo4j)Judges- Neo4j Leonhard Euler Idea ContestNow, without further delay, let’s find out who the winners are and learn a bit more about their ideas and submissions.And the Winner Is… Drumrolls”…The Grand Prize of US $3,500 cash goes to Developers.Zed — A web application to find software developers in Zambia through easy links and recommendations.”Developers.ZedHome page Developers Developers in Zambia(in Southern Africa) tend to be overlooked by local people hiring simply…devpost.comThe creator of Developers.Zed, Kacha Mukabe’s inspiration in his own words: Developers in Zambia (in Southern Africa) tend to be overlooked by local people hiring simply because they are not as visible. Most dev work is outsourced and I want to make it a little easier for employers to find developers locally.” Check out the Developers.Zed public workspace here.Grand Prize: US $3,500Developerz.Zed: A web application to find software developers in Zambia through easy links and recommendations.Developers.ZedHome page Developers Developers in Zambia(in Southern Africa) tend to be overlooked by local people hiring simply…devpost.comVisit this idea’s GitHub Repo.Team Country: ZambiaFirst Runner-Up, 2nd Place: US $2,500SeedFundMe: A tool that provides funding for small businesses of all ethnic backgrounds.SeedFundMeIn our current world situation, small businesses have been hit hard by covid 19. Some are on the brink of bankruptcy…devpost.comVisit this idea’s GitHub Repo.Team Country: United StatesSecond Runner-Up, 3rd Place: US $1,500Help Me: A platform where you can ask for help. Your help request is displayed on the map, and anyone can see your request for help.Help MeGIFNear Me Write a post HelpMe is a platform where you can ask for help. Your help request is displayed on the map…devpost.comVisit this idea’s GitHub Repo, or try out the app here.Team Country: IndiaWinners of the Leonhard Euler Idea ContestMost Creative Ideas: US $500 eachIndustrial Pollution Analysis: GitHub Repo (India)Identify the possible industries responsible for the pollution in an area by linking them with their respective area’s air/water quality measures and disease cases, so that immediate actions can be taken on a root level with proof.Graphic Details: GitHub Repo (US)A graph-based IDE for coding that helps you visualize, build, and run Javascript Code. All code is broken down into functions, which become nodes. All dependencies between functions become links.reSKILL: GitLab Repo (India)An adult learning platform that would help students upskill/reskill by training industry-focused courses. The platform would also run recruitment drives that would help businesses, organizations, or industries hire people based on the skills acquired.Disaster Prevention Web: GitHub Repo (Malaysia)Gather real-time data from users and historical data to predict the occurrence of natural disasters and help mitigate the damage caused. Based on this information, the authorities of the responsible region will be able to send out warnings to the users in the affected area.NeoHistoricity: GitHub Repo (India)An app for researchers of history and other people to determine historicity. The app allows recording statements. Every statement is a record made by a person about themselves or about what another person said or did. Statements are either made in meetings or in books authored by the person. We can then query statements based on filters.Rookie of the Idea Contest: Neo4j Swag BagTracing Patient Zero: GitHub Repo (India)A web application that helps to visualize how infections spread across people and identities who was the patient zero — the first person who got infected by disease.Congratulations to Our Winners — and Thank YouAltogether, a total of US $10,000 was awarded to eight winning teams.Thank you to the fantastic DevRel team here at Neo4j, our developer community, and participants for making this a contest experience that exceeded our expectations. We look forward to hosting more online and in-person idea contests and hackathons, and we can’t wait to see what you build next with the Neo4j Graph Data Platform.P.S. Winners will be contacted soon on how they will be receiving their prize money. For questions or concerns please reach out to — support_hackathon@neo4j.com;Jun 23, 2021;[]
https://medium.com/neo4j/new-neo4j-4-0-features-copy-a-database-and-more-c51d1744a7e3;Ljubica LazarevicFollowMay 15, 2020·3 min readNew Neo4j 4.0 features — copy a database (and more)Fine-grained control in database replication, compaction, reorganisationMarkus Spiske on unsplashThere have been a number of big new features that have been introduced into Neo4j 4.0 — multi database, fabric, fine-grained security… the list is long.I thought I’d take the opportunity to write some short posts about some of the other features that have also snuck in. In this post we’re looking at copying a database.ThenBack in the good old days the process to copy a Neo4j database could invariably involve using store-utils, a plugin developed by Michael Hunger.This is a very powerful tool. Not only does it enable you to copy your database, but you could also reduce its size (which was growing through continuous writes and deletes) through ID reuse and reorganisation of relationships and properties. It defragmented and co-located nodes and relationships. And it was able to skip defect records in case you somehow got inconsistencies in your store. Store-utils also allowed you to skip nodes with certain labels or skip certain labels, properties and relationship-types entirely during the process.Who can resist the therapeutic blinking of block swaps?Like with all powerful tools, store-utils required careful handling. Apart from considerations around what version of the database you were using, you’d also have to think about repopulating and rebuilding indexes.Now with 4.0As part of the replacement to the store-utils, you can now run neo4j-admin copy. As well as enabling you to copy a database to a new destination, it also has a number of fine-grained options to configure how the copy is created, including:Filtering out nodes with a specific labelIgnoring certain labelsIgnoring specified relationship typesSkipping over property keysThis brings a whole suite of flexible options when it comes to replicating, duplicating and sampling existing databases. For example, you can think about generating sub graphs (for Neo4j fabric sharding) or graph samples for distributing across different databases — an extremely powerful feature.This version also implemented several of the changes that were prototyped in store-utils but never released, like parallel execution for reads and writes using the same fast, parallel ingestion pipeline that the neo4j-admin import -mode csv uses.The latest version of Neo4j also brings with it automated ID reuse as well to keep the database as compact as possible. Now would be a good time to remind you not to depend on internal IDs for your external processes and users!ExampleLet’s have a quick look at an example. Imagine we have a database, with the default name neo4j, containing data with node categories of Movie and Person (much like the :play movie-graph dataset). We wish to make a copy of this database, but we only want the Person data, and we want to call this new database persons.We’ll be using neo4j-admin for this, which you will find in the bin folder. To to make the above copy of the data, we would do something like this:bin/neo4j-admin copy --from-database=neo4j --to-database=persons --delete-nodes-with-labels= Movie This will produce the persons database with just the Person nodes and any relationships that may interconnect them.Check out the documentation!Happy compacting.;May 15, 2020;[]
https://medium.com/neo4j/european-natural-gas-network-via-knowledge-graph-3c3decb5f2ec;Ali Emre VarolFollowApr 26, 2022·12 min readExploring the European Natural Gas Network as a Knowledge GraphIn this blog post, we’ll use Neo4j to turn the European Gas Network into a knowledge graph to analyze the data.Photo by Rostislav Artov, UnsplashThe crisis between Ukraine and Russia caused relations between Russia and the E.U. to fall to the lowest point since the Cold War. The U.S. and E.U. imposed some sanctions on Russia over the Ukraine invasion. The financial measures are designed to damage Russia’s economy and penalize President Putin, his high-ranking officials, and the people who have benefited from his regime.Europe relies on Russia to keep warm, and Russia needs revenue from the gas trade — hence, both still need each other, despite the conflict. The Foreign Minister of Germany recently announced that Germany would stop all Russian oil imports by the end of 2022 [1].The main focus of this post is to turn the European Gas Network via Neo4j into a knowledge graph, and then explore and visualize it. If you’re a developer and are not familiar with Neo4j, you should start here to acclimate yourself. In short, Neo4j is one of the industry-standard graph databases that offers alternative solutions for developers. Products include Neo4j Desktop, AuraDB, AuraDS, Bloom, Graph Data Science, etc.Russia’s major gas pipelines to Europe (from Wikimedia)The above picture clearly shows Russia’s major gas pipelines to Europe. Russian natural gas arrives on the European continent via pipelines, and it makes up about a third of all gas used. Therefore, the natural gas of Russia plays a significant role in the energy mix of European nations. The below picture describes essential transportation routes in detail, and the legend of the image contains the number of elements for each component.Map of INET dataset (from elib.dlr.de)In this article, we will cover:Definitions of Components and Element StructureCreation of Knowledge GraphExploratory Data Analysis and Some QueriesVisualization via NeoDashDefinitions of Components and Element StructureYou can access the dataset that I will use to create a knowledge graph from this link. For simplicity, I shipped all the related data files to my GitHub account and used them while creating. To understand the fields inside the dataset, I will use the SciGRID_gas: The raw EMAP data set” report published by the DLR Institute for Networked Energy Systems.” I will be sharing the definitions in general terms in this post, so readers don’t have to read the mentioned report again.Gas transmission networks consist of different components, such as pipelines, compressors, LNGs, etc. With the help of the main report, let’s briefly describe these components.ComponentsNodes: In a gas network, gas flows from one point to another point, which are given through their coordinates. Elements of all other components (such as compressor stations and power plants) have an associated node, which allows for the geo-referencing of each element. Overall, the term Nodes will be used throughout this blog post, as it aligns with graph theory aspects.PipeLines: PipeLines allow for the transmission of gas from one node to another. PipeLines are georeferenced by an ordered list of nodes.PipeSegments: PipeSegments are almost identical to PipeLines — however, they are only allowed to connect two nodes. Hence, any PipeLines element (with three or more nodes) can easily be converted into multiple PipeSegments elements.Compressors: Compressors represent compressor stations, which increases the pressure of the gas, and thus allows the gas to flow from one node to another node. A gas compressor station contains several gas compressor units (turbines).LNGs: LNGs is the acronym for Liquefied natural gas. There are several LNG terminals and LNG storage in Europe, as some gas gets transported to Europe via ships.Storages: Storages are another network component. Surplus gas can be stored underground (e.g. in old gas fields or salt caverns) and used during low supply or high demand periods.Consumers: Consumers is the term used for gas users, which can include households, industries, and commercial uses. This data set will be generated through a master project, and it excludes power plants.PowerPlants: PowerPlants is the term for gas used by power plants only.Productions: These can be wells inside a country where gas is pumped out of the ground. Most of the gas used in Europe comes from outside of the EU. However, there are several smaller gas production sites scattered throughout Europe.BorderPoints: BorderPoints are facilities at borders between countries, which are mainly used to meter the gas flow from one country to another.Element StructureAs mentioned above, elements are describing individual facilities, such as compressors or LNG terminals. However, the overall structure of those elements is the same for all elements of all components, and is described as follows:id: A string that is the ID of the element, and must be unique.name: A string that is the name of the facility, such as Compressor Radeland.” In most cases this is not supplied.source_id: A list of strings that are the data sources of the element. As several elements from different sources could have been combined into a single element, one might need to know the original data sources.node_id: The ID of a geo-referenced node to which an element of the network is associated to. For a compressor, this will be just a single node_id. However, for a gas pipeline this entry would be a list of at least two node_id values: the starts node id and the end node id.lat: The latitude value of an element. For elements of type PipeLines and PipeSegments, lat is a list of latitude values. Throughout the SciGRID_gas project, the projection World Geodetic system 1984 (epsg:4326) will be used.long: The longitude analog to lat.country_code: A string indicating the two-digit ISO country code (Alpha-2 code, see Chapter 10.6 for list of countries and their codes) of the associated node of elements or list of nodes in case of PipeLines or PipeSegments.comment: An arbitrary comment that is associated with the element. In most cases this is not supplied.tags: This dictionary is reserved for OpenStreetMap data. It contains all associated key:value-pairs of an OpenStreetMap item.Creating the Gas Network Knowledge GraphBefore running this section, you need to create a Neo4j sandbox to run the codes in the browser or communicate via notebook. First, we will define the constraints, and then we will jump into creating the components. I will create all the components whether or not they are part of the pipeline, since I will use all of them for visualization purposes.On the other hand, you can find all the code snippets of all components in the notebook, but only border points” will be here as an example of not being a part of the pipelines.Creating constraintsBorder points:Now, we will create the nodes” as the junction points” of the pipelines. Then we will connect them to form the pipelines.Nodes:Creation of nodesForming the pipelines by connecting the existing nodesExploratory Data Analysis (EDA) and Graph Data Science (GDS)In the EDA section, the first question could be How many nodes does KG consist of and what are their types?” The second would be the same question for relationships. For this example, the relationship is not meaningful since there is only one relationship among the nodes, but I want to cover this part to share the related code snippets.For nodes:Node distribution queryNode distributionFor relationships:Relationship queryRelationship distributionYou can also use the following meta stats” code snippet to get the same picture as a different output format. You can also check the type and number of the nodes and relationships.meta stats” querymeta stats”Neo4j Graph Data Science (GDS)Neo4j released the 2.0 version of Graph Data Science on March 24, 2022. You can check out the release notes through the link below:Graph Data Science 2.0.0GDS 2.0.0 is compatible with Neo4j 4.3 and 4.4 but not Neo4j 3.5.x, 4.0, 4.1, or 4.2. For a 3.5 compatible release…neo4j.comI decided to utilize the GDS 2.0 for multiple graph-based analyses as a part of EDA, such as page ranking, degree centrality, etc. Upfront, we need to create a GDS object, and we will plug the graph-based queries into this instance and check out the results.Creation of GDS objectBefore jumping into the GDS algorithms, I want to share some details about the algorithm syntax execution modes: Once you have created a named graph projection, there are four different execution modes provided for each algorithm:stream: Returns the results of the algorithm as a stream of records without altering the databasewrite: Writes the results of the algorithm to the Neo4j database and returns a single record of summary statisticsmutate: Writes the results of the algorithm to the projected graph and produces a single form of summary statisticsstats: Returns a single record of summary statistics but does not write to either the Neo4j database or the projected graphIn addition to the above four modes, it is possible to use estimate to forecast how much memory a given algorithm will use.A special note on mutate mode: When it comes time for feature engineering, you will likely want to include some quantities calculated by GDS into your graph projection, and mutate takes its place on the scene. It does not change the database itself but writes the calculation results to each node within the projected graph for future calculations. This behavior is functional when using more complicated graph algorithms or pipelines. It’s beyond the scope of this blog but is covered in more detail in the API docs.PageRankThere are many ways to determine the centrality or importance, but one of the most popular ways is through the calculation of PageRank. PageRank (PR) algorithm measures the significance of each node within the graph. PR computes the ranking of the nodes based on the number of incoming relationships (links). Initially, it was designed to rank the web pages. Generally speaking, the underlying assumption is that a web page is only as important as the web pages that link to it.The related code snippet to get the top 10 nodes in the gas pipelines according to PageRank:and the result is:According to the results, the most critical node is node number 5305. It is worth noting that this id” is not the id we set when creating the nodes in our actual graph. id” shown in the results is the internal id space of each node set by the Neo4j engine during the creation period by default. So, how can we find this node in the graph?The most important node by PR algorithmTo alleviate that problem, we will slightly modify our GDS PR code. First, we will write the pagerank” of each node by the GDS algorithm, and then we will query them by the pagerank” value.GDS write page rank on nodesQuery of top 10 nodes by PageRankTop 10 nodes and corresponding maximum annual gas volume by PageRankAs a result, the most important junction points of the gas pipelines of Europe can be found by using the GDS Page Rank function, and we also showed their corresponding incoming max annual gas volume.Degree CentralityThe Degree Centrality algorithm measures the number of both incoming and outgoing relationships from a node to find popular nodes within a graph. For example, we can identify the most influential users on Twitter or help separate fraudsters from legitimate users of an online auction. Degree Centrality is an essential component of any attempt to determine the most critical nodes in a network. The core part of this algorithm is the orientation parameter that shapes its working principles. There are three types of orientation:UNDIRECTED : scores both incoming and outgoing relationshipsREVERSE : scores only incoming relationshipsNATURAL : scores only outgoing relationshipsI will use the UNDIRECTED orientation to count all incoming and outgoing relationships for this use case. If I was trying to detect the influencers on Twitter, I should use the REVERSE orientation to calculate the users’ followers.Betweenness CentralityThe Betweenness Centrality algorithm measures the centrality within a graph based on the shortest paths. According to graph theory, in a connected graph there exists at least one shortest path between nodes for every pair of nodes. The betweenness centrality for each node is the total number of these shortest paths that pass through the node.In other words, it is highly used to find the bridge nodes that connect one part of a graph to another. In network theory, the betweenness centrality algorithm applies to a wide range of problems related to social networks, biology, transportation, telecommunications, etc.For example, a node with higher Betweenness Centrality in a transportation network would have more control over the net because more items/passengers will pass through that node. The decision makers can utilize Betweenness Centrality scores to determine the hubs in a transportation network.Top 10 nodes by Betweenness Centrality ScoreCluster Detection Via Louvain ModularityThis method is used to find the communities/rings in large networks. It is one of the fastest modularity-based algorithms and works well in large networks. Modularity is a measure of how well groups have been partitioned into clusters. The Louvain algorithm recursively merges communities into a single node and executes the modularity clustering.I will use the pipes” projected graph created at the beginning of this section.Top three clusters by Louvain ModularityTo read the country codes of the clusters more efficiently, you can find the long form of the country codes for some countries below. Exploring this list, we can see that the most prominent community corresponding to junction points are Russia, the natural gas provider, and the closer countries to Russia, such as Estonia, Belarus, and Ukraine. The junction points in these countries play an essential role in distributing natural gas to Europe.EE: EstoniaBY: BelarusUA: UkraineRU: RussiaNL: NetherlandsBE: BelgiumXX: No country — under seaDE: GermanyAT: AustriaCH: SwitzerlandPathFindingLike all other graph algorithm categories we have explored, there are several alternatives for pathfinding. Predominantly, the goal of pathfinding algorithms is to find the shortest path between two or more nodes. In the case of our natural gas pipeline graph, the pathfinding algorithm would help us determine which junction point (node) would be required for the minimum overall distance.Dijkstra’s algorithm is one of the most common shortest path algorithms used. A* and Yen’s algorithm are the other alternative guns in the Neo4j GDS arsenal.Unlike the previous GDS examples, we will need a weighted graph projection, since Dijkstra’s algorithm supports weighted graphs with positive relationship weights, such as distance. It begins by finding the lowest weighted relationship from the source nodes to all reachable nodes. It then performs the same calculation from that node to all nodes connected to it, and so on. It always chooses the relationship with the lowest weight until the target node is reached.Shortest path between two nodesAs you can see in the above query, we specify a source and target node and use the relationshipWeightProperty of length_km. At this point, I arbitrarily chose two nodes — the source node is INET_N_856” and the target node is NutsCons_1003” — and ran the pathfinding algorithm. Many things are returned, including the total distance and a listing of the junction points along this path. In this case, we see that the shortest route is 18 hops long, and the total distance is minimized.Visualization Via NeoDashNeoDash is a graph app to build dashboards by Neo4j graphs in minutes. You can create multi-page visualizations using the graph database. There are multiple options to present the data, such as a map, table, bar chart, pie chart, graph, line chart, etc. It also allows setting a dynamic parameter that affects other visualizations. After creating the dashboard, you can save it to your graph database as a node and fire up it again. To learn more, check out the resources below.Building Interactive Neo4j Dashboards with NeoDash 1.1Introducing NeoDash 1.1: Interactive multi-page dashboards, Geographical map visualizations, and embeddable web-apps.medium.comNeoDash 2.0 — A Brand New Way to Visualize Neo4jVisualizing graph data is often key to the success of a Neo4j project. This post will go over the big changes in the…medium.comHere is one example of the dash created by using NeoDash.Snapshot from the dashboardMap representation of some pipelines in GermanyThe notebook we’ve worked through can be found here. I hope you fork it and modify it to meet your needs. Pull requests are always welcome!Thank you for reading! You can reach out me on LinkedIn, GitHub, and Twitter!Referenceshttps://www.forbes.com/sites/dereksaul/2022/04/20/germany-stopping-all-russian-oil-imports-by-end-of-2022-foreign-minister-says/?sh=549c582b32aahttps://neo4j.com/developer/get-started/https://elib.dlr.de/139217/1/scigrid_gas_EMAP.pdfhttps://zenodo.org/record/5079748#.YmZRbS8RqLehttps://neo4j.com/release-notes/gds/graph-data-science-2-0-0/https://www.brandwatch.com/blog/react-influential-men-and-women-2017/https://link.springer.com/chapter/10.1007/978-3-319-23461-8_11https://en.wikipedia.org/wiki/Dijkstras_algorithmhttps://medium.com/neo4j/building-interactive-neo4j-dashboards-with-neodash-1-1-2431489a864ehttps://medium.com/neo4j/neodash-2-0-a-brand-new-way-to-visualize-neo4j-ec8dee689e9b;Apr 26, 2022;[]
https://medium.com/neo4j/bamboohr-to-neo4j-integration-bf4de3938f87;Dana CanzanoFollowFeb 2, 2022·5 min readBambooHR to Neo4j IntegrationPhoto by Mufid Majnun on UnsplashBambooHR software collects and organizes all the information you gather throughout the employee life cycle, and then helps you use it to achieve great things. Whether you’re hiring, onboarding, preparing compensation, or building culture, BambooHR gives you the tools and insights to focus on your most important asset — your people.That company information is often just provided and shown as tables or individual records, but it inherently forms a graph — your organizational structure together with its locations, teams, departments, and more.BambooHR does provide a REST API as documented, which means we can use Neo4j Cypher along with APOC to extract data from Bamboo and load into a graph.The examples below will demonstrate how to extract the Employee records from BambooHR and effectively build the graph equivalent of an organization chart.And once it’s loaded, the data can either be visualized with Neo4j Browser or Neo4j Bloom. With Bloom — and specifically with its usage of Hierarchical Graph Layout — the result will be a traditional Organization Chart layout. It should be noted that BambooHR natively offers a similar visual Organization Chart layout. But in Neo4j you cannot just visualize the data in one way, you can also query, explore, and extend it in myriad ways.For a graph model we would use these nodesEmployeeLocationDepartmentAnd these relationships(Employee)-[:SUPERVISES]->(Employee)(Employee)-[:LIVES_IN]->(Location)(Employee)-[:WORKS_IN]->(Department)Here is a visual graph model with example dataCompany Graph ModelThe purpose of the examples below are not to just extract data from BambooHR but also to possibly build out relationships to other data from other sources (email, Slack, etc.).The example below was constructed using the following software:Neo4j version: 4.4.3APOC version: 4.4.0.1Bloom version: 2.0.0And for the most part, I suspect Cypher will work with prior Neo4j versions with little to no modifications.// create schema indexescreate constraint on (e:Employee) assert e.id is uniquecreate index on :Employee(name)create constraint on (l:Location) assert l.name is uniquecreate constraint on (d:Department) assert d.name is uniqueLoad the data from Bamboo and insert into the Graph Model as shown before. For each piece of data, it creates the nodes uniquely (using MERGE) and the relationships, too. The supervisor field from the API doesn’t contain an id but the display-name of the supervisor, so it might trip up if you have duplicate names.// consult employee directorywith ‘https://api.bamboohr.com/api/gateway.php/neo4j/v1/employees/directory as uri,‘Basic ODI4ZDZmYjViYjNlNmVkYzQ0YmZkNzEyM2Q2YTc0OTlkMzIwOng=’ as bamboo_identifierCALL apoc.load.jsonParams(uri, {Authorization: bamboo_identifier, Accept: ‘application/json’},null)YIELD valueUNWIND value.employees as rowMERGE (e:Employee {id:row.id})SET e.firstName=row.firstName, e.lastName=row.lastName,e.workEmail=row.workEmail, e.name=row.displayName,e.jobTitle=row.jobTitleMERGE (l:Location {name:row.location})MERGE (e)-[:LIVES_IN]->(l)MERGE (d:Department {name:row.department})MERGE (e)-[:WORKS_IN]->(d)MERGE (boss:Employee {name:row.supervisor})MERGE (boss)-[:SUPERVISES]->(e)If we want to, we can fetch additional information, e.g. the hire-date.// now GET each employeeMATCH (e:Employee) WHERE e.hireDate IS NULLWITH e,https://api.bamboohr.com/api/gateway.php/neo4j/v1/employees/ + e.id + ‘/?fields=hireDate’ AS uri,‘Basic ODI4ZDZmYjViYjNlNmVkYzQ0YmZkNzEyM2Q2YTc0OTlkMzIwOng=’ AS bamboo_identifierCALL apoc.load.jsonParams(uri,{Authorization: bamboo_identifier, Accept: ‘application/json’},null)YIELD valueset e.hireDate=value.hireDateNOTES:There are 2 references toBasic ODI4ZDZmYjViYjNlNmVkYzQ0YmZkNzEyM2Q2YTc0OTlkMzIwOng= AS bamboo_identifier represents my Bamboo API key. This key is generated from within BambooHR upon clicking on your Profile, in the upper right corner of the BambooH UI, and selecting API Keys.Using the generated key, you then need to base64 encode and append a :x to the key. For example, if the generated key was 828d6fb5bb3e6edc44bfd7123d6a7499d320 then you would need to run:echo -n 828d6fb5bb3e6edc44bfd7123d6a7499d320:x | base64And this would result in:ODI4ZDZmYjViYjNlNmVkYzQ0YmZkNzEyM2Q2YTc0OTlkMzIwOng=There are 2 references to neo4j https://api.bamboohr.com/api/gateway.php/neo4j/v1/employees/. That company name or id will need to be updated to the domain of your organization.Now that the data is loaded into Neo4j what types of queries might we want to run?How many people work in the customer support” department?MATCH (:Employee)-[:WORKS_IN]->(d:Department) WHERE d.name = ‘Customer Support’ RETURN count(*)Find all employees who are at most 2 relations from the Chief Executive Officer:MATCH path=(boss:Employee)-[:SUPERVISES*..2]->(e:Employee) WHERE n.jobTitle=’Chief Executive Officer’ RETURN pathShortest path between me and another employee:MATCH path=shortestPath(   (me:Employee)-[:SUPERVISES*..100]-(other:Employee) ) WHERE me.name = My Name AND other.name = Jane DoeRETURN pathIn these days of COVID, find all employees from a given location:MATCH (e:Employee)-[:LIVES_IN]->(l:Location) WHERE l.name STARTS WITH ‘San Mateo’ RETURN e.name, n.workEmailAnd to demonstrate how the data would appear visually in both Neo4j Browser and Neo4j Bloom — and when using a common dataset — I will effectively use the International Olympic Committee data. I’ve chosen to use this common dataset, as internal employee data cannot be shared.Org Chart IOCThis is what that org chart would look like in a default graph representation in:Neo4j Browser:and Neo4j Bloom:Hope that was helpful. If you are running BambooHR, try it out with your own company’s data and surprise your colleagues with some new insights or visualizations that you could create.;Feb 2, 2022;[]
https://medium.com/neo4j/gram-a-data-graph-format-a480a5d31a75;Andreas KolleggerFollowJan 25, 2021·3 min readGram: A Data Graph FormatGram is a textual format for data. We have CSV for tables, JSON for documents, and gram for data graphs.Use gram when [a,b,c] becomes (a)-->(b)<--(c).Why Another Data Format?Anything worth talking about is worth inventing a language for. — probably not L. WittgensteinGraphs are a powerful and increasingly popular way to think about and work with data. Graph technology includes databases, analytics libraries, visualization software, and a host of other tools.How do we use these things together? We need a common format to connect tools when performing the usual operations:import/export data filesend/receive network transmissionscopy & paste across applicationsWe all expect these actions to just work. It’s frustrating when any of them are not possible. Tools with limited interoperability feel locked-in and isolated. That’s not the graph way.Gram can make it possible for data graphs to be used in normal everyday work.Credit to XKCDData Graphs as Information ArchitectureWhile there are existing graph formats, gram is designed to complement the Property Graph Model defined by ISO GQL (Graph Query Language).Gram is interested in information architectures. Data graphs are the means, not the goal. Rather than talking about graphs, gram speaks in graph to talk about other things.Some motivating examples include:Association class: How do we describe the relationship between two things? Not just that they are connected, but how they are connected.Decorator pattern: Can multiple applications share data while reserving space for their own use?U.S. Route 66: How can we represent path membership and also information about the path itself?Bill of materials with finished goods: How do we describe both the plan to build stuff and the stuff that has been built?Gram attempts to address these scenarios while being friendly to read and write by adopting a path-based representation. Paths all the way down, or all the way up with path composition.Gram is intuitively simple, inspired by Cypher path expressions.Gram FormatInformation starts with a familiar looking data record of nested properties:({ name:”Andreas Kollegger”,  address: {   city:”Malmö”, country:”Sweden” }})The extra set of parenthesis wrapping the record provides space for an identifier and a set of labels:(a:Person:Author {     name:”Andreas Kollegger”,     address: { city:”Malmö”, country:”Sweden” }})Data records can be composed into information structures using relationships:(a)-[:Wrote]->(b:Blog {title:”Gram: a data graph format”})We can complete this scene with another related record:(b:Blog)<-[:Read]-(c:Person)Putting it all together we could write:(a:Person)-[:Wrote]->(b:Blog)<-[:Read]-(c:Person)(a:Author {     name:”Andreas Kollegger”,     address: { city:”Malmö”, country:”Sweden” }})(b {title:”Gram: a data graph format”})(c {when:date`2021-01-21`})Nice. The first path is readable as a sentence that a Person Wrote a Blog that was Read by another Person.” Notice in this rewrite we accumulated information — each of the as, bs and cs are the same records by id.Like writing prose, you don’t have to say everything at once. Paths appear in an ordered stream that merges forward into a final result. In fact, because any path or sequence of paths forms a valid result, you can time-travel or page through, or slide a window along a graph.Use Gram for Data GraphsGram is:friendly to read and writecomposable, sliceable, pageable and streamablevendor-neutrala work-in-progress :)There is a reference implementation called gram-js (with data format model and parser) which you can see in action along with d3.js in a collection of ObservableHQ notebooks.Next up is integration with other Javascript libraries and frameworks and conversion to/from other formats.Gram of GraphsGram is a textual format for data graphs, easy as (a)-->(b)<--(c) / An Observable collection by Andreas Kolleggerobservablehq.comgram-data/gram-js(text)-->(ast)-->(text) gram.js is a collection of packages for working with the gram textual format for graph data…github.comCheck it out and let me know what you think!;Jan 25, 2021;[]
https://medium.com/neo4j/cypher-query-optimisations-fe0539ce2e5c;Kees VegterFollowNov 28, 2018·9 min readCypher Query OptimisationsMake your slow cypher queries running fastIn this post I will explain about common causes of query performance degradation in the Neo4j server. This is a follow up on the Meet the Query Log Analyzer story earlier this week.Query log Analyzer App in the Neo4j DesktopThe Query Log Analyzer App helps you to find quickly the slow queries on the Neo4j server.Important Things To Remember About Cypher ExecutionCaching and Disk IOThe Neo4j database maps the data files from disk to the Page Cache. When you do a query and the data is not in Cache the data will be loaded from disk. The second time the query is executed the data can be read from Cache, which will result in a faster query. Any query that needs the same data will now benefit from having the data in cache. If possible make sure that your Page Cache can contain the database.Query PlanningWhen the Cypher engine receives a query string it compiles an execution plan for it. This execution plan is stored in the Query Cache and can be reused when the same query is fired again. The plan will be removed from that cache if the cache exceeds its limits or the data in the database has changed too much, so that estimates made during planning don’t hold true anymore. Using parameters instead of literal values will allow use of that cache, otherwise the query has to be re-parsed time and again.First Query ExecutionRunning a query for the first time, will always be slower than running the query for the second time due to cache misses, and query planning as described above.Checking the Query PlanYou can check queries by prefixing the query with the EXPLAIN or PROFILE keyword. When you find a slow query you can profile/explain a query by copying the query and the eventual parameters from the Query Log tab to the Neo4j Browser and use explain/profile to check how the query will be/is executed.Query ChecksIn the following sections we handle common causes which slow down the query performance. Note that there can be multiple causes which slow down the query performance, and even good queries can slow down because bad queries taking all the system resources.Query ParametersAre query parameters used for the queries where they should be? When you do not use query parameters every time a value changes the database will make a new plan for the query.Example queries without parameters, each time a new plan is generated:MATCH …. WHERE n.name = John RETURN …MATCH …. WHERE n.name = Ellis RETURN …`Example queries with parameters, the same plan is used again:Parameters: { name :  John  }MATCH …. WHERE n.name = $name RETURN …Parameters: { name :  Ellis  }MATCH …. WHERE n.name = $name RETURN …Index UsageWhen a query is unexpectedly slow it may be caused by a typo in the query which make the query planner decide to not use an Index. Use explain in the Neo4j browser to see if all the correct indexes are used in the query and change the query if needed.Big Result Sets (and slow networks)When your query is returning a lot of data, for instance megabytes of data, then the query will take a longer time to finish. This will be worse when you have also a slow network connection between the client and the server or the client is not consuming the results quickly enough.These may be valid queries, however returning a lot of data in one query triggers me always to do a functional check on why you need these big result sets. Maybe a functional redesign is needed.Note that when you load a lot of data to populate a cache in your application layer, you may do things ‘double’. The database is also caching the data.LockingThe database places lock’s on nodes and relationships when you change Node values (lock on Node) or add and remove relationships (lock on start and end Node and Relationships) or change Relationship values (lock on Relationship). The Lock will be released when the transaction is committed or rolled back.When at the same time multiple write queries are executed on the database and they are writing on the same Node/Relationship structure than the writes have to wait for the other writes to complete.This is desired behaviour of a database but how can we optimise the writes to avoid unnecessary lock’s.An example model can be as follows:Assume that in your application you want to add 50 new persons to a specific Group. You will then fire 50 calls to the database to add each person to that specific group in the same time frame (In this example calls as are send parallel to the database each having their own transaction). The first call will place a lock, and while the first call is being executed the other calls have to wait until the lock is removed. Then you will get the following pattern (the green is executing the query, with a lock, blocking the other queries):This can be avoided to send a list with 50 Persons in a parameter in one cypher statement with the use of UNWIND.Example (parameters: personUuid list, groupUuid)MATCH (g:Group { uuid: $groupUuid })UNWIND $personUuidList as personUuidMATCH (p:Person { uuid : personUuid })MERGE (p)-[:IS_MEMBER]->(g)This will be only one transaction on the server to process all the 50 members. Which is fast, and it has not to wait for locks.With the Query Log Analyzer you can find this scenario by using the ‘Highlight’ function on a query. This will mark the lines in the query log file yellow for this query, and they tend to form ‘yellow’ blocks, with almost the same log time.Query LoadBesides Locking, queries may also be slow because the query has to wait for cpu-resources for execution because the server has too many queries to process at the same time.With the Query Log Analyzer you can use the ‘Highlight’ function and inspect the query log lines around the same log date time. Another option can be the Timeline which shows all the queries in the log file over time.The amount of queries which can be handled sufficiently on a server is dependent on the single query execution time and the functional requirements. When you have a lot long running queries than the amount of queries per second the server can handle will be lower. Therefore it is very important that all the queries in the application are tuned to be as fast as possible, and that the server has enough capacity to handle the queries.Tuning QueriesQueries may be slow because the cypher statement is not optimal. Therefore it is important to analyse the query log when you develop your application. Then in an early state you can find inefficient queries. The goal of query tuning is to get the lowest possible amount of db hits. In the following non-exhaustive list I mention some tips to improve your queries:Errors in variable namesWhen you have an error in the variable names in the cypher statement, you can have unexpected results. If there is in the query a reference to a variable which is not initialised like ‘…MATCH (a)-[:’ or ‘…MERGE (a)-[:’ than this may result in a full node scan, which can slow down your query a lot. Use explain to check the query plan for your query.Missing colon ‘:’ for a Label or Relationship TypeThis is easy to check, if you miss the colon then the label or relationship type will be seen as a variable name, which results in full node scans etc. as described in the previous section, e.g. MATCH (Person) ... instead of MATCH (:Person) ... .Try to reduce the query working set as soon as possibleThere is a lot to say about this depending on the query. In general you can ask yourself the following things:Can I move a distinct to an earlier point in the query?Can I move a limit to an earlier point in the query?Can I use collect on places in the query to reduce the amount of rows to be processed during execution of the query?Do I use order by on the right place in the query?Multiple UNWIND statements Multiple UNWIND statements after each other will lead to cartesian products of the contents of the arrays you are unwinding. This is shown in the cypher statement below.Multiple OPTIONAL MATCH statementsOPTIONAL MATCH is a power full possibility in Cypher, however it should be handled with care. When there are multiple OPTIONAL MATCH statements in one query then there may be a cartesian product which gives the database a lot of work. Assume we have the following model and cypher statement:MATCH (a)OPTIONAL MATCH (a)->(b)->(c )OPTIONAL MATCH (a)->(d)->(e)OPTIONAL MATCH (a)->(f)...Each OPTIONAL MATCH results in a stream of paths. So when for one (a) the (a)->(b)->(c ) path produced 10 entries, the (a)->(d)->(e) path produces 10 entries and the (a)->(f) path produces 10 entries this will result in 10 * 10 * 10 = 1000 intermediate rows per (a) node.Depending on the requirements there are several ways to handle this properly:Use WITH and COLLECT and DISTINCT to reduce the intermediate resultsMATCH (a) OPTIONAL MATCH (a)->(b)->(c ) WITH a, collect(DISTINCT b) as bb, collect(DISTINCT c) as cc OPTIONAL MATCH (a)->(d)->(e) WITH a, bb, cc,  collect(DISTINCT d) as dd,  collect(DISTINCT e) as ee ...Use Pattern ComprehensionWhen a nested structure needs to be returned then you should use Pattern Comprehension (3.2.11.3) where you can ‘execute’ cypher patterns to build up the a tree after the RETURN statement. This is very fast:MATCH (a) RETURN      { a:a,       blist : [ (a)-->(b) | {b:b, clist : [(b)-->(c) | c ]],       dlist : [ (a)-->(d) | {d:d, elist : [(d)-->(e) | e ]],       flist : [ (a)-->(f) | f]}Instead of returning the full nodes, you can also use map-projections to only return a subset of properties: RETURN a { .name, b: b { .*, c: [(b)-->(c) | c ]}}DISTINCT and Returning tree structuresWhen you return properties or create a tree structure with Pattern Comprehension then you should not use that in combination with DISTINCT or aggregation like this:MATCH (a:Label)...RETURN DISTINCT a { and build up your nested tree structure with Pattern Comprehension and Map Projections}It is much more efficient for the database to do the DISTINCT operation on a node or a simple variable before building the JSON structure:MATCH (a:Label)...WITH DISTINCT aRETURN a { ... }LinksIf you have questions regarding the query performance, you can always head to the #help-cypher channel on the Neo4j Users Slack or on the neo4j community.;Nov 28, 2018;[]
https://medium.com/neo4j/create-a-data-marvel-develop-a-full-stack-application-with-spring-and-neo4j-part-2-12186b929cb2;Jennifer ReifFollowDec 5, 2018·8 min readCreate a Data Marvel: Develop a Full-Stack Application with Spring and Neo4j — Part 2*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)Last week, we covered the start of this project — the background, the API evaluation process, and the data model creation. If you haven’t and want to catch up on that information, you can read the post for Part 1.This week, I want to talk about how we actually imported the data. Data is extremely messy, so I will admit that these steps in the process took the longest to wade through and finally come up with import statements that worked and produced clean, unified data to match our data model (image below for review).Marvel comics data model in Neo4jData ImportNow that we created a sensible data model, we needed to import the data from the API into Neo4j to see if the model actually worked or if we needed to adjust it. Often, you will not know for certain if a data model works until you start reviewing the real-world data in that model. This is where Neo4j’s flexible data modeling is really valuable, allowing us to create and tweak the model, as well as refactor the data to match any model changes.For our project, the data resided in an API, and we needed to call the endpoints to retrieve segments and insert them into our Neo4j model. Enter API difficulties!Some public APIs will limit the number of calls a user can make per day in order to ensure the data is accessible and service remains stable for everyone. Marvel is no exception to this rule. They limit each user to 3,000 calls per day. This number may not seem small, but when you look at the number of comics in the system (around 43,122) and each of the other entities attached to those, that number quickly depletes!So, we needed to be a bit more creative in our approach to pulling data. We could look at comics starting with each letter of the alphabet. That would mean 26 calls (26 letters in English alphabet) to retrieve them in batches. However, when we ran some testing in Marvel’s interactive documentation, we found that each call would only return a maximum of 100 results, and there was no way to page through for more.Since there are over 43,000 comics in the database, divided by 26 letters in the alphabet, there would be massively more than 100 results — especially for letters like T” (for comics starting with ‘the’) and other common ones. Pulling by comic was not going to work.Goal: we are trying to get maximum information within 100 results and under 3,000 call limit!What if we pulled by comic character? There would be fewer characters than comics, since a single character often has many comics associated with them. There would definitely be less than 100 superheroes for many letters of the alphabet. We checked each letter to see how many total results it found, and there were only a couple of letters that found more than 100 characters (M” and S”, to be specific). This gets us as close as we can to everything!Retrieving characters wouldn’t allow us to retrieve information for other entities, but it got our initial group for retrieving the rest of the data. Great! Now what do we use to get the data from one source to another?Tools for Neo4j ImportThere are a variety of tools available for all kinds of import scenarios. One of the best options is to use the APOC (Awesome Procedures on Cypher) standard library that includes utility procedures and functions for various capabilities. For this API import, we used apoc.load.json procedure, which reads the JSON-formatted responses from the API and uses Cypher statements you write for specifying how you want to insert or update the data in Neo4j.Because APOC is an extension library, you can either download the GitHub project or install it within the Neo4j Desktop application and use it just as you would any other Cypher procedure or function. For this scenario, we opened Neo4j Browser and read some sample data from the API to test the results. Then, we added Cypher statements to craft the data into the shape of our graph model.Let’s Import Characters!Before we start importing data, though, we want to set our API keys (private and public key for Marvel API) as parameters, so that we can use variables in our calls and not copy/paste the key each time or accidentally expose the keys to others reviewing our code. This is easy enough in Neo4j. Simply call the params keyword and type in the parameter name you want and the value you want it to be. The statement is shown below.:params {marvel_public:<your public API key here>,         marvel_private:<your private API key here>}* Tip: if you are calling the same API endpoint often, you could also set up a URL parameter and reference that variable in any Cypher statements. It didn’t make sense in our case because we call various endpoints to retrieve different pieces of information, so the URL changes.Finally, we are ready to start loading the first bit of data for the comic characters! The entire statement is in the code block below. We will walk through each section of this statement in the next paragraph.WITH apoc.date.format(timestamp(), ms”, ‘yyyyMMddHHmmss’) AS tsWITH &ts=” + ts + &apikey=” + $marvel_public +      &hash=” + apoc.util.md5([ts,$marvel_private,$marvel_public]) as suffixCALL apoc.periodic.iterate(‘UNWIND split(ABCDEFGHIJKLMNOPQRSTUVWXYZ”,””) AS letter RETURN letter’,‘CALL apoc.load.json(http://gateway.marvel.com/v1/public/characters?nameStartsWith=+letter+ &orderBy=name&limit=100 +$suffix)YIELD valueUNWIND value.data.results as resultsWITH results, results.comics.available AS comicsWHERE comics > 0MERGE (char:Character {id: results.id})ON CREATE SET char.name = results.name,   char.description = results.description,   char.thumbnail = results.thumbnail.path +                     ”.” + results.thumbnail.extension,   char.resourceURI = results.resourceURI’,{batchSize: 1, iterateList:false, params:{suffix:suffix}})The first line of the code above starts by setting a couple of parameters for the timestamp and some required tokens for the end of the URL string. The WITH keyword allows you to pass along results or values from one part of a query to the next. In this case, we are setting these parameters and passing those to the main part of the query in the next couple of lines.The next section of code calls the APOC procedure apoc.periodic.iterate. This was suggested by my colleague to list all of the letters of the alphabet and let the procedure loop through them automatically (rather than executing a statement for each letter manually). The procedures takes the first statement within the parentheses and, for each result returned from that statement, executes the second statement. In this case, it unwinds all the letters in the alphabet (first statement: UNWIND...RETURN letter) and retrieves data from the API for each letter (second statement: CALL apoc.load.json...).Let’s dive into a bit more detail about the apoc.load.json and what that bit of code is doing. First, the procedure expects a few parts to its structure. We start with the API endpoint url we need for characters and include a few url parameters Marvel needs to retrieve the data.The nameStartsWith= url parameter allows us to insert the starting letter from our previous statement loop and retrieve characters who have a name starting with that letter. We also sort the characters in alphabetical order and include the limit of 100. The API defaults to 20 in the return results, so we want to ensure it pulls the maximum number and not the default.The YIELD value syntax brings back the JSON object from the called endpoint. Now, we need to sift through the object and insert the data to Neo4j. The Marvel API result gives us some high-level details (number retrieved, call status, etc) and nests the character data under a results[] section, so the next line unwinds that object and navigates the nested structure to get the subsection that has the character data.Once we have gotten here, we pass that object (using WITH) to the WHERE criteria to check if the characters have any comics or not. Our goal is to look at Marvel’s comics and how the comics relate to other types of entities. If a character does not have any comics, then it will not have any relationships to other entities, and therefore is not meaningful data for this project. For our project, we only cared about the characters who have comics because we wanted to focus on the relationships between entities.The next section of code actually inserts the data into Neo4j using Cypher statements. MERGE does a select-or-insert function where it checks if the node exists in the database before trying to create it. It looks for a node of type Character based on the id coming from the API. If the node exists, it just returns it. If the node does not exist, it creates the new node and sets any properties. ON CREATE SET only executes if the node does not already exist and adds the properties to a newly-created node. This ensures no duplicates get inserted for characters.Finally, the last code line has some config options you can set for the apoc.periodic.iterate procedure we used earlier in the statement. You can check out the configuration options in the APOC documentation. The configuration I want to point out is the params one. This simply allows the procedure to use the parameter set outside the procedure call (suffix was set outside the CALL apoc.load.json()).* Tip: If you forget to add parameters to the params config that you want the internal statement to use, you will see an error.Running that statement will insert all the characters! If you run a couple of quick queries like the ones below, you can verify that data was inserted and that the values look good and the translation worked.MATCH (n:Character)RETURN n LIMIT 25MATCH (n:Character)RETURN count(n)Ok, everything looks good, and we now have plenty of characters to work with!What I LearnedAfter quite a bit of research and query tests, the query given and explained above is what we used to get the first round of character data from a finicky API hosted by Marvel into our local instance of Neo4j as a graph data model. Below are my key takeaways from this part of the data import process.It took a lot of time to maneuver through the Marvel Developer Portal restrictions on their API and find the best approach to gathering as much data as possible within the bounds.My Cypher improved a LOT from seeing the data go from one source to another and finding where I hadn’t expressed the Cypher correctly.You need a practical example to apply what you have learned. Even finding something small and simple (which this was not) will allow you to experiment with a data set hands-on and gain deeper understanding.Next StepsIn the next post, we will cover the remaining steps to import the rest of the Marvel data set and show the fully-populated database. Things get even more fun once we have all the data to play with. Stay tuned for more info!ResourcesFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifNeo4j Data ImportAPOC standard libraryDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j Guide;Dec 5, 2018;[]
https://medium.com/neo4j/graph-data-modeling-all-about-relationships-5060e46820ce;David AllenFollowOct 23, 2019·10 min readGraph Data Modeling: All About RelationshipsIn my last article on graph data modeling, we talked about categorical variables, and how to choose whether to model something as a node, property, or label.With that out of the way, it’s now time to go deep on relationships. In this article, let’s go deeper on what relationships are, what they mean in a domain, and how to use them.In this article, we’re going to take an example of a social network site, because it makes an easy to understand graph. People create friend” relationships among their accounts, people post” content, and like” content.Relationships are verbs!The simplest way is to think about relationships is to just write declarative sentences about our domain. Write some true facts, and isolate the nouns” (in bold) and verbs” (in italics). Example:A person posts an articleA person friends another personA person has an interestIn this simplified view of the domain, all of your nouns are nodes, and all of your relationships are verbs. The relationship type is the singular form of the verb. And so this implies a graph that looks like (:Person)-[:POSTS]->(:Article) and so on.If you’ve ever seen RDF, this approach should strike you as pretty similar to the RDF triple concept of subject-predicate-object, where the relationship is basically the predicate of the sentence. Grammatically, the first noun in the sentence is the subject, and the second noun is the object. So boiling your data model down into these sets of 3 elements is a common graph approach (in RDF-land they actually call them triples!)Under the simple explanation, the task is to decompose your domain into a large batch of simple declarative sentences. This gives you a pile of nodes and relationships to work with. You then have most of your model, and mostly have to make naming decisions.This is nice…but also too simple. Let’s go deeper and talk about what relationships do in a model, and what they really mean aside from the simple comparison to verbs in a sentence.Relationships normalize dataData Normalization is often a topic that’s discussed for relational databases, but it applies to graphs as well. What’s data normalization all about?The goal of data normalization is to reduce and even eliminate data redundancy, an important consideration for application developers because it is incredibly difficult to stores objects in a relational database that maintains the same information in several places. ReferenceWhen we factor a piece of data (let’s say a Person’s primary interest) and put it into a separate node, linked by a relationship, one of the things we’re implicitly doing is storing that interest once, instead of with every single node. Here’s a simple example.Denormalized:CREATE (p1:Person { name:  David , interest: [ Guitar ] })CREATE (p2:Person { name:  Sarah , interest: [ Guitar ] })Normalized:CREATE (s:Interest { name:  Guitar  })CREATE (p1:Person { name:  David  })CREATE (p1)-[:HAS]->(s)CREATE (p2:Person { name:  Sarah  })CREATE (p2)-[:HAS]->(s)Notice that in the normalized example, Guitar” is stored once, and in the denormalized example, it’s stored twice. The fact that we could link the Person nodes to the separate Interest allowed us to normalize our data.Going back to that data normalization article, the advantages of doing it this way are higher cohesion, and less data duplication. What if we wanted to change the name of the interest from Guitar” to Six String Guitar”? With the normalized example, we only have to update one string and we’re done. In the denormalized example, we might have to update millions of nodes! That’s where low cohesion and duplication starts to hurt.How do you know if your data is normalized enough”? How far should you go with this? This raises the topic of Normal Forms” which is always taught in data modeling for relational databases. Just as traditional relational databases have 1st, 2nd, 3rd, things like Boyce-Codd Normal Form, graph databases can do the same.The staircase of normal formsAll of the theory that applies here translates directly to graphs, but is usually written in other sources as being about tables. So here in this image we see it talking about columns” but if we translate that to node property” we’re pretty much still good. The full translation of how to apply normal forms to graph data modeling will have to wait for another article, but you should check out these techniques, they have direct relevance to graphs. The definitions though of what it means for one property to depend” on another and so forth though can be carried over directly.Know your relationships’ semanticsWhen we think about what relationships actually mean in a model, there are several different kinds. When modeling it’s useful to know which of these is your desired relationship, because it constrains how many instances of the relationship can exist, and what it actually means in the real world.A relationship has a domain (the thing it’s coming from) and the range (the thing it’s going to). We think of a relationship as like a function that maps one node to another.Tip: for each relationship in your domain, figure out what kind of relationship it is in the real world, because it will help you and your model users understand what they can and can’t do with this data.HAS A”: this expresses a part/whole relationship, otherwise known as composition”. For example in our social network, HAS_INTEREST is a HAS A” relationship, because people may have many. If what you’re trying to do is create a bag of items” you’re probably dealing with a HAS A relationship.IS A”: this expresses an inheritance relationship between a parent and a child. Believe it or not, these don’t come up that often in property graph modeling partially because they’re so easy to do with labels. For example I could have either (:Person:Employee { name:  John  }) or I could have (:Person { name:  John  })-[:IS_A]->(:Employee). The former comes up more often.Functional: the relationship acts like a true function, meaning that given a single domain, there can be only one range node. An example of a functional relationship is HAS_HOME_ADDRESS. You should probably have one of those, not 5. But a relationship like HAS_INTEREST clearly is not functional, because our users can have many different interests!Transitive: if the relationship is true from A to B, and from B to C, then it’s also true of A to C. An example of this would be IS_RELATED_TO. My grandfather is related to my father. My father is related to me. So IS_RELATED_TO is transitive, because my grandfather is related to me. Going back to our HAS_INTEREST relationship, this one isn’t transitive. I might be interested in guitar, but that doesn’t mean that it‘s interested in me!Reflexive: this one doesn’t come up as often in property graph modeling, but it means that the relationship implies every node has one of these to itself. For example (:Person)-[:KNOWS]->(:Person) is reflexive, because all people know themselves. It turns out that IS_RELATED_TO would be reflexive too! But HAS_HOME_ADDRESS clearly is not. Note that with reflexive relationships, the target label (:Person) is going to be the same as the source label…because it’s reflexive!Symmetric: if the relationship is true one way it’s true the other way too. Again, in the case of (:Person)-[:KNOWS]->(:Person), it’s symmetric because if A knows B, you can be pretty sure B knows A. But you can see how a relationship like HAS_HOME_ADDRESS is not symmetric, because an address can’t have a person as it’s home address, that would make no sense.Vanilla: I just made this term up. You heard it here first folks. But I’ll refer to any relationship that doesn’t have any of the above properties as vanilla”.In other graph modeling disciplines they might add others, such as asymmetric, irreflexive, and others, but we don’t typically need to consider these in property graph modeling because we can traverse relationships bi-directionally, and we’re not usually trying to use them for logical inference as in the RDF/OWL world.Tip: A good practice is to minimize your vanilla relationships. Knowing what other type applies is useful to the semantics of your model, and useful to application constraints, so ideally you’d like to have that with all of your relationships if possible. But sometimes it isn’t! So don’t be a purist about it. Rule of thumb: if you’re not sure, it’s vanilla.One thing that may have to wait for another article is how to decompose vanilla relationships into a collection of potential different or other relationships of the various types. It’s enough for now to hint that you can do this, and it’s an interesting exercise to see how you might in your model.Hint: You do it by refining what the vanilla relationship means and getting more specific. In a later section, we’ll give one concrete example of how reification can be used to refactor vanilla relationships.Deciding on relationship propertiesThese relationship types impact an important downstream consideration: whether to put properties on your relationship. When we assert properties about our relationship, we’re putting metadata on it.The most important things to know are:Most commonly, relationships don’t have properties at all.The next most likely scenario is that they only have administrative metadata, such as when the relationship was created, updated, who created the relationship, or a version” integer to let us version relationships. This administrative metadata” can be placed on any kind of relationship, because it’s generally divorced from the semantics of the relationship type, and so it can fit with any relationship type.The next likely scenario is that the relationship property will actually be path metadata, for example a weight or a distance.Some systems such as Neo4j at present cannot index relationship properties with the exeception of specialized full-text indexes. This means there’s a built in reason to avoid them when you can, because your queries won’t necessarily get faster or more selective by placing criteria on a path that hinge on a relationship property value.Drake knows his property graph data modelingBecause all of this, if you find yourself wishing you could put an index on a relationship property, you should most likely think about factoring that relationship out into a node. You are probably starting to think about this relationship as a first-class object in its own right, rather than just a relation of two other things.As always, maintain flexibility to do what’s right for your domain, but in general try to minimize properties on relationships unless you have a compelling reason you can clearly explain, and then that’s what they’re there for.Reifying relationshipsTo reify” means to take something abstract and make it concrete. When we think of relationships as objects themselves (for example, using a relationship as a bank transfer, smuggling a noun into a verb” so to speak), things start to get hard for the previous reasons. So the answer is to reify the relationship or turn it into a first-class node all its own, and then simplify.Reifying a relationship” is also sometimes referred to as making an intermediary node, or creating a hyper-edge”, where the node is standing in for a relationship that can itself have relationships.An example is a bank transfer:/* Very bad */CREATE (a1:Account)-[:TRANSFER {    id: 555,    amount: 123,    currency: USD,    bank: Wells Fargo,    time: 2pm}]->(a2:Account)Designs like this start out seeming like they make sense, because an account transfer is a flow of money between two accounts. But this is a very bad model, because:An account transfer is itself a thing in our domain with rich properties, not just a verb.It lacks the opportunity to normalize data. We can’t link to a separate pre-established :Bank or :Currency node, because relationships can’t have relationships!It’s going to paint us into a corner. When it comes time to denote that transaction 555 has been cancelled, how will we do that? It’s not extensible.The :TRANSFER relationship is vanilla. Does it have any of the properties above that we discussed? It’s not symmetric, functional, transitive, or reflexive. It’s also not HAS_A or IS_A! It’s a mess.You know have a bunch of unindexed properties, so looking up all USD-based transfers from Wells Fargo is going to be painful.Better is to reify the :TRANSFER relationship into a separate node:/* Much better */MATCH (wf:Bank { name:  Wells Fargo  })MATCH (c:Currency { name:  USD  })CREATE (a:Account)-[:INITIATES]->   (b:BankTransfer { id: 555, time: 2pm, amount: 123 })      -[:RECEIVES]->(b:Account)CREATE (b)-[:ORIGIN_BANK]->(wf)CREATE (b)-[:CURRENCY]->(c)Notice what happened here![:TRANSFER] was reified to :BankTransfer.Data normalization was improved by reusing Bank and Currency nodes, improving cohesion and reducing redundancy.Relationships got more numerous, but they all got simpler, and easier to explain: ORIGIN_BANK, CURRENCY are functional relationships!By redefining the TRANSFER relationship sides to INITIATES vs. RECEIVES, it clarifies the intent of what’s going on. RECEIVES becomes a functional relationship (only one account can receive a transfer) and INITIATES arguably becomes HAS A.Standard indexing applies, so looking up all USD-based transfers from Wells Fargo will be fast!Don’t let this be youConclusionIn this article we’ve covered data normalization, the connection between relationships and verbs, the different types of semantics a relationship can take on, and how to reify complicated relationships into more simple ones.In teaching articles, we’re always limited by needing to show simple examples the challenge for the modeler will be to apply the principles to your domain in real work, and there’s only one way to do that: practice.Happy graph hacking!This article is part of a series if you found it useful, consider reading the others on labels, relationships, super nodes, and categorical variables.;Oct 23, 2019;[]
https://medium.com/neo4j/healthcare-analytics-sandbox-load-and-analyze-fda-adverse-event-reporting-system-data-with-neo4j-29b7b71a6ef4;Chintan DesaiFollowAug 17, 2022·4 min readHealthcare Analytics Sandbox: Load and Analyze FDA Adverse Event Reporting System Data With Neo4jHealth care analytics is an analysis activity that can be undertaken as a result of data collected from four areas within healthcare:Claims and Cost DataPharmaceutical and Research and Development (R&D) DataClinical Data (collected from electronic medical records (EHRs))Patient Behavior and Sentiment Data (patient behaviors and preferences, (retail purchases — e.g. data captured in running stores).Health care analytics is a growing industry and is expected to grow to even more with time.The connected data capabilities of a graph database can help us achieve what is either impossible or complicated with the traditional relational databases, other NoSQL databases, or even big data solutions like Pig and Hive.Demo Use CaseI have developed a self-explanatory example use case to explain the capabilities of Graph Database in healthcare. In the demo guide, we are performing data ingestion and analytics of the FDA Adverse Event Reporting System Data.The FDA Adverse Event Reporting System (FAERS or AERS) is a computerized information database designed to support the U.S. Food and Drug Administration’s (FDA) post-marketing safety surveillance program for all approved drug and therapeutic biologic products.The FDA uses FAERS to monitor for new adverse events and medication errors that might occur with these products. It is a system that measures occasional harms from medications to ascertain whether the risk–benefit ratio is high enough to justify continued use of any drug and to identify correctable and preventable problems in health care delivery (such as the need for retraining to prevent prescribing errors).FDA Adverse Events Reporting System (FAERS) Public DashboardReporting of adverse events from the point of care is voluntary in the United States. The FDA receives some adverse event and medication error reports directly from health care professionals (such as physicians, pharmacists, nurses, and others) and consumers (such as patients, family members, lawyers, and others).Health professionals and consumers may also report these events to the products’ manufacturers. If a manufacturer receives an adverse event report, they are required to send the report to the FDA as specified by regulations.Data, Modeling, and Graph IngestionWe downloaded one of the publicly available FDA FAERS datasets, and massaged and articulated the demographics for the United States. FAERS data is traditional RDBMS-based tabular data. We translate it to a Graph-based data model.Data ModelNext, we perform data ingestion to prepare the FAERS graph and run a few example analytics queries to see the interesting output. Some interesting queries are:What are the top five drugs reported directly by consumers for the side effects?What top ten drug combinations have the most side effects when consumed together?What age group reported the highest side effects, and what are those side effects?What are the most common side effects reported in children and what drugs caused these side effects?You’ll notice these queries are truly analytical in nature — additionally, they cannot be easy to prepare and produce with a traditional RDBMS data and querying language. With Neo4j and the power of Cypher, this becomes extremely easy.Example AnalysisReadymade Neo4j Sandbox for FDA FAERS explorationWe have a pre-deployed Neo4j sandbox to walk you through this example of Healthcare Analytics. Neo4j Sandbox is a great — and free — online tool that lets you try Neo4j’s graph database without installing anything locally.Neo4j SandboxGet started with your own healthcare analytics sandbox.Full source code for this example and guide is available on GitHub.ReferencesNeo4j Life Sciences Use-CasesNeo4j Healthcare ProjectsHealthcare AnalyticsFDA Adverse Event Reporting System WikiFAERS DatasetsNeo4j Life Sciences Workshop;Aug 17, 2022;[]
https://medium.com/neo4j/cypher-sleuthing-dealing-with-dates-part-5-apoc-a556dd5fbda7;Jennifer ReifFollowSep 24, 2021·10 min readCypher Sleuthing: Dealing with Dates, Part 5 — APOCThis post takes a brief departure from Cypher temporals to the APOC library, where much of the temporal functionality for Neo4j beganFrom part 1 introducing Cypher dates and formats to part 4 where we combined duration functions and temporal components to translate amounts in one unit to another unit, we have covered a lot of ground in this series! Filling in the gaps, part 2 showed us how to truncate dates for searches and use durations for adding/subtracting amounts of time, and part 3 gave us an intro to temporal components and translations within component groups.This post will be the final in the series. We will take a brief departure from Cypher temporals to the APOC library, where much of the temporal functionality for Neo4j began (before it was available in Cypher). Many of the original procedures and functions in the APOC library still exist, along with new ones that are not yet implemented in Cypher. This post will cover all the APOC temporal functionality and note which ones have been replaced by Cypher or are still uniquely provided by APOC.APOC Temporal FunctionsIn our part 1 post, we covered 4 APOC functions that could format epoch time (to any string and to an ISO8601 string), translate various string formats, and convert a string to a Neo4j (ISO8601) temporal. In the remainder of this post, we will briefly cover the rest of the existing apoc.date.* and apoc.temporal.* functions to explain their purpose and note which ones are replaced by ones in Cypher.Let’s start our list! Ones that are crossed out were covered in the Part 1 post of this series.apoc.date.add()apoc.date.convert()a̶p̶o̶c̶.̶d̶a̶t̶e̶.̶c̶o̶n̶v̶e̶r̶t̶F̶o̶r̶m̶a̶t̶(̶)̶apoc.date.currentTimestamp()apoc.date.field()apoc.date.fields()a̶p̶o̶c̶.̶d̶a̶t̶e̶.̶f̶o̶r̶m̶a̶t̶(̶)̶apoc.date.fromISO8601()apoc.date.parse()apoc.date.parseAsZonedDateTime()apoc.date.systemTimezone()a̶p̶o̶c̶.̶d̶a̶t̶e̶.̶t̶o̶I̶S̶O̶8̶6̶0̶1̶(̶)̶apoc.date.toYears()apoc.temporal.format()apoc.temporal.formatDuration()a̶p̶o̶c̶.̶t̶e̶m̶p̶o̶r̶a̶l̶.̶t̶o̶Z̶o̶n̶e̶d̶T̶e̶m̶p̶o̶r̶a̶l̶(̶)̶apoc.date.add()This function adds or subtracts values from epoch time. Since we can work with epoch time in Cypher and add or subtract durations, then this functionality should all be available in Cypher. Here is an example.Example: Cypher vs APOC epoch subtract timeRETURN datetime({epochSeconds: 1626149356}) — duration(‘P1D’) as cypher,  apoc.date.add(1626149356,’s’,-1,’d’) as apocNotice that the return type for Cypher is still a temporal value, while the return type for the APOC procedure is an epoch time. That is the only difference between the two.apoc.date.convert()This function converts a time-based integer value from one format to another. For instance, we can convert 120 minutes to 2 hours. One difference to keep in mind is that Cypher’s version of using duration functions and components deals in duration syntax (strings with literal P and T), whereas APOC’s version deals entirely with integer math.Example: Cypher vs APOC convert unitsMATCH (c:Conference)RETURN duration.inSeconds(c.startDatetime, c.endDatetime).hours as cypher,  apoc.date.convert(c.endDatetime.epochSeconds-c.startDatetime.epochSeconds, ‘s’,’hours’) as apocThis example is a bit trickier. If you are dealing within Neo4j and Cypher temporals, then the Cypher functionality is a bit less verbose (even though I’m still using Cypher temporal components to get the datetimes for subtraction). However, if I’m coming from an external source and already have an epoch value, then the APOC function is actually far simpler. Unless I’m missing something, I don’t believe Cypher can take an epoch value and convert it from seconds to hours at all.So, whether you use Cypher or APOC here depends on your incoming value and desired output.apoc.date.currentTimestamp()This function might seem like the Cypher clock options might render APOC’s version obsolete. However, apoc.date.currentTimestamp() tracks the live time, which will update even inside of a transaction. Cypher’s currently will not.There are several good examples of this APOC function in the documentation, so I will allow you to explore those on your own.apoc.date.field()The apoc.date.field() function takes an Epoch time and retrieves the specified field. This can be replaced by Cypher using the epochMillis component to accept the input and using other components and/or functions to select the appropriate unit.It’s easier to follow using an example.Example: Cypher vs APOC select component from datetimeWITH datetime() as datetime, datetime().epochMillis as epochRETURN datetime().hour as cypher,  datetime({epochMillis: epoch}).hour as cypherEpoch,  apoc.date.field(epoch, ‘hours’) as apocEpochIn this example, I’m showing Cypher’s functionality with both a Neo4j datetime input, as well as an epoch time input. Cypher’s syntax (second value in the return) is slightly more verbose, but still simple to understand. For this function, I think I would lean toward using the built-in Cypher functionality unless there is an edge case that Cypher won’t accept and APOC will.apoc.date.fields()Similar to the previous function, apoc.date.fields() deals with components of the date, but it returns all of them as a map. While it is possible to do this in Cypher, it’s not built-in. Let’s take a look.Example: Cypher vs APOC list all date fields as mapWITH datetime() as dRETURN { year: d.year, month: d.month, day: d.day, hour: d.hour, minute: d.minute, second: d.second } as cypher,  apoc.date.fields(toString(d),”YYYY-MM-dd’T’HH:mm:ss.SSS’Z’”) as apocThe APOC function also takes a string value, which isn’t too difficult to convert with Cypher’s toString() function. However, if you already have a string value, remember that Cypher’s temporal functions (datetime(), date(), etc) will also accept strings, as long as they are in the ISO8601 format, so this would also be an easy conversion.apoc.date.fromISO8601()With this function, we can take an ISO8601 string and convert it to epoch time. We are also able to do this with the built-in Cypher functionality. Let’s see what that looks like.Example: Cypher vs APOC convert ISO8601 to epochWITH datetime() as datetimeRETURN datetime.epochMillis as cypher,  apoc.date.fromISO8601(toString(datetime)) as apocNote that the APOC function automatically converts to epoch with milliseconds, where we can actually control whether we want seconds or milliseconds with Cypher’s .epochMillis or .epochSeconds components.apoc.date.parse()This function takes a date string and converts it to a time unit — milliseconds, seconds, minutes, hours, or days. There isn’t currently a way to do this in Cypher, as the only way to convert from a date to a time unit is by calculating a duration between two dates and using the duration functions and components.Example: Cypher vs APOC convert date to timeWITH datetime() as dRETURN duration.inSeconds(datetime(‘1970–01–01T00:00:00.000Z’),d).hours as cypher,  apoc.date.parse(toString(d),’h’,”yyyy-MM-dd’T’HH:mm:ss.SSS’Z’”) as apocThis one was a bit cumbersome in Cypher. First, in order for me to translate to a different time unit, I had to use one of the duration functions (inSeconds()) to convert the years and years of time to something I could convert to hours (remember we have to use the function to get us to the proper component category from the left column in the Cypher manual table). Those functions require two arguments — a start time and an end time to calculate duration between. Since I know the APOC is converting to epoch time, then I can use the start of epoch time as the starting date (1970–01–01T00:00:00.000Z). Once that duration is converted to seconds, I can use the .hours component to convert the whole value to hours.Now, the APOC version is much simpler because it makes a couple of assumptions. First, since apoc.date.parse() converts a string to a time unit, the epoch start time is already used. Second, APOC does the conversion between larger durations behind the scenes, so I don’t need to specify component categories and such. I only need to specify the unit for output (hfor hours).To Cypher’s credit, it has to be flexible enough to handle a myriad of scenarios accurately with a few functions and components. However, if I was looking to make this specific kind of conversion, APOC is much simpler at this point in time.apoc.date.parseAsZonedDateTime()This is similar to the apoc.date.parse() in that it parses a string to another value, but the output is actually going straight to an ISO8601 temporal value. This means we can take any type of string and pass its format and timezone and get a temporal value in return! You might ask how this would be different than passing a date string to Cypher’s temporal instants (e.g. datetime(2021–07–19T09:45:00)), but I’ll show that in just a minute.Example: Cypher vs APOC convert string to ISO8601 temporalWITH 2021–07–19T09:45:00–06:00 as strDatetimeRETURN datetime(strDatetime) as cypher,  apoc.date.parseAsZonedDateTime(strDatetime,”yyyy-MM-dd’T’HH:mm:ss”,”-06:00 ) as apocThe results are different! This is because Cypher is making the assumption that the value you pass is the time for the timezone specified. The APOC function, however, is translating the value provided to the timezone specified, assuming that the value passed is UTC zone (+00:00).Cypher and APOC calculate the same results if you remove timezone entirely — from the input string, as well as the 3rd argument in the APOC function call.apoc.date.systemTimezone()This function returns the timezone of the server system, while Cypher’s pulls the database timezone, which is UTC by default. You can alter the database’s internal time with a configuration, if needed.Example: Cypher vs APOC system timezoneRETURN time().timezone as cypher,  apoc.date.systemTimezone() as apocUnless you were running this query in different systems across regions where all the configurations were still defaulted (you’d get all UTC for results), then the Cypher version is probably the better method.apoc.date.toYears()The apoc.date.toYears() function takes an epoch time and calculates the number of years since the start of epoch time (1970–01–01T00:00:00Z). There is currently no way to replicate this exactly in Cypher, as the APOC function returns a floating point number (precise calculation), while everything in Cypher converts to whole values only — no fractions. We can get close, though, so it depends on your use case. Let’s see it.Example: Cypher vs APOC convert epoch time to yearsWITH datetime().epochMillis AS datetimeRETURN duration.inMonths(datetime(1970–01–01T00:00:00.000Z),datetime()).years as cypher,  apoc.date.toYears(datetime) as apocapoc.temporal.format()Now that we’re in the apoc.temporal.* realm (rather than in apoc.date.*), we are dealing directly with temporal values as input and trying to get a different output. In the case of apoc.temporal.format(), we are trying to get a string in another format. There isn’t currently any way to replicate this in Cypher because Cypher does not output dates in other formats besides ISO8601.Example: APOC format temporal to stringWITH datetime() as datetimeRETURN apoc.temporal.format(datetime, yyyy-MM-dd HH:mm)apoc.temporal.formatDuration()I actually didn’t realize this existed until I was going through documentation. Similar to the function just above, you can also format durations into other units with this function. There are a couple of gotchas, though. While it will accept other duration formats (unit-based and date/time-based), I’m not sure on use cases for those particular durations. It won’t accept unit-based for a duration like P05D or P1M.Example: Cypher vs APOC format durationWITH duration({minutes: 150}) as dRETURN d.hours as cypher,  apoc.temporal.formatDuration(d,hour)Note that the return for the APOC output is a 2-digit string value. This is specified in the built-in formats. There is also another example showing a use case to calculate the difference between realtime and transaction time.Wrapping up!Throughout this series, we have taken a journey through nearly all aspects of temporal values related to Neo4j — both Cypher and APOC.In our part 1 post, we saw how complex programming for dates and times could actually be and how to create instants using functions like date(), datetime(), time(), etc. Temporal components also got a brief mention before we took a look at how to get to Neo4j-supported formats from epoch time and strings and capped our post with showing how to use multiple conversions in a single line. APOC saw some spotlight, as we relied on it for some of the conversions.In our part 2 post, we walked through truncating temporal types in Cypher and scratched the surface of creating basic durations, plus adding and subtracting them from dates. We then covered duration precision, which forms the foundation of most operations with durations. The last section of that post calculated differences between two dates with duration.between().In our part 3 post, we did another quick review of temporal components (begun in Part 1), and then spent some time working through duration conversions using duration functions. Lastly, we talked about components again, showing which ones we could use based on the duration category (months, days, seconds).In the previous post (part 4), we have seen how to combine duration functions and components in order to translate durations in one component category to another one.Then in this part 5, we circled back to APOC for a step-by-step review of each date and temporal function in the library, making note of which ones are obsolete with functionality provided in Cypher or which are still valuable.I hope this series has helped you understand Cypher temporals as much as it has for me. I’ve learned so much and asked so many questions. Thank you for taking this journey with me! Happy coding!ResourcesCypher Sleuthing: Part 1Cypher Sleuthing: Part 2Cypher Sleuthing: Part 3Cypher Sleuthing: Part 4APOC docs: TemporalAPOC docs: Date functionsAPOC docs: Temporal functions;Sep 24, 2021;[]
https://medium.com/neo4j/a-neo4j-path-through-the-christmas-holidays-56a3b847d872;Dan FlavinFollowDec 21, 2020·4 min readA Neo4j Path Through the Christmas Holidays.Dashing through the snow with arrows, triangles, paths, stumps, Cypher queries and Lee Majors.Image drawn with arrows.appThe classic Neo4j arrows web tool has been used to sketch out initial Neo4j graph constructs for what feels like a long and never ending winters nap. It is great tool built by Neo4j engineers in their spare time as a Neo4j Labs project, but it is getting a little past its expiration date. Fortunately those same wonderful engineers took the spirit of the holiday season to heart. There is now available a much more functional version of the arrows tool available on the arrows.app site (versus the old tool was found at the apc jones site). There will probably be a blog post soon detailing the new arrows app, but until then you can go to the Neo4j Labs arrows.app documentation page to for an introduction.Our Christmas tree image above was created using the new arrows.app and shared internally by one of the Neo4j engineers. It brought joyous thoughts of Christmas gifts and warm family holidays past, present, and future. Obviously it also brought happy thoughts of Cypher query language patterns.The image could be used as the graph in the Using the Neo4j Graph Database and Cypher To Solve This Brain Teaser. Why Argue? post. I use that post as background material for explaining Cypher queries and graph pattern concepts during Neo4j field engineer led education sessions¹. The post attempts to illustrate the fundamental power of graph patterns in a Cypher query counting the number of triangles embedded in a triangle stored as a graph. The graph used as an example in the post is symmetrical and looks like the tree image above, minus the bottom stump”. Attendees to the sessions have asked if the Cypher query used to count triangles would still return valid results if the triangle symmetry is broken by adding or removing nodes and relationships from the graph. The stump” in the tree diagram above is an example of one such modification to the graph data. Spoiler: Of course the query would still return the correct answer.Creating a graph using the new arrows.app to add the stump” following the triangle graph pattern is represented by the :Point nodes with the properties of pID 9 and 10:A small xmas” tree to for counting embedded trianglesHow many embedded triangle patterns exist in the this graph? There are 12 without the bottom stump” and 13 with. Why? Because there’s one more triangle pattern starting from the :Point node pID:0 that includes the nodes with the properties pID:9 and pID:10 that is part of our triangle counting query:MATCH (top:Point) WHERE NOT ( (top:Point)<--(:Point) )  // Find first node of triangle, has no incoming relationshipsMATCH path=(top)-[:DOWN*]->(:Point)-[:ACROSS*]->(:Point)<-[:DOWN*]-(top)RETURN count(path) AS triangleCountSee the original post for an explanation and to see how the graph and query pattern were developed.What would happen if a mean old Grinch tries to ruin our graph Christmas by stealing all the :Nodes and :Relationships that make up our tree?Of course the Grinch would puzzle and then say I must start with node (:Point {pID:2}) because it belongs to little Cindy Lou Who. A Who who is no more than two”. Just to keep things interesting, let’s mash-up and and say that the mean old Grinch was only able to get away with Cindy Lou Who’s node (:Point {pID:2}) before his nefarious plan is thwarted by Lee Majors as in the movie Scrooged.Scrooged (1988)Lee Majors, Jean Speegle Howard, and Al Red Dog Weber in Scrooged (1988)www.imdb.comWe’d end up with a sad looking tree graph like so:You’re a mean one, Mr. GrinchNot to worry! We lost a node, but little Cindy Lou Who and the Whos down in Whoville are still happy, because they know that it’s the triangles that count! Our query returns 4, the correct number of triangles based on our query pattern.Best wishes and here’s to everyone having a safe, healthy and happy year!¹ There are thorough and free online Neo4j courses covering Cypher and the Neo4j graph database in detail available from the Neo4j Graph Academy. You can even become a Neo4j Certified Developer through the Graph Academy!;Dec 21, 2020;[]
https://medium.com/neo4j/will-it-graph-identifying-a-good-fit-for-graph-databases-part-2-665be1803120;William LyonFollowJul 28, 2021·9 min readWill It Graph? Identifying A Good Fit For Graph Databases — Part 2Good and Poor Fits for Graph DatabasesHow do you know when the application you’re building is a good fit for a graph database?Graph databases are not always a good fit for everything. In this second part of the Will It Graph blog series, we’re going to show you some examples of good and poor fits for graph databases, how to identify a graph-shaped problem, and how the graph-native architecture helps solve graphy problems.Last time, we discussed how graph databases work under the hood and how they’re different from relational databases. (Or listen to the GraphStuff.FM podcast!)Will It Graph? Identifying A Good Fit For Graph Databases | GraphStuff.FM: The Neo4j Graph Database…How do you know when the application youre building is a good fit for a graph database? How do graph databases work…graphstuff.fmGood Fits for Graph DatabasesI think at a high level, graph databases are a good fit when we have the equivalent of lots of JOINs in our typical workload in relational databases. In this episode, we’re focusing on transactional use cases, so we’ll ignore the analytical use cases.Personalized RecommendationsThe canonical example that I always think of is the concept of personalized recommendations. If a customer is browsing an e-commerce store, we want to show them personalized recommendations based on their shopping history, the item that they’re currently looking at, etc.One of the things I can do is traverse my graph of orders, users, and products to see people who bought this thing that the customer is currently looking at — and what other things those users bought — which might be a good recommendation for the current user.Traversal through the graph is very performant in a graph database, whereas if I look at the massive order and product database in a relational database, it might need some very expensive JOINs to do that multi-hop traversal.Typically, those have been overnight batch processes — that’s how a lot of retail companies have dealt with the challenge around slow queries. You run this process overnight. You get some data. Those are your recommendations. That’s what you use the following business day.But obviously, the big challenge behind that is you’re potentially dealing with stale data. What happens when products go out of stock, etc. The real game changer is to be able to do it in real time. You can adapt according to any promotions you have, or do some real-time dynamic pricing, and so forth.Fraud DetectionAnother really powerful one is fraud detection. We touched on this idea of looking for patterns. A great example of this would be looking for fraud rings. If you think about retail banking, where typically you would have an account customer who would have a number of products with you — maybe a bank account, a loan, or credit cards. What do you do to look out for warning signs if somebody’s looking to request a loan? How do you know if it’s a typical situation or if it’s out of the blue?To start looking for patterns of fraud rings, you take a step back and have a look at things such as details being recycled, social security numbers being recycled — pretty straightforward. What if we step back a little bit further to understand, for example, whether the same phone number for a landline for a house is being used by two different people. There’s nothing unusual there, but if you then keep traversing through the graph, you discover that the same phone number is being split across different properties, so that might trigger alarm bells.There’s lots of different things that you can start to piece together by being able to look for certain patterns. In the example of the fraud on here, you wouldn’t expect to find long connections from a retail customer. You’d expect to find a fairly small, sparse, star-shaped graph.When you start to see a long line of connections, you can query it as a pattern that may bring back something that you need to investigate further. You can start to think of it from a real-time perspective as people putting applications in, or as they’re trying to set up a new account. You can have a look at the data you have in your existing graph compared to the new data coming in to see if there’s something that needs to be investigated further.Focusing again on this concept of real-time query performance, fraud detection, just like personalized recommendations, is a great example where we need to have the results immediately. Is this a fraudulent transaction? Yes or no? The answer should be in milliseconds, because when someone swipes their credit card, they don’t want to be standing there waiting multiple minutes for their credit card transaction to be approved.There’s a very small window of time where the bank, or the credit card processing company, needs to go through this process, just like in the personalized recommendations use case where I need to be able to serve those personalized recommendations to the user in real time, as they’re browsing the website. They’re not going to wait two minutes for me to go fetch and find relevant recommendations as they’re browsing my product catalog I need to be able to show those in milliseconds to the user.Network and IT ManagementAnother really graphy use case would be network and IT management. It’s all about the devices you’ve got in your network: servers, routers, the load balances, all the applications you’re running, virtual machines, etc.If you have an outage or a series of events happening within your network that may be triggering an outage, it’s not going to know that a certain server’s going to go out. You want to be able to determine what the impact is going to be.Your network can have varied levels of depth. You’re not necessarily going to know how far you need to traverse into your network to find impacted resources. But if you can do this easily in real time, you can start to trigger services to deal with that.If you start to see things happening in your network, you can do real-time root cause analysis — then avert or redirect resources. Eventually, you would even be able to predict outages. So that’s a really powerful graphy real-time application that would be quite hard to do in a relational database.What Does a Graph-Shaped Problem Really Look Like?Now that we’ve talked through a few examples where graph databases make sense, we can start to see some general themes to help us identify if we have a graph-shaped problem to work with.At a high level, any time that we’re trying to understand how our different entities are connected to each other, where the relationships in the data are just as important as the entities in the data, graph databases really offer an advantage.In personalized recommendations, this is traversing the orders and users. The connections in the data are important to answer my question: What products would the current user looking at this product be interested in based on the orders of the other users who bought the same product?Another aspect is we don’t necessarily know how many connections we’re interested in at query time. This is the concept of a variable length graph traversal. In the network management use case, let’s say we have a service that goes down and we want to know what are all of the applications that are somehow dependent on this service. There may be nested dependencies in the graph that represents the dependencies for this service and the different applications, maybe through different data providers or dependencies on applications that depend on other services.The final applications and products that may be impacted on my site could be directly connected to the service that’s going down, or they may be the dependency of a dependency of this service. And I want to just traverse that piece of the graph connected to this service that is impacted at multiple depths. A graph database is going to be really efficient at finding all of those downstream impacted applications, where I don’t know ahead of time how deep, but I just want to know all of the children dependencies of this service.There’s also this idea of finding the pattern. In the fraud detection example, we’re looking for suspicious patterns in the graph that might represent a fraud ring. If we see multiple accounts sharing a social security number or an address, and we see suspicious transactions connected to those accounts, it might be a fraud ring. We probably wouldn’t have a specific starting point for our graph traversal. Rather, we’re looking for the bigger patterns. This is another case where the graph databases can be extremely helpful.Index-Free Adjacency Doesn’t Mean No Index at AllWe talked about this idea of index-free adjacency in a graph database in our previous blog post. In short, it means that we’re not using an index at query time to traverse relationships in the graph. But that doesn’t mean that we don’t use indexes at all in a graph database.There is still a place for using indexes in a graph database. Rather than using them to find the nodes that are connected, a graph database typically uses an index to find the starting point for the traversal in our graph.For example, we may create an index on the unique ID for a node. Going back to our person example, if we’re using their social security number as the unique identifier for the person node, we may create an index on social security numbers so that when we’re looking up an operation, we can quickly find the node with that social security number using an index. But once we start traversing out to maybe the address or whatever other pieces of our graph that we’re interested in, we’re not using the index to see where those relationships exist.Poor Fits for Graph DatabasesWe’ve talked a bit about the good uses for graph databases, but there are other situations where the performance of graph databases may not be as good as you might hope.There’s one example I always think about from a colleague of ours, Max & Ozzie, who always do this talk about the Hollywood actors’ heights. There’s a list of the really tall actors down to the not-so-tall actors — this is a really great way to talk about the various strengths and weaknesses of different databases as well.For example, things that are really great fits for graph databases would be things around who knows who, or which actors are friends with which other actors. If we’re trying to find new co-actors for the latest movie that’s going to be recorded, we could leverage those connections, like who’s worked with who previously, what works well, and then try and generate some recommendations from that. Or if we want to try and figure out what films we should watch based on actors that we’ve liked again, that’s another good fit, too. So this is all about using those connections and understanding the relationships between data.But there are also other instances where the performance of a graph database will be less than great for the query. If you want to ask questions such as what is the average height or average salaries of the actors, you can still run these queries in a graph database, but it wouldn’t be as performant as you’d expect a relational database.Why is that the case? A graph database has nodes and relationships, which can have labels as well as properties stored as key-value pairs, and pointers that point to where they connect. What happens is when you run a query and you want to bring back the properties of a node, the engine will go away and pick up all the nodes that are related to that query. And then, for each node, it has to go away and look up in the store the property that has been requested.So in this example of average heights, we would bring back all of the nodes, (all of the actors) for our query, and then for each node we would then go off and do a lookup in the store to bring back the value for property. Once we have gathered all of those up, we would then perform the operation. This is quite different from how a tabular database works, where all it needs to do is pull up that single table, which would have all of the actors’ heights and then run down the column to do the aggregation or calculation required. This means that you are going to have a slower query doing that in a graph database than in a relational database.But does this mean we should never do averages on a graph database? Not at all. This is just a reminder about how you’re using the database. Next week, we’ll move on to discuss using a graph database as a general purpose database.Don’t miss Part 3 of this Will It Graph? series coming up next!;Jul 28, 2021;[]
https://medium.com/neo4j/testing-your-neo4j-based-java-application-34bef487cc3c;Michael SimonsFollowJan 17, 2019·11 min readTesting your Neo4j-based Java applicationIn this post I’m presenting different approaches how you can test your Neo4j-based application. As you might know, Neo4j can be accessed from a variety of languages — Go, Python and JavaScript being some of them — this post focuses on Java based applications. I’m an engineer on Neo4j’s Spring Data team, so this is where my focus usually is.This post has been reviewed in detail by my colleagues Gerrit Meier and Michael Hunger. Thanks a lot!There are several aspects that needs to be considered while making the decision for or against a certain test setup.Are you developing a stored procedure for the database itself?Are you using the official Java-driver to issue Cypher-queries directly over Bolt?Are you using an Object-Graph-Mapping library like Neo4j-OGM to build an application-side domain model?Did you add Spring Data Neo4j (SDN) to the mix to take advantage of the Spring ecosystem?I’ll cover those four scenarios which allow for a good comparison of the options we have at hand for testing queries or a domain model against the Neo4j database.I’m going to use JUnit 5 in all scenarios. At the beginning of 2019, there’s hardly any reason not to use JUnit 5 in new projects. All Neo4j specific techniques demonstrated in this post can be be applied with some adaption to JUnit 4 as well.The examples in this article will all handle spatial values and work with the functions defined there on. The spatial datatype point is new in Neo4j 3.4 and Neo4j-OGM will support it out-of-the-box in Neo4j-OGM 3.2 and Spring Data Neo4j 5.2.Neo4j test-harnessThe full example how to use the test-harness for custom Neo4j extension is on GitHub: using-the-test-harness.Testing custom Neo4j extensionsNeo4j can be extended with custom procedures and functions. One can also add unmanaged server extensions to Neo4j, that expose arbitrary JAX-RS endpoints from the database. In all three cases, one can interact directly with the database Java API for all kinds of operations that require direct interaction with the database engine for the highest degree of performance or flexibility.This is where the Neo4j test-harness comes in. The test-harness is a special variant of an embedded Neo4j server instance with hooks to provide test fixtures and adding your custom procedures and extensions.Given the following user defined procedure, that converts spatial attributes:public class LocationConversion {    @Context    public GraphDatabaseService db    @Procedure(        name =  examples.convertLegacyLocation ,         mode = Mode.WRITE)    public Stream<NodeWrapper> apply(                                @Name( nodes ) List<Node> nodes) {        return nodes.stream()            .filter(LocationConversion::hasRequiredProperties)            .map(LocationConversion::convertPropertiesToLocation)            .map(NodeWrapper::new)    }    static boolean hasRequiredProperties(Node node) {        return node.hasProperty(PROPERTY_LONGITUDE) &&               node.hasProperty(PROPERTY_LATITUDE)    }    static Node convertPropertiesToLocation(Node node) {        Map<String, Object> latLong =            node.getProperties(            PROPERTY_LATITUDE, PROPERTY_LONGITUDE)        PointValue location = Values.pointValue(            CoordinateReferenceSystem.WGS84,            (double) latLong.get(PROPERTY_LONGITUDE),            (double) latLong.get(PROPERTY_LATITUDE)        )        node.removeProperty(PROPERTY_LATITUDE)        node.removeProperty(PROPERTY_LONGITUDE)        node.setProperty(PROPERTY_LOCATION, location)        return node    }}The LocationConversion operates directly on the graph database nodes for optimal performance. It is meant to be executed from Cypher with a call like this: CALL examples.convertLegacyLocation(nodes). If you followed the instructions on how to package your stored procedures you would have ended up with a JAR file containing the executable code. Do you want to package it, stop your server and upload it everytime for testing it? Probably not.Enter the test-harness:<dependency>    <groupId>org.neo4j.test</groupId>    <artifactId>neo4j-harness</artifactId>    <version>${neo4j.version}</version>    <scope>test</scope></dependency>There’s a variant neo4j-harness-enterprise that matches the commercial enterprise version of Neo4j, too.With JUnit 5 you don’t need a @Rule to start it as JUnit 5 supports non-static initialization methods for tests when the lifecycle of the test is set to PER_CLASS.@TestInstance(TestInstance.Lifecycle.PER_CLASS) // <1>class GeometryToolboxTest {    private ServerControls embeddedDatabaseServer // <2>    @BeforeAll // <3>    void initializeNeo4j() {        this.embeddedDatabaseServer = TestServerBuilders            .newInProcessBuilder()            .withProcedure(LocationConversion.class) // <4>            .withFunction(GetGeometry.class)            .withFixture(   // <5>                +   CREATE (:Place {name: Malmö, longitude: 12.995098, latitude: 55.611730})                 +   CREATE (:Place {name: Aachen, longitude: 6.083736, latitude: 50.776381})                 +   CREATE (:Place {name: Lost place})             )            .newServer()     }}The lifecycle of this test should be PER_CLASS, so that initializeNeo4j(), annotated with @BeforeAll runs exactly once.The variable embeddedDatabaseServer to hold a reference to the server during all tests.initializeNeo4j() runs before all tests and uses a builder to create a test server, the builder provides interfaces for registeringthe class GetGeometry.class for custom procedures and functions as well asfixtures, either through Cypher statements like here, files or even functions.finally, start the embedded serverNow it’s really easy to use the server provided by the harness. I have added the Java-Driver as test-dependency to the project and open up a connection as against a standalone server-instance or cluster:@Testvoid shouldConvertLocations() {    try (Driver driver = GraphDatabase.driver(           embeddedDatabaseServer.boltURI(),driverConfig)        Session session = driver.session()    ) {        StatementResult result = session.run(            +   MATCH (n:Place) WITH collect(n) AS nodes           +   CALL examples.convertLegacyLocation(nodes) YIELD node           +   RETURN node ORDER BY node.name )        assertThat(result.stream())            .hasSize(2)            .extracting(r -> {                Value node = r.get( node )                return node.get( location ).asPoint()            })            .containsExactly(                Values.point(4326, 6.083736, 50.776381).asPoint(),                Values.point(4326, 12.995098, 55.611730).asPoint()            )    }}Using the test-harness for application level testsTechnically, the test-harness and the embedded server, reachable through ServerControls, can be used for application-level-testing. Besides the Bolt-URI it exposes the HTTP-URI as well as the embedded graph database instance itself. Both URIs use random, free ports and thus allow tests in parallel. The ServerControls are an autoclosable resource and as they start relatively quick, they can be fired up multiple times.It comes with a price, however: In the end it is a full blown Neo4j server instance with all the dependencies. You might not want those dependencies in your application, not even in the test scope. For instance compatibility with Scala versions can be an issue. The other disadvantage is the fact, that you’re running the test database inside the same JVM as your application. Most of the time this is not what production looks like. While being on the same JVM is the correct place for stored procedures, it is not for applications. Using an embedded database for testing your application code might lead you into a false sense of safety.Neo4j TestcontainerThe full example how to use the Testcontainers Spring Data Neo4j based applications is on GitHub: using-testcontainers.What are Testcontainers?Testcontainers is a Java library that supports JUnit tests, providing lightweight, throwaway instances of common databases, Selenium web browsers, or anything else that can run in a Docker container.František Hartman from GraphAware wrote a very detailed article about Integration testing with Docker Neo4j image and Testcontainers. František already covered a lot here and you should check this out.In the meantime, our pull request to add Neo4j support has landed in a recent Testcontainers release. Neo4j container describes the basic usage of the official container.General setup of Testcontainers with JUnit 5As stated earlier, I have become a big fan of JUnit 5. Nice assertions are one reason, package private test methods, better lifecycle management and extensions are another.Testcontainers comes with support for JUnit 5, the following listing shows all dependencies:<dependency>    <groupId>org.junit.jupiter</groupId>    <artifactId>junit-jupiter-engine</artifactId>    <version>${junit-jupiter.version}</version>    <scope>test</scope></dependency><dependency>    <groupId>org.testcontainers</groupId>    <artifactId>neo4j</artifactId>    <version>${testcontainers.version}</version>    <scope>test</scope></dependency><dependency>    <groupId>org.testcontainers</groupId>    <artifactId>junit-jupiter</artifactId>    <version>${testcontainers.version}</version>    <scope>test</scope></dependency>At the time of writing, testcontainers.version is 1.10.5 and junit-jupiter.version is 5.3.2.I recommend the following setup for an integration test with Testcontainers:@Testcontainers // <1>public class PlainOGMTest {    @Container // <2>    private static final Neo4jContainer databaseServer       = new Neo4jContainer()  // <3>}When run, extend this test-class with the @Testcontainers extensionThe extension will look for all attributes marked as @Container and start and stop them according to their lifecycle. Here, databaseServer points to our Neo4j Testcontainer.Create (but don’t start) a Neo4jContainerAs JUnit 5 tests have a default lifecycle of PER_METHOD, shared state needs to be defined as static attributes of the test. Hence, the definition of the Testcontainer as a staticattribute. This way, the container is started before all tests and closed afterwards. If the container is defined as an instance attribute, it’s restarted before each individual test.While it is possible to change the lifecycle of the test class to PER_CLASS instead of PER_METHOD, it’s a bit harder later on to configure it for Spring Boot Test-Slices.Also applicable for both plain Neo4j-OGM and SDN test is the way to provide a test-fixture. This can be done in a @BeforeAll method like this:static final String TEST_DATA =       +   MERGE (:Thing {name: Thing  })     +   MERGE (:Thing {name: Thing 2})     +   MERGE (:Thing {name: Thing 3})     +   CREATE (:Thing {name: A box, geometry: [     +     point({x:  0, y:  0}),      +     point({x: 10, y:  0}),      +     point({x: 10, y: 10}),      +     point({x:  0, y: 10}),      +     point({x:  0, y:  0})] }     +  ) @BeforeAllstatic void prepareTestdata() {    String password = databaseServer.getAdminPassword() // <1>    AuthToken auth = AuthTokens.basic( neo4j , password)    try (        var driver = GraphDatabase.driver(            databaseServer.getBoltUrl(), auth)         var session = driver.session()    ) {        session.writeTransaction(work -> work.run(TEST_DATA))    }}databaseServer is the container we defined and started above. It provides access to the database password (1). The container also provides an accessor to the Bolt-URI which contains a random port.The @BeforeAll method is invoked once before all tests. I provide the test data over Bolt, so I have the Neo4j Java-Driver on the classpath. Having a static string here is one option, but you can read in your test-data anyway you want.Using with Neo4j-OGMThe only thing you need to test your business logic based on Neo4j-OGM and queries is a Neo4j-OGM SessionFactory. I recommend defining it as a static variable through a second @BeforeAll method in the test as well:private static SessionFactory sessionFactory@BeforeAllstatic void prepareSessionFactory() {    var ogmConfiguration = new Configuration.Builder()        .uri(databaseServer.getBoltUrl())        .credentials( neo4j , databaseServer.getAdminPassword())        .build()    sessionFactory = new SessionFactory(        ogmConfiguration,         org.neo4j.tips.testing.using_testcontainers.domain )}Again: No hardcoded password, no hardcoded Bolt-URI. The Neo4j-Testcontainer provides this. One possible test with the above data could be this:@Testvoid someQueryShouldWork() {    var query =  MATCH (t:Thing) WHERE t.name =~ $name RETURN t     var result = sessionFactory.openSession()        .query(query, Map.of( name ,  Thing \\d ))    assertThat(result).hasSize(2)}This test runs over the network against a  real , server-mode Neo4j-instance. Just as your application hopefully will.Using with Neo4j-OGM and SDNFor me there’s no good reasons to start new Spring projects without Spring Boot. Spring Boot brings you — among other nice things — autoconfigured tests and more important, test slices. Test slices deal specifically with certain, technical layers of your application. Being either database layer, service layer or just the web-frontend.Regarding the database layer it’s an integration test very much focussed on interaction with the database.The Neo4j test-slice is called @DataNeo4jTest :@Testcontainers@DataNeo4jTest // <1>public class SDNTest {    @TestConfiguration // <2>    static class Config {        @Bean // <3>        public org.neo4j.ogm.config.Configuration configuration() {            return new Configuration.Builder()                .uri(databaseServer.getBoltUrl())                .credentials(                     neo4j , databaseServer.getAdminPassword())                .build()        }    }    private final ThingRepository thingRepository    @Autowired // <4>    public SDNTest(ThingRepository thingRepository) {        this.thingRepository = thingRepository    }}This activates Spring Datas repository layer and also provides Spring Boot’s JUnit 5 extensionsA @TestConfiguration adds to Spring Boot’s config but doesn’t prevent autoconfigurationA bean of Neo4j-OGMs configuration will be created to configure the SessionFactory of Spring Data Neo4jJUnit 5 together with Spring Boots extension allow constructor based injection, even in testsNow you can test against the ThingRepository as shown below:public interface ThingRepository    extends Neo4jRepository<Thing, Long> {    List<Thing> findThingByNameMatchesRegex(String regexForName)    @Query(value        =   MATCH (t:Thing) WHERE t.name = $name         +   RETURN t.name AS name, examples.getGeometry(t) AS wkt )    ThingWithGeometry findThingWithGeometry(String name)}A boring” test would look like this:@Testvoid someQueryShouldWork() {    var things = thingRepository       .findThingByNameMatchesRegex( Thing \\d )    assertThat(things).hasSize(2)}Why is this boring? Because we’re basically testing whether Spring Datas query derivation works or not.Testing findThingWithGeometry is much more interesting, as you may recognize examples.getGeometry(t) as our own, custom procedure. How do we get this into the test container? Turns out the authors of Testcontainers thought of a method to mount paths in the container before it starts.I packaged the custom stored procedures from the beginning of this article into a JAR files name geometry-toolbox.jar and added it to the test resources. With this the Testcontainer can be created like this:@Containerprivate static final Neo4jContainer databaseServer =     new Neo4jContainer<>()    .withCopyFileToContainer(        MountableFile.forClasspathResource( /geometry-toolbox.jar ),         /var/lib/neo4j/plugins/ )    .withClasspathResourceMapping(         /test-graph.db ,         /data/databases/graph.db , BindMode.READ_WRITE)    .withEnv(         NEO4J_dbms_security_procedures_unrestricted ,         apoc.*,algo.* )The plugin-jar gets copied into the right place inside the container and is recognized by Neo4j during startup. The test data for the second test isn’t hardcoded like in PlainOGMTest.java. I copied over the graph.db folder from my  production  instance to the test resources. Calling withClasspathResourceMapping() maps it into the containers /data/ volume, where Neo4j expects the database. In a real-world test you probably have that data folder somewhere else and not in your project. In such cases, you would use withFileSystemBind() of the Testcontainer.In the setup above, withEnv() is used to remove any security restrictions from APOC and algorithms extensions by setting the environment variableNEO4J_dbms_security_procedures_unrestricted accordingly.Anyway, given the same test data as before, a not so boring test now is green:@Testvoid customProjectionShouldWork() {    var expectedWkt        =  LINESTRING (0.000000 0.000000,         +  10.000000 0.000000,         +  10.000000 10.000000,         +  0.000000 10.000000,         +  0.000000 0.000000)     var thingWithGeometry = thingRepository       .findThingWithGeometry( A box )    assertThat(thingWithGeometry).isNotNull()        .extracting(ThingWithGeometry::getWkt)        .isEqualTo(expectedWkt)}And with that, we just have tested a Spring Data Neo4j based Neo4j-application including custom plugins end-to-end. Starting with the plugins and ending up with an integration test.SummaryWhen writing custom extensions, you want a quick feedback loop for all of your tests. You’re also very close to the server in all cases. The test-harness provides you with the fastest feedback loop possible and doesn’t expose your code to more than you actually need. Your code is right there at the server level. The test-harness and the embedded, customizable instance of Neo4j should be your first choice when testing custom Neo4j extensions. It is also a good choice for infrastructure code like Neo4j-OGM and Spring Data Neo4j itself. Neo4j-OGM runs against an embedded graph, over Bolt and HTTP, so it must be tested against all of those. The test-harness provides good support for that.The main advantage of using a Testcontainer is the fact that it resembles your later application setup the best. While there are a few use-cases, most applications should not run an embedded version of Neo4j. Think about it: In a microservices world, where you have usually more than one instance of an application running, should each instance bring it’s own database? You cannot run Neo4j in Causal Cluster mode in an embedded scenario, so you have to synchronize those instances. Furthermore: If your application goes down, so would your database.The generic Testcontainer or the dedicated Neo4j-Testcontainer gives an easy way to bring up new, clean database instances for each test. Thus, your tests are independent of each other and you won’t have interference in your test data from concurrent tests.So please keep the following in mind while your design your integration tests:The topology of your test should resemble your target topology as much as possibleTry to use a dataset for integration tests, that is comparable in size to your production datasetTestcontainers help a lot to achieve the first item. Whether you can get your hands on a dataset that is similar to your production data set, depends probably on your surroundings and organization. If it is possible however, you could create a custom Neo4j Docker image and use that one as a basis for the Testcontainer in your CI.Images:Harness: https://unsplash.com/photos/N_3CHNdliVsLogo Testcontainers: https://testcontainers.org;Jan 17, 2019;[]
https://medium.com/neo4j/introducing-neo4j-drivers-for-the-next-gen-database-96981f65e8b8;Ljubica LazarevicFollowJan 22, 2020·3 min readIntroducing Neo4j drivers for the next gen databaseRacing Around The Curve — Joe Neric on unsplashThe latest Neo4j 4.0 drivers are here! The new drivers have been designed to work together with Neo4j 4.0, Neo4j’s Database-as-a Service solution Aura, and Neo4j’s federation framework Fabric. The 4.0 drivers are built to provide a user-friendly and unified API across all languages to take advantage of these new features and services.What’s in a name?To better align with Neo4j versions, the decision was made to release the driver as 4.0.0 rather than 2.0.0 (which would have followed the previous convention, and would have been the next version up from 1.7).The following languages have drivers available as version 4.0:Java.NETJavaScript4.0 drivers for Python and Go will be available soon.The current stable 1.7 driver release for these languages will work in fallback mode with Neo4j 4.0, which means that all functionality that is available in Neo4j 3.5 will continue to work. However, new functionality introduced in Neo4j 4.0 will not exist.New featuresThe biggest driving feature for the latest version of the drivers is support for Neo4j 4.0Bolt 4.0 is implemented for drivers and Neo4j 4.0Reactive streams is now available, including client side back-pressure supportThe new feature detection method driver.supportsMultiDb() for 4.0, and connectivity checking method driver.verifyConnectivity() methodSession need to be acquired against a specific database. This is due to the multi-database feature now available in Neo4j 4.0You can use the new drivers with 3.5 series of the Neo4j database. Do be aware you won’t be able to use the new features such as multi-database, back pressure, and reactive sessions, you’ll need Neo4j 4.0 for that!Breaking changesAs with all major releases, there can be breaking changes, and with the latest version of the Neo4j drivers, the following changes have been made:Default configuration for encrypted is false. if you enable encryption, the default trust mode is determined by operating system (e.g. self-signed certificates will fail on certificate validation by default)Hostname verification is turned on by default when encryption is turned onv1 is removed from the driver’s package name. You only need the package name of org.neo4j.driver, rather than org.neo4j.driver.v1You no longer need to use the protocol bolt:// or bolt+routing:// in connection strings when working with clustered deployments. This is replaced by neo4j://. You can still use bolt:// for a single-instance, or with a single member in a clusterFor the Java and .NET drivers, asynchronous methods have been extracted out and put in AsynchSession, whereas synchronous methods remain in Session. This change ensures that blocking and non-blocking APIs don’t accidentally get mixed.For synchronous Transaction API, Transaction#success and Transaction#failure have been replaced with Transaction#commit and Transaction#rollback. A transaction in Neo4j 4.0 can only be committed or rolled back once. If a transaction is not committed explicitly using Transaction#commit, Transaction#close will roll back the transaction.There has also been some name simplification for clarity Statement has been renamed to Query StatementResult has been changed to Result StatementResultCursor has been changed to ResultCursor.Driver#session method now uses a session configuration object or option building rather than method argumentsBookmark has changed from a String or a List<String> to a Bookmark objectA result can only be consumed once. This occurs either if the query result has been discarded by invoking Result#consume and/or the outer scope where the result is created, such as the transaction or session, has been closedLoadBalancingStrategy is removed from Config class and the drivers always default to LeastConnectedStrategy.Racing to the resourcesYou can get the latest driver documentation from here, including example code snippets.;Jan 22, 2020;[]
https://medium.com/neo4j/spring-data-neo4j-6-0-8b92164fff32;Gerrit MeierFollowNov 18, 2020·4 min readSpring Data Neo4j 6.0Photo by Jorge Ibanez on UnsplashWe are happy to announce the all new Spring Data Neo4j 6.0.Spring Data Neo4j version 6 (or shorter SDN 6) is built on the foundation of SDN/RX that got released earlier this year. It is the successor of SDN 5 (with Neo4j-OGM) and RX.The Spring Data module has some history and went through several major changes. For about four years now it followed the Spring Data JPA principle: Using Spring Data as a thin layer to orchestrate an underlying object mapper.While Neo4j-OGM is quite flexible, it comes with a price. It does not only abstract over our query language like any object mapper would do, but also over the transport.The new Spring Data Neo4j 6.0 major release addresses all of this. We started that endeavour around March 2019, very much inspired by the great work of the Spring Data Team around Spring Data JDBC.SDN 6 started as a side project named SDN/RX. As SDN/RX we gathered feedback from customers and early adopters, among them JHipster users. Eventually it became SDN 6 and we announced it during Spring One 2020:Introducing Spring Data Neo4j 6What’s in it for youFull support of reactive programming, from the mapping layer up to the database (requires Neo4j 4.x).Compatible with a wide range of the Neo4j Java Drivers: From version 4.0 to 4.2 and future versions, thus supporting Neo4j 3.5 up to 4.2.A JDK 8 baseline in accordance with Spring Framework and Spring Data, but also runs great in JDK 15, including using the Records-preview as domain classes.Speaking of records: The mapping is record oriented, thus pipelined and mapped data can be retrieved fully immutable. This applies to Kotlin data-classes, Java 15 records and of course to Java data” classes with withers”, either manually created or through Lombok.For the first time with Spring Data Neo4j: Support for findByExample based methods.Introduction of our new Cypher-DSL. We use this internally but you can create custom queries with it, too.neo4j-contrib/cypher-dslA Java DSL for the Cypher Query Language and an optional Query DSL mode - neo4j-contrib/cypher-dslgithub.comOne More Thing: Support for the experimental Spring Native project: Spring Data Neo4j, the Cypher-DSL and the Neo4j Java driver are fully compatible with GraalVMs native image.The return of the Neo4j TemplateWell-defined levels of abstractions: SDN 6 is not responsible for creating a database connection anymore. Nor it can be held responsible for bringing up an embedded database instance.SDN Building BlocksInstead, you provide the driver or let Spring Boot handle this for you. Do whatever you want with the driver. Manage your transactions the way you need.SDN 6 gives you a thin client over the driver. We hope that this client has some nice and usable hooks for your own mapping. At least we are happy with it and use it under the hood. The client is integrated with Spring’s transaction managers. On top of the client sits the Neo4jTemplate. The Template instance knows about your domain model and is used by the repository abstraction.Spring BootAlong with a larger update of the starter for SDN we added configuration support for the pure Neo4j Java Driver in Spring Boot. It is now possible to create a managed Driver Bean on its own by declaring the Neo4j Java Driver dependency and configuring it in the application.properties with the new spring.neo4j.* parameters. Also we introduced better health endpoint information and hopefully soon great Micrometer metrics support.Let’s talk about embeddedOne point that we like to address: The support for Neo4j embedded.While Neo4j-OGM can not only start an embedded instance but also use the Graphdatabase API directly, we don’t recommend that approach at all. Starting any database from the object mapping layer opens a world full of problems.This begins at the sheer amount of dependencies a database brings into your application, continues with the synchronization of multiple instances of the same service and possible ends with shutdown timing issues, especially when something is run on Kubernetes, which has a tendency to kill things quickly.If you are aware of those drawbacks, you can still bring up an embedded Neo4j. This embedded instance then can be used by SDN 6 by opening up a local Bolt connection to it.What do you need to do to use SDN 6?The annotations have changed and we offer a dedicated page in the documentation where annotation of OGM has been replaced by a Spring Data Neo4j annotation.The mapping is a bit more opinionated and you might need to rework your domain model in a couple of places.We think that we managed to lay the groundwork for a stable framework for the next years and look forward to your feedback.You can find more information about the new features and migration in the Spring Data Neo4j documentation and Spring Boot’s Neo4j section. If you have any questions, reach out to us in the Neo4j community.Many thanks to Mark Paluch (Spring Data team lead), Sergei Egorov (Project Reactor), Stéphane Nicoll (Spring Boot team) and to you for giving us feedback and ideas all the time ❤;Nov 18, 2020;[]
https://medium.com/neo4j/building-an-educational-platform-on-neo4j-d7dc195b9466;Adam CowleyFollowJul 19, 2022·8 min readBuilding an Educational Platform on Neo4jThe data model for the GraphAcademy databaseIn a previous article, I went into detail on the reasons why we rebuilt Neo4j GraphAcademy. If you are not aware, GraphAcademy is a free, self-paced online training platform that acts as the first port of call for many developers starting with Neo4j.Introducing the New GraphAcademyOver the past couple of months, we have been working on a new version of Neo4j GraphAcademy, a free, self-paced online…medium.comIt was always my intention to build the new site in the open on the Neo4j Twitch Channel, but in the end, time got the better of me and I ended up quietly developing the backend in stealth mode. So, in lieu of a Twitch stream, I thought I would take some time to write a follow-up post with some of the more technical aspects of the build.Educating users about Neo4j. Using Neo4j.Naturally, the platform is built on top of a Neo4j Aura database. It may be a surprise that many companies don’t use their own products but I thought it was important to eat our own dog food with this project. With that said, there were some moments of serendipity along the way that justified the decision.I’ll go through some of those moments in more detail.The Data ModelAs this is a Neo4j blog, let’s start with the data model used in the graph.The approach we recommend in the Graph Data Modeling Fundamentals course is to split the data model out into statements and use the verbs and nouns to form a data model. This being an educational platform, we are storing data on users who enrol in courses. To make the courses more manageable, we decided to split out the new courses into modules, each of which would contain one or more lessons.Here is how those parts translated into the graph:A Course is listed in one or more Categories. Categories form a hierarchy, so a Category may also have child categories.A Graph diagram produced with arrows.app.Serendipity: This may seem pretty straightforward, but one thing I discovered at this point was how easily Neo4j can handle hierarchies. Back in my early years of development, I struggled with a few product in child category of X” style problems. With Neo4j, you can quickly find paths through a hierarchy of relationships until you find a leaf with a single line of Cypher.A Student enrols to a Course — or a student has an enrolment for a course.A screenshot of my enrolment for the Neo4j Fundamentals course from Neo4j Browser.Relationships in Neo4j have a type, direction, and single start and end nodes. Extracting the enrolment out into a node in its own right gave me the ability to link that enrolment to other facts. For example, as part of the enrolment, a user may attempt a quiz. That attempt would contain one or more correct or incorrect answers to questions.A Lesson may have one or more Questions — a simple one here, defaulting back to a has” or is” relationship. When in doubt…In this screenshot, the red Enrolment node is linked to a yellow Attempt node. The Attempt has an additional SuccessfulAttempt label applied to it. The two green answer nodes represent an Answer to the question.In order to judge the quality of our quizzes and challenges, each attempt that the user makes is stored in a node along with their answers. The success rates for each question are monitored, and where pass rates are low, we make changes to the course content and the pass criteria in order to make the questions as easy as possible.Courses have prerequisites.Progression was a major factor during this rebuild. One piece of feedback was that the previous version of the website didn’t communicate clearly enough the order in which the courses should be completed, or what a user should do next once they have completed a course.Follow your path from Neo4j Fundamentals to Importing DataEach course now has recommended prerequisite courses and courses to advance to. These have been curated to help guide the user towards a Neo4j Certification (and a free t-shirt!).Showing the learning path through the Cypher Fundamentals course. Once you have learned the fundamentals of Cypher, you can either progress to Data Modeling or learn more advanced CypherIt takes a single line of Cypher to find a list of prerequisites for that course, and another to filter out any courses that the user is already enrolled on. This allows us to provide better recommendations and guide the user on what they should be doing next.MATCH (u:User {id: $sub})-[:HAS_ENROLMENT]->()-[:FOR_COURSE]->(c)WITH collect(c) AS enrolledToMATCH (:Course {id:  importing-data })-[:PREREQUISITE*]->(prerequisite)WHERE not prerequisite IN enrolledToRETURN prerequisiteIn fact, there are linked lists all over the database. I almost add linked lists as a force of habit now but they are extremely useful.For example, we’ve made a lot of assumptions in our course catalogue. The Beginners courses were ordered this way due to a combination of intuition and experience. But as more users register and enrol, we can analyse :NEXT_ENROLMENT relationships between their enrolments to see how users are actually using the platform.We can also look at the attempts that learners are making during the quizzes or how they attempt code challenges. If there is a common pattern of multiple :UnsuccessfulAttempt nodes linked to a question before a :SuccessfulAttempt, that could be a sign that we need to revise the lesson or make some questions clearer.If you want to learn more about how to model your data in Neo4j, you can enrol to the Graph Data Modeling Fundamentals course.Tech StackHere are the highlights of the technologies I used to build the site:Built with TypeScriptThe website is built using TypeScript — I wanted to build something as quickly and simply as possible, so rather than use any of the libraries or frameworks I had put together over the years (neode, nest-neo4j, use-neo4j react hooks), I thought why not just run Cypher queries directly through the Neo4j JavaScript Driver.As the complexity of the website grew, this decision was justified — there is some pretty complex Cypher in there at points.At first, I was on the fence about TypeScript. But the more I’ve worked with it, the more I‘ve enjoyed the experience. Many potential bugs are squashed early on in the development process. A few months on, it feels strange to write vanilla JavaScript without type definitions.In terms of the proper way” of using TypeScript with Neo4j, I asked a couple of colleagues for their suggestions — there were a couple of ways to look at things. The first (bullet-proof way) would be to create classes with getters and setters — this had the drawback of having to somehow find a way of hydrating the response from the driver into these classes. But again, as the complexity grows, this takes more maintenance, and it seemed like overkill for an object that would just be passed through to a pug template.The driver also supports typescript generics, but there is no check on the result itself so these just become syntactical sugar.The way I settled on was to create an interface to reflect the object that would be returned by the query — loosely mapped to the properties of the nodes themselves.export interface Course {    slug: string    title: string    link: string    video?: string    duration?: string    redirect?: string    thumbnail: string    caption: string    status: CourseStatus    interested?: string    usecase: string | undefined    modules: Module[]    categories: Category[]    prerequisites?: Course[]    progressTo?: Course[]    badge?: string}Rather than returning Nodes from the driver, I settled on returning a map projection of the properties I was interested in. In the app, I have a function that generates the Cypher needed to return information about the course, modules and lessons. Here is an abridged version of the output:MATCH (c:Course {slug:  neo4j-fundamentals })OPTIONAL MATCH (:User {username:  adam.cowley })-[:HAS_ENROLMENT]->(e)-[:FOR_COURSE]->(RETURN c {    .slug,    .title,    .thumbnail,    .caption,    .status,    .usecase,    .redirect,    .link,    .duration,    .video,    enrolled: e IS NOT NULL,    categories: [         (c)-[:IN_CATEGORY]->(category) |         category {             .id,             .slug,             .title,             .description,             link: /categories/+ category.slug +/         }    ],    modules: [         (c)-[:HAS_MODULE]->(${module}) |         m { .id, .title }     ],    prerequisites: [        (c)-[:PREREQUISITE]->(p) WHERE p.status <> disabled |         p { .link, .slug, .title, .caption, .thumbnail }     ],    progressTo: [         (c)<-[:PREREQUISITE]-(p) WHERE p.status <> disabled |         p { .link, .slug, .title, .caption, .thumbnail }     ]} AS courseAs you can see, the output of the Cypher query maps to the TypeScript interface.Express.js & PugWe use Asciidoc across our documentation and with the previous site. So building a site that was served by Express.js with Pug templates meant that I could easily integrate Asciidoctor.js to turn the courses into HTML.Straight up HTMLThe UI isn’t built with a front-end framework, and it is not a SPA — a colleague had put together a prototype using Gatsby, but in reality, the nature of the course files meant that these frameworks ended up complicating things. Some of the courses were generating a few KB’s of HTML.Some may think this is an antiquated way of building a website, but to be honest, after a few years of building apps with React & Vue — not having to deal with state management, etc. — it was a breath of fresh air. Everything just worked.Building on Neo4j AuraI know that you’d expect me to be full of praise for Neo4j & Aura, but in truth, it gave me some peace of mind. I like to do things myself, and I’ve been installing and tuning Neo4j databases for years. But GraphAcademy was developed and is currently being hosted on Aura Free, although as we scale up I expect that we will move over to the Professional tier. That’s a limited-size graph (200k nodes, 400k relationships) free of charge.As soon as we hit those limits, it’ll be $65 a month — you wouldn’t get much time from a DevOps or Server Admin for that amount.The fact that I could just scale up the database with a couple of clicks in the Aura Console made the job easy.What’s next for GraphAcademy?We’ll continue to develop the GraphAcademy platform, integrate existing Neo4j tools and build out the course catalogue with new courses. If you think there is something missing from the course catalogue, feel free to email us at graphacademy @ neo4j.com with your suggestions.If you are interested in getting started with graph databases but are not sure where to start, please feel free to reach out to me on Twitter. I would be happy to explore the possibilities with you.Otherwise, GraphAcademy is a great place to learn everything you need to know to be successful with Neo4j.Happy learning!;Jul 19, 2022;[]
https://medium.com/neo4j/discover-auradb-free-week-19-the-wordle-graph-8a6870b094f9;Michael HungerFollowFeb 14, 2022·11 min readDiscover AuraDB Free: Week 19 — The Wordle GraphI know I’m late to the game as Wordle was sold to the New York Times last week. I haven’t played it much myself just saw it popping up on my twitter stream a lot.If you haven’t played Wordle, it is a guessing game similar to mastermind, only with (5-letter) English words (there are now many clones in other languages). Correctly guessed letters in the right position appear green, wrong position yellow and incorrect letters gray. The goal is to guess the word in 6 attempts, i.e. make all 5 letters green.Our creative folks even created a Neo4j version of it, see if you can spot the mistake.So I thought one late night that it would be fun to represent the Wordle world as a graph. And here we go.If you missed our live-stream here is the recording, we had a lot of fun both playing the game but also exploring the graph version of it:That Wordle of the day was elder”, which we got to in attempt 3, after someone from the stream hinting at it.You can find past live sessions and deep dive blog write-ups.All the Cypher statements and the source CSV can be found in this GitHub repository:GitHub - jexp/wordle-graph: Wordle Solver as Neo4j graph databaseWordle Solver as Neo4j graph database. Contribute to jexp/wordle-graph development by creating an account on GitHub.github.comBut let’s create our database instance first, so that you can follow the modeling and import.Create a Neo4j AuraDB Free InstanceGo to https://dev.neo4j.com/neo4j-aura to register or log into the service (you might need to verify your email address).After clicking Create Database you can create a new Neo4j AuraDB Free instance. Select a Region close to you and give it a name, e.g. {db-name}.Choose the blank database” option as we want to import our data ourselves.On the Credentials popup, make sure to save the password somewhere safe. The default username is always neo4j.Then wait 2–3 minutes for your instance to be created.Afterwards you can connect via the Open Button with Neo4j Browser (you’ll need the password).Then also the connection URL: neo4j+s://xxx.databases.neo4j.io is available and you can copy it to your credentials as you might need it later.If you want to see examples for programmatically connecting to the database go to the Connect” tab of your instance and pick the language of your choiceDatasetI found a scraped Wordle list in this repository of an R solver, which I turned into a CSV with 12k entries.Here are the first few words, none of which I would have associated with English :)aahedaaliiaarghaartiabacaThe CSV is available on GitHub, it doesn’t have a header and only a single column.ImportWe can load the CSV data directly with LOAD CSV into Word nodes. First we create a constraint to make this and future lookups fast and the import repeatable.// create constraintCREATE CONSTRAINT word_name IF NOT EXISTS ON (w:Word) ASSERT w.name IS UNIQUE// load csv and turn each row into a Word nodeLOAD CSV FROM  https://raw.githubusercontent.com/jexp/wordle-graph/main/wordle.csv  AS rowMERGE (:Word {name:row[0]})That’s a lot of 5-letter words (12k) nodes.I also put a dump file on s3: https://data.neo4j.com/wordle.dump that you can load into your AuraDB or Neo4j Desktop.Tale of 2 ModelsThe idea is to split the words into their constituent letters and represent those character positions” in the graph, sharing letters and positions as needed.That will allow us to later have fun with things like:Character frequencyFollow frequenciesTop-starter wordsPossible solution words if we have already some positive and negative cluesPlaying the gameModel 1 — Letter Positions as NodesInitially I represented characters at positions with dedicated nodes labeled CharAtPos with a char and an idx property connected to the word via HAS and to each other with an NEXT relationship and the first node (idx=0) having an extra STARTS relationship.As it turns out the model might have been a bit overengineered :)Here is the code to split the words into characters and then create the first CharAt pos and subsequent (1..4) nodes and connect them.To make it work with memory-constrained environments I wrapped it in a CALL {} IN TRANSACTIONS subquery, but even in AuraDB Free that was actually not necessary.MATCH (w:Word)CALL { WITH wWITH w, split(w.name,  ) AS charsMERGE (start:CharAtPos {idx:0, char:chars[0]})MERGE (w)-[:STARTS]->(start)MERGE (w)-[:HAS]->(start)WITH *UNWIND range(1,size(chars)-1) AS idxMERGE (next:CharAtPos {idx:idx, char:chars[idx]})MERGE (w)-[:HAS]->(next)WITH *MATCH (prev:CharAtPos {idx:idx-1, char:chars[idx-1]})MERGE (prev)-[:NEXT]->(next)} IN TRANSACTIONS OF 1000 ROWSIF we query a word like crash now based on our model:match p=(:Word {name: crash })--()return pWordle Solver v1To solve a word, you pass in the letters you know with their positions and the letters that you don’t have the right position for and match any words that fit this pattern.MATCH (c1:CharAtPos {idx:0, char:c}),      (c5:CharAtPos {idx:4, char:h}),      (c:CharAtPos {char:a})match (w:Word)-[:HAS]->(c1),      (w)-[:HAS]->(c5),      (w)-[:HAS]->(c)return w.name╒════════╕│ w.name │╞════════╡│ clach  │├────────┤│ clash  │├────────┤│ caneh  │├────────┤│ coach  │├────────┤│ catch  │├────────┤│ crash  │└────────┘If we have more information, then we can extend the query by excluding letters or positions and get a smaller result set. It takes a bit longer to query due to the exclusions.match (c1:CharAtPos {idx:0, char:c}), // correct      (c2:CharAtPos {idx:1, char:a}), // wrong pos      (c3:CharAtPos {char:l}),  // incorrect      (c4:CharAtPos {char:i}),  // incorrect      (c5:CharAtPos {idx:4, char:h}), // correct      (c:CharAtPos {char:a})match (w:Word)-[h1:HAS]->(c1),      (w)-[h2:HAS]->(c5), (w)-[h3:HAS]->(c)WHERE not exists { (w)-[:HAS]->(c2) } and not exists { (w)-[:HAS]->(c3) } and not exists { (w)-[:HAS]->(c4) }return *Model 2 — Positions in RelationshipsAn alternative model represents just the 26 characters and puts the position onto the relationship either as a property or as the rel-type.Because we know we have 5 letters we can just spell it out.MATCH (w:Word)WITH w, split(w.name,  ) AS charsMERGE (c0:Char {char:chars[0]})MERGE (w)-[p0:POS0]->(c0) SET p0.idx = 0MERGE (c1:Char {char:chars[1]})MERGE (w)-[p1:POS1]->(c1) SET p1.idx = 1MERGE (c2:Char {char:chars[2]})MERGE (w)-[p2:POS2]->(c2) SET p2.idx = 2MERGE (c3:Char {char:chars[3]})MERGE (w)-[p3:POS3]->(c3) SET p3.idx = 3MERGE (c4:Char {char:chars[4]})MERGE (w)-[p4:POS4]->(c4) SET p4.idx = 4Model 2 ExplorationWe can first look at the representation of a word (diver”) in this model.match (w:Word {name: diver })-[r]->(c:Char)return *Then see how shared characters between two words (diver” and elder”) look like, here c are the shared letters and c1, c2 the other letters respectively.match path = (c1:Char)<--(:Word {name: diver })-->(c:Char)match path2 = (c:Char)<--(:Word {name: elder })-->(c2:Char)return path, path2Looking at that frequent suffix of er, we wanted to see what the letter frequencies look like.Letter FrequenciesWith this model we can also easy look at character frequencies and follow probabilities.For the letter frequencies we just count the number of relationships pointing to a character node (aka the in-degree).MATCH (c:Char)RETURN c.char, size((c)<--()) as degORDER BY deg DESCAs expected for the English language the vowels, and R,S,T are pretty high up.╒════════╤═════╕│ c.char │ deg │╞════════╪═════╡│ s      │6665 │├────────┼─────┤│ e      │6662 │├────────┼─────┤│ a      │5990 │├────────┼─────┤│ o      │4438 │├────────┼─────┤│ r      │4158 │├────────┼─────┤│ i      │3759 │├────────┼─────┤│ l      │3371 │├────────┼─────┤│ t      │3295 │├────────┼─────┤│ n      │2952 │├────────┼─────┤│ u      │2511 │├────────┼─────┤│ d      │2453 │├────────┼─────┤│ y      │2074 │├────────┼─────┤│ c      │2028 │├────────┼─────┤│ p      │2019 │├────────┼─────┤│ m      │1976 │├────────┼─────┤│ h      │1760 │├────────┼─────┤│ g      │1644 │├────────┼─────┤│ b      │1627 │├────────┼─────┤│ k      │1505 │├────────┼─────┤│ f      │1115 │├────────┼─────┤│ w      │1039 │├────────┼─────┤│ v      │694  │├────────┼─────┤│ z      │434  │├────────┼─────┤│ j      │291  │├────────┼─────┤│ x      │288  │├────────┼─────┤│ q      │112  │└────────┴─────┘We can now use that information to find good starting words, by summing up the character frequencies for each word.MATCH (w:Word)MATCH (w)-->(c:Char)RETURN w.name, sum(size((c)<--())) as totalORDER BY total DESC LIMIT 5We see there are quite a lot of cheater” words which contain the high frequency characters multiple times.╒════════╤═══════╕│ w.name │ total │╞════════╪═══════╡│ esses  │33319  │├────────┼───────┤│ sasse  │32647  │├────────┼───────┤│ sessa  │32647  │├────────┼───────┤│ asses  │32647  │├────────┼───────┤│ eases  │32644  │└────────┴───────┘If we want to avoid that we can state, that we want to only look at words with 5 distinct characters.MATCH (w:Word)MATCH (w)-->(c:Char)RETURN w.name, sum(size((c)<--())) as total, count(distinct c) = 5 as uniquesORDER BY uniques DESC, total DESC LIMIT 10╒════════╤═══════╤═════════╕│ w.name │ total │ uniques │╞════════╪═══════╪═════════╡│ arose  │27913  │true     │├────────┼───────┼─────────┤│ soare  │27913  │true     │├────────┼───────┼─────────┤│ aeros  │27913  │true     │├────────┼───────┼─────────┤│ serai  │27234  │true     │├────────┼───────┼─────────┤│ arise  │27234  │true     │├────────┼───────┼─────────┤│ reais  │27234  │true     │├────────┼───────┼─────────┤│ aesir  │27234  │true     │├────────┼───────┼─────────┤│ raise  │27234  │true     │├────────┼───────┼─────────┤│ aloes  │27126  │true     │├────────┼───────┼─────────┤│ stoae  │27050  │true     │└────────┴───────┴─────────┘Still not perfect, most of these are just variations of the same set of letters. Let’s group them by the sorted set of letters and show the first few.MATCH (w:Word)MATCH (w)-->(c:Char)RETURN apoc.coll.sort(split(w.name,)) as letters, sum(size((c)<--())) as total, count(distinct c) = 5 as uniques, collect(w.name)[0..2] as wordsORDER BY uniques DESC, total DESC LIMIT 10╒═════════════════════╤═══════╤═════════╤═════════════════╕│ letters             │ total │ uniques │ words           │╞═════════════════════╪═══════╪═════════╪═════════════════╡│[ a , e , r , s , t ]│348010 │true     │[ aster , arets ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ a , e , p , r , s ]│331422 │true     │[ apres , apers ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ a , e , l , s , t ]│311796 │true     │[ salet , taels ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ a , e , l , p , s ]│247070 │true     │[ pales , lapse ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ d , e , o , r , s ]│243760 │true     │[ doser , deros ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ a , e , l , r , s ]│241614 │true     │[ arles , earls ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ a , c , e , r , s ]│229527 │true     │[ acres , acers ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ d , e , i , l , s ]│229100 │true     │[ delis , diels ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ a , i , r , s , t ]│214803 │true     │[ artis , astir ]│├─────────────────────┼───────┼─────────┼─────────────────┤│[ a , e , l , s , v ]│210438 │true     │[ avels , valse ]│└─────────────────────┴───────┴─────────┴─────────────────┘Follow FrequenciesFor example, in position 1 we can use this for the follow frequencyMATCH (c1:Char)<-[:POS0]-(w)-[:POS1]->(c2:Char)RETURN c1.char, c2.char, count(*) as freqORDER BY freq DESC LIMIT 5╒═════════╤═════════╤══════╕│ c1.char │ c2.char │ freq │╞═════════╪═════════╪══════╡│ c       │ o       │220   │├─────────┼─────────┼──────┤│ m       │ a       │193   │├─────────┼─────────┼──────┤│ r       │ e       │187   │├─────────┼─────────┼──────┤│ s       │ t       │183   │├─────────┼─────────┼──────┤│ b       │ o       │175   │└─────────┴─────────┴──────┘Globally we can either do a big union and sum them up as we did on the stream. Or with the positions on the relationships, we can also generalize our statement from above.MATCH (c1:Char)<-[p1]-(w)-[p2]->(c2:Char)WHERE p1.idx +1 = p2.idxRETURN c1.char, c2.char, count(*) as freqORDER BY freq DESC LIMIT 10╒═════════╤═════════╤══════╕│ c1.char │ c2.char │ freq │╞═════════╪═════════╪══════╡│ e       │ s       │867   │├─────────┼─────────┼──────┤│ e       │ r       │765   │├─────────┼─────────┼──────┤│ a       │ r       │659   │├─────────┼─────────┼──────┤│ r       │ e       │646   │├─────────┼─────────┼──────┤│ e       │ d       │642   │├─────────┼─────────┼──────┤│ r       │ a       │562   │├─────────┼─────────┼──────┤│ i       │ n       │556   │├─────────┼─────────┼──────┤│ a       │ n       │544   │├─────────┼─────────┼──────┤│ a       │ l       │536   │├─────────┼─────────┼──────┤│ a       │ s       │532   │└─────────┴─────────┴──────┘These can help us finding that missing letter, but I guess you have internalized it already from your language knowledge, except for some rare combinations.One interesting question we wanted to answer is, which letters appear frequently in succession as double-letters.MATCH (c:Char)<-[r1]-(w:Word)-[r2]->(c)WHERE r1.idx = r2.idx +1RETURN c.char, count(*) as freqORDER BY freq DESC LIMIT 8As expected o and e lead the pack, l was a bit surprising to me.╒════════╤══════╕│ c.char │ freq │╞════════╪══════╡│ o      │328   │├────────┼──────┤│ e      │308   │├────────┼──────┤│ l      │209   │├────────┼──────┤│ t      │114   │├────────┼──────┤│ f      │111   │├────────┼──────┤│ r      │108   │├────────┼──────┤│ s      │105   │├────────┼──────┤│ n      │91    │└────────┴──────┘For generic, repeats and re-occurrences one could generalize that toWordle Solver v2For resolving our wordle puzzle (v2) we could use this Cypher using this time the relationships as structuring means.MATCH (c:Char {char:c}),      (h:Char {char:h}),      (a:Char {char:a})MATCH (wordle:Word)-[p0:POS0]->(c),      (wordle)-[p4:POS4]->(h),      (wordle)-[px]->(a)WHERE not exists { (wordle)-[:POS1]->(a) }  AND not exists { (wordle)-[:POS2]->(:Char {char:l}) }  AND not exists { (wordle)-[:POS3]->(:Char {char:i}) }RETURN *If we have more information, then we can extend the query by excluding letters or positions and get a smaller result set.MATCH (c:Char {char:c}),      (h:Char {char:h}),      (a:Char {char:a})MATCH (wordle:Word)-[p0:POS0]->(c),      (wordle)-[p4:POS4]->(h),      (wordle)-[px]->(a)WHERE not exists { (wordle)-[:POS1]->(a) }  AND not exists { (wordle)-[:POS2]->(:Char {char:l}) }  AND not exists { (wordle)-[:POS3]->(:Char {char:i}) }RETURN *We can even go so far as to implement a generic solver, that takes an structured input (we could also split an parse a marked up word as shown in the 2nd statement) and for each position, includes or excludes the letter (at position) as needed.// Laser 🟩🟨⬜🟩⬜WITH [{char:l,match:true},{char:a},{char:s,match:false},{char:e,match:true},{char:r,match:false}] as inputMATCH (w:Word)CALL {    WITH w, input    UNWIND range(0,size(input)-1) as idx    WITH size(input) as total, idx, w, input[idx].match as m, input[idx].char as char    // for matching must match position    WHERE (m AND exists { (w)-[{idx:idx}]->(:Char {char:char}) })    // for non-matching must not contain      OR (m = false AND NOT exists { (w)-->(:Char {char:char}) })    // existing must contain somewhere      OR (m IS NULL AND exists { (w)-->(:Char {char:char}) })    // all conditions need to match for this word    WITH total, count(*) as count WHERE count = total    RETURN true AS found}RETURN w.nameFor ‘laser’ this returns 9 suggestions for the next word: label, laced, lacet, lacey, laded, laden, laked, lamed, lapel, lated, laten, latex, laved, lawed, laxed, layed, lazed, lutea, lycea.We could now rank them by the information gain, i.e. which of those have high frequency new characters or reduce most of the uncertainty.I leave that for you valued reader for now, so that we can have a look at (the much simpler) wordle game implemented using Cypher.Playing Wordle in Your TerminalIf you just want to play, run ./wordle-neo4j.sh in your terminal, it sends a Cypher query to a wordle database in demo.neo4j.labs.com (username, password, database = wordle) to see if your guesses were right.Playing wordle in the terminal backed by Neo4jThe statement that it’s running with the guesses in a loop (via http but could also do via cypher-shell) is:match (w:Word)with w skip $word limit 1// turn guess and word into listswith split($guess,) as guessed, split(w.name,) as letters, w.name as name// iterate and combine for each positionreturn reduce(res=, idx in range(0,size(letters)-1) | res +  // when correct  case when guessed[idx] = letters[idx] then 🟩  // when contained  when name contains guessed[idx] then 🟨  // otherwise  else ⬜ end) as resHappy guessing and playing with the Wordle data!Let us know if you have more ideas / suggestions on how to have more fun with this or other datasets on AuraDB Free.;Feb 14, 2022;[]
https://medium.com/neo4j/using-the-bi-connector-to-query-neo4j-with-sql-372eacb08fbc;David AllenFollowApr 6, 2020·4 min readUsing the BI Connector to Query Neo4j with SQLNeo4j recently released the BI Connector, which is a general JDBC driver that can process SQL queries against a Neo4j graph and return meaningful results. This enables users of tools like Tableau, that generate SQL queries, to plug directly into graphs. In this article, we’re going to show how you can query Neo4j directly using SQL rather than cypher.Test Neo4j DatabaseFor my test data, we’re going to load this cypher snippet into a Neo4j 3.5 series database, which just loads a list of cities and countries worldwide.If you’re using Neo4j 4.0, you can also use this sample, just remember to change the toInt() in the cypher to toInteger() as that changed in Neo4j 4.0.Once we have the data loaded in, we can see a simple graph of cities & their countries. Here, we’re looking at all US cities with more than 3 million population.Connect to Neo4j with the BI ConnectorI’m going to be using an app called SQuirrel SQL Client in these screenshots, which can act as a general SQL command shell for any database. But you don’t have to use this, the general approach that we’re using applies to most tools. Neo4j published a knowledge base article as well on how to use another tool called SQL Line in a similar way.We simply fill out a few settings, with the path to the driver JAR file, the driver class, and so on.Add JDBC driver to SQuirrel SQLThis lets our tool know about the BI Connector in general, and how to connect to Neo4j. Notice the driver class, and also the extra class path, which needs to point to the JAR file that comes packaged with the BI Connector. Take note of the class name argument, this is needed for all JDBC drivers.Now that the driver is set up, we can create a connection / alias to the machine we want to use, like this:Setting up a connectionImportant: notice the StrictlyUseBoltScheme=true part. Because I’m connecting to a local database with Neo4j Desktop, this is how I force the driver to use the bolt:// connection scheme. By default, it will try to use bolt+routing:// or the neo4j:// scheme. So make sure you using the appropriate setting depending on your Neo4j deployment. Use of this setting will work with any Neo4j install, but it is only required for stand-alone instances.After testing the connection, we’re looking good!Active connectionAs you can see, once we’ve connected, to the SQL client it looks like this is a single relational database with two schemas named Node” and Relationship”. Under the Node” schema, we have two tables:CityCountryEach table has its own set of attributes, which are the properties of the node label in Neo4j.In the Relationship” schema, we have a single relationship table. Just by its name, you can guess that the City_IN_Country table represents the (:City)-[:IN]->(:Country) path in Neo4j.Simple QueriesLet’s try an easy one:SELECT _NodeId_, name, population FROM City WHERE population > 5000000 ORDER BY name ASC LIMIT 10This allows us to get a certain selection of fields, for only cities with more than 5 million in population, ordered by their name. Behind the scenes, the BI Connector translates the SQL into the necessary Cypher bits, and fetches data from Neo4j in real time.We can also do much more complex SQL queries as well. Here is a join that connects three tables (City, City_IN_Country, and Country) via an equijoin in SQL, and places other constraints as well.This query will show an important pattern you’ll see consistently throughout the BI Connector. Every node gets a _NodeId_ column, which corresponds to the ID of the node in Neo4j. Every relationship table will get _SourceId_ and _TargetId_ columns that allow you to join and navigate from one node to the other. Effectively, relationships become join tables.Happy graph & table hacking!Further ReadingDownload the BI Connector — The BI Connector is available at no extra charge for Neo4j Enterprise Edition customers.Check out the Neo4j Blog Post on how the technology works!Have a look at instructions on using it with Tableau Desktop;Apr 6, 2020;[]
https://medium.com/neo4j/put-data-modeling-on-the-table-61650e44141e;Shani CohenFollowSep 13, 2021·7 min readPut Data Modeling on the TableData modeling is out of date. This conclusion became apparent when I tried locating new data modeling books, research studies, and recent developments in data modeling. It makes me wonder whether the question we should all be asking is whether we even have the right way to model data?Photo by Campaign Creators on UnsplashA data models main purpose is to ensure data is understood, stored, and retrieved from databases consistently and in accordance with business rules. Additionally, the visualizations of data models allow different individuals and teams to communicate with each other more efficiently. Effective visualization fosters high-level cognitive abilities, including classification, unification, knowledge completion, and reasoning.Who Is the Data Modeler?We embrace modeling, but not in order to file some diagram in a dusty corporate repository.— Agile ManifestoHave you ever had the feeling of playing a game of Chinese Whispers while you’re trying to figure out what the original requirement was? The data model is intended to prevent this. By creating intuitive visualizations, we can share knowledge and improve communication between engineers, QAs, product managers, data analysts, DevOps, technical writers…Agile isn’t an excuse for poor design. Yes, we need to move fast, adapt to changes, and provide flexible systems. We cannot afford the whole, long design phase like we did in the past. Yet our task is even more difficult — we must build a puzzle without knowing how the whole picture fits together.Software engineers/DevOps/product managers took responsibility for modeling as the design phase began to break up into chunks. For the model to be an effective communication tool for both technical and non-technical persons, it must be clear and understandable.Relational databases — with their rigid schemas and complex modeling process — aren’t a good fit for rapid change. Schema migration is literally no one’s favorite database activity. Once the database goes live and is in production, we just don’t want to change it. This is what the agile support data model is meant to remedy — it supports ongoing evolution while maintaining the integrity of your data.Relational Schema Abstraction LeaksFor years, relational data models have been the standard. What if normalized relational schema models can’t represent logical entities efficiently?Normalizing the schema reduces redundancies and eliminates anomalies. Writing to the database efficiently is dependent on this. The normalized schema is structured in such a way that there is no redundancy or duplication. When some piece of data changes, you only need to change it in one place, with no need to ensure the change is applied across many copies of the data in many different locations. However, many simple read queries require expensive JOIN operations.Impedance mismatch is a term, adopted from the electronic industry, that describes the mismatch between objects and tables. An object-oriented model connects objects by inheritance or composition, while a relational model is just a collection of tables connected by foreign keys. We are using ORM as a middleware to translate and abstract the differences between the layers to close this gap.Application layer objects are actually written to multiple tables in the relational database. Every time an object is needed, a JOIN table creates the original object. In this split and merge way, an ORM layer abstracts the data structure of the database.Object-relational mapping (ORM) frameworks like Hibernate reduce the amount of boilerplate code required for this translation layer, but they can’t completely hide the differences between the two models.Data Is in a Relationship, and It’s ComplicatedWhat are the methods for storing relationships of different types within normalized databases?One-to-many/many-to-one relations are stored in separate tables. Each identifier of the one” entity is stored in the many” table for each row that belongs to the entity.One-to-many, and sometimes many to one, are often implemented by embedding or nesting the many”, in schemas other than relational. While nesting objects allows reading and writing to be done in one operation, thus solving the impedance mismatch problem, the RDBMS supports for nesting objects are poor.Many-to-many relationships add complexity and confusion to the relational model and the application development process. The key to resolving many to many relationships is to separate the two entities and create two one-to-many relationships between them with a third intersect entity. The intersect entity usually contains attributes from both connecting entities.There’s no doubt that we can use RDBMS to implement almost any case, but is that really the general approach? If hierarchical, nested objects are stored, the depth of nested objects is missed. If a reach, connected domain object is being used, it will struggle to navigate these relationships effectively.Graph Model Can Do ItAs time goes on, more and more domains become more complex, relationships among entities are stronger, and the world generally becomes more interconnected. Only a database that natively embraces relationships can store, process, and query connections efficiently. A graph database stores connections alongside the data in the model.In a graph database, the relationships between the data are just as important as the data itself. It is intended to hold data without constricting it to a pre-defined model. Instead, the data is stored like we first draw it out, showing how each entity connects with or is related to others.For highly interconnected data, the hierarchical/document model is awkward, the relational model is acceptable, and graph models are the most natural.Whiteboard-Friendly ModelingData modeling building blocks are the three independent schemas: Conceptual, Logical, and Physical (which in many cases, is just the DB schema).Conceptual Data ModelA concept map is a tool that visualizes relationships between concepts. Concept mapping arose from the field of learning psychology and has proven to be useful to identify gaps and loopholes, and enhances the learning of science subjects.While UML and entity-relationship diagrams failed as business-side tools, concept mapping is readily accepted in its place. Concept mapping allows both businessmen and technical experts to communicate intuitively and visually. Furthermore, current knowledge about early learning emphasizes concept maps.Image from Graph Modeling Guidelines, Neo4jLogical Data ModelWhat graph databases do very well is represent connected data. Representing any model is about communicating these two perspectives:Structure (connectedness)Meaning (definitions)Together they explain the context very well. Nodes represent entity types, which I prefer to call types of business objects. Edges, better known as relationships, represent the connectedness and, because of their names, bring semantic clarity and context to the nodes. Concept maps exploit the experiences from educational psychology to speed up learning in the design phase. That is why the labeled property graph model is the best general-purpose data model paradigm currently available. Expressed as a property graph, the metamodel of property graphs used for solution data modeling looks like the following model:Image from Graph Modeling Guidelines, Neo4jProperty graphs are similar to concept maps in that there is no normative style (e.g. like there is in UML). So feel free to find your own style. If you communicate well with your readers, you have accomplished the necessary.The limits of my language mean the limits of my world.”— Ludwig WittgensteinIn his book Designing Data-Intensive, Martin Kleppmann states: Data models are perhaps the most important part of developing software because they have such a profound effect not only on how it is written but also on how we think about the problem.”There has been a strong focus on relational modeling and normalization for years. As I wrote, however, there’s something wrong with that approach. The normalized tables collection does not fit with what we planned on the board. The relationships between entities do not clear for anyone other than business analysts/data modelers (Crow’s Foot notation — anyone?)Graph schema does not modify the data model to fit a normalized table structure. The graph data model stays exactly as it was drawn on the whiteboard. This is where the graph data model gets its name for being whiteboard-friendly.” Additionally, it embraced schema changes as a fact and prepared for them, instead of striving to avoid them.I suggest talking about graph schemas as intuitive, learning-effective, communication-friendly, and aligning from the moment we draw on the board to the storage layer.All models are wrong, but some are useful.”— George BoxReferences:[1] Martin, Designing Data-Intensive Applications (2017), O’Reilly Media, Inc.[2] F, Thomas. Graph Data Modeling for NoSQL and SQL (2016), Technics Publications.;Sep 13, 2021;[]
https://medium.com/neo4j/spring-data-neo4j-rx-released-into-the-wild-f1473951f91d;Gerrit MeierFollowApr 15, 2020·4 min readSpring Data Neo4j RX released into the wildFirst 1.0 Spring Data Neo4j RX GA Release is now availablePlease note that Spring Data Neo4j RX replaces Spring Data Neo4j in the future. More information can be found here.After we presented the features that came with the third beta of Spring Data Neo4j RX two months ago, we are happy to announce that we have just released the first GA version and are happily waiting for your feedback. Make sure that you also check out the introduction post we have written to get more information about the concepts and ideas that went into the library.Let’s have a look at the lately introduced features and improvements that made it into the 1.0 release.DocumentationEven though it is not a feature of the library itself it is a very important part for every product. The documentation is available at https://neo4j.github.io/sdn-rx/current/The current path will always point to the latest released version.Improved CypherDSLOne of the things we are trying to avoid while developing SDN/RX is String-concatenation. So we came up with the idea to design our own DSL to produce type-safe Cypher statements for us.Somehow this considered internal API got attention from the community and more requests came our direction for improving the API. We heard you and we came to the conclusion to make most of the API publicly available. But for the 1.0 release, the API will be still marked as experimental and should be considered as unstable and changing until it officially reaches stable state.LabelsNow it is possible to let SDN/RX create multiple labels for your annotated entities. You can achieve this in two different ways, depending on your use-case.Use the @Node annotation with additional parameters:This means that you can provide either multiple Strings in the annotation’s value / labels property or make use of the primaryLabel .The first value of the labels array will be considered as the primary label. This means it is the label all the operations will base their work on during query creation, mapping, etc.@Node(primaryLabel =  Cat , labels = { Animal, Pet })public class Cat {   private String name}Or use Java’s inheritance for label-inheritance”:You can annotate an abstract class with @Node(optionally with additional labels) and extend it in another annotated entity. This comes in handy if you consider for example a Person who owns Pets. The Pet is the abstract type of a relationship definition in the Person entity but there are multiple implementations like Dogor Cat .So you can now load the Person and SDN/RX will query for Pet to load the relationships and this will find all :Pet:Cat or :Pet:Dog labeled nodes in the graph.@Nodepublic abstract class Pet {   @Id private Long id}@Nodepublic class Cat extends Pet {   private String name}Causal Cluster BookmarksIf you have multiple transactions in the same thread or the same reactive stream, SDN/RX will take care that you will read your own writes. It makes use of the bookmarking mechanism provided by Neo4j (enterprise) in a transparent way. There is nothing to do on your side if you are running a causal cluster.Optimistic LockingWe support now Spring Data’s @Version attribute for optimistic locking. You can use it to avoid simultaneous modification of a given entity. The field has to be of type Long.@Nodepublic class VersionedThing {   @Id   @GeneratedValue   private Long id   @Version   private Long myVersion   /*...*/}Configurable repository base classThere are some cases when you need to extend the repository base class SimpleNeo4jRepository or its reactive counterpart — for example, if you want to overwrite the default behaviour of the load method or similar.We opened up those classes to give you all the freedom (and responsibilities) you may want to make use of.Get the BinariesRight now Spring Data Neo4j RX is not available on start.spring.io and you have to declare the dependency by yourself in your Spring Boot application:<dependency>   <groupId>org.neo4j.springframework.data</groupId>   <artifactId>spring-data-neo4j-rx-spring-boot-starter</artifactId>   <version>1.0.0</version></dependency>If you have chosen not to use Spring Boot for your Spring application, you can also refer to SDN/RX directly via:<dependency>   <groupId>org.neo4j.springframework.data</groupId>   <artifactId>spring-data-neo4j-rx</artifactId>   <version>1.0.0</version></dependency>This will require (at least) to define the @Enable(Reactive)Neo4jRepositories in your configuration and providing a bean with a configured Neo4j Java Driver.More informationsTo get more informations about Spring Data Neo4j RX:Blog post on Reactive multi-tenancy with Neo4j 4.0 and SDN/RXSDN/RX on GitHubReference documentationA big thank youWe couldn’t have done this without you, lovely community ♥️Please keep up the good work by providing bug reports, feature ideas, and similar on the GitHub issue page.;Apr 15, 2020;[]
https://medium.com/neo4j/goodbye-sdn-%EF%B8%8Frx-eccee8e18d00;Gerrit MeierFollowJun 30, 2020·3 min readGoodbye SDN⚡️RXWelcome Spring Data Neo4j 6.0Last Thursday our friends of the Spring Data team at VMWare Tanzu released the first milestone of the upcoming release train Ockham (2020.0.0). As you have might seen there were not a lot public facing changes to Spring Data Neo4j in it. But there is something big coming to Ockham.Photo by Mark Autumns on UnsplashRemember our posts about SDN/RX?Welcome SDN⚡️RXReactive multi-tenancy with Neo4j 4.0 and SDN/RXSpring Data Neo4j RX released into the wildWe have been working on this new Spring Data Neo4j module for over a year now and after 10 releases, including 4 stable ones, we think it’s time for SDN/RX to replace SDN+Neo4j-OGM in the next big release train of the Spring Data modules. While we dubbed SDN/RX always RX, it has never been about reactive only, but also supports all the same features for the imperative programming model.Let’s start movingBehind the scenes we started to plan the steps forward for migrating Spring Data Neo4j RX to the Spring Data family. Together with the Spring Data team we decided that the best way is to see SDN/RX as the natural successor of SDN. Given the current schedule for the Ockham release train, we aim for the milestone 2 release around mid August.TransparencyIt is important for us to communicate the upcoming changes in a timely matter.The least amount of changes will be required for SDN/RX users themselves. You will have to change the Maven or Gradle coordinates and, most likely, the namespaces of the driver configuration. If you, however, are an existing SDN+Neo4j-OGM user, there will more work involved migrating your existing project.To get familiar with those migration related topics, we prepared a migration guide in our documentation. This will get updated more and more over the time when we release the M2.What will change?Besides the obvious move of the project itself, we would like to outline the changes that come with the move to Spring Data.We will extract the OpenCypherDSL, that is right now a module of SDN/RX but already has its own maven coordinates, as a project on its own.The Spring Boot starter and auto-config will be the update for the existing Spring Data Neo4j starter in Spring Boot.The Spring Boot auto-config for the Neo4j Java Driver will also find its place in the Spring Boot project.The examples will be part of the Spring Data examples.When will it get released?Please keep in mind that this is our roadmap for roughly the next four months and a lot can change during this time. If everything works out as expected, Spring Data Neo4j 6.0 will get released with Ockham by the end of October.This also means that it will get picked up as the Spring Data Neo4j in Spring Boot 2.4 and onwards.What is the impact for me now?Until the official release of the milestone 2 version, we will keep the Spring Data Neo4j RX project active on GitHub. So issues, feature requests, and similar can be posted there. We will migrate open ones after this milestone release to the Spring Data Jira and set the project to archived state.If we encounter a serious problem in SDN/RX, we will, of course, release another bugfix release, but feature-based releases are planned to continue with the official Spring Data versions in the future.Thanks to you allWe know that this move, depending on your current setup, will require changes to your code base. Thanks to all the users of Spring Data Neo4j as it is today and our new user base of Spring Data Neo4j RX for providing us with a lot of valuable feedback.We want to make the move as smooth as possible and we aimed for the earliest possibility to bring SDN/RX into the Spring Data modules. There shouldn’t be a SDN/Neo4j-OGM and SDN/RX release living in parallel for too long to avoid confusions about the versions used and their feature sets.If you have any questions, reach out to us on our community site. We are so happy and thrilled to move Spring Data Neo4j to the next level with you. ♥️;Jun 30, 2020;[]
https://medium.com/neo4j/exploring-neodash-for-197m-chemical-full-text-graph-e3baed9615b8;Tom NijhofFollowDec 28, 2022·2 min readExploring Neodash for 197M chemical full-text graphIn a previous blog, I loaded 197M chemical names into a graph database. All of these are indexed with a full-text index and use the graph properties to improve the search.In order to test the user experience (not just typing queries), I built a python backend with fastAPI and a front end with VUE.js.This works with 2 additional languages and frameworks, but then I saw a talk by Niels de Jong at node2022 about NeoDash.NeoDash promises an easy dashboard to explore your Neo4j database, and it delivers!It is a low-code tool where I can use Cypher queries to populate blocks like line graphs, tables, maps, and more.Besides those, it also has an input field to manipulate the results as you go.Adding NeoDashIn my current solution, I use the python backend for some string manipulations before I use it for the Cypher query. In NeoDash I only have Cypher, but the Apoc package gives the functionality to Cypher I need.# Python implementationimport regex as reall_words = re.findall(r [\p{L}\d]{2,} , chemical_name)Lucende_query =  ~ AND  .join(all_words) +  ~ all_words = re.findall(r”[\p{L}\d]{2,}”, chemical_name)// Cypher implementationWITH apoc.text.split($neodash_chemical_1_name, ‘[^\p{L}\d]’) as cleaned_inputWITH [val in cleaned_input WHERE SIZE(val) > 1] as cleaned_inputWITH (apoc.text.join(cleaned_input, ~ AND ) +  ~ ) as lucende_queryThe VUE part is easier to replace, by just creating 2 blocks, one table, and one parameter selection. This means I have an input field and a result field.Adding the full query to the table block gives me a fuzzy full-text search on chemical names, with graph enhancements, and no additional code.// Full chemical searching Cypher queryWITH apoc.text.split($neodash_chemical_1_name, ‘[^\p{L}\d]’) as cleaned_inputWITH [val in cleaned_input WHERE SIZE(val) > 1] as cleaned_inputWITH (apoc.text.join(cleaned_input, ‘~ AND ‘) + ~”) as lucende_queryCALL {  WITH lucende_query  CALL db.index.fulltext.queryNodes(synonymsFullText”, lucende_query)  YIELD node, score  RETURN node, score limit 50}OPTIONAL MATCH (node)-[:IS_ATTRIBUTE_OF]->(c:Compound)WITH DISTINCT c as c, collect({score: score, node: node})[0] as sWITH DISTINCT s as s, collect(c.pubChemCompId) as compoundIdRETURN s.node.name as name, s.node.pubChemSynId as synonymId, compoundId, s.score as score limit 5Resulting dashboardConclusionNeoDash works great for prototypes, and you can even host them. The setup is much easier than building your own front + back end. I will use it again for a future project.With the update of NeoDahs 2.2.2 this comment is no longer relevant:̶I̶ ̶d̶i̶d̶ ̶n̶o̶t̶i̶c̶e̶ ̶t̶h̶e̶ ̶u̶p̶d̶a̶t̶i̶n̶g̶ ̶o̶f̶ ̶t̶h̶e̶ ̶t̶e̶x̶t̶ ̶d̶i̶d̶ ̶n̶o̶t̶ ̶a̶l̶w̶a̶y̶s̶ ̶g̶o̶ ̶s̶m̶o̶o̶t̶h̶l̶y̶,̶ ̶o̶r̶ ̶t̶h̶e̶ ̶l̶o̶a̶d̶i̶n̶g̶ ̶o̶f̶ ̶t̶h̶e̶ ̶r̶e̶s̶u̶l̶t̶s̶ ̶t̶i̶m̶e̶d̶ ̶o̶u̶t̶,̶ ̶s̶o̶ ̶I̶ ̶w̶o̶u̶l̶d̶ ̶n̶o̶t̶ ̶u̶s̶e̶ ̶i̶t̶ ̶f̶o̶r̶ ̶a̶ ̶u̶s̶e̶r̶-̶f̶a̶c̶i̶n̶g̶ ̶p̶r̶o̶d̶u̶c̶t̶ ̶f̶o̶r̶ ̶n̶o̶w̶.̶;Dec 28, 2022;[]
https://medium.com/neo4j/article-recommendation-with-personalized-pagerank-and-full-text-search-c0203dd833e8;Mark NeedhamFollowFeb 27, 2019·4 min readArticle recommendation with Personalized PageRank and Full Text Search6 months ago Tomaz Bratanic wrote a great blog post showing how to build an article recommendation engine using NLP techniques and the Personalized PageRank algorithm from the Graph Algorithms library.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comIn the post Tomaz extracts key words for each article using the GraphAware NLP library, and then runs PageRank in the context of articles based on these key words.I was curious whether I could create a poor man’s version of Tomaz’s work using the Full Text Search functionality that was added in Neo4j 3.5, and so here we are!Tomaz explains how to import the data in his post, so we’ll continue from there. The diagram below shows the graph model that we’ll be working with. We have articles written by authors, and those articles can reference each other.Graph ModelThe first thing we need to do is create a Full Text Search index for our Article nodes. We’ll index the title and abstract properties on these nodes.CALL db.index.fulltext.createNodeIndex(articlesAll,   [Article], [title, abstract])We can check on the progress of the index creation by running the following query:CALL db.indexes()It will have a state of POPULATING while node properties are being added to the index. This state will change to ONLINE once it’s done. The following query will block until the index is online:CALL db.index.fulltext.awaitIndex( articlesAll )Now that we’ve done this, let’s get on with the algorithms.Social Network Analysis PapersTomaz first explores articles that contain the phrase social networks”. Let’s create a parameter containing that search term::param searchTerm =>  social networks Not that we’ve put the search term in quotes. We do this so that Full Text Search will treat the term as a phrase rather than interpreting each term separately.Now we want to call the PageRank algorithm from the point of view of articles that contain this search term. Let’s first see how many articles the full text index comes back with:CALL db.index.fulltext.queryNodes( articlesAll , $searchTerm)YIELD node, scoreRETURN count(*)Just under 15,000 nodes, or around 0.5% of all articles are returned by the query. The following query will return the top 10 articles for the search term:CALL db.index.fulltext.queryNodes( articlesAll , $searchTerm)YIELD node, scoreRETURN node.id, node.title,  scoreLIMIT 10Now we can feed these nodes into the PageRank algorithm as the sourceNodes config parameter. This will bias the results of the algorithm around these nodes.The following query will find us the most influential articles about social networks:CALL db.index.fulltext.queryNodes( articlesAll , $searchTerm)YIELD nodeWITH collect(node) as articlesCALL algo.pageRank.stream(Article, REFERENCES, {   sourceNodes: articles })YIELD nodeId, scoreWITH nodeId,score ORDER BY score DESC LIMIT 10RETURN algo.getNodeById(nodeId).title as article, scoreAs in Tomaz’s post, Sergey Brin and Larry Page’s paper describing Google shows up in first place.Entropy to me is not entropy to youIn the next part of the post, Tomaz shows how we can write queries to find papers that would be interesting to researchers in different fields.Recommendation of articles described by keyword entropy” from the point of view of Jose C. Principe.Let’s setup parameters::param authorName =>  Jose C. Principe :param searchTerm =>  entropy And now run the query:MATCH (a:Article)-[:AUTHOR]->(author:Author)WHERE author.name=$authorNameWITH author, collect(a) as articlesCALL algo.pageRank.stream(  CALL db.index.fulltext.queryNodes( articlesAll , $searchTerm)   YIELD node   RETURN id(node) as id,  MATCH (a1:Article)-[:REFERENCES]->(a2:Article)    RETURN id(a1) as source,id(a2) as target,   { sourceNodes: articles,     graph:cypher,     params: {searchTerm: $searchTerm}})YIELD nodeId, scoreWITH author, nodeId, score WITH algo.getNodeById(nodeId) AS n, scoreWHERE not(exists((author)-[:AUTHOR]->(n)))RETURN n.title as article, score, [(n)-[:AUTHOR]->(author) | author.name][..5] AS authorsorder by score desc limit 10We’ll see these results:And what about if we run the same query for a different author?:param authorName =>  Hong Wang We’ll see this results:We don’t get exactly the same results as Tomaz, but we do still get a different set of results for the different authors.SummarySo in summary, it does seem that we can get a reasonable approximation of Tomaz’s post using Neo4j’s Full Text Search functionality.If you have any other ideas of what we can do with this dataset, let me know by emailing devrel@neo4j.comFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Feb 27, 2019;[]
https://medium.com/neo4j/graphconnect-coming-to-nyc-in-3-weeks-769664022655;ryan boydFollowAug 31, 2018·2 min readGraphConnect coming to NYC in 3 weeks!This year I’ve had the pleasure, once again, to help organize the GraphConnect annual conference developers, architects, data modelers, business execs and others looking to connect with the graph community.We have awesome content on GraphQL, AI/ML, Neo4j, Discovery and Visualization, Graph Algorithms and ETL/modeling.Every year, we’ve sought to add additional awesome talks to this event- both by extending the day (sorry, not sorry!) and by adding additional tracks. This year, we have 5 tracks of full-length sessions and one track of lightning talks.Ashley Sun from LendingClub talking about microservices Ops with Neo4jWe started doing lightning talks two years ago. Last year, it was a bit crowded due to all the awesome talks, so we moved into a much larger room this year for those that like fast and short 10–15 minute presentations.We’ve also started doing a DevZone at the event for everyone to relax, gather with Neo4j staff and other members of the ecosystem, and get help with graph modeling and querying challenges from our GraphClinic.Our training classes and workshops have expanded too!Workshops availableAnd….. this year our hackathon is a day-long Buzzword Bingo event at StackOverflow HQ, co-sponsored with a number of local NYC tech communities. We already have almost 150 people signed up for a day of hacking.Hope you can join us September 20–22nd in NYC!!!!Register today: https://graphconnect.com/;Aug 31, 2018;[]
https://medium.com/neo4j/hot-reload-your-infrastructure-as-code-with-skaffold-2f2e6ec6a6d0;Max AnderssonFollowJul 21, 2021·3 min readHot Reload Your Infrastructure as Code with SkaffoldIf the Kubernetes bug has bitten you, there are usually some pain points that won’t go away. Setting up a functional development environment that is as close to production as possible may take some effort. Skaffold makes it somewhat effortless.SkaffoldFast. Repeatable. Simple. Local Kubernetes Development. Skaffold handles the workflow for building, pushing, and…skaffold.devSo why would you want to hot-reload your infrastructure? As tools for developing software have become much better over the years, it’s all about shortening those feedback loops until you can see the changes your code has done and reflect upon them. If feedback time goes down, so does your development time. And it’s about time we did the same for our infrastructure.Photo by Xiong Yan on UnsplashContinuous integration and continuous delivery/deployment have been making a lot of headroom the last few years and have enabled companies to ship code with shorter development cycles and pushing features to production faster. But that also assumes that you already have your development/staging and production environments already set up.Sometimes your code depends on quite a lot of infrastructure. For developers to manage that, it quickly becomes a pain to get everything to work properly.Docker Compose definitions are sometimes enough for the development cycle. Still, when it is more advanced setups, docker-compose quickly becomes hard to maintain. If youre not using Docker Swarm in production changes, your production configuration differs significantly from your local development environment.Kubernetes has become the de-facto standard for container orchestration and is offered by all major cloud providers, making it a perfect choice for companies and enterprises to define their deployments on Kubernetes and have the ability to reproduce their cluster set up anywhere and any time.So Skaffold can use these definitions to empower your development experience and get a more fluid integration when running code on Kubernetes.Setting up SkaffoldWe will start with the GitOps project we created in the previous blog post as a starting point.How to Set Up GitOps for a Full-Stack App with Flux & Helm on KubernetesManaging infrastructure can be a pain, so much so that there are many attempts to make configuring, provisioning, and…medium.comYou could use any Kubernetes definition or, in our case, a helm chart to set up a deployment. You might want to use Minikube, Microk8s, Docker Kubernetes, or some other local Kubernetes installation, but you can use any Kubernetes cluster.If you’re running a Mac, you can install Skaffold with homebrew. In any other case, you can find the installation instructions here.brew install skaffoldSince Skaffold does not support the automatic setup for helm charts, we will have to set it up manually. Your finished file skaffold.yaml should look something like this.apiVersion: skaffold/v2beta13kind: Configbuild: local: {} artifacts:   - image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/flexcapacitor-ui     context: src/fluxcapacitor-ui   - image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/flexcapacitor-db     context: src/fluxcapacitor-db   - image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/flexcapacitor-api     context: src/fluxcapacitor-api deploy: helm:   releases:   - name: dev-release     namespace: dev-release-fluxcapacitor     createNamespace: true     chartPath: charts/fluxcapacitor     artifactOverrides:       api.image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/flexcapacitor-api       db.image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/flexcapacitor-db       ui.image: xxxxxxxxxx.dkr.ecr.us-east-1.amazonaws.com/flexcapacitor-ui portForward: - resourceType: service   resourceName: api   namespace: dev-release-fluxcapacitor   port: 4001   localPort: 4001 - resourceType: service   resourceName: neo4j   namespace: dev-release-fluxcapacitor   port: 7687   localPort: 7687 - resourceType: service   resourceName: ui   namespace: dev-release-fluxcapacitor   port: 3000   localPort: 3000And then you will be able to run.skaffold dev --port-forwardThat will deploy your stack in a separate namespace and proxy the services back to your local machine. At the same time, it watches for changes to your application, then builds and deploys a new revision as soon as you make code changes — perfect for developing an application on a more complicated architecture, because you can make changes to the infrastructure to fit your use case.It’s as simple as that!Check out my other blog post on getting started with GitOps, another tool for infrastructure development. Or check out the DockerCon ’21 session where we put this to use.;Jul 21, 2021;[]
https://medium.com/neo4j/the-power-of-the-path-part-1-aa23254d89f6;Kees VegterFollowMar 27·8 min readThe Power of the Path — part 1IntroductionIn the Neo4j Database the Path is the data type which represents the Graph Structure. The Path is different than the classic data types from other database types. Working with Paths gives you extra possibilities, this is the beginning of a series of posts about the Cypher Path. First we start with explaining the Path.The Path data typeCypher has several types representing data in the database like property value types (String, number, etc) and the Node and the Relationship. The Path type is not a specific Entity to be stored in the database but a type to capture the structure of nodes and relationships as a result of a query.The Path can be obtained by assigning a pattern to an identifier like MATCH p=<pattern> for example:The Neo4j browser can handle the Path data type and displays the results of this query as follows:The Neo4j browser is showing the ‘sub-graph’ as a result of this query hiding the fact that the result of this query has multiple records with a path in it. For every occurrence of the pattern you will get a record with a path returned by this query.Path StructureNow we have seen a query returning paths what is the actual structure of a path? The internal structure of the path is an array of nodes and relationships in the way the relationship directions are specified in the cypher statement. For the query above the Paths returned have the following structure:For each relationship the start and end nodes are given in the order as specified in the pattern.Path FunctionsNow we explained the structure of a Path, how can we ‘work’ with it? The following functions are available:length(path)The length of a Path is the number of relationships in the path. In the example above it is:length(p) = 2nodes(path)This function returns an array of nodes in the path in the order they are traversed:nodes(p)=[{Node (Person Tom Hanks)} , {Node (Movie Cast Away)} , {Node (Person Robert Zemeckis)} ]Note that we don’t have the ‘duplicate’ {Node (Movie Cast Away)} anymore.relationships(path)This function returns an array of relationships in the path in the order they are traversed:relationships(p)=[{Relationship (ACTED_IN)}, {Relationship (DIRECTED)}]With the Nodes or Relationships in an array we can apply normal array functions on them for instance getting the last Node by using nodes(p)[-1] or getting the last relationship by using relationships(p)[-1].Relationship Functions and Path DirectionWhen working with Paths, you can for example obtain the last relationship and check if the start node of the relationship has a certain label. In this case you have to be aware that the startNode(relationship) and endNode(relationship) functions are relative to the relationship and not to the Path as shown in the following example:MATCH p=(a:Person)-[rel1:FOLLOWS]->(b:Person)<-[rel2:KNOWS]-(c:Person) RETURN pp=[a:Person, rel1, b:Person,b:Person, rel2, c:Person]We can now get the start node of the last relationship like this:startNode(relationships(p)[-1]) = c:PersonWhen you look at the Path perspective the first node of the last relationship is b:Person. Be aware of this, the relationship startNode/endNode functions are always relative to the relationship, not to the Path!UniquenessThe Paths returned by a query are the results of a traversal over the graph. In a Graph everything is connected and it is not hard to imagine that we may walk over the same relationship twice. Take for example the following data with Person Nodes and a name property:If we use the querywe expect to have the results a,b,c,d and a,b,a,b for personNames but we get:The reason is that the query engine guarantees that per MATCH pattern it walks over a relationship once per direction in a Path. In other words the query engine is implementing the RELATIONSHIP_PATH ‘uniqueness’ in mathematical terms it is the TRAIL traversal.It is good to know that Cypher behaves like this, take for instance the following query to find the coActors of Tom Hanks in the movie Cast Away:Due to the RELATIONSHIP_PATH uniqueness we know that the ‘tom ACTED_IN m’ relationship will not be walked over again when we walk to the co-actor.It is very important to realise that the RELATIONSHIP_PATH uniqueness applies per MATCH <pattern>. When we put an extra MATCH in the query we get a different result:The second MATCH initiates a new uniqueness rule/check giving back all the actors in the movie Cast Away.Now that the basics of the Path is explained we dive a bit deeper in the graph by variable length paths.Variable Length PathsIt is possible to specify patterns where you don’t know how deep the path will be. Because Neo4j persists the data as a graph in nodes and relationships it is possible to specify patterns like ()-[*]-() where we just walk over the graph without a limit on the depth. The database does not have to calculate the ‘joins’ between nodes at query time, it can just walk over the persisted relationships without using indexes.As mentioned you can specify a variable path with a ‘*’ in the relationship section. A ‘*’ means paths with a length 1 to infinite and is a shorthand for *1..”. It is also possible to specify the minimum and maximum depth of the traversal with ‘*min..max’. If max is omitted then the length is infinite.[*1..3] means minimal length is 1 and maximum length is 3. [*3] (is *3..3) means only paths with length 3.Imagine that we have a database with the following nodes and relationshipsWhen we do a MATCH p=(a:Person {name :”Kees” })-[:KNOWS*1..2]->(b) we will get back paths with a length of 1 and a length of 2:Kees,TomKees,Tom,JohnKees,JuliusWhen we do a MATCH p=(a:Person {name :”Kees” })-[:KNOWS*]->(b) we will get back the following paths:Kees,TomKees,Tom,JohnKees,Tom,John,MarieKees,JuliusAs you see the Cypher engine goes first to the deepest level also known as Depth First Search (DFS).When we do a MATCH p=(a:Person {name :”Kees” })-[:KNOWS*3]->(b) we only searching for paths with a length of 3. So with this data we will get back the following path:Kees, Tom, John, MariePath Length 0It is possible to specify a 0 for the minimum length (or fixed length) of a Path. This is in some cases useful where you just want to let the match <pattern> be successful even if there is no path found from a node.MATCH (tom:Person {name : ”Tom Hanks” )-[:FOLLOWS*0..1]->(ff:Person) …In this way the pattern behaves like an OPTIONAL MATCH:OPTIONAL MATCH (tom:Person {name: ”Tom Hanks” )-[:FOLLOWS]->(ff:Person)…It can be useful to apply the path length 0 when using pattern comprehensionMATCH (tom:Person {name : ”Tom Hanks” )WITH tom, [(tom)-[:FOLLOWS*0..1]->(f:Person) | f.name ] as tomFollows…Typically this construct can be applied to avoid cartesian products when you have a query with a lot of optional matches after each other as explained in the Cypher Query Optimisations article (under Multiple OPTIONAL MATCH statements”).An important thing to remember when using path length 0 is that when the Path length is 0 the ‘single’ Node must have all the Labels specified in the pattern. In the example above we expect the Person Node in the following pattern something else is happening:MATCH (tom:Person {name : ”Tom Hanks” )-[:FOLLOWS*0..1]->(ff:Writer)…This query will only return a result for path length 0 when there is a Node tom with a Person and a Writer label! It is possible to avoid this by not specifying the Writer label:MATCH (tom:Person {name : ”Tom Hanks” )-[:FOLLOWS*0..1]->(ff)It is important to Model your graph properly so that in this case the FOLLOWS relationship is already enough information to find the requested pattern.Traversal ExplosionAs mentioned the query engine engine will give you back ‘every possible path’ which is in most cases preferred. But there are use cases where it is not beneficial. Let assume the following graph where the red nodes are Locations and the beige nodes are Customers.When we want to traverse the graph from node ‘a’ with a variable depth then we see a lot of possible paths as in shown in the table below:Using a count of the returned paths is a good practice to explore the shape of the data in the graphSo even with this set of 10 nodes and 16 relationships we already get 1102 possible paths. Imagine if you have a cluster of 30 nodes pointing to each other instead of the 6 nodes above.When we want to know to which customers something is transported in this example using the default query engine the query engine has to evaluate a lot of paths to get to the Customer Nodes. In the next article dive deeper on some use case where we use other UNIQUENESS approaches via the apoc.path.expand procedure.If you have questions regarding Cypher Paths, you can always head to the Cypher topic on Neo4j Discord channel to chat directly or go to the Neo4j Community Forum.;Mar 27, 2023;[]
https://medium.com/neo4j/wardley-mapping-with-neo4j-27c0e9acb142;Alexander ErdlFollowFeb 15·3 min readWardley Mapping with Neo4jTom Asel from Tangible Concepts joined me recently to discuss the concept of Wardley Maps: What they are, where they are used, and why they (as well as maps in general) are different from graphs. In a demo, Tom demonstrated how Wardley Maps can be analyzed in Neo4j to find the hidden knowledge in these maps.To watch the full episode scroll to the end of this blog post!What is a Wardley Map?Source: https://www.map-camp.com/_pages/what_is_mapping/A Wardley Map is a visual representation of the value chain of a business, product, or service, showing the progression from raw materials to customer value. It is used to analyze and understand a systems components and identify opportunities for improvement or innovation. Wardley Maps were named after Simon Wardley. See Wikipedia for more information.Every Wardley Map has these core concepts:AnchorThe user from whose perspective the map is drawn.VisibilityThis is in relationship to the anchor and depending on their role, responsibility, and hierarchy the connected components are closer or further away — e.g. a backend developer has more visibility on the database than on the front-end of an app.EvolutionComponents move from left to right depending on the context of the map — e.g. a new AI-Advisor is experimental and therefore on the very left and can move over time with additional development to the right.Wardley Mapping OverviewAfter placing these components on the map you can make assessments on risks and chances. By adding Movement (components becoming more mature on the evolution axis) and Inertia future improvements and blockers become visible.During the video, Tom shows a few advanced Wardley Map examples and explains how they are used for software development.ParsleyWardley Maps in Neo4jWith Parsley you can import Wardley Maps created with the tool Online Wardley Maps into Neo4j.Tom created Parsley to better analyze their Wardley Maps and understand the context.Once imported in Neo4j you lose the visibility of the position on the map, but you have all the connections (relationships) between the components (nodes), which you can now use for deeper analysis of your map. With larger datasets, and more intertwined components, it gets more difficult to see evolution, inertia, or dependencies — especially across various Wardley Maps (remember: they show the view per anchor).Opening Wardley Maps to structured querying was a game changer for Tom: Navigating the graph, finding connections, and through that gaining access to the knowledge that is hidden in the maps.For Tom, the next steps are using pattern matching to find key components in the maps as well as analyzing the flows of the maps with graph algorithms, like the shortest path or link prediction.Watch the full episodeSubscribe to the Neo4j YouTube Channel for more live streams!Interesting LinksTom AselTangible ConceptsParsleyOnline Wardley Maps Wardley Map Awesome List;Feb 15, 2023;[]
https://medium.com/neo4j/integrating-graph-data-visualizations-into-no-code-workflows-with-bloom-2-0-18bf145dc41e;Jeff GagnonFollowDec 13, 2021·9 min readIntegrating Graph Data Visualizations into No-Code Workflows with Bloom 2.0This month sees the release of Neo4j Bloom 2.0, with a number of useful new features. For Enterprise users, this includes the ability to save and share scenes.Of course, Bloom’s other useful features remain — such as sharing perspectives, search phrases, and deep linking.In this article, I’ll discuss how to leverage Bloom’s out-of-the-box utility to easily empower domain experts in your organization with the power of no-code graph data visualization & exploration.When finished, we’ll have created some perspectives to offer different business views of the same retail dataset, added some search phrases that will enable analysts to conduct useful queries without writing Cypher, and used deep links as a simple entry point into Bloom for those same users. All of this can be done without the need to build and maintain a custom visualization solution.Creating, Managing, and Sharing PerspectivesOur sample dataset contains a variety of retail products and transactions with a schema that looks like this:Retail Graph DatamodelThinking about how domain specialists in our imaginary organization might want to use this data, we could imagine that there will be analysts concerned with customers, and others concerned with products. Of course, there might be other requirements, but let’s focus on these two for now.I’ll create two perspectives from the Perspective Gallery:Customer Perspective: includes the Country, Customer, and Transaction labels, each with their own Category in Bloom.Item Perspective: includes the Category and Item labels, each with their own Category in Bloom.Note that in a Bloom perspective, you can assign more than one label to a Category. In this example, we’re using a 1:1 assignment, which suits our needs and is quite common.To do this, we can select an empty Create Perspective” box, choose Blank Perspective” and rename it using the three dot dropdown. Then we can select each perspective in turn, choose Use Perspective” to load it into Bloom, and modify it to suit our needs. We can always return to the Gallery using the nine-dot icon in the perspective drawer, at the top left corner of Bloom.Perspectives Gallery in BloomThis is also a good time to consider the captions you’d like to see on nodes and in tooltips, and default styling including node coloring (and any rule-based styling). These can be set up in the perspective designer at the left, and the legend panel at the right, respectively.Perspectives Designer (left)Legend Panel (right)If we have the full access version of Bloom running on an Enterprise instance of Neo4j with the Bloom plugin, we can now also share these perspectives with users that have specific roles.In this case, I’ve added the reader role to the Customer Perspective, so that any user with that role will have read access to the perspective and the underlying data per the role’s permissions (as configured in Neo4j).Sharing Perspectives with different User RolesMore on these topics can be found in the Bloom Manual (Perspectives).Search PhrasesNow that we’ve created a couple of perspectives, assigned roles, and configured captions and styling, we can move on to considering any search phrases our business users might need.While domain specialists or analysts in your organization may not be well versed in the Cypher query language, developers can provide easy to use searches for specific purposes using Bloom’s search phrases.Note that search phrases are attached to the perspectives, and users will be limited by the permissions of their assigned roles when attempting to run search phrases.Continuing with the Customer Perspective, let’s create a search phrase that lets users of the perspective identify transactions between certain dates for customers from a specific country.When we’re done it can be used like this:Cypher Phrase input for the Business UserWe’ll need four things:The search phrase itself that users will type into the search bar, along with any parameters we specify (preceded by a $): Transactions by customers from $country between $from and $toA description for the search phrase. This will appear in all caps under the suggestion when the user starts typing the phrase in the search box: Lists customers from the selected country and transactions between the indicated datesThe Cypher query that will be executed when the search phrase is run. We’re looking for a graph pattern where a Country node is connected via a FROM relationship to a Customer node, which in turn is connected via a MADE_TRANSACTION relationship to a Transaction node. We then use an apoc.date.parse function to normalize date inputs with the database InvoiceDate parameter on Transaction nodes (which happens to be stored as a String value), so that we can use less than and greater than operators. Care should be taken writing certain search phrase queries to ensure they’ll be performant in the context of your underlying database.MATCH (c:Country)<-[f:FROM]-(cu:Customer)-[m:MADE_TRANSACTION]->(t:Transaction)WHERE c.Name = $countryAND apoc.date.parse($from, ms”, mm/dd/yyyy”) <= apoc.date.parse(t.InvoiceDate, ms”, mm/dd/yy hh:mm”) <= apoc.date.parse($to, ms”, mm/dd/yyyy”)RETURN c, cu, t, f, m4. Optionally, any suggestions we want to display when the user is about to enter parameters. This can be done using a label-key combination, or by writing another custom Cypher query. In our case, we’ll use the Country label and Name property for suggestions when the user is typing the $country parameter, and the Transaction label’s InvoiceDate property when entering the $to and $from parameters.Cypher Phrase EditorHere is the Cypher Phrase in the input field about to be selected, parameterized, and run.It’s possible to add multiple search phrases to each perspective. Now let’s switch to the Item Perspective though, and add a search phrase to it, too.This time we’ll add a simple search phrase, to return item nodes where the description contains the specified string value:Search phrase: Items where description contains $paramCypher query:MATCH (i:Item)WHERE toLower(i.Description) CONTAINS toLower($param)RETURN iIn this query, we’re normalizing user input and the parameter values to lowercase, since we want results to be case insensitive. We’re also taking advantage of the CONTAINS operator to find the value searched anywhere in the Description property of Item nodes.More details about this feature are again in the Bloom Manual (Search Phrases)Deep LinksAt this stage, we’ve set up a couple of perspectives, customized their styling, shared one with users with the reader role, and created search phrases for our domain specialists to use. Next, let’s talk about how we can make all of this easy for users to access.Deep links allow developers to build parameterized links to Bloom, and these can be shared in any number of ways, including via existing applications from which users may want to open a specific graph visualization.Of course, you can generate these links programmatically, and used in conjunction with search phrases this can be very powerful.Here are the parameters you can configure in a Bloom deep link:perspective: the name of a Bloom perspectivesearch : any search that you can run in the search bar, including a search phraserun : an optional boolean parameter, when set to true, Bloom will run the search as soon as it opens and display any resultsThere are also a couple of different protocols you can use to launch a search phrase:neo4j://graphapps/neo4j-bloom/?perspective=Item%20Perspective&search=Items%20where%20description%20contains%20mysearchterm — this approach will launch Bloom via Neo4j Desktop, if it’s installed on the local machinehttp://myServer:port/bloom/index.html?perspective=Item%20Perspective&search=Items%20where%20description%20contains%20mysearchterm — this approach will launch Bloom in a web browser, whether hostedvia the Bloom plugin on a Neo4j database,on Aura, ora standalone webserver hosting Bloom in your organization.More details on Bloom’s different installation options can be found in the Bloom docs.Putting it all together, we can easily integrate links to specific search results in Bloom, styled by preconfigured perspectives with access to specific search phrases. You may want to generate alert emails with deep links, create spreadsheets with links to Bloom, or even integrate Bloom links into existing corporate applications.Customizing & EditingOnce looking at some data of interest, for example in the Item Perspective, analysts can continue to make use of Bloom’s multitude of exploration and analysis features including applying node expansions, revealing relationships, rule-based styling, and so on. Two improvements of note in Bloom 2.0 include histograms for certain data types (found in filters as well as rule-based styling), and improved editing capabilities.Let’s look briefly at histograms — in this data set, some Neo4j Graph Data Science Library (GDSL) algorithms have already been applied and noted as node properties. If we want to filter Items on scene by looking only at those with the highest closeness centrality, we can now easily see how that measure is distributed and interactively select the range of interest per the image below.Version 2.0 also has enhancements to the editing features in Bloom. You can directly edit properties in the Inspector, as before. Additionally, Bloom can handle creating nodes when existence, uniqueness, or node key constraints exist. The ability to delete nodes and relationships is also new.Save & Share a SceneWrapping things up, let’s look at how users of perspectives, search phrases, and deep links can save and share their findings with others.Export Menu OptionsTwo of the most common ways available in all versions of Neo4j Bloom are exporting screenshots or CSVs.These options are found in the Export menu in the top right corner of Bloom.Exporting a screenshot exports a high quality PNG image of the current scene with a transparent background, free of any UI elements or other artifacts.Exporting to CSV generates a ZIP file with two CSVs, one containing all nodes on scene and their properties (except properties ‘excluded’ in the perspective designer), as well as one containing all relationships. If these files don’t meet your needs, they can always be transformed by your own tooling or procedures.Starting with Bloom 2.0, Full Access Enterprise users with the Bloom plugin installed on their Neo4j database can also start taking advantage of the new Scene Saving and Sharing features.Users with write access to the database can create, save, and modify multiple scenes. Furthermore, they can choose to share them via the same export menu with users who have access to the perspective.Users viewing a scene shared with them won’t be able to save changes, but they will be able to make a copy of the scene (duplicate) and make changes to their own copy (as long as they have write access to the database).For Full Access installations, scenes can be accessed via the new Scenes icon below the Filter icon on the upper left of the screen.Bloom Scene ManagementConclusionWe’ve made use of a number of simple yet highly flexible Bloom features to demonstrate how developers can leverage Bloom as an out-of-the-box graph visualization solution working seamlessly with their Neo4j database.In many cases these techniques can save a lot of time and effort building and maintaining custom solutions. You can also check out our explainer video for Developers for a recap of these features and get started with Neo4j Bloom on the product page.Neo4j Bloom - Graph Visualization and CollaborationBloom gives graph novices and experts the ability to visually investigate and explore their graph data from different…neo4j.comIf you have feedback for Neo4j Bloom, please share it here.Neo4j Bloom FeedbackGive feedback to the Neo4j Bloom team so we can make more informed product decisions. Powered by Canny.neo4j-bloom.canny.io;Dec 13, 2021;[]
https://medium.com/neo4j/quick-import-relato-business-graph-database-from-data-world-into-neo4j-1b9f16404951;Michael HungerFollowNov 11, 2017·9 min readQuick Import: Relato Business Graph Database from Data.World into Neo4jLate Night TweetOn Friday night I saw this tweet by Russel Jurney which sparked my interest. Him open-sourcing graphy business data that he has been been working on for quite some time trying to build a business is something we have to be grateful for.He links to a longer blog post explaining the history and the content of the data.As part of that he points us to the Data.World repository where he made the roughly 400k connections available to everyone to use.As I already had created an account on Data.World for our [Graphs for Good Hackation] in October in NYC, I could access the data immediately.You can query the data using SparQL, download the JSON or even grab an URL to access it remotely, which was what I did for ease of use. So please do the same and copy that URL somewhere safe.Neo4jAs I wanted to import it into Neo4j, I grabbed the Neo4j Desktop package for the latest 3.3.0 release from the website.This comes with a free develper license of Neo4j Enterprise, which is really useful. The Neo4j Desktop is an Electron app that allows you to manage multiple databases of different versions and install extensions and run the visual database browser.Neo4j Desktop & BrowserSo I created a new Project & Database with Neo4j 3.3.0 Enterprise and installed the APOC” procedures Plugin, which is a really useful collections of tools.Now I started my database, opened my browser and could start accessing the data. To make it easy we set the URL as parameter, which we can access later with $url.:param url: https://query.data.world/s/xxxxxxIf you want to learn more about Neo4j, check out http://neo4j.com/developer/get-startedFor an overview of the Cypher query language, the reference card is really useful.Now we can start querying our URL (you can also download it locally and load it from there for faster turnaround).The DataWe use the apoc.load.json procedure which will give us a stream of JSON records, so we can use this already to query and analyse remote data. Similar procedures are available for most other databases as well as CSV and XML.CALL apoc.load.json($url) YIELD valueRETURN count(*)Our file has 374k records╒══════════╕│ count(*) │╞══════════╡│373663    │└──────────┘We can also look at the first few json objects, to see which data they contain.call apoc.load.json($url) yield valuereturn value limit 1{   update_time : {     $date :  2015-02-12T21:23:12.929Z   },   home_name :  F5 Networks ,   home_domain :  f5.com ,   _id : {     $oid :  54dd19c08899a4c549dc71cf   },   link_name :  Dell ,   type :  partnership ,   link_domain :  dell.com ,   username :  rjurney }Data Import — BusinessesNow we can start creating our data. Basically 2 businesses are linked by a relationship. We could create the full graph in one go, but to make it easier to follow, we take smaller steps here.Let’s start with the businesses first.As in the links, most businesses appear multiple times, we can use their distinct occurrence. We start with the left sidewhich Russell calls home.CALL apoc.load.json($url) YIELD valueWITH distinct value.home_domain as domain, value.home_name as nameRETURN count(*)╒══════════╕│ count(*) │╞══════════╡│24266     │└──────────┘I want to use domain as the business-id, as it is more unique. So we create a constraint for it.CREATE CONSTRAINT ON (b:Business) ASSERT b.domain IS UNIQUEThen we load our businesses and create nodes for them.CALL apoc.load.json($url) YIELD valueWITH distinct value.home_domain as domain, value.home_name as nameCREATE (:Business {domain:domain, name:name})Now we continue with the right side, which is called link. Note that businesses on the right might already have appeared on the left, side, so if we used CREATE as before, we could get a constraint violation for the duplicates. That’s why we’ll use MERGE.call apoc.load.json($url) yield valuewith distinct value.link_domain as domain, value.link_name as namemerge (:Business {domain:domain}) ON CREATE SET b.name = nameSo now we have a lot of Business nodes in our database, but they are not connected, so that’s not a graph.Data Import — LinksLet’s create the connections, between home and link, using the type and storing date and user as properties on the relationship.Easiest would be to do a batch at a time, we can look at the distribution of links by type and that’s quite evenly spaced out.call apoc.load.json($url) yield valuereturn toUpper(value.type), count(*)╒═════════════════════╤══════════╕│ toUpper(value.type) │ count(*) │╞═════════════════════╪══════════╡│ CUSTOMER            │80465     │├─────────────────────┼──────────┤│null                 │114       │├─────────────────────┼──────────┤│ SUPPLIER            │79401     │├─────────────────────┼──────────┤│ INVESTMENT          │71630     │├─────────────────────┼──────────┤│ PARTNERSHIP         │112953    │├─────────────────────┼──────────┤│ COMPETITOR          │29100     │└─────────────────────┴──────────┘We have to deal with the null value though, which we can also look at.call apoc.load.json($url) yield valuewith * where value.type IS nullreturn value limit 10╒═════════════════════════════════════════════════════════════════╕│ value                                                           │╞═════════════════════════════════════════════════════════════════╡│{ link_domain : navinet.net , home_domain : csc.com , _id :{ $oid││ : 574e15023bf9e624d32e1e0a }}                                   │├─────────────────────────────────────────────────────────────────┤│{ link_domain : cisco.com , home_domain : pgi.com , _id :{ $oid :││ 574e16043bf9e624d32e25d0 }}                                     │├─────────────────────────────────────────────────────────────────┤│{ link_domain : pgi.com , home_domain : micron.com , _id :{ $oid ││: 574e16043bf9e624d32e25d1 }}                                    │├─────────────────────────────────────────────────────────────────┤│{ link_domain : pgi.com , home_domain : ibm.com , _id :{ $oid : 5││74e16283bf9e624d32e26c2 }}                                       │We see for these records there are things missing:the names of the companies (good that we used domain as identifiers)the link typethe date and userLet’s start with these records then, as they are easier. We need to MATCH businesses by domain and then CREATE relationships between them, for the unspecified ones, we just use RELATED_TO.call apoc.load.json($url) yield valuewhere value.type is nullmatch (from:Business {domain:value.home_domain})match (to:Business {domain:value.link_domain})create (from)-[:RELATED_TO]->(to)If Neo4j complains about Out-Of-Memory, the Neo4j-Desktop configures your database initially only with small memory settings. If you go to the Settings” tab, you can increase the dbms.heap.* values to 500M or 1G.We can do the same import for the other types we’ve seen, e.g. CUSTOMER.This time we also want to store the user and the date, but actually not as ISO-8601 in UTC, but as seconds since Epoch. For that we parse the string with this function apoc.date.parse(value.update_time.`$date`,s,  yyyy-MM-ddTHH:mm:ss.SSSZ ).call apoc.load.json(https://query.data.world/s/_kQZwISTfInOevAL2Cy2SelUkue4NS) yield valuewith toUpper(value.type) as type, value WHERE type = CUSTOMERmatch (from:Business {domain:value.home_domain})match (to:Business {domain:value.link_domain})create (from)-[:CUSTOMER {user:value.username,        date:apoc.date.parse(value.update_time.`$date`,s,                             yyyy-MM-ddTHH:mm:ss.SSSZ )}]->(to)We run a similiar statement for the other types:SUPPLIERINVESTMENTPARTNERSHIPCOMPETITORAs some of these records would create two links, one per direction, we can choose to use MERGE instead and leave off the directional arrow-tip.CALL apoc.load.json($url) YIELD valueWITH value WHERE toUpper(value.type) = COMPETITORMATCH (from:Business {domain:value.home_domain})MATCH (to:Business {domain:value.link_domain})CREATE (from)-[rel:COMPETITOR]->(to)ON CREATE SET rel.user = value.username,   rel.date = apoc.date.parse(value.update_time.`$date`,s, yyyy-MM-ddTHH:mm:ss.SSSZ )Data InspectionSo now that we have all our data imported, we can have a first look.For instance just showing a bunch of businesses and their `SUPPLIER`s.MATCH (b1:Business)-[s:SUPPLIERS]->(b2)RETURN b1,s,b2LIMIT 10match (n:Business) return count(*)╒══════════╕│ count(*) │╞══════════╡│51104     │└──────────┘match (n:Business)-[r]->() return type(r), count(*) order by count(*) desc╒═════════════╤══════════╕│ type(r)     │ count(*) │╞═════════════╪══════════╡│ PARTNERSHIP │139998    │├─────────────┼──────────┤│ CUSTOMER    │95405     │├─────────────┼──────────┤│ SUPPLIER    │89401     │├─────────────┼──────────┤│ INVESTMENT  │78085     │├─────────────┼──────────┤│ COMPETITOR  │30660     │├─────────────┼──────────┤│ RELATED_TO  │114       │└─────────────┴──────────┘To see a larger part of the graph, we can chose a few businesses that don’t have too many relationships (we don’t want to see a hairball). And follow their connections 2 hops out, using the same condition on the whole path.MATCH (b:Business) where size((b)--()) < 20MATCH path = (b)--(b2)--(b3) WHERE size((b2)--()) < 20 and size((b3)--()) < 20RETURN path LIMIT 700Parts of the Relato Business GraphAlternative Import ApproachAs mentioned before, we can also combine all of these steps into one load operation.Here we use:apoc.periodic.iterate for batching work on a stream of records (almost 400k)coalesce to provide defaults null valuesapoc.create.relationship to create relationships with dynamic typesapoc.map.clean to remove null values from propertiescall apoc.periodic.iterate( call apoc.load.json($url) , merge (from:Business {domain:value.home_domain}) ON CREATE SET home.name = value.home_namemerge (to:Business {domain:value.link_domain}) ON CREATE SET link.name = value.link_namewith *, apoc.date.parse(value.update_time.`$date`,s,                        \ yyyy-MM-ddTHH:mm:ss.SSSZ\ ) as datecall apoc.create.relationship(from, toUpper(value.type),          apoc.map.clean({updated:date,id:value._id.`$oid`,                          user:value.username},[],[null])     ,to) yield relreturn count(*) ,{batchSize:10000,iterateList:true, params:{url,$url})Data AnalyticsWe can look at the distribution of degrees in the data using apoc.stats.degrees which takes an optional argument of the relationship-type. Here we see that while most nodes only have around 20 relationships, some go up to 1200 and even to 26000 as maximum.call apoc.stats.degrees()What are the nodes with the biggest competition in our graph, i.e. the biggest degree ?MATCH (b:Business)WITH b, size( (b)-[:COMPETITOR]-() ) as degreeRETURN b.name, b.domain, degreeORDER BY degree DESC LIMIT 5Obviously the usual suspects.╒═══════════╤═══════════════╤════════╕│ b.name    │ b.domain      │ degree │╞═══════════╪═══════════════╪════════╡│ Google    │ google.com    │598     │├───────────┼───────────────┼────────┤│ Microsoft │ microsoft.com │533     │├───────────┼───────────────┼────────┤│ Facebook  │ facebook.com  │462     │├───────────┼───────────────┼────────┤│ Apple     │ apple.com     │417     │├───────────┼───────────────┼────────┤│ IBM       │ ibm.com       │379     │└───────────┴───────────────┴────────┘For partnerships it looks a bit different, Cisco is a clear leader here and AWS in the top 5.╒═════════════════════╤════════════════╤════════╕│ b.name              │ b.domain       │ degree │╞═════════════════════╪════════════════╪════════╡│ Cisco               │ cisco.com      │22982   │├─────────────────────┼────────────────┼────────┤│ Microsoft           │ microsoft.com  │3582    │├─────────────────────┼────────────────┼────────┤│ Rackspace           │ rackspace.com  │3572    │├─────────────────────┼────────────────┼────────┤│ Amazon Web Services │ aws.amazon.com │2336    │├─────────────────────┼────────────────┼────────┤│ IBM                 │ ibm.com        │1918    │└─────────────────────┴────────────────┴────────┘We can also apply ranking (e.g. page-rank) and clustering on this data.To enable that we install the neo4j-graph-algorithms library. The latest release can be found here. From there we grab the graph-algorithms-algo-3.3.0.0.jar file and drop it into the plugins folder when you Open Folder.For this to work, we have to add this config setting dbms.security.procedures.unrestricted=algo.*and restart the server.call algo.pageRank()This call computed the page-rank in 2 seconds and wrote the results to our business nodes.What are the highest ranking nodes in our graph?MATCH (b:Business) where exists(b.pagerank)RETURN b.name, b.domain, b.pagerankORDER BY b.pagerank DESC LIMIT 5Again, not surprising.╒═══════════╤═══════════════╤══════════════════╕│ b.name    │ b.domain      │ b.pagerank       │╞═══════════╪═══════════════╪══════════════════╡│ Microsoft │ microsoft.com │184.0849425       │├───────────┼───────────────┼──────────────────┤│ Google    │ google.com    │174.71597049999997│├───────────┼───────────────┼──────────────────┤│ IBM       │ ibm.com       │124.25576300000002│├───────────┼───────────────┼──────────────────┤│ Facebook  │ facebook.com  │124.16897800000001│├───────────┼───────────────┼──────────────────┤│ Apple     │ apple.com     │89.4175015        │└───────────┴───────────────┴──────────────────┘What does it look like for betweenness centrality, i.e. businesses which connect other clusters of businesses. Let’s try that for the PARTNERSHIP relationship.call algo.betweenness(Business,PARTNERSHIP)This one takes longer to compute (3 mins on my Mac) as it needs to run all shortest paths in the graph to see which nodes most frequently appear on them.MATCH (b:Business) where exists(b.centrality)RETURN b.name, b.domain, b.centralityORDER BY b.centrality DESC LIMIT 5╒═══════════════════════════════╤═══════════════════╤══════════════╕│ b.name                        │ b.domain          │ b.centrality │╞═══════════════════════════════╪═══════════════════╪══════════════╡│ CA Technologies               │ ca.com            │21474.83647   │├───────────────────────────────┼───────────────────┼──────────────┤│ NVIDIA                        │ nvidia.com        │21474.83647   │├───────────────────────────────┼───────────────────┼──────────────┤│ Mphasis Australia Pty Limited │ mphasis.com       │21474.83647   │├───────────────────────────────┼───────────────────┼──────────────┤│ Datapipe                      │ datapipe.com      │21474.83647   │├───────────────────────────────┼───────────────────┼──────────────┤│ MicroStrategy                 │ microstrategy.com │21474.83647   │└───────────────────────────────┴───────────────────┴──────────────┘Last but not least we can also cluster our businesses, e.g. the CUSTOMER graph.call algo.labelPropagation(Business,CUSTOMER,OUTGOING,{iterations:10})This returns after 2 seconds. We have 42559 partiions.match (b:Business)return count(distinct b.partition) as partitionsHow big are the top-largest partitions and what are their most highly ranked nodes.We can clearly see the different industries being separated, with software being the largest.╒═════════════╤══════╤════════════════════════╕│ b.partition │ size │ topRanked              │╞═════════════╪══════╪════════════════════════╡│68           │7900  │ Microsoft              │├─────────────┼──────┼────────────────────────┤│13441        │7     │ UC Davis               │├─────────────┼──────┼────────────────────────┤│25359        │6     │ Delta Dental           │├─────────────┼──────┼────────────────────────┤│7463         │6     │ LPL Financial Services │├─────────────┼──────┼────────────────────────┤│2508         │6     │ MillerCoors            │├─────────────┼──────┼────────────────────────┤│19829        │6     │ conde-nast             │├─────────────┼──────┼────────────────────────┤│18813        │4     │ RadiumOne              │├─────────────┼──────┼────────────────────────┤│39918        │4     │ Vail Resorts           │├─────────────┼──────┼────────────────────────┤│48510        │4     │ Arch Coal              │├─────────────┼──────┼────────────────────────┤│48922        │4     │ IIT                    │└─────────────┴──────┴────────────────────────┘I hope this was useful for you to get up and running with a graph database quickly, both in terms of getting data imported but also analyizing it quickly.;Nov 11, 2017;[]
https://medium.com/neo4j/neo4j-graph-algorithms-release-multiple-relationships-and-properties-ann-f05ae37d1ae2;Mark NeedhamFollowNov 1, 2019·4 min readNeo4j Graph Algorithms Release — ANN, In memory graph improvements, Bug fixesThe latest releases of the Graph Algorithms Library add new functionality to the in memory graph.Over the last month or so we’ve released new functionality for the Neo4j Graph Algorithms Library, in versions 3.5.11.0, 3.5.12.0 and 3.5.12.1.tldrThese releases see the following features and bug fixes:support for loading multiple relationship typessupport for loading multiple node and relationship propertiesCypher projections no longer write to the databaseintroduction of the Approximate Nearest Neighbors algorithm that I’ve previously written about.Note that this release is only compatible with Neo4j 3.5.9 and above. You can read more about the release in the release notes.You can install the latest release directly from the Neo4j Desktop in the ‘Plugins’ section of your project. Jennifer Reif also has a detailed post explaining how to install plugins if you haven’t used any yet.Installing the Graph Algorithms LibraryIf you’re installing the library on a server installation of Neo4j you can download the JAR from the Neo4j Download Center.Neo4j Download CenterMultiple relationship typesIt’s now possible to load multiple relationship types into the in memory graph. Previous to this release, the in memory graph treated every relationship loaded as if it was of the same type.Let’s see how to use this feature.The following query creates an in memory graph for a Twitter dataset where we have User nodes connected by FOLLOWS and RETWEETED relationships:CALL algo.graph.load(twitter,   User,   FOLLOWS | RETWEETED   {direction: OUTGOING, concurrency: 8 })We could now choose to run an algorithm using both these relationships, or just one of them. For example, the following query would compute the PageRank on our graph based on followers:CALL algo.pageRank(null, FOLLOWS, {graph: twitter})And the following query would compute it based on the retweets that users have received:CALL algo.pageRank(null, RETWEETED, {graph: twitter})Multiple properties on loaded graphsIt’s also possible to load multiple properties into the in memory graph for both nodes and relationships.This feature is useful when we want to run the same algorithm multiple times with different configurations. The following query loads a transport graph with the node properties population and area:CALL algo.graph.load(transport, City, , {  graph: huge,  nodeProperties: {    population: population,    area: area  }})We could run then run Label Propagation algorithm with weightProperty: population if we want to bias label selection based on the population property:CALL algo.beta.labelPropagation(null, null, {  iterations:10,  weightProperty:population,   graph: transport)Or with weightProperty: area if we want to bias label selection based on the area property:CALL algo.beta.labelPropagation(null, null, {  iterations:10,  weightProperty:area,   graph: transport)We can take a similar approach when loading relationship properties. We could then run the Weight PageRank algorithm or path finding algorithms with different configurations.Cypher projections no longer write to the databaseWhen creating in memory graphs from Cypher projections, it was possible to include write operations in those projection queries.For example the following query would be valid, and would create one Person node per relationship:CALL algo.pageRank.stream(  MATCH (n) RETURN id(n) AS id,   MATCH (n)-->(m)    CREATE (:Person)    RETURN id(n) AS source, id(m) AS target ,  {graph:  cypher })This would be a weird thing to do, but was possible. Not anymore! Now, if we write a Cypher query containing any write operations, we’ll get an error message like the following:Failed to invoke procedure `algo.pageRank.stream`: Caused by: java.lang.IllegalArgumentException: Query must be read only. Query: [MATCH (n)-->(m) SET n.bar = foo RETURN id(n) AS source, id(m) AS target]We hope these changes improve your experience with the library, and if you have any questions or suggestions please send us an email to devrel@neo4j.com;Nov 1, 2019;[]
https://medium.com/neo4j/neo4j-for-django-developers-efd0e39e5f2e;CristinaFollowMar 26, 2021·8 min readNeo4j for Django DevelopersFrom zero to deployment with Django-Neomodel and Neo4j’s Paradise Papers dataset.Making the leap from a relational database to a graph database for an existing project is not easy. Questions arise:What happens to the existing data? If the data is migrated, will the whole system have to be rewritten? What if we like our existing stack but still want the benefits of a graph database? Should we try a microservice? Will there be overhead? Will the overhead be worth it?Thankfully for Django developers, the Neo4j ecosystem provides a quick and painless way to spin up a test implementationThe Paradise Papers Search App, based on the Paradise Papers dataset, is conveniently available on Neo4j Sandbox. By following along with the steps of this post, you will be able to spin up a small Django application to explore the Paradise Papers dataset either locally or on Heroku.neo4j-examples/paradise-papers-djangoA simple Django web app for searching the Paradise Papers dataset backed by Neo4j Welcome to Paradise Paper Search…github.comAbout the Paradise Papers Search AppBackgroundThe ICIJ published a fraction of the Paradise Papers data as part of their Power Players visualization at the same time as the reported stories. Some time later, Neo4j added a version of the Paradise Papers dataset, with 163,414 nodes and 364,456 relationships, to the sandboxes available on Neo4j Sandbox.Paradise Papers - ICIJA new leak of confidential records reveals the financial hideaways of iconic brands and power brokers across the…www.icij.orgData ModelThe model includes:A company, trust or fund created in a low-tax, offshore jurisdiction by an agent ( Entity )People or companies who play a role in an offshore entity (Officer)Addresses (Address)Law firms or middlemen (Intermediary) that asks an offshore service provider to create an offshore firm for a clientEach of these carries different properties, including name, address, country, status, start and end date, validity information, and more.Relationships between the elements capture the roles people or other companies play in the offshore entities (often shell companies), we see many OFFICER_OF relationships for directors, shareholders, beneficiaries, etc.Other relationships capture similar addresses or the responsibility of creating a shell company by a law firm (INTERMEDIARY_OF).Data Model for Paradise PapersFor a detailed description of the data model, check out An In-Depth Graph Analysis of the Paradise Papers.Paradise Papers: an in-depth graph analysisToday, the ICIJ has publicly released data from its most recent year-long investigation into the offshore industry…neo4j.comDjango-NeomodelThe Paradise Papers app uses Django-Neomodel, a Django module allows you to use the Neo4j database with Django using neomodel.Who uses Django, and why? Many python developers (or their employers) prefer to use an ORM such as Django as it allows them to focus on the business logic, the money query,” or other more complex features of their product while the object mapper handles the relationship between the database, the Python models, and other aspects of web development. Django’s introspection-generated admin portal is another benefit, avoiding the boilerplate code for administrator pages.What about neomodel? Similar to Django, neomodel, an object mapper, allows Python developers to focus on solving problems for their users by thinking in terms of domain objects and less in terms of writing Cypher statements.Neomodel documentation - neomodel 3.3.2 documentationAn Object Graph Mapper (OGM) for the neo4j graph database, built on the awesome neo4j_driver Familiar Django model…neomodel.readthedocs.ioNeomodel uses the concepts of StructuredNode and StructuredRel to formalize the relationship between domain objects, Python classes and Neo4j nodes and relationships. (Read more about extending StructuredNodes)But would anyone use neomodel with an existing ORM? If an ORM-based product would like to leverage Neo4j as well, the Django-neomodel plugin would allow them to add Neo4j-based functionality to their existing application.The Example App: Paradise Papers SearchThe Paradise Papers search app allows users to search and filter for Entities, Officers, Intermediaries, and Addresses, providing results in a list. It is not intended to be a fully-fledged app (although it can be considered such), but is rather an example of how a Python developer could integrate Neo4j into a Django application.List view and SearchUpon clicking a list item, the user can see some details about the item, in addition to Entities, Officers, Intermediaries, or Addresses that might be connected to the item.Detail ViewThe StructuredNodes”EntityIn the entity.py file, you will find a class Entity corresponding to Entity nodes in the Neo4j database.The Entity StructuredNode (GitHub)A bit below the Entity definition, take a look at the example serializers. In this example, serializing the Officers, Intermediaries, Addresses, Entities, and Others that have relationships with a particular Entity:The Entity serialize_connections function (GitHub)AddressSimilarly, you can find the Address StructuredNode in address.pyThe Address StructuredNode (GitHub)Intermediaries, Officers, and Other”Intermediaries, Officers, and Other” StructuredNodes can be found in the models directory of the repo.Read more in the paradise-papers-django tutorialLocal App: Sandbox DatabaseFirst step, set up your local environment.Clone the repo:neo4j-examples/paradise-papers-djangoA simple Django web app for searching the Paradise Papers dataset backed by Neo4j Welcome to Paradise Paper Search…github.comgit clone git@github.com:neo4j-examples/paradise-papers-django.gitIn a virtual environment, install the requirements:pip install -r requirements.txtNext, you’ll need to point your app to a sandbox instance of the Neo4j database. In Neo4j Sandbox, create an account and select Paradise Papers by ICIJ.Sandbox Launch Project ScreenCheck out the graph in the browser by clicking the Open” button. You will find a informative Graph Guide detailing the dataset:Back in the Sandbox UI, tap Connection details to find the database’s bolt URL, Username, and Password.Since the Paradise Papers Search App uses Django Neomodel, which is built on the official Python driver, it is a good idea to double-check the suggested implementation under Connect via drivers > Python.In your local environment, set the DATABASE_URL environment variable using the credentials found in the Sandbox admin:export DATABASE_URL=bolt://<username>:<password>@<ip>:7687Run the app!python manage.py runserver \— settings=paradise_papers_search.settings.devStart searching at http://127.0.0.1:8000/Local Setup, Local DataSandbox comes with an expiration date, so if you want to do some additional experimentation, you will have to get the data and import it to your local database. We have made the sandbox datasets, browser guides and code examples available as GitHub repositories, that you can import into your Neo4j installation.neo4j-graph-examples/icij-paradise-papersDescription: The Paradise Papers dataset and guide from the International Consortium of Investigative Journalists…github.comFortunately, Neo4j Desktop now supports that out of the box. If you have not yet done so, please download and install Neo4j Desktop.Import Sample ProjectImport the repository into desktop, via the New -> Import sample project. And pick the icij-paradise-papers from the list. That should prompt you for the database import and create a database and project for you.Back in your app, update the database environment variables to reflect this new database.export DATABASE_URL=bolt://neo4j:<password>@localhost:7687Deploy to HerokuAn app deployed locally is okay, but what about deployment?Create a new Heroku app, (for example, paradise-papers)Go to the app’s settings and add the following config vars:ALLOWED_HOST : paradise-papers.herokuapp.com(change to your app name)DATABASE_URL: the credentials from your sandbox databaseOver in your local terminal inside the repository, add the Heroku app as a remote, then push to Heroku:git remote add heroku https://git.heroku.com/paradise-papers.gitgit push heroku masterView your app at the URL you specified in the configuration.Use a Neo4j Aura DatabaseNow that you’ve seen the app in action locally and on a remote server, if you want someone else to be able to visit the app without your computer running, you may want to use a database on Neo4j Aura.After creating an Aura account, you can import by following the wizard on the Aura console. You can drag and drop your dump-file onto the browser to seed your database.Next StepsAfter testing out the ICIJ Panama Papers database, you may want to test out using some of the data native to your Django application. Test out converting your relational data set using the Neo4j ETL Tool.Looking for more details on how Neo4j was used in the Panama Papers investigation? Check out our full coverage here:An In-Depth Graph Analysis of the Paradise PapersHow the ICIJ Used Neo4j to Unravel the Panama PapersAnalyzing the Panama Papers with Neo4j: Data Models, Queries & MoreThe Panama Papers: Why It Couldn’t Have Happened Ten Years AgoResourcesneo4j-contrib/neomodelAn Object Graph Mapper (OGM) for the neo4j graph database, built on the awesome neo4j_driver Familiar class based model…github.comneo4j-contrib/django-neomodelThis module allows you to use the neo4j graph database with Django using neomodel Install the module: $ pip install…github.comneo4j/neo4j-python-driverThis repository contains the official Neo4j driver for Python. Each driver release (from 4.0 upwards) is built…github.comGetting Started with NeomodelNeomodel is an Object Graph Mapper (OGM) for the Neo4j graph database built on py2neo and provides a Django ORM style…thobalose.co.zaAbout the AuthorsCristina and Alisson work at The SilverLogic, a software development company based in Boca Raton.Alisson is a software engineer at The SilverLogic. Passionate about plants, he is the driving force behind Que Planta, a GraphQL-based social network for plants.;Mar 26, 2021;[]
https://medium.com/neo4j/link-prediction-with-neo4j-part-1-an-introduction-713aa779fd9;Mark NeedhamFollowMar 8, 2019·10 min readLink Prediction with Neo4j Part 1: An IntroductionThis is the beginning of a series of posts (part 2) about link prediction with Neo4j. We’ll start the series with an overview of the problem and associated challenges, and in future posts will explore how the link prediction functions in the Neo4j Graph Algorithms Library can help us predict links on example datasets.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comLet’s start at the beginning…what is link prediction?What is link prediction?Link prediction has been around for ages, but as far as I can tell was popularised by a paper written by Jon Kleinberg and David Liben-Nowell in 2004, titled The Link Prediction Problem for Social Networks.Kleinberg and Liben-Nowell approach this problem from the perspective of social networks, asking the question:Given a snapshot of a social network, can we infer which new interactions among its members are likely to occur in the near future? We formalize this question as the link prediction problem, and develop approaches to link prediction based on measures for analyzing the proximity” of nodes in a network.More recently, my colleague Dr Jim Webber has been explaining how the evolution of a graph can be determined from its structure in his entertaining GraphConnect San Francisco 2015 talk where he explained World War 2 using graphs.Apart from predicting World Wars or friendships in a social network where else might we want to predict links?We could predict future associations between people in a terrorist network, associations between molecules in a biology network, potential co-authorships in a citation network, interest in an artist or artwork, to name just a few use cases.In each these examples, predicting a link means that we are predicting some future behaviour. For example in a citation network, we’re actually predicting the action of two people collaborating on a paper.Link Prediction AlgorithmsKleinberg and Liben-Nowell describe a set of methods that can be used for link prediction. We can see a list of these in the diagram below.Algorithms described in Kleinberg and Liben-Nowell’s paperThese methods compute a score for a pair of nodes, where the score could be considered a measure of proximity or similarity” between those nodes based on the graph topology.The closer two nodes are, the more likely there will be a relationship between them.Let’s have a look at a few of these measures in more detail so we can get a feel for what they do.Common NeighborsOne of the simplest measures that we can compute is common neighbors, as described by Ahmad Sadraei:The common-neighbors predictor captures the notion that two strangers who have a common friend may be introduced by that friend.This introduction has the effect of closing a triangle” in the graph [i.e. a triadic closure as described in Jim’s talk]As the name suggests, this measure computes the number of common neighbors that a pair of nodes share.In the graph above, nodes A and D have 2 common neighbors (nodes B and C), whereas nodes A and E only have one common neighbor (node B).Therefore nodes A and D would be considered closer and more likely to be connected by a link in future.Adamic AdarThis algorithm was introduced in 2003 by Lada Adamic and Eytan Adar while researching how to predict links in a social network.This measure builds the common neighbors, but rather than just counting those neighbors, it computes the sum of the inverse log of the degree of each of the neighbors.The degree of a node is the number of neighbors it has, and the intuition behind this algorithm is that when it comes to closing triangles, nodes of low degree are likely to be more influential.For example, in a social network, for two people to be introduced by a common friend, the probability of that happening is related to how many other pairs of friends that person has. An unpopular person may therefore be more likely to introduce a pair of their friends.Preferential AttachmentThis is one of the most well known concepts amongst network scientists, having been popularised by Albert-László Barabási and Réka Albert through their work on scale-free networks.The intuition is that nodes with lots of relationships will gain more relationships.This measure is one of the easiest to compute — we take the product of the degree of each node.Link Prediction — Neo4j Graph Algorithms LibraryThe Neo4j Graph Algorithms library currently contains 6 link prediction algorithms — Adamic Adar, Common Neighbors, Preferential Attachment, Resource Allocation, Same Community, and Total Neighbors.We chose to start with these ones as they are some of the most common ones we encountered while researching this topic. These algorithms are used to compute values that will be used as part of a larger process.Link Prediction algorithms in the Neo4j Graph Algorithms libraryLet’s quickly learn how the other 5 algorithms work:Adamic Adar — the sum of the inverse log of the degree of common neighborsPreferential Attachment — the product of the degree of each nodeResource Allocation — the sum of the inverse of the degree of common neighborsSame Community — checks whether two nodes are in the same community computed by one of the community detection algorithmsTotal Neighbors — total unique neighbors of the two nodesNow let’s have a quick look at how to use them. If you haven’t installed the Graph Algorithms library before, it’s super easy — just a one click install from the Neo4j Desktop.You can also read Jennifer Reif’s excellent blog post for a more detailed explanation.Now let’s see how to use the common neighbors function from the library by revisiting the example graph from the previous section.We’ll first create the graph in Neo4j by executing the following Cypher query:UNWIND [[ A ,  C ], [ A ,  B ], [ B ,  D ],         [ B ,  C ], [ B ,  E ], [ C ,  D ]] AS pairMERGE (n1:Node {name: pair[0]})MERGE (n2:Node {name: pair[1]})MERGE (n1)-[:FRIENDS]-(n2)And then compute common neighbors for nodes A and D using that function:neo4j> MATCH (a:Node {name: A})       MATCH (d:Node {name: D})       RETURN algo.linkprediction.commonNeighbors(a, d)+-------------------------------------------+| algo.linkprediction.commonNeighbors(a, d) |+-------------------------------------------+| 2.0                                       |+-------------------------------------------+1 row available after 97 ms, consumed after another 15 msThese nodes have 2 common neighbors, so they receive a score of 2. Now let’s do the same for nodes A and E. We expect them to receive a score of 1 as they only have a single common neighbor.neo4j> MATCH (a:Node {name: A})       MATCH (e:Node {name: E})       RETURN algo.linkprediction.commonNeighbors(a, e)+-------------------------------------------+| algo.linkprediction.commonNeighbors(a, e) |+-------------------------------------------+| 1.0                                       |+-------------------------------------------+As we expected, a score of 1 — good times!By default the function assumes that it can compute its score using relationships of any type and in either direction. We could also choose to specify this explicitly by passing in parameters:neo4j> WITH {direction:  BOTH , relationshipQuery:  FRIENDS }        AS config       MATCH (a:Node {name: A})       MATCH (e:Node {name: E})       RETURN algo.linkprediction.commonNeighbors(a, e, config)        AS score+-------+| score |+-------+| 1.0   |+-------+Just for good measure, let’s try one of the other functions to check we’ve got the hang of this.The preferential attachment function returns the product of the degrees of the two nodes. If we run that for nodes A and D we would expect to get a score of 2*2=4 as nodes A and D both have two neighbors. Let’s give it a try:neo4j> MATCH (a:Node {name: A})       MATCH (d:Node {name: D})       RETURN algo.linkprediction.preferentialAttachment(a, d)        AS score+-------+| score |+-------+| 4.0   |+-------+Great! So we know how to compute these scores for our Neo4j graphs. What now?What should we do with the link prediction scores?So we’ve now determined that our can be solved by link prediction and we’ve computed the relevant proximity measures described above, but we still need to decide how to use these measures to predict links.There are two approaches that I came across in the literature:Using the measures directlyWe can use the scores from the link prediction algorithms directly. With this approach we would set a threshold value above which we would predict that a pair of nodes will have a link.In the example above we might say that every pair of nodes that has a preferential attachment score above 3 would have a link, and any with 3 or less would not.Supervised learningWe can take a supervised learning approach where we use the scores as features to train a binary classifier. The binary classifier then predicts whether a pair of nodes will have a link.Guillaume Le Floch describes these two approaches in more detail in his blog post Link Prediction In Large-Scale Networks.In this series of posts we’re going to focus on the supervised learning approach.Building a machine learning classifierOnce we’ve decided to use a supervised learning approach, we need to make some decisions related to our machine learning workflow:What machine learning model do we want to use?How are we going to split our data into train and test sets?Machine learning modelMany of the link prediction measures that we’ve covered so far are computed using similar data, and when it comes to training a machine learning model this means there is feature interaction issue that we need to deal with.Some machine learning models assume that the features they received are independent. Providing such a model with features that don’t meet this assumption will lead us to predicts things with low accuracy. If we choose one of these models we would need to exclude features with high interaction.Alternately, we can simplify our life by choosing a model where feature interaction isn’t so problematic.Ensemble methods tend to work well as they don’t make this assumption on their input data.This could be a gradient boosting classifier as described in Guillaume’s blog post or a random forest classifier as described in a paper written by Kyle Julian and Wayne Lu.Train and test data setsThe tricky thing with the train and test set is that we can’t just randomly split the data, as this could lead to data leakage.Data leakage can occur when data outside of your training data is inadvertently used to create your model. This can easily happen when working with graphs because pairs of nodes in our training set may be connected to those in the test set.When we compute link prediction measures over that training set the measures computed contain information from the test set that we’ll later evaluate our model against.Instead we need to split our graph into training and test sub graphs. If our graph has a concept of time our life is easy — we can split the graph at a point in time and the training set will be from before the time, the test set after.This is still not a perfect solution and we’ll need to try and ensure that the general network structure in the training and test sub graphs is similar.Once we’ve done that we’ll have pairs of nodes in our train and test set that have relationships between them. They will be the positive examples in our machine learning model.Now for the negative examples.The simplest approach would be to use all pair of nodes that don’t have a relationship. The problem with this approach is that there are significantly more examples of pairs of nodes that don’t have a relationship than there are pairs of nodes that do.The maximum number of negative examples is equal to:# negative examples = (# nodes)² - (# relationships) - (# nodes)i.e. the number of nodes squared, minus the relationships that the graph has, minus self relationships.If we use all of these negative examples in our training set we will have a massive class imbalance — there are many negative examples and relatively few positive ones.A model trained using data that’s this imbalanced will achieve very high accuracy by predicting that any pair of nodes don’t have a relationship between them, which is not quite what we want!So we need to try and reduce the number of negative examples. An approach described in several link prediction papers is to use pairs of nodes that are a specific number of hops away from each other.This will significantly reduce the number of negative examples, although there will still be a lot more negative examples than positive.To solve this problem we either need to down sample the negative examples or up sample the positive examples.What next?We’re now covered the theory behind link prediction, and we’re ready to try out this approach on a dataset. That’s exactly what we’ll be doing in the next post of the series, which will be coming out next week.In the mean time, if you’re interested in this topic the following links may be of interest:Amy Hodler and I will be presenting on this topic at the Neo4j Online Meetup on 21st March 2019.The O’Reilly Graph Algorithms Book, which is currently in early access mode, has a chapter covering link prediction with Spark MLlib and Neo4jDownload the Neo4j Graph Algorithms Library and give the link prediction algorithms a try yourselfFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Mar 8, 2019;[]
https://medium.com/neo4j/create-your-own-dashboard-using-retool-with-neo4j-458ed6b6a780;Jennifer ReifFollowApr 8, 2021·6 min readCreate-Your-Own Dashboard: Using Retool with Neo4jMy latest research for integrations with Neo4j brought me to Retool: a low-code tool for building applications with drag-and-drop components and Javascript customizations. Still not sure what that means?We have this very popular concept (in the U.S., at least) of create-your-own-food-item bars (taco bar, ice cream bar, pizza bar, sandwich bar, etc.), where you pick and choose customizations for your perfect food item. For instance, in building an ice cream sundae, I can pick my ice cream flavor, then choose from an array of toppings (see the header picture). Retool is kind of like this for UIs. You pick the components that you want and where you want them to get a customized application, but all the items are provided for you to pick.From relational databases to GraphQL APIs, Retool can connect to diverse data sources, so users can access data when they need it without writing code. Retool offers simple, yet configurable, components that users can map to show results from various data sources and queries. From dashboards to analysis and more, Retool provides a quick and easy ramp for users to show their data in meaningful ways.The nice thing about Retool is that it can be used by developers, analysts, and others alike. It is broadly applicable for different use cases, but these are likely a few of the most common:Quick and pretty UI: built with the drag-and-drop components for a demo or presentationInternal statistics: tracked by pulling data from various sources into one, clean UIEmbedded application-builder: built into an iframe to share with the publicIn my work with Retool so far, I’ve focused mostly on the first two in the above list, but the third option doesn’t seem too complicated. I hope to circle back in the future and create materials for it, but for now, let’s focus on getting a UI built and mapping it to statistics from a database.SetupRetool has a variety of data source integrations, but not one that’s Neo4j-specific. How can we connect to Neo4j then, you might ask? Fortunately, both Retool and Neo4j share an integration: GraphQL. We will need a GraphQL API deployed on top of a Neo4j instance, then connect Retool to the API.While you can spin up any type of Neo4j database and deploy a GraphQL API manually, we will choose from options that are much simpler.Neo4j Sandbox with personalized Twitter data.Hosted graph example with Neo4j Twitter data.With the first option, sandbox, does not require any download in order to spin up an instance. It also comes pre-built with the APOC extension library, as well as a one-click integration to deploy a code sandbox that includes a GraphQL API and playground.Launching a Twitter sandbox allows you to import your Twitter account data into the database (after granting access) to explore your own social network and find influential users, analyze popular tweets, and build a personalized news feed. This gives you the ability to explore personalized data and gather valuable insights.Once the sandbox is launched, copy the contents of this Cypher script and execute it in Neo4j Browser. This will enrich the data for our Retool application. Then, on the main sandbox page, click the dropdown on the instance, navigate to the Connect via drivers tab, choose the GraphQL radio button, and click the Deploy to CodeSandbox button.The second option lets you try this out using a pre-packaged Github repository. While this is data from a certain point in time, if you don’t have a Twitter account or don’t feel comfortable pulling your social media data, this gives you a good secondary option.We can deploy the prepared GitHub repository with one click to a free GraphQL codesandbox (click on the Open in CodeSandbox at the top of the readme from the link). You can still find valuable insights with Neo4j’s data to apply to your own projects later.Either way you choose, we now have Neo4j with a GraphQL API deployed and running. With that, we are ready to add our application in Retool on top.Retool ApplicationWe have already built a starter application that you can use as a template. This is on Retool’s website. Let’s see what all is in the application.First, we have some statistics in the panel at the top. Here, we have some calculations around our Twitter followers. All of these numbers are either calculated in Retool or simply printed from data in Neo4j retrieved with GraphQL queries (with the exception of the input for goal number of followers). We can adjust the goal number to see how close to the target we are.In the panel of stats along the left side, we have more information that helps us better understand our network and things we can do to improve it. First, we pull our most popular followers, so we can see the top influencers. Perhaps we could work on collaborations or content that appeals to these users in order to reach their networks, as well.Next, we retrieve some recommendations for people we aren’t following, but maybe should. The last two statistics show us our habits and topics — which people we mention often and hashtags we commonly use. Perhaps we can vary our hashtags a bit or see how many other users are also using those same hashtags (are we influencing others?).The large pane on the right is our Twitter newsfeed. The query prioritizes the tweets of users most similar to us at the top of the feed, then everything else is sorted by date (newest to oldest). This is the way social media news feeds work — Twitter, Facebook, Instagram, etc.More ExplorationIf you’re interested in the data in Neo4j, feel free to walk through our Twitter browser guide that steps through different queries and data analysis. You can open Neo4j Browser and type the below command into the top input line, then click the play button to execute the guide.:play https://guides.neo4j.com/sandbox/twitter/index.htmlAll of the queries that we mapped to our Retool components can be found in the GraphQL Playground for this demo. You can tweak the queries and test them there, as well as copy the Retool application template and create your own additions.Next StepsIn today’s post, we started by outlining the architecture of the components for our project using GraphQL as the common connector between Retool’s low-code application-builder and Neo4j’s graph database. We also walked through each layer and how they were put together. Finally, we found out that we can alter the queries and the application for ourselves and explore the data.ResourcesWhat is Retool?What is Neo4j?What is GraphQL?Retool’s GraphQL integrationWorking with Retool componentsBlog post: Building a Shopify dashboard with GraphQL and RetoolLearn Cypher — Neo4j’s graph query language;Apr 8, 2021;[]
https://medium.com/neo4j/neo4j-devtools-its-finally-spring-here-release-e2d8d6b7eddb;GregFollowApr 20, 2022·4 min read🐇🌱🌦 Neo4j DevTools It’s Finally Spring Here” Release 😅With winter officially behind us, we at Neo4j have been busy in the workshop sharpening our (Dev)Tools with meticulous precision to bring you the latest releases. In this release, we’ve got new features to further improve your data load experience in the new Neo4j Data Importer. Thank you to our wonderful users for all the feedback that we’ve received since its release in February. Without further ado, read on to find out more.Improved Empty ID” HandlingIn previous releases of Data Importer, we treated empty string fields quite naively when it came to using them as IDs. If we saw an empty string in a file, we treated that as a unique identifier and created a node for it. If your data had many missing IDs, this could easily result in a supernode with the lonely-looking empty ID appearing in your graph.To solve this (and since flat files don’t have a concept of null values) we now treat empty strings as null values when it comes to node IDs. We will filter out rows where the empty string is the ID for a node import — meaning we no longer create a node for those rows.Additionally, we now provide a setting for your load configuration that allows you to specify other string values you’d like to be treated as nulls. This could be useful if, for example, you have actual string values like null” or undefined” in your ID columns. The setting is specific to your model/mapping and will be exported along with your configuration when you share it with others.Node ID exclude list settingGenerated Cypher that now skips Empty IDs”Improved Representation of Partial Mappings and ErrorsA key part of using Data Importer is making sure you have your model and mapping correctly defined. For the most part, this means you need to ensure you have all the fields completed in the mapping part of the UI. While the previous version of Data Importer helped with this, it wasn’t always clear from the graph model what was partially mapped, and errors could be a bit shouty,” painting the graph model red when you hadn’t quite got around to mapping something yet.In the latest release, we now show nodes and relationships that you haven’t quite got around to fully mapping yet with a dashed line. So from now on, if you see a dashed line, it’s a good reminder that you haven’t yet provided all the required information.If you try to run an import with incomplete mappings, Data Importer will still remind you which parts of your model or mapping require your attention:Other ImprovementsWe’ve also made a few small improvements to the way load progress is shown (it’s now in the form of a progress bar) and made some subtle changes to buttons and menus.Loads that previously failed as a result of a type conversion failures on an id field (e.g. converting the string Abc” to an Integer) will now behave similarly to properties with an empty string — meaning no nodes will be created where the property used as the ID fails type conversion. We’re considering improving reporting of occurrences like these in the load summary to help you better understand how Data Importer is reacting to sub-optimal data. Feel free to let us know more about what you’d like to see on our feedback page.And Finally…Patch releases of Neo4j Browser and Desktop also went out recently, addressing a number of bugs. We’ve also been busy working on some big things for the future, and while we can’t quite share full details just yet, rest assured that we’re working on things to make your developer experience smoother than ever. Stay tuned for more updates!Start Uploading Your Data to Neo4j AuraDB — No Coding Required;Apr 20, 2022;[]
https://medium.com/neo4j/building-a-modern-web-application-with-neo4j-and-nestjs-b51ffd8268fa;Adam CowleyFollowJul 8, 2020·10 min readHalacious on UnsplashBuilding a Web Application with Neo4j and NestJSThis article is the introduction to a series of articles and a Twitch stream on the Neo4j Twitch channel where I build an application on top of Neo4j with NestJS and a yet-to-be-decided Front End. This week I built a Module and Service for interacting with Neo4j.Watch the first stream in full on YouTube, or head to the Neo4j Twitch Channel for the next session.TLDR: I’ve pushed the code to Github and created a Neo4j module for NestJS to save you some time.Over the past few weeks I have been spending an hour live streaming something that I have found interesting that week, but from this week I thought I would change things up and start to build out a project on Neo4j.While the poll for the subject of the stream is still running, I thought I’d spend some time putting together the scaffolding so that we’re ready to go next week…Tech StackNeo4jIf you’re subscribed to this channel then you are likely familiar with Neo4j, but if not then Neo4j is the world’s leading Graph Database. Rather than tables or documents, Neo4j stores its data in Nodes — those nodes are categorised by labels and contain properties as key/value pairs. Those Nodes are connected together by relationships, which are categorised by a type and can also contain properties as key/value pairs.(a:Person {name:  Adam })-[:USES_DATABASE {since: 2015}]->(neo4j:Database:GraphDatabase {name:  Neo4j , homepage:  neo4j.com })What sets Neo4j apart from other databases is its ability to query connected datasets. Where traditional databases build up joins between records at read time, Neo4j stores the data in such a way that each Node is aware of the relationships connected to it — this is known as Index Free Adjacency.As relational tables grow, join queries become slower because the size of the Set grows. With Neo4j, the query time is only proportional to the number of nodes/relationships that have been touched as part of the query rather than the size of the data as a whole, and query times will remain consistent regardless of whether you have a hundred or a million nodes.Neo4j is schema-optional — meaning that you can enforce a schema on your database if necessary by adding unique or exists constraints on Nodes and Relationships.If you want to find out more specifically, there are some great, free, online training you can check out.TypescriptI’ve been experimenting with Typescript for a while now, and the more I use it the more I like it.Typescript is essentially JavaScript but with additional static typing. Under the hood, it compiles down to plain JavaScript but it improves the developer experience a lot, and allows you to identify problems in real-time as you are writing your code.NestJSBy far the best framework I have seen that supports typescript is NestJS. NestJS is an opinionated framework for building server-side applications. It also includes modern features you’d expect in a modern framework like Spring Boot or Laravel — mainly Dependency Injection.Integrating Neo4j into a NestJS ApplicationNest comes with a CLI with many helpers for starting and developing a project. You can install it by running:npm i --global @nestjs/clinest --helpOnce it’s installed, you can use the new or n command to create a new project.nest new apiAfter selecting the package manager of your choice, the CLI command will generate a new project and install any dependencies. Once it’s done, you can cd into the directory and then run npm run start:dev to fire up the development server.In the generated src/ folder, youll see:main.ts - The main entry point of the file, this creates a Nest application instanceapp.module.ts - This is the root module of the application, where you define the child modules that are used in the applicationapp.controller.ts - This is a basic controller, where you can define REST endpoints on the serverNest modulesFunctionality in Nest is grouped into modules, the official documentation uses Cats as its example. Modules are a way of grouping related functionality together. In the Cats example, the module provides a CatsService which handles the applications interactions with Cats, and a Cats controller which registers routes which define how the Cats are accessed.Module classes are defined by a @Module annotation, which in turn defines which child modules are imported into module, any controllers that are defined in the module, and any classes that are exported from the module and made available for dependency injection.Take the annotation on the Cats example in the documentation, this is saying that the CatsModule registers a single controller CatsController and provides the CatsService.@Module({  controllers: [CatsController],  providers: [CatsService],})The CatService is registered with the Nest instance and can then be injected into any class.@Injectable() classesClasses annotated with @Injectable() are automatically injected into a class using some under-the-hood Nest  magic . For example, by defining the CatsService in the constructor for the CatsController, Nest will automatically resolve this dependency and inject it to the class without any additional code.This is identical to how things work in more mature frameworks like Spring and Laravel.import { Controller } from @nestjs/commonimport { CatsService } from ./cats.service@Controllerexport class CatsController {  constructor(private catsService: CatsService) {}}Dependency Injection is a software technique where a class will be injected” with instances of other classes that it depends on. This makes the testing process easier where instead of instantiating classes. It also promotes the principles of DRY — don’t repeat yourself and SOLID. Each class should have a single responsibility — for example a User service should only be concerned with acting on a Users record, not be concerned with how that record is persisted to a database.Creating a Neo4j Driver InstanceIn order to use Neo4j in services across the application, we can define a Neo4jService for interacting with the graph through the JavaScript driver. This service should provide the ability to interact with Neo4j but without the service itself needing to know any of the internals. This service should be wrapped in a module which can be registered in the application.The first step is to install the Neo4j Driver.npm i --save neo4j-driverThen, we can use CLI to generate a new module with the name Neo4j.nest g mo neo4j # shorthand for `nest generate module neo4j`The command will create a neo4j/ folder with its own module. Next, we can use the CLI to generate the Service:nest g s neo4j # shorthand for `nest generate service neo4j`This command will generate neo4j.service.ts and append it to the providers array in the module so it can be injected into any application that uses the module.Configuration & Dynamic ModulesBy default, these modules are registered as static modules. In order to add configuration to the driver, we’ll have add a static method which accepts the user’s Neo4j credentials and returns a DynamicModule.The first thing to do is generate an interface that will define the details allowed when instantiating the module.nest g interface neo4j-configThe driver takes a connection string and an authentication method. I like to split up the connection string into parts, this way we can validate the scheme.The scheme (or protocol) at the start of the URI should be a string, and one of the following options:export type Neo4jScheme = neo4j | neo4j+s | neo4j+scc | bolt | bolt+s | bolt+sccThe host should be a string, port should either be a number or a string, then username, password should be a string. The database should be an optional string, if the driver connects to a 3.x version of Neo4j then this isn’t a valid option and if none is supplied then the driver will connect to the default database (as defined in neo4j.conf — dbms.default_database).export interface Neo4jConfig {    scheme: Neo4jScheme    host: string    port: number | string    username: string    password: string    database?: string}Next, for the static method which registers the dynamic module. The documentation recommends using the naming convention of forRoot or register. The function should return a DynamicModule - this is basically an object that contains metadata about the module.The module property should return the Type of the module — in this case Neo4jModule. This module will provide the Neo4jService so we can add the class to the providers array.// ,,export class Neo4jModule {    static forRoot(config: object): DynamicModule {        return {            module: Neo4jModule,            providers: [                Neo4jService,            ]        }    }    // ...}Because we are providing a configuration object, we’ll need to register it as a provider so that it can be injected into the Neo4jService. For providers that are not defined globally, we can define a unique reference to the provider and assign it to a variable. We will use this later on when injecting the config into the service. The useValue property instructs Nest to use the config value provided as the first argument.// Reference for Neo4j Connection detailsconst NEO4J_OPTIONS = NEO4J_OPTIONSexport class Neo4jModule {    static forRoot(config: object): DynamicModule {        return {            module: Neo4jModule,            providers: [                {                    // Inject this value into a class @Inject(NEO4J_OPTIONS)                    provide: NEO4J_OPTIONS,                    useValue: config                },                Neo4jService,            ],        }    }    // ..}If the user supplies incorrect credentials, we don’t want the application to start. We can create an instance of the Driver and verify the connectivity using an Asynchronous provider. An async provider is basically a function that given a set of configuration parameters, returns an instance of the module that is configured at runtime.In a new file: neo4j.utils.ts, create an async function to create an instance of the driver and call the verifyConnectivity() to verify that the connection has been successful. If this function throws an Error, the application will not start.import neo4j from neo4j-driverimport { Neo4jConfig } from ./interfaces/neo4j-config.interfaceexport const createDriver = async (config: Neo4jConfig) => {    // Create a Driver instance    const driver = neo4j.driver(        `${config.scheme}://${config.host}:${config.port}`,        neo4j.auth.basic(config.username, config.password)    )    // Verify the connection details or throw an Error    await driver.verifyConnectivity()    // If everything is OK, return the driver    return driver}The function accepts the Neo4jConfig object as the only argument. Because this has already been defined as a provider, we can define it in the injects array when defining it as a provider.// Import the factory functionimport { createDriver } from ./neo4j.utils.ts// Reference for Neo4j Driverconst NEO4J_DRIVER = NEO4J_DRIVERexport class Neo4jModule {    static forRoot(config: object): DynamicModule {        return {            module: Neo4jModule,            providers: [                {                    provide: NEO4J_OPTIONS,                    useValue: options                },                {                    // Define a key for injection                    provide: NEO4J_DRIVER,                    // Inject NEO4J_OPTIONS defined above as the                    inject: [NEO4J_OPTIONS],                    // Use the factory function created above to return the driver                    useFactory: async (config: Neo4jOptions) => createDriver(config)                },                Neo4jService,            ],        }    }}Now that the driver has been defined, it can be injected into any class in it’s own right by using the @Inject() annotation. But in this case, we will add some useful methods to the Neo4jService to make it easier to read from and write to Neo4j. Because we have defined NEO4J_DRIVER in the providers array for the dynamic module, we can pass the NEO4J_DRIVER as a single parameter to the @Inject directive in the constructor.import { Injectable, Inject } from @nestjs/commonimport { NEO4J_DRIVER } from ./neo4j.constants@Injectableexport class Neo4jService {    constructor(        @Inject(NEO4J_CONFIG) private readonly config,        @Inject(NEO4J_DRIVER) private readonly driver    ) {}}Each Cypher query run against Neo4j takes place through a Session, so it makes sense to expose this as an option from the service. The default access mode of the session allows the Driver to route the query to the right member of a Causal Cluster — this can be either READ or WRITE. There is also an optional parameter for the database when using multi-tenancy in Neo4j 4.0. As I mentioned earlier, if none is supplied then the query is run against the default database.So the user doesn’t need to worry about the specifics of read or write transactions, we should create a method for each mode — both with an optional parameter for the database. There is also a database specified in the Neo4jConfig object, so we should fall back to this if none is explicitly specified.import { Driver, Session, session, Result } from neo4j-driver//...export class Neo4jService {    constructor(@Inject(NEO4J_DRIVER) private readonly driver) {}    getReadSession(database?: string): Session {        return this.driver.session({            database: database || this.config.database,            defaultAccessMode: session.READ,        })    }    getWriteSession(database?: string): Session  {        return this.driver.session({            database: database || this.config.database,            defaultAccessMode: session.WRITE,        })    }}These methods make use of NEO4J_CONFIG and NEO4J_DRIVER which were injected into the constructor.So with that in mind, it would be useful to create a method to read data from Neo4j. The driver accepts parameterised queries as a string (eg. queries with literal variables replaced with parameters — $myParam) and an object of parameters so these will be the arguments for the query. Optionally, we may want to specify which database this query is run against so it makes sense to include that as an optional third parameter. The query then returns a Result statement which includes the result and some additional statistics.read(cypher: string, params: Record<string, any>, database?: string): Result {    const session = this.getReadSession(database)    return session.run(cypher, params)}Over the course of the application, this will save us a few lines of code. The same can be done for a write query:write(cypher: string, params: Record<string, any>, database?: string): Result {    const session = this.getWriteSession(database)    return session.run(cypher, params)}Using the Service in the ApplicationNow we have a service that is registered in the main application through the Neo4jModule that can be injected into any class in the application. So as an example, lets modify the Controller that was generated in the initial command. By default, the route at / returns a hello world message, but instead lets use it to return the number of Nodes in the database.To do this, we should first inject the Neo4jService into the controller:import { Controller, Get } from @nestjs/commonimport { Neo4jService } from ./neo4j/neo4j.service@Controller()export class AppController {    constructor(private readonly neo4jService: Neo4jService) {}    // ...}Now, we can modify the getHello method to return a string. The constructor will automatically assign the Neo4jService to the class so it is accessible through this.neo4jService. From there we can use the .read() method that weve just created to execute a query against the database.async getHello(): Promise<any> {    const res = await this.neo4jService.read(`MATCH (n) RETURN count(n) AS count`)    return `There are ${res.records[0].get(count)} nodes in the database`}Navigating in the browser to http://localhost:3000 should now show a message including the number of nodes in the database.There are 1038 nodes in the databaseTune in to the Neo4j Twitch channel Tuesdays at 13:00BST, 14:00CEST for the next episode.;Jul 8, 2020;[]
https://medium.com/neo4j/automating-deployment-of-neo4j-java-extensions-to-self-managed-google-cloud-377c8a6bed9b;Gaston GuitartFollowDec 16, 2022·9 min readAutomating Deployment of Neo4j Java Extensions to (self-managed) Google CloudNeo4j Extensions (namely: Stored Procedures, Functions, plugins, and Triggers if for any reason Java is favored over APOC) are ultimately Jar artifacts that require deployment to the plugins” folder on the file system of the Server running your Database Instance.This article presents a method to address some challenges that may be observed during the manual deployment process:Developers usually possess the means to code and test locally but deploying artifacts to a Server in the Cloud (e.g. a Compute Engine Instance) requires configuring SSH and a local O/S user account on it with higher permissions. Provisioning these resources becomes cumbersome and difficult to track as the team gets bigger.Each account and/or SSH Key has to be shared with an individual person, thus opening a door to a security breach.Additionally, each user would require privileges to restart the Database Service, increasing the complexity of the provisioning process.The manual nature of this process sets a path to a customized CI/CD pipeline, which on Cloud environments might produce higher resource costs.The upcoming sections present a design for automating the deployment of Neo4j Extensions on self-managed Google Cloud environments (i.e. not Neo4j Aura). This design provides a framework that can be used to build solutions that address the Administration and Security challenges mentioned above.You may also deploy and test the solution with the provided code for a complete end to end workflow.High Level DesignGoogle Cloud Platform ComponentsThe Table below presents a first look at the GCP Components required to implement this automation — which will be referred to as a Pipeline” from now on.*Javascript/Nodejs is the language of choice for the Function and the Web Service implemented here but Google Cloud does offer more options.NOTE: Although it may be a possible to instantiate all of these components while keeping costs within the Google Cloud free tier, instructions to do so will not be covered. It is strongly recommended to leverage the pricing calculator by including the components in the table above and your existing services.Pipeline Workflow SequenceThe figure below illustrates the Pipeline and the steps are as follows:The developer (authenticated with Google Cloud) uploads the jar artifact to a specific Storage Bucket.A Cloud Function, configured to listen for finalize” events in the aforementioned bucket, is triggered.The Cloud Function relays the Event Information to a Web Service on Cloud Run.The Web Service obtains the jar file from the Bucket (action not displayed in the diagram for readability), deploys it directly via SSH to the Server hosting the Neo4j Database Instance, and restarts the Database Service.Services OverviewIn the following subsections we will examine code snippets for each solution component. Familiarity with Javascript/Nodejs and Docker is assumed. We focus on explaining the essential tasks behind each architectural component rather than presenting how to create the Nodejs Applications from the ground up.The code base is in two git repositories, namely the Cloud Function (git repo here) and the Cloud Run App (git here).GitHub - ggasg/neo4j-plugindeploy-function: GCP Function for relaying Storage Trigger DataGCP Function for relaying Storage Trigger Data to Cloud Run Web Service and execute a Neo4j Extension deployment…github.comGitHub - ggasg/neo4j-plugindeploy-service: Web App for deploying Neo4j Extensions inbound from…Nodejs Web App that receives a Storage Event payload with information about the Jar artifact that was uploaded to GCS…github.comPre-requisitesBefore any code-related activities, three components must exist in your GCP Project:Cloud Storage Bucket.Service Account. A key must be generated and its .json downloaded as it will be used as part of the automation.Compute Engine VM running with the Service Account, and with Neo4j installed. See here for debian installation instructions.While there are no particulars about the Storage Bucket and the VM besides noting down their names (they will be used as configuration parameters later), the Service Account must be set up with the following IAM Roles:* (If complying with the principle of least privilege is a must, a designated O/S user within the VM can be created as part of your instance provisioning, with write privileges on the plugins folder and leverage the Compute OS Login Role instead)Storage Event FunctionThe Cloud function has two main responsibilities:Monitor for the google.storage.object.finalize event on the designated bucket.Relay the received payload to the Web Service.Therefore, the logic behind this function is simple:async function run(funcPayload) {  let targetAudience = <YOUR_CLOUD_RUN_APP_ENDPOINT_URL>  let authUrl = `http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/identity?audience=${targetAudience}`  const authRes = await fetch(authUrl, {    method: get,    headers: { Metadata-Flavor :  Google },  })  let authToken = Buffer.from(authRes.body.read()).toString()  let serviceUrl = targetAudience  return await fetch(serviceUrl, {    method: post,    headers: {Authorization: `Bearer ${authToken}`,      ce-subject: sample,      ce-type: ggoogle.cloud.storage.object.v1.finalized,      content-type: application/json    },    body: JSON.stringify(funcPayload)  })}It is important to point out that in order to relay the Storage Object payload to the Web Service, an Authentication Token must be supplied since in this solution there are no endpoints exposed outside of the Project (Public Access is not enabled anywhere). A method that consists in querying an internal GCP metadata endpoint has been chosen for this. All options are reviewed in this excellent documentation page about Service-to-Service” Authentication.The trigger bucket for this function is configured at deployment time. As noted in the documentation, there is no need to explicitly state the …finalize” event when running this command for deploying 1st Gen Functions.Deployment Web ServiceThis Service is essentially a Containerized Nodejs Web Application deployed to Cloud Run. In a nutshell, its tasks are sequenced as follows:Download the jar file from the Storage Bucket (as stated in the inbound payload) locally.Deploy the file to the Compute Engine VM.Restart the Neo4j DB Service via an SSH command.Clean up.Task #1 is implemented with the Cloud Storage Client Library. This will execute as long as the client (the Service Account that was previously created) is authenticated. Notice how the key’s .json file for the Service Account is used for this purpose:async function downloadFromGcs(bucketName, fileName) {  const storage = new Storage({keyFilename: assets/key.json})  await storage.bucket(bucketName).file(fileName)    .download({destination: destLocal + / + fileName})}Task #2 is quite different in nature and uses the Command Line Interface due to the fact that at the moment of writing, there is no support for compute scp” in the Cloud Compute Client Library. Notice usage of Nodejs’ spawnSync” to run the gcloud compute command as a Child Process:function sendToComputeInstance(fileName) {  const sp = spawnSync(gcloud, [compute, scp, fileName,    `neo4j@${appConfig.dbServerName}:${appConfig.destPluginFolder}`,    ` - zone=${appConfig.computeZone}`])    if (sp.status && sp.status !== 0) {    console.error(sp.stderr.toString())    if (sp.error) {      throw new Error(sp.error)    }    throw new Error(Cannot process request)  }}Before attempting to run gcloud compute” successfully on any environment, it is mandatory to install and configure the CLI. To do so, we encapsulate the configuration in a Docker image which is straightforward and gives plenty of tuning flexibility:RUN curl -sSL https://sdk.cloud.google.com | bashRUN gcloud auth activate-service-account ${SERVICE_ACCOUNT} - key-file=./assets/key.jsonRUN gcloud config set account ${SERVICE_ACCOUNT}RUN gcloud config set project ${PROJECT_ID}Deploying this Cloud Run Application is more involved than simply running gcloud run deploy because the docker build command to build the image will require some environment variables as inputs that will be used by the configuration tasks. At the time of writing, gcloud run deploy does not support supplying environment variables to the build process, but this can be overcome by incorporating Simon Willison’s fantastic approach, which consists in running a shell script that will create and submit a cloudbuild.yml file to Google Cloud prior to building and deploying.Lastly, in task #3 we issue a Neo4j direct Service restart:const sp = spawn(gcloud, [compute, ssh,  `neo4j@${appConfig.dbServerName}`, ` - zone=${appConfig.computeZone}`,   - command=sudo systemctl restart neo4j ])You may replace the O/S account neo4j” above with your designated account and avoid the sudo if following a principle of least privilege.SetupFirst of all, ensure the pre-requisites, presented in the first section, are implemented. If you are working with the code explained in this document, clone the git repositories for the Cloud Function and the Web Service.Second, go to IAM/Service Accounts in your GCP Console, generate a json key file for your Service account. Ensure the resulting .json file is saved as <YOUR_CLOUD_RUN_APP_BASE_DIR>/assets/key.jsonThen, update config.json to match your parameters{ appName :  <NAME_OF_CLOUD_RUN_SERVICE , projectName :  <YOUR_PROJECT_NAME> , dbServerName :  <YOUR_NEO4J_COMPUTE_INSTANCE_NAME> , computeZone :  <YOUR_REGION_NAME> , destPluginFolder :  /var/lib/neo4j/plugins , serviceAccountName :  <YOUR_SERVICE_ACCOUNT_NAME> }The destPluginFolder” element should not be updated unless you have customized this setting in neo4j.conf.Next, in the Cloud function, update its config.json{ deployWebServiceEndpoint :  <YOUR_CLOUD_RUN_SERVICE_ENDPOINT_URL> }Continue to the next subsections to see how each component is started locally and then deployed to GCP.Testing LocallyLocal testing can involve running either or both the Cloud Function and the Web Service.The Web Service can be started like any other Nodejs App built with Express. A script based on nodemon has been provided for convenience:npm run startAppLocalNext, a request can be issued with the following sample format (some json elements have been omitted for simplicity):curl - location - request POST http://localhost:3000 \ - header ce-subject: localTest \ - header Content-Type: application/json \ - data-raw {     bucket :  gg-neo-plugin-deploy ,     contentType :  text/plain ,     crc32c :  rTVTeQ== ,     etag :  CNHZkbuF/ugCEAE= ,     metageneration :  1 ,     name :  plugin.jar ,     selfLink :  https://www.googleapis.com/storage/v1/b/sample-bucket/o/folder/Test.cs ,     size :  352 ,     storageClass :  MULTI_REGIONAL ,     timeCreated :  2020–04–23T07:38:57.230Z ,     timeStorageClassUpdated :  2020–04–23T07:38:57.230Z ,     updated :  2020–04–23T07:38:57.230Z }A 200 response code with a plaintext Process Done” message should be received.The Cloud Function can be started similarly:npm run funcStart# Or just run npm start to see it in action without nodemonAnd since the Function acts as a relay to the Web Service, the same json payload can submitted on both tests.At this point a critical eye may be questioning why the workflow is split between a Cloud Function and a Cloud Run App when the entire logic could be written in a single Web Application. The main reason behind this decision is that running the essential ssh operations (like scp and restarting services) is not supported by the Compute Client Application Library. This introduces a need to obtain more control over the O/S to install and configure the gcloud CLI (see Dockerfile for more information). Now, even with this limitation, it is entirely possible to implement everything in a single Web Application that is virtually identical to the Cloud Run Web Service presented here, but hosted by the Google Kubernetes Engine. Then, an EventArc trigger can be configured to send Storage Events to a GKE Endpoint. However, the simplicity and maintainability of a single code base comes at the higher cost of keeping a GKE cluster running.Improving the Image SpecIf for any reason you require to alter the environment for the Cloud Run Application (there are many improvements that can be made), feel free to use the following commands as a starting point to build and the image locally:docker build - build-arg SERVICE_ACCOUNT=<YOUR_SERVICE_ACCOUNT_EMAIL> \ - build-arg PROJECT_ID=<YOUR_PROJECT_ID> -t plugin-cloudrun .docker run -dp 3000:3000 plugin-cloudrunDeploying and Running the Pipeline on GCPOnce the local endpoints have been successfully tested, the Cloud Run Service must be deployed first:npm run deployToCloudRunThe command above runs the shell script that will update the environment variables for the build process if necessary:Do you need to update environment variables? (Y/N)If deploying for the first time, choose Y. Whenever this choice is made, the process may take a few minutes. Once it is complete, take note of the URL for the Service by running:gcloud run services listUpdate appConfig.json and then proceed to deploy the Cloud Function:gcloud functions deploy <FUNCTION_NAME> \ - trigger-bucket=<YOUR_GCS_BUCKET_NAME> - runtime=nodejs12 - region=<REGION_NAME> - service-account=<SERVICE_ACCOUNT_EMAIL>Triggering the pipeline is straightforward. Just upload a file to your designated bucket:cp ~/<YOUR_JAR_FILE>.jar gs://<YOUR_GCS_BUCKET_NAME>MonitoringWe can monitor the following aspects during the development, deployment, and testing phases:The Cloud Build History: This will keep the logs for the Cloud Run Container build process triggered by the shell script mentioned previously. If there was a typo with one of the environment variables that set up the CLI, it will be called out here.Cloud Run Logs: All the console.log” messages from the Deployment Web Service.Cloud Function Logs: Same as above, but with the Function receiving events from Cloud Storage.Improvement OpportunitiesYou are encouraged to improve this code base, adapt it and make it your own. And if you share feedback and/or suggestions with the author, it will be greatly appreciated.For a more robust design, here are some open questions for inspiration:What design changes would you suggest to support deployment of Neo4j Extensions to a Clustered Environment?How can the SSH operations be replaced or eliminated?How can the Cloud Run App evolve to a Service that also provides metadata about the artifact that is currently deployed in a particular environment? -> This could be potentially of great help to customized CI/CD pipelines.If the size of the .jar file becomes significantly larger, which components would have to be refined and how?Thank you for reading.;Dec 16, 2022;[]
https://medium.com/neo4j/turn-a-harry-potter-book-into-a-knowledge-graph-ffc1c45afcc8;Tomaz BratanicFollowJul 20, 2021·9 min readTurn a Harry Potter Book into a Knowledge GraphLearn how to combine Selenium and SpaCy to create a Neo4j knowledge graph of the Harry Potter universeMost likely, you have already seen the Game of Thrones network created by Andrew Beveridge.Andrew constructed a co-occurrence network of book characters. If two characters appear within some distance of text between each other, we can assume that they are somehow related or they interact in the book.Network of ThronesIve added the Network Analysis for Season 8. Like the storylines, the network is unifying as the characters turn their…networkofthrones.wordpress.comI decided to create a similar project but choose a popular book with no known (at least to me) network extraction. So, the project to extract a network of characters from the Harry Potter and the Philosopher’s Stone book was born.I did a lot of experiments to decide the best way to go about it. I’ve tried most of the open-source named entity recognition models to compare which worked best, but in the end, I decided that none were good enough.Luckily for us, the Harry Potter fandom page contains a list of characters in the first book. We also know in which chapter they first appeared, which will help us even further disambiguate the characters.Armed with this knowledge, we will use SpaCy’s rule-based matcher to find all mentions of a character. Once we have found all the occurrences of entities, the only thing left is to define the co-occurrence metric and store the results in Neo4j.We will use the same co-occurrence threshold as was used in the Game of Thrones extraction. If two characters appear within 14 words of each other, we will assume they have interacted somehow and store the number of those interactions as the relationship weight.AgendaScrape Harry Potter fandom pagePreprocess book text (Co-reference resolution)Entity recognition with SpaCy’s rule-based matchingInfer relationships between charactersStore results to Neo4j graph databaseI have prepared a Google Colab notebook if you want to follow along.Harry Potter Fandom Page ScrapingWe will use Selenium for web scraping. As mentioned, we will begin by scraping the characters in the Harry Potter and the Philosopher’s Stone book. The list of characters by chapter is available under the CC-BY-SA license, so we don’t have to worry about any copyright infringement.Scraping list of characters from HP fandom siteWe now have a list of characters with information on which chapter they first appeared. Additionally, each of the characters has a web page with detailed information about the character.Hermione GrangerMinister Hermione Jean27 Granger (b. 19 September, 1979)1 was an English Muggle-born3 witch born to Mr and Mrs Granger…harrypotter.fandom.comFor example, if you check out the Hermione Granger page, you can observe a structured table with additional information. We will use the alias section for the entity extraction and add other character details like house and blood type to enrich our knowledge graph.Enrich character details by scraping character’s sitesI haven’t added all code used to enrich information in this gist for readability and clarity. I have also added some exceptions for aliases. For example, Harry Potter has the following alias:Gregory Goyle (under disguise of Polyjuice)We want to ignore all the aliases under the disguise of Polyjuice. It seems he also told Stanley Shunpike that he was Neville Longbottom, which we will also skip.Before we continue with named entity extraction from the book, we will store the scraped information about the characters to Neo4j.Attributes:nameurlaliasesnationalityblood-typegenderspeciesRelationships:HouseLoyaltyFamilyStore character information to Neo4jIf you have some experience with Cypher, you might have noticed that we have stored information such as blood, gender, aliases, but also family and loyalty relationships to our graph.Example subgraph of Hermione Granger. Image by the author with Neo4j Bloom.It seems that Hermione Granger is also known as Little Miss Perfect and is loyal to the Society for the Promotion of Elfish Welfare. Unfortunately, the data has no timeline, so Fred Weasley is already brother-in-law to Little Miss Perfect.Text PreprocessingFirst of all, we have to get our hands on the text from the book. I’ve found a GitHub repository that contains the text of the first four Harry Potter books.There is no license attached to the data, so I will assume we can use the data for educational purposes within the limits of fair use. If you actually want to read the book, please go and buy it.Getting the text from a GitHub file is quite easy:Read a text from a file on GitHubWe have to be careful to provide the link to the raw text content, and it should work.When I first did the entity extraction, I forgot to use the co-reference resolution technique beforehand. Co-reference resolution is the task of determining linguistic expressions that refer to the same real-world entity.In simple terms, we replace the pronouns with the referenced entities.For a real-world example, check out my Information extraction pipeline post.From text to knowledge. The information extraction pipelineImplementation of information extraction pipeline that includes coreference resolution, entity linking, and…towardsdatascience.comI’ve been searching for open-source co-reference resolution models, but as far as I know, there are only two. The first is NeuralCoref that works on top of SpaCy, and AllenNLP provides the second model. Since I have already used NeuralCoref before, I decided to look at how the AllenNLP model works.Unfortunately, I quickly ran out of memory (Colab has 16GB RAM) when I input a whole chapter into the AllenNLP model. Then I sliced a chapter into a list of sentences, but it worked really slow, probably due to using the BERT framework.So, I defaulted to use NeuralCoref, which can easily handle a whole chapter and works faster. I have copied the code I have already used before:Now that we have our text ready, it is time to extract mentioned characters from the text.Entity Recognition with SpaCy’s Rule-Based MatchingFirst, I wanted to be cool and use a Named Entity Recognition model. I’ve tried models from SpaCy, HuggingFace, Flair, and even Stanford NLP.None of them worked well enough to satisfy my requirements. So instead of training my model, I decided to use SpaCy’s rule-based pattern matching feature.We already know which characters we are looking for based on the data we scraped from the HP fandom site. Now we just have to find a way to match them in the text as perfectly as possible.We have to define the text patterns for each of the character.First, we add the full name as the pattern we are looking for. Then we split the name by whitespace and create a pattern out of every word of the term. So, for example, if we are defining matcher patterns for Albus Dumbledore, we will end up with three different text patterns that could represent the given character:Full name: Albus DumbledoreFirst name: AlbusLast name: DumbledoreThere are some exceptions. I have defined a list of stop words that should not be included in the single word pattern for a given character.For example, there is a Keeper of the Zoo” character present in the book. It might be pretty intuitive not to define words like of” or the” as the matcher patterns of an entity.Next, we want all single words to be title-cased. This is to avoid all mentions of the color black” being a reference to Sirius Black”. Only if the Black” is title-cased, will we assume that Sirius Black” is referenced. It is not a perfect solution as Black” could be title-cased due to being at the start of a sentence, but it is a good enough solution.A particular case I’ve introduced is that Ronald Weasley” is mainly referred to as Ron” in the text. Lastly, we do not split entities like Vernon Dursley’s secretary” or Draco Malfoy’s eagle owl”.There are two obstacles we must overcome with this approach. The first issue is that the following text: Albus Dumbledore is a nice wizard”will produce three matches. We will get three results because we have used the whole and parts of names in our matcher pattern. To solve this issue, we will prefer more extended entities.If there is a match that composes of multiple words and another match consisting of a single word in the same place, we will prefer the one with multiple words.Matcher pattern results based on longer entities prioritization.My implementation of longer-word entities prioritization is very basic. First, it checks if a more extended entity already exists. It then checks if the current result is longer than any existing entities in the same position, and lastly appends a new result if there are no existing entities yet.What is interesting is the very last else” clause. Sometimes, more entities are assigned to a single position. Consider the following sentence:Weasley, get over here!”There are many Weasley characters that we can choose from. This is the second issue we must overcome. It mostly happens when a person is referenced by their last name, and there are many characters with that last name.We must come up with a generic solution for entity disambiguation.Disambiguate when multiple options are available for a single entity.This is a bit longer function. We start by identifying which entities require disambiguation.I introduced a unique logic disambiguating the Dursley” term. If Mr.” is present before the Dursley” term, then we reference Vernon, and if Mrs.” is present, we choose Petunia”.Next, we have a more generic solution. The algorithm assigns the reference to the nearest neighbor out of the options.For example, suppose we can choose between Harry Potter”, James Potter”, and Lily Potter”. In that case, the algorithm identifies the nearest of those three entities in the text and assigns the current item its value. There are some exceptions where their full or first name is not referenced within the same chapter, and I have added hard-coded options as a last resort.Infer Relationships Between CharactersWe are finished with the hard part. Inferring relationships between characters is very simple. First, we need to define the distance threshold of interaction or relation between two characters. As mentioned, we will use the same distance threshold as was used in the GoT extraction.That is, if two characters co-occur within the distance of 14 words, then we assume they must have interacted. I have also merged entities not to skew results.What do I mean by joining entities? Suppose we have the following two sentences:Harry was having a good day. He went to talk to Dumbledore in the afternoon.”Our entity extraction process will identify three entities, Harry”, He” as a reference to Harry, and Dumbledore”. If we took the naive approach, we could infer two interactions between Harry and Dumbledore as two references of Harry” are close to Dumbledore”.However, I want to avoid that, so I have merged entities in a sequence that refers to the same character as a single entity. Finally, we have to count the number of interactions between the pairs of characters.Count the number of interactions between characters based on the text co-occurrence.Store Results to Neo4j Graph DatabaseWe have extracted the interactions network between character, and the only thing left is to store the results into a graph database. The import query is very straightforward as we are dealing with a monopartite network.If you are using the Colab notebook I have prepared, then it would be easiest to create either a free Neo4j Sandbox or free Aura database instance to store the results.Store interaction network to Neo4j.Let’s visualize the interaction network to examine the results.Interaction network visualized with NEuler. Image by the author.At first glance, the results look pretty cool. In the center of the network is Harry Potter, which is understandable as the book is mostly from his point of view.I’ve noticed I should have added some additional stop words in the matcher pattern, but hey, you live, and you learn.ConclusionI am quite proud of how good the rule-based matching based on predefined entities turned out.You could try this approach on the second or third book. Probably the only thing that would need a bit of fine-tuning is the entity disambiguation process.Stay tuned till next time, when we will perform a network analysis of the Harry Potter universe.You can use the Neo4j graph algorithms playground application to test out graph algorithms on the Harry Potter knowledge graph if you don’t want to wait.As always, the code is available on GitHub.;Jul 20, 2021;[]
https://medium.com/neo4j/neo4j-get-off-the-ground-in-30min-or-less-3a226a0d48b1;Jennifer ReifFollowApr 11, 2018·8 min readNeo4j — get off the ground in 30min or less!As with any new technology, the first step into the unknown is uncertain and sometimes scary. Questions on where to start or how to learn can sometimes overwhelm the motivation to dive in. The toughest part of any new thing is starting. Today, I hope to ease the uncertainty and get you past the initial obstacles and pass along some resources for further development.What is a graph database?Graph technology has been around for a long time, but graph databases are a relatively new entry to the technology market. Similar to the relational model, graph model is a way to store and retrieve data. However, the difference between relational and graph is how the data is stored in the database. While relational organizes the data into table and column structures and crunches data that falls outside the norms into this format, graph accepts the data exactly as it exists and creates a model that fits the data.From this explanation, it may seem that graph would likely be very unstructured, but that is actually not the case. A property graph model consists of entities (nodes) and connections between the entities (relationships). Each node and relationship is able to include properties that contain additional metadata about itself. To better visualize the models, they are compared in the image below.Image credit: William Lyon (https://www.slideshare.net/lyonwj/natural-language-processing-with-graph-databases-and-neo4j)Instead of creating an intermediary table of Person-Friend in the relational model and using it as the reference point between the Person and Friend tables, a graph stores each relational row as a separate entity and eliminates the need for reference/lookup IDs that a relational structure relies upon. This allows for exceptionally fast lookups of data segments from a graph database, because the database is not traversing 1m+ rows in the Person table to find a particular row.In the example above, the relational model would traverse the Person table until it finds Andreas’s row (assuming only one row exists for Andreas), then search the Person-Friend table for any connections to Andreas that reference his id, then look up those reference rows to find the details about each friend.In contrast, a graph model simply finds Andreas’s node (which is stored in a particular place in memory, so that the query does not need to search the entire graph), then looks for all of the relationships with type `KNOWS` to find all of the nodes that know Andreas.As you can imagine, the more straightforward graph approach results in much faster traversal times that provide results back to the user faster. While the relational model is perfectly suited to structured, tabular data, the graph model’s strength lies in highly-connected data.These last few paragraphs are a very high-level overview of the structure. For additional information and depth, I have included resources at the bottom of this post that more thoroughly explain graph databases and their models.How to download Neo4jNeo4j’s main database product follows the property graph model and provides ways for users to store and retrieve their data in a graph database. There is both an open source and commercially-supported version. Neo4j for desktop includes a free license to Neo4j enterprise edition, which provides development and POC capabilities that are normally part of a commercial license. Developers are able to download and install Neo4j for desktop to learn how to use it and create smaller-scale or proof-of-concept projects without any fees or strings attached.Neo4j also provides a basic way to interact with the graph database without any downloads via the Neo4j sandboxes. Several use cases and tutorials have been created in a web-based interface for developers to interact with Neo4j without adding any software to devices.For this post, we will walk through downloading and using the Neo4j for desktop application. This sets developers up to create applications and interact with a Neo4j database or run a small cluster of local Neo4j servers for testing.To install Neo4j for desktop, follow the link and click `Download`.This will take you to a page that downloads the software and shows steps for installing Neo4j on your device (displays Windows, Mac, or Linux instructions, as applicable). Step 1 on the page simply covers this installation step for your device. Step 2 walks you through how to open and use the application, which we will cover here, as well.Start and navigate Neo4j for desktopWhen first loading the application, there is a brief info form to create an account for yourself in the application. This allows different users to access and work with their own projects without interference from others. You can sign in with an existing Google, GitHub, LinkedIn, or Twitter account, or you can create a separate account for Neo4j.Once signed in, the main screen for the application is presented (screenshot below). Along the left-hand side of the window are some high-level icons that show the list of projects for the user (folder icon), allow users to adjust application settings (gear icon), show the user profile (person icon), and give basic info for Neo4j as a company (Neo4j logo icon).Under the project folder icon is a list of any or all projects for that user. If you are just starting with Neo4j, then there is only one default project in the list called `My Project`. You can create a new project with the `+ New` at the top, or work in the default project, which we will do here.In the right pane, you can create, modify, run, and delete databases, as well as install plugins to use. Plugins allow developers to unlock additional capabilities, depending on the use case they are trying to solve.`APOC` (Awesome Procedures on Cypher) is a group of procedures and functions that give developers extra functionality that may not be included in the product package. This blog post gives an introduction to more details on APOC, and more resources are linked at the end of this post.The `Graph Algorithms` plugin includes a set of algorithms to analyze a data set for patterns and data science purposes. More information can be found on the Neo4j developer pages.`GraphQL` allows users to interact with a GraphQL endpoint to specify and return the exact data needed. For more information on the GraphQL plugin, there is an excellent blog post by William Lyon to get started with it.Creating and working with a databaseTo create a database, you can click the `New Graph` section under the `My Project` header and choose `Create a Local Graph`.Create a graph databaseCreate a local graphThen, you can fill out a name and password for your database (can be more creative than mine below), as well as choose a version and click the `Create` button. Just note that it may take a few minutes to download the version of Neo4j and create the database.Graph database detailsGraph created!To start the database, simply click the `Start` on your new database. To view details and interact with the database, click on `Manage`. This brings you to a new pane with a top and bottom pane.The top panel shows some basic functions you can execute, such as `Start`, `Stop`, `Restart`. It also has two additional functionalities to open the directory where the application holds all its files (`Open Folder` button) and to open a Neo4j Browser window to interact with any data in the database (`Open Browser` button).The bottom panel has several more options. In the `Details` section, it shows the version of Neo4j running and the status of the database. It also shows the ports that you can use to interact with the database via HTTP, HTTPS, and Bolt protocols.If you click on the `Logs` tab, it shows all of the streaming log output from the database. The `Terminal` tab allows you to interact with the database via the command line (including to retrieve data from an exposed endpoint).`Settings` allows you to adjust the configuration for the database, as needed. You can also search this tab using `Ctrl+F`/`Cmd+F`. The `Plugins` tab shows the available/installed plugins on that particular database.The `Upgrade` tab contains a list of Neo4j database versions, giving you the opportunity to adjust the version or upgrade from here. The final tab for `Administration` lets users update the password for the database.Interacting with the databaseThere are a couple of different ways to interact with your newly-created Neo4j database. We have already mentioned opening the Neo4j Browser through the `Open Browser` button within Neo4j for desktop. You can also open a new window in your preferred browser and type http://www.localhost:7474 into the URL. To connect, you will need to enter the password you entered for your database and click `Connect`.From here, you can run tutorials (for more info, see our browser guide tutorial). You can also insert your own data and run queries from the browser to interact with the data.Another way to interact with the database is by creating an application in your preferred programming language, then interacting via the application. If any endpoints are exposed in the application, you can also interact using the Neo4j Browser. To learn more, check out our language guides for your particular language.RecapIn this post, we have given a foundation for how to get started with Neo4j, as well as providing a few additional resources and ideas for deeper learning. Neo4j for desktop application allows you to easily start an instance of Neo4j and interact with data loaded into the graph database in a user-friendly manner.From here, there are endless possibilities to load your own data, create simple and useful applications to interact with the database, or apply your own proof-of-concept and test out a solution to a valuable business problem. Whatever your need, I hope that this post has helped and given you the stepping stone to doing incredible things with Neo4j. Happy learning!ResourcesDeveloper Getting Started GuidesAPOC documentationGraph Algorithms documentationGraphQL documentationLoading Data to Neo4jNeo4j SandboxNeo4j Use Case Examples;Apr 11, 2018;[]
https://medium.com/neo4j/graphs-for-good-where-graph-technology-is-tackling-complex-real-world-problems-4e0d860a5901;Jennifer ReifFollowMay 4, 2022·5 min readPhoto by Ian Schneider on UnsplashGraphs4Good: Where Graph Technology is Tackling Complex, Real-World ProblemsIn our world today, we seem to encounter crises and global struggles on a daily basis. But what if we could help others and improve life around us using the data and technology at our fingertips?Graph databases like Neo4j help us do exactly that. By storing data and the connections between them we can interrogate that data to see underlying context and relationships that get missed in other data formats.We will look at the technical aspects of three project areas where graphs help solve these complex, real-world problems. While we will only cover three projects in today’s article, there are many more. To find out more about current projects or submit a project, check out the Graphs4Good page.Graphs are being used to bring varying data stores together into a meaningful map for cancer research. The constantly changing field of medicine necessitates a flexible data model that can adapt to new entities and their relationships to existing data. Neo4j minimizes model constraints and captures relationships between many different types of entities, eliminating costly JOIN operations in traditional relational models and queries.Graphs also manage massive family trees of plants to increase food sources and decrease natural resource consumption. Hierarchical data often means recursive queries over entities of the same type. Modeling the same data structure as a graph in Neo4j presents the tree structure naturally and removes iterating over the same dense paths.Finally, graphs connect disparate data to identify and track space debris for future space travel and cleanup. Humans collect vast amounts of data, but extracting meaningful information is a challenge. Traditional data stores create a single-model view of the data and require many-to-many relationships across entity types. Neo4j integrates all kinds of data, connecting the pieces of the puzzle and presenting a unified view for transforming data into knowledge.Let’s talk tech!Working Towards a CureMedicine does amazing things, but humans still suffer from a number of illnesses. From survivable health conditions to deadly diseases, health sciences are a continuous field of study and improvement. How can we prevent health problems, minimize pain and symptoms, and cure illness?Research scientists and doctors at the IRCCS utilize graphs for information management to conduct advanced cancer research that provides treatments and continues searching for a cure. Neo4j was initially brought in to maintain consistency between relational data stores. Complex entities and contexts made data integration and synchronization a difficult task. The graph became a way to connect disparate data and definitions across an entire system.However, the project integrated Neo4j further when they combatted changing data models, complex relationships, and sluggish query performance in a relational solution. Queries across many types of entities meant many JOIN operations in SQL, and keeping the data model in sync with the latest medical research created obstacles for data coherence. By using graphs, the institute could integrate and analyze data from MySQL, MongoDB, and new data sources. Users are now harnessing Neo4j’s flexible data model and optimized pattern-matching to adapt to changes in the medical field, analyze experimental procedures, and model complex concepts in semantic knowledge.IRCCS sets the stage for continued improvements with hopes of eliminating human disease through understanding complex biological data.Feeding the World’s Growing PopulationAs the number of people on Earth continues to increase, many are looking for ways to maximize the output of food while reducing the consumption of other natural resources in the process. Bayer is working on solving this problem with graph databases to help feed the expected 9.5 billion humans worldwide by 2050.They are studying plant ancestry to understand breeding cycles that produce certain traits (drought tolerance, high yield, etc). Two parent plants cross to produce several child plants, of which only a select few make it to the next breeding cycle. Starting with many parent plants that produce at least hundreds of thousands of offspring, this generates a massive family tree” very quickly. All of this hierarchical data is extremely hard to navigate, and researchers want to find the entire tree of ancestors for a single plant.Doing this in the relational model required 11 tables and many recursive joins that caused exponential query times at and beyond nine hops. In contrast, Neo4j’s relationship traversal created constant query times. This allowed researchers to also query plant features and return genealogy for one million plants, rather than only one plant, within the same timeframe. Neo4j provides this information as a REST API that the group named Ancestry-as-a-Service”, which integrates with Oracle and Kafka. In addition, the graph is being used for predictive analytics, alongside Spark and HBase. Predicting traits of plant offspring will narrow down the list of test subjects, reducing the resource consumption of testing plants in the field.Bayer is planning for the future by navigating dense plant ancestry data with decreasing natural resource consumption to feed an increasing population.Keeping Current and Future Space Exploration SafeWith over 70 years of global space exploration, there is a lot of space debris from dead satellites, exploded rockets, and unusable equipment floating around and beyond Earth. How can we make sure we protect the safety of future missions (for equipment and people), as well as work on ways to clean up existing debris?Moriba Jah has a project for just this purpose called ASTRIAGraph. It uses Neo4j to identify ownership of space objects to companies/countries, match objects to registries, and track changes over time. Graphs help them ingest widespread data sources to track object details, predict collisions, and potentially fingerprint objects for identification. The project also utilizes technologies such as computer-aided design (CAD) models and predictive analytics to gain comprehensive information about the object’s geospatial location and trajectories.ASTRIAGraph consists of three technical layers — the landing zone for data ingestion, the mezzanine for interpreting information (Neo4j knowledge graph), and the inquiry layer for querying answers to questions. The amount of diverse data creates traditional many-to-many relationships in other data storage formats, which makes a knowledge graph that connects the disparate entities cumbersome. These difficulties become manageable and useful in a graph, where the schema is stored clearly with the data it organizes.Jah’s ASTRIAGraph can inspire us to do more with existing datasets to promote safety and efficiency in research and exploration.What Other Problems Can Graphs Solve?These three projects are only a small sample of the myriad projects currently in the works, plus many unknown or unimagined others waiting to take advantage of the power of a graph database. From natural resources to healthcare to improving the lives of creatures and humans, storing and traversing data as nodes and relationships allows users to enhance their understanding of context, hierarchy, meaning, dependencies, and much more.Many Graphs4Good projects are still active and looking for your help and input. There are also many unnamed or unknown projects out there. If you have a project or know of one, don’t hesitate to contact us. New to graphs? Check out Neo4j’s Developer page for more information and learn how to get started with the technology!;May 4, 2022;[]
https://medium.com/neo4j/importing-your-data-into-neo4j-just-got-even-easier-3712e531ca70;GregFollowJul 22, 2022·2 min readImporting Your Data Into Neo4j Just Got Even EasierEarlier this year, we introduced a new tool, Data Importer, to help users easily import their flat file data into Neo4j’s graph database without writing a single line of code.It works by simply providing your flat file data (CSV or TSV) to the Data Importer web interface, modeling the nodes and relationships visually, and mapping the files to your model.In this latest release, we’ve added a Preview” button to help validate your data model before importing the data into Neo4j, saving you time and increasing overall productivity.Check out this video for a quick demo of how Preview works.To keep the Preview as performant as possible we’ve made some optimizations you should be aware of so you understand the subtleties of what you see:The tool uses the first few rows of the files associated with relationships to preview up to 300 relationships in total. If the most linked parts of your graph aren’t towards the top of each file, you may not see the depth of connections that materialize in the fuller load.We don’t currently preview the properties you have mapped. We show the columns mapped to IDs in the relationships as node properties. So don’t worry if you don’t see the properties you have mapped.We currently only preview relationships and their linked nodes. You won’t see it in the preview if you have an isolated node in your model or one that doesn’t yet have a fully mapped relationship. But we’re working to fill that gap in a future release.We hope this new feature helps you iterate more quickly, testing out ideas before committing to a full data load.Additionally, you’ll notice there have been a few styling changes and minor bug fixes trickling through to make Data Importer tidier and more pleasant.As always, you can find the latest version of Neo4j Data Importer from the Neo4j AuraDB console, and it comes with every Free and Professional instance of Neo4j.Looking to get started with Neo4j? AuraDB is totally FREE — check it out here.For feedback regarding Neo4j Data Importer, leave your thoughts here, where you can also upvote other ideas from the community.Goodbye for now! We look forward to delivering you the next DevTools update soon!;Jul 22, 2022;[]
https://medium.com/neo4j/register-your-neo4j-based-models-to-the-django-admin-b58ebcd6dfc3;CristinaFollowJul 9, 2021·4 min readRegister Your Neo4j-Based Models to Django AdminA Paradise Papers ExampleIf you’re a Django developer, you may have already found the popular py2neo toolkit and may have some experience using Neomodel and/or the Neo4j python driver directly. But how well do these tools play with Django?Thankfully for Django developers, the Neo4j ecosystem provides a quick way to spin up an example app — the Paradise Papers Search App — based on the Paradise Papers dataset, conveniently available on Neo4j Sandbox.This post will be a quick walkthrough of how to register a model on the Paradise Papers Search app. For more details, check out our previous post on the topic here:Neo4j for Django DevelopersFrom zero to deployment with Neo4js Paradise Papers dataset. Making the leap from a relational database to a graph…neo4j.comLocal Setup — Sandbox DatabaseFirst step, set up your local environment:clone the repo:git clone git@github.com:neo4j-examples/paradise-papers-django.gitIn a virtual environment, install the requirements:pip install -r requirements.txtNext you’ll need to point your app to a sandbox instance of the database.In Neo4j Sandbox, create an account and select Paradise Papers by ICIJ.Back in the Sandbox UI, tap Connection details” to find the database’s Bolt URL, username, and password.In your local environment, set the DATABASE_URL variable using the credentials found in the Sandbox admin:export DATABASE_URL=bolt://<Username>:<Password>@<IP Address>:7687In paradise_papers_search/fetch_api/admin, add the models you would like to explore using the admin:from django.contrib import admin as dj_adminfrom django_neomodel import admin as neo_adminfrom .models import Entityclass EntityAdmin(dj_admin.ModelAdmin):    list_display = (name”,)neo_admin.register(Entity, EntityAdmin)Add the admin URL to urls.py (if you haven’t done so already):from django.conf.urls import url, includefrom django.views.generic import TemplateViewfrom django.contrib import adminurlpatterns = [    url(r^$, TemplateView.as_view(template_name=index.html), name=index),    url(r^fetch/, include(fetch_api.urls)),    url(r ^admin/ , admin.site.urls)]Because Django’s admin authentication still goes through Django models, you will you need to set up a (relational) user database in your settings file and run the migrations.Example settings snippet:DATABASES = {    default: {        NAME: papers.db,        ENGINE: django.db.backends.sqlite3,        USER: ,        PASSWORD: ,        PORT: ,    },}Running the migrations:./manage.py migrateAfter that, create the admin superuser:./manage.py createsuperuserRun the app!python manage.py runserver — settings=paradise_papers_search.settings.devStart searching at http://127.0.0.1:8000/Log in to the admin at http://127.0.0.1:8000/adminThat’s it!Next StepsAfter testing out the ICIJ Panama Papers database, you may want to try using some of the data native to your Django application. Test out converting your relational dataset using the Neo4j ETL Tool:Neo4j ETL Tool - Interactive Relational Database Data Import - Neo4j LabsMany people want to import the data from their relational systems into Neo4j. The Neo4j ETL tool was developed to make…neo4j.comResourcesneo4j-contrib/neomodelAn Object Graph Mapper (OGM) for the neo4j graph database, built on the awesome neo4j_driver Familiar class based model…github.comneo4j-contrib/django-neomodelThis module allows you to use the neo4j graph database with Django using neomodel Install the module: $ pip install…github.comneo4j/neo4j-python-driverThis repository contains the official Neo4j driver for Python. Each driver release (from 4.0 upwards) is built…github.comLooking for more details on how Neo4j was used in the Panama Papers investigation? Check out more coverage here:An In-Depth Graph Analysis of the Paradise PapersHow the ICIJ Used Neo4j to Unravel the Panama PapersAnalyzing the Panama Papers with Neo4j: Data Models, Queries & MoreThe Panama Papers: Why It Couldn’t Have Happened Ten Years Ago;Jul 9, 2021;[]
https://medium.com/neo4j/its-true-it-s-the-largest-graphhack-in-history-755834050d4;Karin WolokFollowSep 17, 2019·2 min readIt’s True! It’s the Largest GraphHack in History!!!This year, as the Developer Relations Team at Neo4j announced the first ever, Neo4j Online Developer Expo & Summit (NODES 2019), I am happy to make the community activities awesome for everyone.Since it’s somewhat of a tradition to host a GraphHack around every GraphConnect conference… I thought, let’s host a GraphHack! Given that NODES 2019 is a virtual conference, we decided to make it a virtual hackathon. After brainstorming and receiving some feedback from our community members (shout-out to Jhonathan Soares & Michael McKenzie!) and members from my team (big shout-out to DevRel and especially Will Lyon!), we announced our first-ever Global GraphHack!Prizes:Flight and hotel to GraphConnect 2020 (max $2000 per person)Free tickets to GraphConnect 2020 in Times Square (includes training sessions)Private access to the invite-only Ecosystem Summit (at GraphConnect 2020)Meet-and-greet event access with Neo4j Executive Staff at GraphConnect 2020An invitation to present your project at a dedicated GraphConnect 2020 DevZone sessionEven more visibility by showcasing your work on the Neo4j blog and newsletterIf you have travel restrictions, don’t worry, we’ll have amazing prizes for you too! Winners will be selected by the community votes and a panel of judges from Neo4j staff and announced immediately following the NODES 2019 keynote (given by Neo4j CEO, Emil Eifrem).This beautiful design was created by our good friend, Nicole ForresterWe made a Google form to collect registrations, and to my amazement, we received over 500 registrations!!!! (509 to be exact), which officially made it the Global GraphHack! We decided to have the hackathon live on DevPost.com, as it would make it easier for hackers to find team members, submit their projects, and vote (so, GraphHack registration has moved there).The hackathon lasts for a whole month and we’re only 2 weeks in! So, if you’re still interested in participating, it’s not too late to join!!!! Sign-up on DevPost.com. (teams can be between 1–4 hackers)The theme for this year’s GraphHack is: Extending the Graph Ecosystem”.The task is to build something using or extending Neo4j that can help others in the community. Watch the video below for more details or head straight to DevPost to read more about it.Shout-out to my very busy Director of DevRel, Ryan Boyd, took much time out of his day to create for us!We hope you can participate. At the very least, I hope you join us for the awesome interactive virtual conference. :)PS. Most of these community ideas come from inspiration from our community, so if you have any ideas for programs, projects, things you would like to see more of, etc. please share it with us! You can post it directly on the community feedback category on the Neo4j community site or email us directly at community@neo4j.com. Want to share the feedback anonymously or just generally prefer a form format? We have that too, community feedback form.;Sep 17, 2019;[]
https://medium.com/neo4j/maximising-efficiency-the-power-of-chatgpt-and-neo4j-for-creating-and-importing-sample-datasets-de563e40de51;David StevensFollowFeb 6·4 min readMaximizing Efficiency: The Power of ChatGPT and Neo4j for Creating and Importing Sample DatasetsAs a Customer Success Manager at Neo4j, I work with customers on a daily basis, helping them achieve their business goals with graph technology.Our Customer Success Architects spend time reviewing data models, tuning queries, and outlining approaches to optimize their solutions. However, one challenge we often face is access to their data. Reviewing the data model is often possible, but accessing usable data is not always possible.Can ChatGPT help me to create a set of usable sample data and import it into Neo4j?Use Case and Data ModelI’ll start with a simple banking data model, where account holders are making transactions between other accounts.ChatGPTI knew I wanted to use Visual Studio Code to write” my scripts and node.js to run them, as I am familiar with both of these. But let’s assume I’m not — can ChatGPT get me started from almost nothing?Okay, this looks like a promising start.The First ScriptsI started simply to see what I could achieve, and was also fairly explicit in what I wanted the data to look like…The first generated scriptMaking a number of revisions to this within the same conversation thread, it’s impressive how the key information is retained throughout the conversation.Updated version to include writing to CSV filesAdding TransactionsWhat I found impressive here is that I no longer had to ask for the output to be written to a CSV file — it was already included for meBut I have a problem, not with ChatGPT, but with how I’ve approached my scripts — there is no relationship between my random accounts and my random transactions. Thanks to a chat with my colleague (Thanks James!), I realized you still need to have some level of understanding of how the script needs to be structured to get the correct results.Updated the question to connect the random accounts to the random transactionsThe generated script within Visual Studio CodeSnapshot of some of the generated dataImporting into Neo4jI looked at two options here, a packaged node.js script to import directly into my local instance of Neo4j and the simple Cypher scripts to import using Neo4j desktop.Some Minor ReworkBefore importing into Neo4j, I did take some steps of my own, as I knew importing such a large dataset (I had over 1 million sample transactions) without having any indexes in place wasn’t going to work well.I also updated the larger of the two imports (transactions) to use PERIODIC COMMIT to import as a batch of 1000 rows from the CSV file. The final update was to revise the relationship information to match my target data model.The ResultsThe sample data loaded without any issues into my Neo4j instance, and the generated schema and a snapshot are shown below.Generated schema within Neo4jSample dataset presented in BloomConclusionThe current dataset is fairly simple and I could have (if given the time) written the scripts myself, but the speed at which I was able to turn this around is impressive — there is no more than one hour of work here!However, I think it’s imperative to note that while ChatGPT delivers amazing results, there is still an underlying need for some knowledge of the systems and processes being used. This is from a base understanding of programming to some of the finer details of Neo4j and graph data structures. ChatGPT is very powerful at refining and revising the generated output, but you need to know the areas where improvements can be made.Next steps for this project are to review the dataset with Cypher and the Graph Data Science library from Neo4j and see if any interesting patterns have been generated, such as potential mule accounts (of course with ChatGPT alongside).The generated scripts are available on my GitHub account here.;Feb 6, 2023;[]
https://medium.com/neo4j/neo4j-data-importer-introducing-file-filtering-e41e583dfd2a;GregFollowFeb 1·4 min readNeo4j Data Importer — Introducing File FilteringIf your CSV file contains several segments of data indicated by a different column value, you can now treat one file as several, creating separate Nodes and Relationships.The latest release of Neo4j Data Importer introduces a new way to load more data sources without the need for pre-processing. By allowing you to apply simple filters to files we’re enabling loads in more scenarios, including:Generally keeping data relevant from only certain rows in a file while skipping the othersLoading data from aggregate node lists and relationship lists where information on all nodes and all relationships is encapsulated in just two files. (Like from the Bloom export)We’re going to take a quick look at how file filtering can help you with the latter example.Consider the following subset of the Northwind data model showing Orders, the Products they contain, and the Shippers they are shipped by.Northwind Model SubsetIn the classic Northwind dataset, these are represented by different tables like Shippers.csv, Orders.csv, Products.csv, and Order-Details.csv. When extracting data from more graph-like sources, it is not uncommon to be provided with wide node lists and relationship lists that contain all the nodes and all the relationships in just two files.Here’s what an example exported from our very own Neo4j Bloom looks like (but could equally apply to any other graph-like export):bloom-nodes-export.csvbloom-relationships-export.csvIn the nodes file you’ll notice the node types are identified by the ~labels column and relationship types in the relationships file by the ~relationship_type column.Before the file filtering feature, you needed to manually separate the nodes files into three files upfront, representing the three node types and the relationships file into two files representing the two relationship types.With file filtering, now optionally available under the File dropdown in the Mapping Panel, you can apply include filters to keep rows only relevant to the Nodes or Relationships in your model.Here’s an example of applying the file filter to the bloom-relationships-export.csv file to ensure it only keeps the rows where the ~relationship_type column has ORDERS values.Animation of file filtering in actionYou’ll notice the filter when applied, gives you feedback as you type on how many matches were found. For performance reasons only the first 10,000 rows of any file are scanned, so even if you don’t see matches in this feedback, there may still be matches further down your file.The same filtering principle applies for the nodes file, in this example mapping to the Product node and only keeping the rows in the bloom-nodes-export.csv file where values in the column ~labels equal Product.For now, file filtering supports exact string matches, but we’d love to hear your feedback on the utility of the filtering functionality and other things you’d like to see.Data Importer is available in AuraDB Free and Professional — if you don’t have one already, get started with your free instance here.Data Importer continues to be available as both a standalone app in Workspace and within the Import tab of Neo4j Workspace.If you need to use Data Importer against a local dev Neo4j instance, you can continue to use the app hosted here (for non-certificate secured instances only).That’s all for now.As always, please head over to https://feedback.neo4j.com/data-importer to leave us your feedback.;Feb 1, 2023;[]
https://medium.com/neo4j/creating-graphville-6ab927c37cfa;Vlad BatushkovFollowDec 15, 2022·7 min readCreating GraphvilleGraph educational platform with Neo4j courses to teach Cypher to beginnersMy name is Vlad, and I am a Neo4j graph enthusiast. What does it mean? I love to solve problems using graphs, I write about graphs in the Neo4j blog, and I consult and collaborate with other engineers across the globe.Recently I launched an EdTech project for Neo4j beginners, called Graphville. And here is my story.Season 1, Episode 1: Welcome to GraphvilleGraphville is a graph educational platform for engineers to learn Neo4j Database and Cypher query language.The project was in a passive development for the last three years. I know the pain of working on something for a long time, and it’s always in the not finished yet” state. In this article, I want to summarize the most important aspects of the development process and share my story with other engineers. I want to motivate people like me to build and release more indie projects.Don’t be afraid to invest your time into something new. Creating a unique thing — is a privilege.The IdeaIn May 2019 I started learning Neo4j through the One Month Graph Challenge.” For one month, I learned something new every single day and wrote a short article about it. After my initial stage of learning, I ended up with 30 articles.The most insightful stories about OMGchallenge - Mediummedium.comPioneriya is an article exploring Betweenness Centrality Algorithm.Sherlock is an article exploring Common Neighbors Algorithm.These two short stories became an inspiration for Graphville — a small virtual town with its own citizens and problems that can be solved using graphs.From idea to implementationYou probably know that Neo4j, Cypher, and APOC were inspired by the Matrix” movie. You also might remember that once Thomas Anderson became Neo he could learn anything just in a few seconds: Kung Fu, how to drive a helicopter, or the Dijkstra Shortest Path algorithm. What you probably don’t know yet is that in Graphville you can learn graph theory, Neo4j basics, and level up your Cypher skills within quite a short period of time! (Not in a few seconds, of course, but you will enjoy the process.)Know Your AudienceLater on, I wrote quite a popular article, Learn Cypher in 30 minutes,” based on the materials from my Neo4j workshop in Bangkok. At the end of the tutorial, I asked readers to solve several practical Cypher tasks. I was pleased with the rather large number of participants, many of whom solved all the tasks correctly!Learn Neo4j Cypher basics in 30 minutesPractical tutorial for Neo4j graph database beginners, based on Neo4j Workshop in Bangkok.vladbatushkov.medium.comI received 81 responses (the number grew still), and these provide useful insights into the learning curve. Sure, it’s a very small dataset, but it still gave me enough of a signal to act.The majority of engineers from various software development fields want to learn graph technologies to expand their knowledge and skills. In most, they want to learn basic concepts, how to do data modeling, how to write queries, and be aware of the most popular graph algorithms. This is how I can formulate the mission of Graphville:The mission of Graphville is to help engineers to enter the field of graph technologies.Key PrinciplesOnce you establish a mission, it’s time to build up a philosophy for your project. The philosophy of Graphville’s educational platform is based on three key principles: teach the right things, teach in a fun way, and set challenging problems.EducateGraphville theory meets the guidelines from the official documentation. The explanations are simple and easy to understand. Query examples follow naming conventions and best practices. Each lesson is focused on a single topic and includes Tests and Tasks for a better understanding of learning materials.Graphville, Task ViewEngaging UXReading documentation can be boring. How to make it fun for engineers? Create a joyful learning process by mixing theory and practice. User experience should be the same compared to official tools like Bloom or Neo4j Browser.With Graphville, you don’t need to install anything, no need to prepare a dataset, or do a setup for your local machine. Open Graphville, select a topic, and jump into the theory and practice.BloomNeo4j BrowserGraphville GraphUISeasons and EpisodesGraphville is something like a Netflix series with several seasons. Every lesson is an episode in a small story with a plot and heroes. Stories have unique illustrations that help you to turn on your imagination and have some extra fun with the learning process.Mr. Hank Teasle, Sheriff of GraphvilleChallengeHow can you gain stronger knowledge? Tasks and Tests. Combining theory and practice, Graphville lessons ask students to explore graphs, answer questions, and write a bunch of Cypher queries. Tasks are not complex, but also not trivial. I strongly believe that challenge is a required part of any learning process.Without a challenge, you will not fully enjoy a result.TeamNow I want to talk about the importance of a team. The big project is a marathon, not a sprint. Sooner or later you will need help, you will need extra energy, ideas, and motivation to move on. These people impact the project and change it in a better way. Without these people and their contributions, Graphville would be an absolutely different story.Graphville TeamAlenaShe is my sister and a hardworking frontend engineer, and she is an expert in HTML and CSS. She learned D3.js from scratch and built her own GraphUI: clickable, zoomable, and draggable graph. Decent result!Graphville, Page ViewIrinaGraphville illustrator and author of all visual materials. She has fantastic taste and always understands how to bring the best ideas alive. With her help, Graphville has its own visual style. And as proof, just take a look at this beauty!Fantastic Beasts and Where to Find Their OwnersRobThis Neo4j expert literally lives on the opposite side of the world (we have a 12-hour time difference). Rob joined Graphville as a lesson author and helps to create tests, tasks, and content for lessons. He is a creative person who changed Graphville’s structure from a plain Storybook into a conceptual series of Seasons and Episodes.MATCH (e:Episode)-[:CREATED_BY]->(a:Author)WHERE a.name =  Rob RETURN e.titleMashaThe creator of Graphville teaser for YouTube, Masha helped at a very early stage, when there was nothing at all. The video is no longer in use, but the Graphville teaser was the first done” thing.Season 1, Episode 2: HobbiesWelcome to GraphvilleGraphville officially launched on September 1, 2022. The first season is released. Some episodes are already published, and some episodes are still under construction.Season 1Episode 1: Welcome to GraphvilleEpisode 2: HobbiesEpisode 3: Summertime CallsEpisode 4: Furry Road 🚧Episode 5: Graph Detective 🚧Episode 6: Leaving Zombieville 🚧Here is several important checkpoints from the three-year roadmap of the project:Graphville Roadmap 2019–2022Community and SponsorsGraphville growth is only possible with the growth of the Graphville Community. Enroll in the episode, share your experience, and provide feedback. We are happy to hear your suggestions on how to improve the quality of learning materials and lessons.Graphville is created for engineers who loves modern technologies and creative education. Do you like Graphville and want to try more episodes? You’re welcome to join Graphville ❤️ Sponsorship at GitHub Sponsors. Sponsors of Graphville have early access to new episodes and participate in a content creation process — new lessons and stories.Welcome to GraphvilleOutroThank you for reading. I hope you found this piece useful and inspiring. Also, I hope that Graphville will find sponsorship for many seasons and spark the hearts of many software engineers wanting to learn graph technologies.Cheers.;Dec 15, 2022;[]
https://medium.com/neo4j/your-in-depth-guide-to-neo4j-graphql-library-2-0-0-126e0929c57e;Daniel StarnsFollowAug 16, 2021·5 min readYour In-Depth Guide to Neo4j GraphQL Library 2.0.0In this blog I will take you on a technical dive into the new and improved Neo4j GraphQL Library. Since releasing 1.0.0, in April 2021, we have been working hard taking on board all your ideas, suggestions and applying them to the new and improved 2.0.0 release.Before we dive into whats changed and how you can migrate to 2.0.0, I would like to take this opportunity to say thanks to all the community members that showed patience and commitment in both reporting issues and also contributing towards making 2.0.0 a reality.Whats Included? — TLDRCheck out Darrell’s Blog post here for a quick overview of whats included and what has changed in 2.0.0.Relationship propertiesCursor-based paginationCount queriesImprovements to the developer experience with union relationship fieldsMore comprehensive validation of type definitionsInternal refactoring and bug fixesMotivationNow that the Neo4j GraphQL integration has a full-time dedicated team behind it, we are able to shape and implement many more features than that of the older Neo4j Labs neo4j-gaphql-js implementation, which has been deprecated. Our aim with this release was to support relationship properties — something that was missing in the 1.0.0 release. We deliberately left out relationship properties so we could get something out the door” and with a small team it was a lot to implement. Since our 1.0.0 release, we have received extensive requests to support properties on relationships and so this was high on our agenda.What Are Relationship Properties?As stated in the previous paragraph, our aim with this release was to support relationship properties. This section will dive deeper into the motivation and use cases around relationship properties and you should walk away knowing how to implement them in your graph.Lets dive deeper here. Here we look at the Movie and Actors model:You could represent this with the following Neo4j GraphQL schema:Then, you could create a Movie connected to a Person like so:Workaround Using an Extra NodeNow, this is all good and what not however, what if we wanted to store information about how much screen time a Person had on a Movie? In a data store where relationship properties are not present you would have to construct two relationships with three nodes, the third node would be a ‘join’/’meta’ node with the screen time as a property as such:screenTime is represented as a property on the Movie Meta node, of type Int.You could represent this model with the following Neo4j GraphQL schema:Then, you could create a Person, Movie Meta and a Movie like so:Using Relationship PropertiesRelationship properties address the problem we face here an extra node and relationship. Instead of creating unnecessary data in our Graph, we can utilize the existing relationship and store properties on it. Here is what our model would look like:You could represent this with the following Neo4j GraphQL schema, using an interface, ActedIn, to represent the relationship property fields:Then you could create a Movie, and a Person, with relationship properties, like so:I hope this section showcases the power of using relationship properties, and now you have the power and know-how to put them in your data model.Where Does Relay Come into It?https://relay.dev/In order to get relationship properties into our existing auto-generated schema, we knew one of two things we needed to either break the existing projections or create new fields for each relationship on a node. In 2.0.0, we opted for the latter approach. What this means is that for each relationship on your node you have both the field name and also fieldNameConnection. The connection field is what’s relevant to the relay spec. Each connection is a Relay compliant field where you use cool stuff such as pagination and have access to the relationship properties. This does mean however, you need to traverse the connection in order to access the relationship properties due to the properties being a part of the Relay edge.Lets take a closer look at the new fields. Say you have this schema:You would normally be able to project the following:Now, with 2.0.0, you can also project the actorsConnectionfield:The query above is asking for all the Relay compliant fields. You also have the ability to filter, sort and paginate on the connection fields using cursor-based pagnation like so:The cursor is great for implementing something like infinite scroll.As we mentioned earlier, in order to access the relationship properties you need to traverse the connection field. Here we have our schema, thats been slightly adapted to include relationship properties:Then here we perform a query to get not only the Movies and Actors but also the properties on the relationship between each Movie and Actor:screenTime is a property on the edgeWhat Else Can I Do with 2.0.0?Other than being able to define relationship properties and use Relay-style pagination we added one last feature: count queries.Count QueriesWe realized that when using 1.0.0 that performing paged-based pagination was difficult and you had to define your own custom resolver using the Cypher directive to be able to compute the total number of pages. Now, this release comes with auto-generated count queries. Using the movies schema, used throughout this post, you can perform the follow count query:Is That a Moth?World’s First Computer BugOther than the ability to count, I’d like to point out that we did an extensive backlog grooming session fixing many bugs reported by you the community. We feel that this 2.0.0 release unblocks many users both for migrating to the lib but also people stuck on 1.0.0 with existing bugs.Thanks for reading, I hope this blog equips you with enough information to get started using all our new cool features. Go and check out our Github today and or read our new revamped documentation here. Cheers 🍻;Aug 16, 2021;[]
https://medium.com/neo4j/graphhackers-lets-unite-to-help-save-the-world-graphs4good-2020-fed53562b41f;Karin WolokFollowMar 20, 2020·4 min readGraphHackers, Let’s Unite to Help Save the World — Graphs4Good 2020During these times of uncertainty and solidarity, a lot of courageous heroes in our society have stepped up. From healthcare professionals caring for sick patients to the hard-working individuals working overtime delivering groceries, people are taking on a role in supporting our society when we need it the most.These selfless people inspire us and we want them to know that we’re in this together. It’s time for us as a community to collaborate and do something positive.So — we invite you to join us and the global development community in an effort to unite our skills and bring some good to our world. ❤❤Let’s hack for good, together.Image source: CNN MoneyAbout Graphs4Good (GraphHack) 2020WHAT: any project that has a positive goal and can help others, qualifies. ❤WHERE: virtually, of course!Here’s the overview from the Graphs4Good Project Kick-Off Session from Friday, March 27th.WHEN: the world needs us since… yesterday — so, you can start now! We’ll host a showcase of the projects as part of Global Graph Celebration Day 2020 on April 15th (birthday of Leonhard Euler, inventor of graph theory). More info below.WHY: because the world needs us!HOW: hack solo or in collaboration with others in the community (friends or ‘strangers’). You can either: 1) start a new project 2) find a project to contribute to.More info on the two options in next two sections below.Have an Idea for a Project?To get your project started:Create a GitHub project/repoAdd a readme description of your projectAdd ‘graphs4good’ topic to your projectRegister yourself and your project for Graphs4Good GraphHackIf you’re open to collaborators, you should also:Enable issues, discussions, wikiAdd existing dataset/code (if you already have something)Optional: create tickets (issues) with help-wanted” labelWant to Find a Project to Contribute To?Register yourself as a Graphs4Good hackerBrowse ‘graphs4good’ topic on GitHub to find a repo/project you’re interested in.Contact project owner via GitHub discussions for selected project (or ping owner via GitHub issue)Graphs4Good Project ShowcaseOn April 15th, (the birthday of the inventor of Graph Theory, Leonhard Euler), we (Neo4j staff) will create a couple of options for contributors to showcase their projects and work.These are just opportunities to spread the word to the world on what you’ve built, you’re welcome to take any, all, or none of the suggestions. :DMore details will come to you via email (just make sure you register yourself as a hacker!).Register for the GraphHack for UpdatesSome IdeasWorking with COVID-19 data in GraphXR and Neo4j - KinevizExploratory analysis on COVID-19 data to understand how we got to where we are. What caused SARS-CoV-2 spread to so…www.kineviz.comKnowledge Graphs for Social Good Workshop: Helping the United Nations achieve Sustainable…The UN Sustainable Development Goals (SDGs), set in 2015, are a collection of 17 shared global goals intended to…towardsdatascience.comcotect/cotectCrowd-sourced COVID-19 reporting and assessment system. Problem Statement * Solution * Architecture * Challenges *…github.comHetionet - An integrative network of biomedical knowledgeHetionet is an integrative network of biomedical knowledge assembled from 29 different databases of genes, compounds…het.io;Mar 20, 2020;[]
https://medium.com/neo4j/week-35-discover-auradb-free-nodes-2022-sessions-1d8f0d96d5a3;Michael HungerFollowSep 27, 2022·8 min readWeek 35 — Discover AuraDB Free — NODES 2022 SessionsWe’re really excited and looking forward to our annual online developer conference NODES 2022 on November 16 and 17.The event will run around the globe with talks in all time zones.You can save your spot now by registering early for some of the cool goodies that are coming your way.We ran our Call for Papers in August and got 150 really great submissions by 130 speakers.We selected 90 for the event, which was really not easy. Some of the sessions that didn’t make it for the conference will be run in later live streams.Today, let’s look at the NODES data model and import the data for sessions and speakers into Neo4j from CSV.If you’d rather watch the video of the live stream, here you go:Next time we’ll import the data from a REST API, and then look at the schedule and session recommendations.Let’s start by creating and connecting to our AuraDB Free instance.If you want to try out the Workspace Beta too and provide feedback, please go to https://neo4j.com/product/workspace and sign up.Create a Neo4j AuraDB Free InstanceGo to https://dev.neo4j.com/neo4j-aura to register or log into the service (you might need to verify your email address).After clicking Create Database you can create a new Neo4j AuraDB Free instance. Select a Region close to you and give it a name, e.g. NODES Sessions.Choose the blank database” option, as we want to import our data ourselves.On the credentials pop-up page, make sure you save the password somewhere safe. It’s best to download the credentials file, which you can also use for your app development.The default username is always neo4j.Then wait two to three minutes for your instance to be created.Afterwards, you can connect via the Query Button with Neo4j Browser (you’ll need the password), or click Import for the Data Importer and Explore for Neo4j Bloom.On the database tile you can also find the connection URL: neo4j+s://xxx.databases.neo4j.io (also contained in your credentials env file).If you want to see examples for programmatically connecting to the database, go to the Connect” tab of your instance and pick the language of your choice.Source Data from SessionizeSessionize offers both an JSON REST API to fetch the data, as well as export as CSV/XLS.Sessionize Export DialogWe saved the CSV for sessions and speakers locally and can use it with Data Importer in Workspace.The fields in our Session CSV are:Session IdTitleDescriptionSpeakersSession formatLevelPrerequisites for attendees?Topic of your presentationNeo4j use caseYour time zoneStatusDate submittedSpeaker IdsThe fields for our Speakers:Speaker IdFirstNameLastNameTagLineBiotimezoneCityCountryNinjaLinkedInBlogTwitterCompany WebsiteProfile PictureData Modeling and ImportTLDRIf you want to shortcut the modeling you can grab the nodes-sessions-data-importer-2022-09-26.zip file from the GitHub repository and load it via the  Open model (with data)  menu entry in the three dots …​.If we open Workspace on our blank database (or Data Importer directly) we can add our CSV files on the left side.Then we start mapping our data by:adding the session nodesetting its label to sessionselecting the sessions CSV in the mapping viewand adding all relevant fields to the mapping from the filethe Session Id is automatically selected as id fieldThen we do the same for the Speaker node, just with the speaker’s CSV.To connect both, we drag out a relationship from speaker to session from the speaker node’s halo.Give it the name PRESENTS and use the fields Session Id and Speaker Ids for the mapping.Data Importer ModelUnfortunately, the comma separated Speaker Ids are not handled by data importer yet, so this will only connect sessions which have a single speaker.But fear not — we will connect the rest with a bit of post-processing.In Preview we see how our data will look in the graph, both the properties and relationships.Neo4j Data Importer PreviewIf we’re satisfied, we can click Run Import” and it will take roughly a second to import the data.If you click on the Show query links in the import report you’ll see the constraints and import query that data importer is running.There you also see that for the relationships it tries to match the existing nodes for session and speaker based on Sessiond Id and Speaker Ids to connect them, which only works if there’s a single speaker for a session.After running the importer we can open the Query” tab by htting Run Queries.”Or just click on it on top.QueryingA single pre-populated query shows us our graph, which should look very similar to the preview.MATCH p=()-[:PRESENTS]->() RETURN p LIMIT 25We can also look for speakers that have more than one session, like Anton, but not yet for sessions with more than one speaker.Let’s first find sessions that have no speakers yet.match (s:Session) where not exists { (s)<-[:PRESENTS]-() }return s.`Session Id` as session, s.`Speaker Ids` as speakersThese are the ones we want to fix in our post processing.Post ProcessingThe approach we take for all these operations is the same:Find the sessions to update, split a field by comma+space `, ` into a list of values. Then turn (UNWIND) this list of values into rows of values, MATCH or MERGE (get-or-create) nodes for the values and connect the session to them via a relationship.List of speaker idsMATCH (s:Session) WHERE NOT EXISTS { (s)<-[:PRESENTS]-() }RETURN s.`Session Id` as session, split(s.`Speaker Ids`,, ) as speakersList to rowsMATCH (s:Session) WHERE NOT EXISTS { (s)<-[:PRESENTS]-() }WITH s, split(s.`Speaker Ids`,, ) as speakersUNWIND speakers as speakerIdRETURN s.`Session Id` as session, speakerIdMATCH speakers and connect themMATCH (s:Session) WHERE NOT EXISTS { (s)<-[:PRESENTS]-() }WITH  s, split(s.`Speaker Ids`,, ) as speakersUNWIND speakers as speakerIdMATCH (sp:Speaker {`Speaker Id`:speakerId})MERGE (sp)-[r:PRESENTS]->(s)RETURN *Delete orphan speakersAs sessionize exported all speakers not just the ones with the accepted sessions, we now how to remove our orphans.MATCH (sp:Speaker) WHERE NOT EXISTS { (sp)-[:PRESENTS]->() }DELETE spCategorize other fieldsWe can now do the same for the other comma separated fields.LevelTopicNeo4j Use caseHere we generally call the list names, and value name and the node n to keep the editing needed to a minimum.LevelMATCH (s:Session)WITH  s, split(s.Level,, ) as namesUNWIND names as nameMERGE (n:Level {name:name})MERGE (s)-[r:OF_LEVEL]->(n)RETURN *Use CasesMATCH (s:Session)WITH  s, split(s.`Neo4j Use-Case`,, ) as namesUNWIND names as nameMERGE (n:UseCase {name:name})MERGE (s)-[r:USECASE]->(n)RETURN *Topicsmatch (s:Session)WITH  s, split(s.`Topic of your presentation`,, ) as namesUNWIND names as nameMERGE (n:Topic {name:name})MERGE (s)-[r:HAS_TOPIC]->(n)RETURN *Now we have a beautiful graph with different nodes for:SessionSpeakerLevelTopicUse Caseand their relationships.Exploring the ResultsWe can open Explore” to visualize our data a bit.In case the Perspective” is not really showing something, click on the Untitled Perspective,” choose delete from the three dots, and then create/generate a new one.First, we set the correct captions for Session (Title) and Speaker (FirstName and LastName) as well as some icons.Then we can pick Show me A Graph from the drop-down to quickly show a graph and explore it a bit.Next we can use our extracted topics and see which sessions share topics by entering: Session<tab>Topic<tab>Session<tab> into the search bar and hitting return.DashboardWe’re using NeoDash, a Neo4j Labs project for creating quick dashboards.You can open it via https://tools.neo4jlabs.com and add your connection URL (from the aura console or your credentials download) to the form and click on the Open button for NeoDash.Neo4j Labs Tools PageYou still need to provide the password — for security reasons it shouldn’t be passed through the URL.In the video we go through the charts and queries for the dashboard. We’ll list them quickly here.Dashboard Charts and QueriesFor the speaker locations we first have to compute the geolocation from their city and country.Add geolocation for Speaker CitiesLet’s check it for a single speaker. We need to see if the city is actually empty or has some characters.Then we can call apoc.spatial.geocodeOnce to geocode the city and country and look at the results.match (sp:Speaker)where size(sp.City) > 1with sp limit 1call apoc.spatial.geocodeOnce(sp.City+   +sp.Country) yield location, data, latitude, longitudereturn *We can use the latitude and longitude from the result directly to create spatial point location properties in our speakers.match (sp:Speaker)where size(sp.City) > 1call apoc.spatial.geocodeOnce(sp.City+   +sp.Country) yield location, data, latitude, longitudeset sp.location = point({latitude:latitude, longitude:longitude})Which we then can put on a Map-Chart in the Dashboard by just selecting the speakers that have a location property.ConclusionWe hope this was a fun and useful sessions and you got excited for NODES 2022.Stay tuned for the next time when we look at importing REST APIs, Schedule Modeling, and computing recommendations.;Sep 27, 2022;[]
https://medium.com/neo4j/whats-cooking-approaches-for-importing-bbc-goodfood-information-into-neo4j-64a481906172;Ljubica LazarevicFollowJan 23, 2019·7 min readWhat’s cooking? Part 1: Importing BBC goodfood information into Neo4jIntroductionMy colleague Mark Needham and I were very keen to get our hands on a new data set: BBC goodfood recipes. Each recipe has a wealth of interesting data from ingredients, diet types, nutrition, and so forth. This gives us a lot of opportunities to try out great graph database use-cases in a fun setting. Before we get started on these use-cases, we need to get that data. In this post we will show you a couple of ways to get that data into Neo4j.Overview of the situationGraph databases are a fantastic fit in many business use-cases. We’d suspect that many of you will have come across them, including:Recommendation enginesMaster Data ManagementEntity resolutionFraud detectionAnd many more!Graph databases are a great fit because relationships between data entities are valued the same as the data entities. Not only do you look at the data entity itself, but also it’s context in relation to other entities. There is a wealth of material on this online. For those of you who are interested in learning more, look here.So we have a great technology and we have some neat examples we want to apply. All we need is that tasty data set to work with. There are a great set of examples ready for you to get your hands on via the Neo4j GraphGists. However, for this post we’ve decided to do something a little different…BBC goodfood has a fantastic selection of recipes. It has long been an extremely popular resource for for budding chefs, and for those looking for something different at meal times. There is an extensive selection covering different courses, cuisines and dietary requirements.BBC goodfood seriously rich chocolate cakeExamining one of the recipes (who doesn’t fancy that seriously rich chocolate cake?), we can see we’ve got a lot of useful information such as ingredients, nutrition, user ratings, and so forth.Investigating the source code for the page, we discover we have a very tidy object that gives us helpful summary information:{page:{article:{author: Good Food ,description: Dark, rich and delicious - the perfect dessert ,id: 97123 ,tags:[]},recipe:{collections:[ Chocolate cake , Boozy bake ],cooking_time:2100,prep_time:1800,serves:10,keywords:[ Cocoa powder , Dark chocolate , Dessert , Decadent , Pudding , Afternoon tea , Booze , Alcohol , Ground almond , Ground almonds , Kirsch ],ratings:81,nutrition_info:[ Added sugar 22g , Carbohydrate 24g , Kcal 401 calories , Protein 10g , Salt 0.66g , Saturated fat 11g , Fat 30g ],ingredients:[ butter , flour , dark chocolate , egg , ground almond , kirsch , salt , caster sugar , cocoa powder ],courses:[ Dessert , Treat , Buffet ],cusine: British ,diet_types:[ Low-salt ],skill_level: More effort ,post_dates: 1009843200 },channel: Recipe ,title: Seriously rich chocolate cake }}Which if we unfurl, would look something like this:{page:{     article:{        author: Good Food ,      description: Dark, rich and delicious - the perfect dessert ,      id: 97123 ,      tags:[]   },   recipe:{        collections:[            Chocolate cake ,          Boozy bake       ],      cooking_time:2100,      prep_time:1800,      serves:10,      keywords:[            Cocoa powder ,          Dark chocolate ,          Dessert ,          Decadent ,          Pudding ,          Afternoon tea ,          Booze ,          Alcohol ,          Ground almond ,          Ground almonds ,          Kirsch       ],      ratings:81,      nutrition_info:[            Added sugar 22g ,          Carbohydrate 24g ,          Kcal 401 calories ,          Protein 10g ,          Salt 0.66g ,          Saturated fat 11g ,          Fat 30g       ],      ingredients:[            butter ,          flour ,          dark chocolate ,          egg ,          ground almond ,          kirsch ,          salt ,          caster sugar ,          cocoa powder       ],      courses:[            Dessert ,          Treat ,          Buffet       ],      cusine: British ,      diet_types:[            Low-salt       ],      skill_level: More effort ,      post_dates: 1009843200    },   channel: Recipe ,   title: Seriously rich chocolate cake }}This would be a worth data set to import. We can ask some very interesting questions, such as:What are the most commonly used ingredients?Recommend me recipes based on author/ingredient/dietI have these ingredients, what can I cook?Now we’ve downloaded some recipes we want to investigate (the onus is on the reader to do this step, including parsing). How do we get this data into Neo4j?There are a number of mechanisms that allow us to get data in to Neo4j. We’re going to explore two approaches below.First approach — LOAD CSVTo start off with, we may just want to explore the ingredients for the recipes. That is, we extract the recipes unique ID, title, and the ingredients list. A quick and easy way to get the data into Neo4j would be through using LOAD CSV (Comma Separated Value). Our graph data model will look something like this:To get the recipe data into an digestible format, we’d suggest the following format of id, title, ingredient. A snippet would look as follows: 97123 , Seriously rich chocolate cake , butter  97123 , Seriously rich chocolate cake , flour  97123 , Seriously rich chocolate cake , dark chocolate  97123 , Seriously rich chocolate cake , egg  97123 , Seriously rich chocolate cake , ground almond  97123 , Seriously rich chocolate cake , kirsch  97123 , Seriously rich chocolate cake , salt  caster sugar  97123 , Seriously rich chocolate cake , cocoa powder Once we’ve got all of the recipe data into this flat file format, we can now load it into Neo4j using Cypher queries. To run this query in Neo4j Browser, you will need to paste each line terminating with a semicolon separately. Alternatively, you can enable the multi-line statement mode (in Browser go to Settings, and tick ‘ Enable multi statement query editor’). Save your file into the import folder, and run the following:CREATE INDEX ON:Ingredient(value)CREATE INDEX ON:Recipe(id)LOAD CSV FROM  file:///bbcgoodfood.csv  AS lineMERGE (r:Recipe {id:line[0]})ON CREATE SET r.title= line[1]MERGE (i:Ingredient {value:line[2]})CREATE (r)-[:CONTAINS_INGREDIENT]->(i)If you’ve got a particularly large file, you may wish to consider periodic commit.With some data now in, we can ask some simple questions, such as, what are the most common ingredients?MATCH (i:Ingredient)<-[rel:CONTAINS_INGREDIENT]-(r:Recipe)RETURN i.name, count(rel) as recipes order by recipes descOr, how about suggesting some other recipes that are similar to that chocolate cake? Here’s a simple query that’ll make some recommendations recipes using mostly the same ingredients:MATCH (r:Recipe {id:97123})-[:CONTAINS_INGREDIENT]->(i:Ingredient)<-[:CONTAINS_INGREDIENT]-(rec:Recipe)WITH rec, COUNT(*) as commonIngredientsRETURN rec.name, rec.id ORDER BY commonIngredients DESC LIMIT 10This is great start and there are lots of things to explore with this data. However it would be nice to get the rest of that page object. Also, rather than having to create a verbose CSV file, wouldn’t it be better if we could use that object as is?Second approach — load.jsonThis is where the APOC library comes in. APOC (A Package Of Components) is a set of user-defined functions packaged together and can be called by Cypher query. To install APOC on your Neo4j database instance, check here.We’re going to use the load.json procedure to ingest all of those page objects. Once you have extracted all of the objects into a file, we can now run the following query (also a multi-line statement). If you wish, you may use our import file. You will notice we load the file several times — this is to reduce the eagerness of the data import:CREATE INDEX ON :Recipe(id)CREATE INDEX ON :Ingredient(name)CREATE INDEX ON :Keyword(name)CREATE INDEX ON :DietType(name)CREATE INDEX ON :Author(name)CREATE INDEX ON :Collection(name):params jsonFile =>  https://raw.githubusercontent.com/mneedham/bbcgoodfood/master/stream_all.json CALL apoc.load.json($jsonFile) YIELD valueWITH value.page.article.id AS id,       value.page.title AS title,       value.page.article.description AS description,       value.page.recipe.cooking_time AS cookingTime,       value.page.recipe.prep_time AS preparationTime,       value.page.recipe.skill_level AS skillLevelMERGE (r:Recipe {id: id})SET r.cookingTime = cookingTime,    r.preparationTime = preparationTime,    r.name = title,    r.description = description,    r.skillLevel = skillLevelCALL apoc.load.json($jsonFile) YIELD valueWITH value.page.article.id AS id,       value.page.article.author AS authorMERGE (a:Author {name: author})WITH a,idMATCH (r:Recipe {id:id})MERGE (a)-[:WROTE]->(r)CALL apoc.load.json($jsonFile) YIELD valueWITH value.page.article.id AS id,       value.page.recipe.ingredients AS ingredientsMATCH (r:Recipe {id:id})FOREACH (ingredient IN ingredients |  MERGE (i:Ingredient {name: ingredient})  MERGE (r)-[:CONTAINS_INGREDIENT]->(i))CALL apoc.load.json($jsonFile) YIELD valueWITH value.page.article.id AS id,       value.page.recipe.keywords AS keywordsMATCH (r:Recipe {id:id})FOREACH (keyword IN keywords |  MERGE (k:Keyword {name: keyword})  MERGE (r)-[:KEYWORD]->(k))CALL apoc.load.json($jsonFile) YIELD valueWITH value.page.article.id AS id,       value.page.recipe.diet_types AS dietTypesMATCH (r:Recipe {id:id})FOREACH (dietType IN dietTypes |  MERGE (d:DietType {name: dietType})  MERGE (r)-[:DIET_TYPE]->(d))CALL apoc.load.json($jsonFile) YIELD valueWITH value.page.article.id AS id,       value.page.recipe.collections AS collectionsMATCH (r:Recipe {id:id})FOREACH (collection IN collections |  MERGE (c:Collection {name: collection})  MERGE (r)-[:COLLECTION]->(c))You may wish to download it and parse it locally. Note that you will need to change the path to be an absolute path to the file on your machine e.g.file:///<absolutepath>/stream_all.jsonThis is an absolute path, not relative one as is the case with LOAD CSV.:params jsonFile =>  file:///<absolutepath>/stream_all.json You’ll also need to add the following line to your Neo4j configuration file:apoc.import.file.enabled=trueAnd there we have it, a rich and interesting data set to explore within a graph database. Going back to the chocolate cake, what else has the author published?MATCH (rec:Recipe)<-[:WROTE]-(a:Author)-[:WROTE]->(r:Recipe {id:97123})RETURN rec.name, rec.idSummaryWe’ve shown the principles of loading recipe data into Neo4j, using either CSVs, or load.json. There’s even a simple recommendation query to get you started with the recipes data.This rich data set gives us even more things to explore. For example, approaches on entity resolution: does cherry tomato belong with tomatoes, or with cherries? Or, can we start creating common recipe communities, such as those for cakes? Some food for thought indeed!;Jan 23, 2019;[]
https://medium.com/neo4j/discover-auradb-free-week-15-the-graph-of-hamilton-cc8d69977102;Michael HungerFollowJan 12, 2022·10 min readDiscover AuraDB Free: Week 15 — The Graph of HamiltonAs part of our ongoing exploration of Neo4j AuraDB Free, we’re looking at a fun dataset every week. This time it is the lyrics of Hamilton, the Musical.I love Hamilton the Musical by Lin-Manuel Miranda, a semi-accurate retelling of the story of the founding father Alexander Hamilton with powerful lyrics and songs.If you haven’t seen it, here is a sneak in the Disney+Trailer (but there are also other uploads on youtube).I got lucky and was able to see it from the last free seat at the SFO performance in 2019.Like Shirley Wu I’ve been listening to the soundtrack on repeat since forever, it’s just a joy.For a long time I’ve been wanting to turn Shirley Wu’s amazing An Interactive Visualization of Every Line in Hamilton” into a Neo4j graph for myself and others to play around with.Please check out her work and play with it, it’s really inspiring.Live StreamIf you missed our live-stream, and rather watch than read the article, enjoy us having fun with this data.Data SourceThankfully she made the data available as part of her GitHub Repository for the visualization.The JSON files can be found here, together with some explanations.https://github.com/sxywu/hamilton/tree/master/src/dataEach JSON file has keys which are usually id’s for the entities pointing to a list of values, some of which can also be nested.Here is the final data model I ended up with for the data:Hamilton Musical Data ModelWe haveSinger that can be Individual or GroupSong, withLines, that a Singer SINGS, are DIRECTED at a Singer or EXCLUDE a SingerCONTAINS WordHAS a Theme, whichis PART_OF a CategoryIndexes and ConstraintsFor faster lookups to connect our data, let’s create some constraints and indexes first.create constraint on (s:Singer) assert s.id is uniquecreate constraint on (l:Lines) assert l.id is uniquecreate constraint on (s:Song) assert s.id is uniquecreate constraint on (t:Theme) assert t.id is uniquecreate constraint on (c:Category) assert c.name is uniquecreate constraint on (w:Word) assert w.word is uniquecreate index on :Singer(name)create index on :Song(title)SingersLet’s load the data one by one, starting with the Singers.char_list.json{ 1 : [ Aaron Burr ,  M ,  individual , true,  rgb(149,65,200) ]}We load the data, and process each key and it’s value as a row to create a Singer with an additional label from the data array.call apoc.load.json(https://raw.githubusercontent.com/sxywu/hamilton/master/src/data/char_list.json) YIELD valueUNWIND keys(value) AS keyWITH key, value[key] AS datacall apoc.create.node([ Singer ,apoc.text.capitalize(data[2])], {id:key, name:data[0], gender:data[1],xx:data[3],color:data[4]}) yield nodereturn nodeSongsSongs are straightforward with just an id and title.song_list.json{ 1 : [ Alexander Hamilton ,  #9541c8 ]}We load the data again and turn them into nodes.call apoc.load.json(https://raw.githubusercontent.com/sxywu/hamilton/master/src/data/song_list.json) YIELD valueUNWIND keys(value) AS keyWITH key, value[key] AS dataMERGE (s:Song {id:key})ON CREATE SET s.title = data[0], s.color = data[1]RETURN sLinesLines is a bit more complicated, here is the description from Shirley’s repositorykey: line key(example) 1:31 would be song 1, line 31(example) 1:1-5 would be song 1, lines 1 through 5value: array of 4 values0: line key1: array0: character(s) singing1: character(s) excluded from singing2: character(s) the lines were directed to2: array of the actual lyrics in those lines3: number of lineslines.json{ 1:1-5 :[ 1:1-5 ,[[ 1 ],[  ],[  ]],[ How does a bastard, orphan, son of a whore and a , Scotsman, dropped in the middle of a , Forgotten spot in the Caribbean by providence , Impoverished, in squalor , Grow up to be a hero and a scholar? ],5]}Here after loading the data, wesplit the id into song-id and linesfind the songcreate a Lines node for each keyconnect sines to the songthen for each type of relationship to a singer (SING, EXCLUDED, DIRECTED)we loop over the id-array and connect the lines to that singer appropriatelyfor the SING relationship we also connect the singer to the actual songcall apoc.load.json(https://raw.githubusercontent.com/sxywu/hamilton/master/src/data/lines.json) YIELD valueUNWIND keys(value) AS keyWITH key, value[key] AS dataWITH key, split(key,:)[0] as song, split(key,:)[1] as lines, data[1][0] as singers, data[1][1] as excluded, data[1][2] as directed, data[2] as text, data[3] as countMATCH (s:Song {id:song})MERGE (l:Lines {id:key})ON CREATE SET l.text = text, l.count = count, l.lines = linesMERGE (l)-[:OF_SONG]->(s)FOREACH (id IN [id IN singers WHERE id <>   ] |    MERGE (p:Singer {id:id})    MERGE (p)-[:SINGS]->(l)    MERGE (p)-[:PERFORMS]->(s))FOREACH (id IN [id IN excluded WHERE id <>   ] |    MERGE (p:Singer {id:id})    MERGE (p)-[:EXCLUDED]->(l))FOREACH (id IN [id IN directed WHERE id <>   ] |    MERGE (p:Singer {id:id})    MERGE (l)-[:DIRECTED]->(p))RETURN count(*)In a memory restricted environment like AuraDB Free, we need to process the data in chunks:All the JSON and String data takes too much of the available, transactional memory.Just increase the SKIP value from 0 all the way to 1400 in steps of 200.:auto call apoc.load.json(https://raw.githubusercontent.com/sxywu/hamilton/master/src/data/lines.json) YIELD valueUNWIND keys(value) AS keyWITH key, value[key] AS dataSKIP 0 LIMIT 200call {with key, dataWITH key, split(key,:)[0] as song, split(key,:)[1] as lines, data[1][0] as singers, data[1][1] as excluded, data[1][2] as directed, data[2] as text, data[3] as countMATCH (s:Song {id:song})MERGE (l:Lines {id:key})ON CREATE SET l.text = text, l.count = count, l.lines = linesMERGE (l)-[:OF_SONG]->(s)FOREACH (id IN [id IN singers WHERE id <>   ] |    MERGE (p:Singer {id:id})    MERGE (p)-[:SINGS]->(l)    MERGE (p)-[:PERFORMS]->(s))FOREACH (id IN [id IN excluded WHERE id <>   ] |    MERGE (p:Singer {id:id})    MERGE (p)-[:EXCLUDED]->(l))FOREACH (id IN [id IN directed WHERE id <>   ] |    MERGE (p:Singer {id:id})    MERGE (l)-[:DIRECTED]->(p))} in transactions of 100 rowsRETURN count(*)Now we can have our first graph query, that shows us who sings which songs.MATCH (p:Singer)-[r:SINGS]->()-[:OF_SONG]->(s:Song)RETURN s.id, s.title, collect(distinct p.name)ORDER by toInteger(s.id) ascLIMIT 25There is also a file character.json which contains the same singing information as the lines, so we don’t need to process it.WordsSomething that’s not used in her visualization but I wanted to include in the graph are the words of the text.We could have extracted them ourselves but stemming etc. was already taken care of in words.json.Here we have the word as key and the line within the lines as array of values.words.json{ how :[ 1:1/1:1-5 , 3:49/3:47-51 ]}When loading the file, the most annoying part is splitting the string to find the lines-key and the data to compute the offset in the text-array. Again we batch the import to not overload the AuraDB Free memory.:auto call apoc.load.json(https://raw.githubusercontent.com/sxywu/hamilton/master/src/data/words.json) YIELD valueUNWIND keys(value) AS keyWITH key, value[key] AS dataCALL { WITH key, data    MERGE (w:Word {word:key})    WITH *    UNWIND data as entries    WITH w, split(entries,  / ) as parts    WITH w, parts[1] as lines, parts[0] as line    WITH *, toInteger(split(line,:)[1])-toInteger(split(split(lines,:)[1],-)[0]) as idx    MATCH (l:Lines {id:lines})    MERGE (w)<-[:CONTAINS {pos:line, idx:idx}]-(l)} IN TRANSACTIONS OF 500 ROWSRETURN count(*)Now we can query for Words within lines of songsMATCH (s:Song)<-[:OF_SONG]-(l:Lines)-[c:CONTAINS]->(n:Word) where n.word = satisfiedRETURN s.title, l.text[c.idx],n.word LIMIT 25Here are the results╒═══════════════════════╤══════════════════════════════════╕│ s.title               │ l.text[c.idx]                    │╞═══════════════════════╪══════════════════════════════════╡│ Non-Stop              │ Satisfied                        │├───────────────────────┼──────────────────────────────────┤│ Satisfied             │ You strike me as a woman who has ││                       │never been satisfied              │├───────────────────────┼──────────────────────────────────┤│ The Reynolds Pamphlet │ You could never be satisfied     │├───────────────────────┼──────────────────────────────────┤│ Satisfied             │ Be satisfied                     │├───────────────────────┼──────────────────────────────────┤│ Satisfied             │ You will never be satisfied      │├───────────────────────┼──────────────────────────────────┤│ Non-Stop              │ Satisfied                        │├───────────────────────┼──────────────────────────────────┤│ Non-Stop              │ To be satisfied                  │ThemesThe theme list has the theme id as key and some text examples and the category name as values Not sure what the true” value means, it’s true in all entiries.theme_list.json{ 1 : [[ Just you wait ,  wait for it ],  Ambition , true]}We create the theme by id and set the text and category, but then also create a category node that we want to connect to.call apoc.load.json(https://raw.githubusercontent.com/sxywu/hamilton/master/src/data/theme_list.json) YIELD valueUNWIND keys(value) AS keyWITH key, value[key] AS dataMERGE (t:Theme {id:key})ON CREATE SET t.category=data[1], t.text=data[0]MERGE (c:Category {name:data[1]})MERGE (t)-[:PART_OF]->(c)RETURN *The actual themes are more complicated. With the theme id as key, we have a triple nested list of lines and lines-ids and the text lines.themes.json{ 1 :[ [[ 1:27/24-27 ],[ But just you wait, just you wait... ]],[[ 1:54/54 ],[ Just you wait! ]]]}But we’re only interested in the lines keys and the offset, so we do our usual spiel of splitting hairs to compute the index. This time they lines-key is not directly there so we need to construct it from song and lines-range.call apoc.load.json(https://raw.githubusercontent.com/sxywu/hamilton/master/src/data/themes.json) YIELD valueUNWIND keys(value) AS keyWITH key, value[key] AS dataMATCH (t:Theme {id:key})UNWIND data as phrasesUNWIND phrases[0] as entriesWITH t, split(entries,  : ) as partsWITH t, parts[0] as song, split(parts[1], / ) as linesMATCH (l:Lines {id:song +  :  + lines[1]})WITH l, t, toInteger(lines[0]) - toInteger(split(lines[1], - )[0]) as idxMERGE (l)-[:HAS {idx:idx}]->(t)RETURN *Now we can see who sings about Death” in Hamilton.MATCH p1=(c:Category {name: Death })<-[:PART_OF]-(t:Theme)<-[h:HAS]-(l:Lines)-[:OF_SONG]->(s:Song), p2=(l)<-[:SINGS]-(p:Singer)RETURN p1,p2Resulting Data ModelAfter importing all the data, we can see that the resulting graph data in the database resembles our original planned model.You can see it by calling call db.schema.visualization().Database data modelExploration: Themes Directed at HamiltonNow we can start exploring this wonderful dataset.E.g. to see who’s most often directing lines at Hamilton, unsurprisingly it’s Eliza followed by Burr.MATCH (p:Singer)-[:SINGS]->(l:Lines)-[:DIRECTED]->(:Singer {name: Alexander Hamilton }),(l)-[:HAS]->(t)-[:PART_OF]->(c)RETURN p.name, count(*) as freq, collect(distinct c.name) as categoriesORDER BY freq DESC╒═══════════════════╤══════╤════════════════════════════════╕│ p.name            │ freq │ categories                     │╞═══════════════════╪══════╪════════════════════════════════╡│ Eliza Schuyler    │20    │[ Contentment , Legacy ,        ││                   │      │  Ambition ,  Death ]           │├───────────────────┼──────┼────────────────────────────────┤│ Aaron Burr        │11    │[ Personality , Relationship ,  ││                   │      │  Ambition , Legacy ]           │├───────────────────┼──────┼────────────────────────────────┤│ George Washington │8     │[ Personality , Miscellaneous , ││                   │      │  Contentment , Legacy , Death ]│├───────────────────┼──────┼────────────────────────────────┤│ Maria Reynolds    │6     │[ Relationship , Personality ]  │├───────────────────┼──────┼────────────────────────────────┤│ James Reynolds    │1     │[ Relationship ]                │├───────────────────┼──────┼────────────────────────────────┤│ Phillip Hamilton  │1     │[ Miscellaneous ]               │└───────────────────┴──────┴────────────────────────────────┘Co-SingingWho does Thomas Jefferson most frequently sing with.MATCH (p:Singer)-[:SINGS]->(l:Lines)<-[:SINGS]-(:Singer {name: Thomas Jefferson })RETURN p.name, count(*) as freqORDER BY freq DESC╒════════════════════╤══════╕│ p.name             │ freq │╞════════════════════╪══════╡│ James Madison      │35    │├────────────────────┼──────┤│ Aaron Burr         │18    │├────────────────────┼──────┤│ Alexander Hamilton │4     │├────────────────────┼──────┤│ Company            │3     │├────────────────────┼──────┤│ George Washington  │1     │├────────────────────┼──────┤│ Angelica Schuyler  │1     │└────────────────────┴──────┘Visuals: Who’s Surrounding WashingtonMATCH path=(p:Individual)-[:SINGS]->(l:Lines)<-[:SINGS]-(:Singer {name: George Washington })RETURN pathTimelineWhat timeframe is a character active in the storyMATCH (p:Singer)-[r:SINGS]->()-[:OF_SONG]->(s:Song)// exclude the first and last song where everyone singsWHERE NOT s.id IN [ 1 , 46 ]// order numericallyWITH DISTINCT p.name as name, s.id as song ORDER BY toInteger(song) ASC// collect all songs of a character into a listWITH name, collect(song) as songs// first and last entry of the listRETURN name, songs[0]+ - +songs[-1] as time// sorted by frequency and first appearanceORDER BY size(songs) DESC, toInteger(songs[0]) asc╒══════════════════════╤═══════╕│ name                 │ time  │╞══════════════════════╪═══════╡│ Alexander Hamilton   │ 2-45  │├──────────────────────┼───────┤│ Company              │ 2-45  │├──────────────────────┼───────┤│ Aaron Burr           │ 2-45  │├──────────────────────┼───────┤│ George Washington    │ 4-32  │├──────────────────────┼───────┤│ John Laurens         │ 2-23  │├──────────────────────┼───────┤│ Eliza Schuyler       │ 5-44  │├──────────────────────┼───────┤│ Men                  │ 5-45  │├──────────────────────┼───────┤│ Hercules Mulligan    │ 2-23  │├──────────────────────┼───────┤│ Marquis de Lafayette │ 2-23  │├──────────────────────┼───────┤│ Angelica Schuyler    │ 5-45  │├──────────────────────┼───────┤│ Thomas Jefferson     │ 24-42 │├──────────────────────┼───────┤│ James Madison        │ 24-42 │├──────────────────────┼───────┤│ Women                │ 5-42  │├──────────────────────┼───────┤│ Phillip Hamilton     │ 26-45 │├──────────────────────┼───────┤│ King George III      │ 7-33  │├──────────────────────┼───────┤│ Charles Lee          │ 14-15 │├──────────────────────┼───────┤│ Peggy Schuyler       │ 5-5   │├──────────────────────┼───────┤│ Samuel Seabury       │ 6-6   │├──────────────────────┼───────┤│ Maria Reynolds       │ 27-27 │├──────────────────────┼───────┤│ James Reynolds       │ 27-27 │├──────────────────────┼───────┤│ George Eacker        │ 39-39 │└──────────────────────┴───────┘WordsFrequency of words usedMATCH (w:Word)WHERE size(w.word) > 4RETURN w.word, size((w)<-[:CONTAINS]-()) as freqorder by freq desc LIMIT 20╒═══════════╤══════╕│ w.word    │ freq │╞═══════════╪══════╡│ where     │82    │├───────────┼──────┤│ don’t     │76    │├───────────┼──────┤│ hamilton  │68    │├───────────┼──────┤│ you’re    │68    │├───────────┼──────┤│ never     │56    │├───────────┼──────┤│ alexander │49    │├───────────┼──────┤│ right     │40    │├───────────┼──────┤│ gonna     │37    │├───────────┼──────┤│ would     │36    │├───────────┼──────┤│ world     │35    │Which words are related to which themes.MATCH (w:Word)<-[:CONTAINS]-(l)-[:HAS]->(t)-[:PART_OF]->(c)WHERE size(w.word) > 4RETURN w.word, c.name, count(*) AS freqORDER BY freq DESC LIMIT 50╒════════════╤═══════════════╤══════╕│ w.word     │ c.name        │ freq │╞════════════╪═══════════════╪══════╡│ enough     │ Contentment   │104   │├────────────┼───────────────┼──────┤│ where      │ Ambition      │102   │├────────────┼───────────────┼──────┤│ would      │ Contentment   │74    │├────────────┼───────────────┼──────┤│ could      │ Contentment   │57    │├────────────┼───────────────┼──────┤│ satisfied  │ Contentment   │54    │├────────────┼───────────────┼──────┤│ happens    │ Ambition      │50    │├────────────┼───────────────┼──────┤│ where      │ Contentment   │47    │├────────────┼───────────────┼──────┤│ alive      │ Contentment   │47    │├────────────┼───────────────┼──────┤│ happened   │ Ambition      │46    │├────────────┼───────────────┼──────┤│ don’t      │ Contentment   │37    │├────────────┼───────────────┼──────┤│ throwing   │ Ambition      │34    │├────────────┼───────────────┼──────┤│ enough     │ Legacy        │28    │├────────────┼───────────────┼──────┤│ story      │ Legacy        │27    │├────────────┼───────────────┼──────┤│ helpless   │ Personality   │26    │├────────────┼───────────────┼──────┤│ world      │ Legacy        │25    │├────────────┼───────────────┼──────┤│ you’re     │ Contentment   │25    │├────────────┼───────────────┼──────┤│ other      │ Ambition      │25    │├────────────┼───────────────┼──────┤│ never      │ Contentment   │23    │├────────────┼───────────────┼──────┤│ wrote      │ Death         │22    │├────────────┼───────────────┼──────┤│ wanna      │ Ambition      │21    │├────────────┼───────────────┼──────┤│ running    │ Death         │20    │You can find our past livestreams here plus the GitHub repository for each week.Discovering AuraDB Free with Fun DatasetsWe run these Discover AuraDB Free” live-streams alternating every Monday alternating at 9am UTC (11am CEST, 2:30 IST…neo4j.comGitHub — neo4j-examples/discoveraurafree: A repo with examples from the Discover Neo4j AuraDB Free…A repo with cypher and examples from the Discover Neo4j Aura Free Twitch stream. All streams will roughly follow the…github.com;Jan 12, 2022;[]
https://medium.com/neo4j/graph-modeling-all-about-super-nodes-d6ad7e11015b;David AllenFollowOct 19, 2020·10 min readGraph Modeling: All About Super NodesThis article is the latest in a series on advanced graph modeling the other articles in the series deal with keys, relationships, and categorical variables.Today we’re going to talk about super nodes, what they are, the problems they cause, and what to do about them. This article is a summary and round-up of those issues and how they touch Neo4j specifically.Super nodes! But not to the rescueThis article will cover what they are, how they happen, and what we can do about them. Let’s get started.What’s a super node?A super node is any node that happens to have a lot of relationships. How many relationships? Well that isn’t really defined, so let’s call it a lot. If a thing has way more relationships than other nodes, relative to what else is in the graph, it’s fair to call it a super node. How many is way too many? Well that is really a subjective thing, since everyone is running a machine with different memory configuration and query patterns.It’s not hard to spot super nodes. If you were visualizing a graph and what you get is a hairball”, then the densely connected nodes that seem like they’re connected to everything are probably the super nodes.Super nodes put the hair into the hairball.In extreme situations you can’t even visualize super nodes, since if you truly had millions of outbound relationships on a node, the visualization would just be a mess of black color with too many overlapping lines to see anything really.How do super nodes happen?There are two key causes: they can arise from the domain that you are modeling, or from your modeling choices for the domain.From the domain itself: Densely connected networksImagine we modeled Twitter as a graph, with a relationship between accounts which follow one another. We know that popular accounts have many followers, so in that kind of graph structure, Lady Gaga’s node would have 82 million+ relationships. That’s pretty super, since the average user has 700 followers.Lady Gaga the Super node was born this way (as a super node, not as a motorcycle)Another example of a densely connected network might be (:Flight)-[:DEPARTED_FROM]->(:Airport). The Hartsfield-Jackson airport in Atlanta could easily do more than 200,000 flights per year. That’s a lot of -[:DEPARTED_FROM]-> relationships.Many other business domains are rich with examples:Big corporate bank accounts which are long-lived, and do hundreds or thousands of financial transactions per day (:Account)-[:TRANSFER]->(:Account) .Other social networks, like those technical recruiters you may know on LinkedIn who seem to have a zillion contacts.From your modeling choices: Categorical variable overloadIn a previous post, I wrote about categorical variables and modeling them in Neo4j. A categorical variable is just a property that can take a small handful of values. The cardinality” of the variable refers to how many different values it can have.Example: Imagine you model the gender field a customer filled out as a separate node. This is low cardinality because the only options might be male, female, didn’t specify, and other. Now suppose you have 1 million customer accounts, then we can be sure that, (:Gender) nodes will be super nodes if you modeled your graph as (:Customer)-[:GENDER]->(:Gender).Gender is just an example it’s a general issue with low-cardinality variables, modeled as nodes, when you have a large graph with millions of nodes needing to reference the variable. Imagine Amazon.com took a handful of product categories (Home and Garden”, Electronics”, Health & Wellness”) and linked every one of their tens of millions of products to a category node — Same situation.Data has outgrown the modelOften you’ll have a simple graph model that works on a small dataset, but that you might have trouble scaling, either immediately for full production data, or later as your total dataset grows with time. Imagine that Customer/Gender example not a big deal with 1,000 customers, and possibly a killer with 10 million customers. Similarly, Lady Gaga’s account wasn’t a problem for Twitter maybe in 2013, but that was a few hundred million users ago.In modeling, note that the super node problem would come from a cardinality mismatch if every product needs a category, and products have a cardinality of say, 20 million, and categories have a cardinality of 12, this massive mismatch creates probably as many super nodes as there are categories. The same pattern can be seen with any (:X)-[:RELATED_TO]->(:Y) where the cardinality of the X’s and Y’s are mismatched.If your data has outgrown your model, it’s time to refactor your model.Why are super nodes bad?The trouble is how fast they branch out, giving us too many paths to consider. Put simply, it’s easy for the leaf of a tree to find the tree trunk. It’s harder for the tree trunk to find an individual leaf. Let’s look at some ways they hurt read performance.Blowing up your traversalsSuppose you wrote a query like this:MATCH (bob:TwitterUser { name:  bob  })WITH bobMATCH p=(bob)-[:FOLLOWS*]-(:TwitterUser { name:  karla  })RETURN pThis asks for all of the paths between Bob and Karla. This query will be a disaster if anyone follows Lady Gaga, because in order for Neo4j to answer the query, it has to branch out to potentially 80 million other accounts. The number of paths to Karla will explode Bob and Karla may both live in New York City, but rest assured, this query will need to look through every Lithuanian and Korean fan of Gaga (as well as all of their followers, and their followers followers, and so on) in order to return.Listen to Boromir, he knows what’s upSlowing down other readsAnother issue is relationship filtering Neo4j (as of version 4.1) doesn’t support secondary indexes on relationship properties. Imagine a query to find all of the followers Gaga gained in 2020.MATCH (gaga:TwitterUser { name: ladygaga })<-[r:FOLLOWS]-(u:User)WHERE r.date > date(2020-01-01)RETURN u.name, u.followersThis query must consider all 80 million+ relationships, because you can’t index the date data type on a relationship. This becomes a bigger deal when you need to use relationship filtering on a path of multiple steps, where any potential step may run through a super node.Slowing down writesAs of Neo4j 4.1, when you MERGE a relationship into the graph, it locks both the source and target node temporarily for that transaction. That’s quite bad for Lady Gaga, because we’re going to want to put new [:FOLLOWS] relationships in for her all the time when her next album drops. When we add a new relationship, she is (for a fraction of a second) locked for the rest of the world to follow, and that’s going to cause problems elsewhere among her international fanbase.This post on DZone covers some other interesting facets of the performance impact, if you’d like to follow up with more detail.Not Always the end of the worldThere are a few situations where super nodes arguably don’t matter.If the super node is at the end of a path, and you are not traversing through it, it will not have much impact.The super node can be harmless if it isn’t part of a high-transaction rate query, or multi-user update.OK, so what do we do?It’s a little bit art and a little bit science — the best answer for you will depend on your domain, but here’s a toolbox of techniques to get the most out of your graph. These techniques range from simple to difficult.Relationship directionalityJoin hintsLucene relationship indexingLabel and relationship segregationSuper node refactoringWe’ll cover each of these in detail, but really they all boil down to two classes of approaches — either help the database to consider fewer relationships when you run a query, (which makes super nodes less super) or else try to break up or eliminate the super node in the first place. Some techniques are a combination of both. And of course remember: You don’t have to choose one approach, feel free to mix and match.Relationship directionalityCypher lets us express paths as undirected (:User)-[:FOLLOWS]-(:User) or as directed (:User)-[:FOLLOWS]->(:User). Always use directed paths, because it cuts down on how many relationships your query has to consider.You can exploit what you know about directionality with super nodes. Lady Gaga has 80 million followers (that is, inbound :FOLLOWS relationships) but she only follows 121,000 accounts (outbound relationships). Actually, when you think about that, it’s crazy how many people she follows, but still — there is a drastic difference between these two queries:(:User)-[:FOLLOWS]-(:User { name:  ladygaga  }) (80 million + relationships)(:User)<-[:FOLLOWS]-(:User { name:  ladygaga  }) (121,000 relationships)Keanu learns about one of the super node’s key weaknessesJoin hintsIn this knowledge base article, Andrew Bowman lays out how to use Cypher JOIN hints to constrain how Cypher does graph traversals. If you know that you have some super nodes in your graph, you can use this technique to make sure that Neo4j’s query evaluation doesn’t run into that traversal problem.Rather than re-capping the entire article, here’s the key bit with a brief explanation of how this works:MATCH (me:Person {name:Me})-[:FRIENDS_WITH]-(friend)-[:LIKES]->(a:Artist)<-[:LIKES]-(me)USING JOIN on aRETURN a, count(a) as likesInCommonThis construct USING JOIN ON a ensures that Cypher can only traverse to the :Artist node, but never through or away from it. This would be a good technique if the Artist in question was Gaga, with her 80 million fans.For more information on this technique, check the planner hints section of the Cypher manual.Lucene relationship indexingIn an earlier section, I said that Neo4j doesn’t support relationship property indexing. That’s almost true. There is actually one special case. It’s possible to use Lucene to index text properties on relationships.Using this documentation here, you can call for example:CALL db.index.fulltext.createRelationshipIndex( taggedByRelationshipIndex ,[ FOLLOWS ],[ date ], { eventually_consistent:  true  })Assuming that the date property on the :FOLLOWS relationship was a String, you could use this to filter relationships down (taking advantage of indexes), which in some limited instances, can help address the issue. This isn’t a silver bullet for the super node problem by itself. And yeah, it’s not great that we need to treat a date as a String in order to make it work.Label and relationship segregationAnother way to approach the problem of super nodes is that the individual densely connected nodes (like Lady Gaga) look and act like the rest of the graph, when really they’re special cases. Most people (coughcough) on twitter don’t have 80 million followers. So we might segregate the super nodes out of the graph by labeling them differently. What if we introduced new labels?Lady Gaga: Kinda not like the regular average node in the graph, kinda special case(:TwitterUser) for the regular schmoes like me(:Celebrity) or (:VerifiedUser) for the mega-accountsThis would make our traversals much easier, because we could still specify to go through all nodes if we wished, but we could exclude celebrities in our traversals easily if they were labeled differently.The same goes for relationships — instead of labeling the nodes differently, we could label the relationships differently: -[:FOLLOWS]-> for regular user to user relationships, and -[:FAN]-> for unreciprocated relationships to a celebrity.Notice what this does to our earlier query example:MATCH p=(:TwitterUser { name:  bob  })-[:FOLLOWS*]-(:TwitterUser { name:  karla  })RETURN pIf we just introduced a [:FAN] relationship type, suddenly this query looks very, very different and we don’t need to consider Lady Gaga. If for some reason we did want to include her, the change is trivial:MATCH p=(:TwitterUser { name:  bob  })-[:FOLLOWS|FAN*]-(:TwitterUser { name:  karla  })RETURN pThe difference is as simple as -[:FOLLOWS|FAN*]->A drawback of this approach though is that it only works with domains that are naturally densely connected. If our problem was a categorical variable like (:Gender), you can’t get out of that problem with this approach. The reason it works is because we can segregate a small population of (:Celebrity) out of our larger graph. That won’t work when all of the (:Gender) nodes are super nodes.Super node refactoringProbably the toughest approach is to refactor your graph to break down a single super node into multiple nodes. You could take Lady Gaga’s single node and break her into, say, 1,000 copies of Gaga. Create a series of equivalence relationships, i.e. (gaga1:TwitterUser)-[:SAME_AS]->(gaga2:TwitterUser) and then have a strategy for distributing your applications relationships between the 1,000 Gaga Clones”.This is a last resort strategy.It will only help with certain query patterns though, for several reasons:Queries need to be rewritten to be able to consider the SAME_AS relationships if they need to know everything about Gaga, not just touch their local clone of that node.If you needed to traverse through Gaga, you’re still back in the same situation after traversing to all of the clones, still you end up with a total cardinality of 80 million+ depending on the query.It’s more complicated to choose and implement a strategy for distributing relationships between Gaga clones” and keeping it balanced so that one of the clones doesn’t gradually grow to become its own super node, as Gaga attracts more fans over time.ConclusionWrapping all of this together, here are the key points to remember:Super nodes are those that are densely connected in your graph, with a very high number of incident relationships relative to the average found in your graph.Super nodes are bad, because they compromise both read and write performance.Super nodes come from several different places, but most usually come from the natural complexity of your domain, and your modeling choices.You’ve got a bunch of different tools in the toolbox to address the issue, and for advanced graph modeling, it’s an issue you definitely want to be aware of to get the best performance out of your graph.Happy graph hacking!This article is part of a series if you found it useful, consider reading the others on labels, relationships, super nodes, and categorical variables.;Oct 19, 2020;[]
https://medium.com/neo4j/announcing-the-release-of-the-neo4j-graphql-library-3-0-0-d41fbbc020c8;Darrell WardeFollowFeb 22, 2022·7 min readAnnouncing the Release of the Neo4j GraphQL Library 3.0.0Neo4j is excited to announce the release of the Neo4j GraphQL Library 3.0.0! Initially driven by the requirement to perform some major dependency upgrades, we have taken the opportunity to work through a backlog of breaking changes to improve the experience with the library, and to perform some much needed cleanup in key areas.Some of you may remember the 2.0.0 release and the significant migration effort that came along with that, but we hope that you’ll find this migration to be significantly lower impact. We’ve tried our best to limit migration activities to the server application, and any client-side changes either impact few users or are not immediately required.I want to take this opportunity to thank the community for their continued support of and contributions to the library. We’re only a small team, and we just couldn’t push out new features and bug fixes at the rate that we do without you. Thank you!Helping You Have Healthier RelationshipsYou’re here reading about the Neo4j GraphQL Library, so you already know that Neo4j is all about relationships. Ensuring that manipulating and querying relationships with the Neo4j GraphQL Library is a good experience is always at the forefront of our minds.We released 1.0.0 with basic support for relationships, albeit without relationship properties, which we added support for in 2.0.0. During the 2.x releases, we ensured that GraphQL Union and Interface types were supported as relationship types.In 3.0.0, we’re tidying up some of those loose ends to really polish the relationship experience. Expect to have much confidence in the integrity of your relationships, and enjoy new ways of querying them entirely! Read on below.New requirements for relationship definitionsStarting with a small migration effort for some, from 3.0.0, if an n..m relationship is defined without both the List and its entries being marked as non-nullable, an error will be thrown.This is the way.Onto some error cases — if we remove the outer Non-Null type modifier, it implies that the List can be Null. This will never happen, as an empty List will always be returned if there are no relationships.While not crazy, this type definition doesn’t really make sense.If we remove the inner Non-Null type modifier, it implies that the target node can be Null.This would be… very strange.This is where we enter a whole world of weird! With the Actor type being nullable, that would imply that the following graph” would be possible.This clearly makes very little sense, so we will throw an error for relationships defined in this way.Relationship cardinalityI would like to start this section with a confession — in hindsight, this is an issue that should have been addressed long ago. Let’s take a look at some type definitions.Given the example above, it had previously been the case that the nullability and 1..m cardinality of the director and assistantDirector fields was not enforced in the underlying database. This was obviously far from ideal.However, from 3.0.0 onwards:director will always have to be set on any Movie object, and cannot have more than one underlying relationship.assistantDirector doesn’t have to be set, but cannot have more than one underlying relationship.This is implemented via checks in the generated Cypher, so you can have high confidence that your database always reflects your GraphQL type definitions.As a heads up, you might experience some error conditions if you have any extraneous relationships created in error by earlier versions of the Neo4j GraphQL Library.Undirected relationship traversalsLet’s say you had a simple graph such as the one below:You would hope that this would be reciprocal!You might define a GraphQL type definition for this as the following:This version of the library introduces a new argument directed on relationship fields, which allows relationships to be included in query results regardless of the direction. So to get all of the people in the graph and their friends, we can do:Better yet, we can make this the default behavior so we don’t need to explicitly specify this argument on every query:New relationship filtersLet’s explore some slightly more rich type definitions to walk through the new relationship filters that are available in the 3.0.0 release.The following is only a snippet of the MovieWhere input type, showing the relationship filtering fields which will be produced from the above.The first thing to note is the deprecation of actors and actors_NOT, which have been superseded by actors_SOME and actors_NONE, respectively. It would be incredibly jarring to remove these without notice, so they have been marked with the @deprecated directive. They will be removed in the next major release. While we lament the loss of the simple actors field, its behavior was always unclear, and these new filters will allow for much more accurate querying of data. With that out of the way, let’s walk through some examples!Perhaps you want to find a potentially amazing movie in which all of the actors are award winners:Or maybe you’re not too fussy about that, and you just want to make sure that some of the actors are at least award nominated:Maybe you have some really weird requirement, and you wanted to find any movies where only a single actor was born before 1970:We might want to find all of the movies in our database that Tom Hanks didn’t act in (sorry Tom):We hope you like these new filters and you use them to query your data in exciting new ways!What Else You Can Expect in This ReleaseQuery limit controlA quite frequently requested feature, you can now set the default and maximum number of entries that will ever be returned for a particular type. Let’s take a look at an example:For the Record type:By default, 10 entries will be returned when queried.A maximum of 20 will ever be returned. If a client requests more than that, only 20 will be returned.Automatically project fields needed for custom resolversWe have renamed the @ignore directive to be @computed in the 3.0.0 release, as we realized that the only real use case was where people were ignoring a field for it to be resolved by a custom resolver.Additionally, a from argument has been added to this new directive, which will automatically project field dependencies, for example:Given a custom resolver:You can now query just the fullName field, while previously you would have also had to select the firstName and lastName fields for it to resolve.DeprecationsIn the Neo4j GraphQL Library, we only promise to support the current supported database versions. As such, we have removed support for 4.1 in this release.Additionally, the peer dependency on the graphql library has been upped to ^16.0.0, so your project will need to have this installed.A bunch of bug fixesWe’ve recently improved our approach to bug triage (which you hopefully noticed with the eight patch releases following 2.5.0!), and you’ll find a bunch of bug fixes in 3.0.0.A Recap of Updates You Might Have MissedWe’ve been fairly quiet on the blogging front since the 2.0.0 release, our heads stuck into the codebase building great new features and fixing bugs. During the 2.x releases, we came out with a number of great features, including (headings are links to documentation):Introspector: Generate GraphQL type definitions from existing data in a Neo4j database, which you can then modify if needed and host in a GraphQL server, such as Apollo Server.Indexes and constraints: Unique node property constraints and full-text indexes are now supported by the Neo4j GraphQL Library. Specify them using the new @unique and @fulltext directives.Aggregations: Supported in both the root-level Query type and for relationship fields, these allow you to perform complex aggregations over your data, including sums, averages, and finding limits.GraphQL Interface relationship fields: These abstract types can now be used as the type of relationship fields, allowing for greater flexibility in your data.Coming SoonFull Relay supportThere are a couple of loose ends to our implementation of the GraphQL Cursor Connections Specification, namely:Connection fields in the root Query type, which will allow users to perform cursor-based pagination at all levels of the schema.A node field in the root Query type, which will allow for the necessary Object Identification for a Relay server.We’re hoping to get this work polished up and released soon.GraphQL subscriptionsA much larger piece of work, we hope to soon implement GraphQL Subscriptions to allow for real-time updates of the data in your Neo4j database through the GraphQL Library.Need a Hand Upgrading?We’ve written a handy migration guide which you can find here. If you have any issues, come and join our community on Discord, and raise any potential issues you find on GitHub!GraphQL libraryThe Neo4j GraphQL Library automatically creates Query and Mutation types from your type definitions that provide…neo4j.com;Feb 22, 2022;[]
https://medium.com/neo4j/neo4j-cybersecurity-auradb-sandbox-graphs-for-cybersecurity-c3c8bde2ec8d;Chintan DesaiFollowAug 2, 2022·3 min readNeo4j Cybersecurity AuraDB & Sandbox: Graphs for CybersecurityComputer security, cybersecurity or information technology security (IT security) is the protection of computer systems and networks from information disclosure, theft of or damage to their hardware, software, or electronic data, as well as from the disruption or misdirection of the services they provide.Cybersecurity is also one of the significant challenges in the contemporary world, due to its complexity, both in terms of political usage and technology.Graph database can help in implementing an effective solution in the field of Cyber Security.Defenders think in lists. Attackers think in graphs. As long as this is true, attackers win.— John Lambert, Engineer from Microsoft Threat Intelligence CenterOur networks are connected graphs. We can use a graph based solution to have a holistic view of enterprise network.Computer Network, Users and Groups: Graph VisualizationThis helps us to minimize the impact by:Quickly identify risks,Detecting anomalies, andProtecting our systems with confidence.Example Use CaseWith help of Dave Voutila, we came up with a very good example in the area of Cybersecurity. This demo is based on the data and themes from the BloodHound Project.BloodHound: Six Degrees of Domain Admin - BloodHound 3.0.3 documentationBloodHound uses graph theory to reveal the hidden and often unintended relationships within an Active Directory…bloodhound.readthedocs.ioBloodhound is a great tool for auditing your Active Directory environment. It uses Neo4j as a backing datastore and querying.BLOODHOUNDWe have conceptualized a BloodHound example to prepare a fictitious Active Directory environment. Active Directory helps IT teams manage and monitor various network resources, machines and users. It allows to grant and revoke different permissions to users and groups.Data ModelNetwork InsightsIn this example, we prepare the graph schema and load the Active Directory data in form of graph nodes and relationships. Further, we demonstrate some interesting queries to gain insights of the network assets and user access.Attack PathsCybersecurity nowadays is seeing a zero trust (trust no one) shift of network defense. This approach allows organization to restrict access controls to network, apps and environment without sacrificing performance and user experience.In a simple terms, we analyze any path that a user can take to reach to a high value resource in the network. Is this access (path) necessary? If not, these unwanted access paths can be revoked or controlled.We have also demonstrated this aspect by showing all possible access paths leading to a network crown jewel (high value object in the Network), and possible attack paths.Cybersecurity Dataset Available on AuraDB FreeThe dataset and guide is available if you create a free AuraDB managed database. Just select the box for the dataset and give it a try.Neo4j AuraDB Free DatasetsNeo4j Sandbox to Explore Cybersecurity ExampleWe have a created a Neo4j Sandbox to walk you through the Cybersecurity use case. Neo4j Sandbox is a great — and free — online tool from Neo4j to try their graph database without installing anything locally.Neo4j SandboxGet started with your own cybersecurity sandbox.Full source code for this example and guide is available on GitHub.GitHub - neo4j-graph-examples/cybersecurity: Graphs in Cybersecurity - BloodhoundGraphs in Cybersecurity - Bloodhound. Contribute to neo4j-graph-examples/cybersecurity development by creating an…github.comNeo4j ReferencesDemo video Cyber SecurityNeo4j Bloom PersectivesBloodHound Datacreator LibraryNeo4j for Cyber SecurityExternal ReferencesBloodHoundBloodhound Handbook PDFCyber Security- WikipediaHackerpocalypse Cybercrime Report-2016Cost of Cybercrime StudyForecast Analysis: Information Security, WorldwideFunctional Post — John Lambert;Aug 2, 2022;[]
https://medium.com/neo4j/whats-new-in-neo4j-desktop-spring-2020-197c43bfcfe1;Adam CowleyFollowApr 2, 2020·4 min readWhat’s new in Neo4j Desktop — Spring 2020The Neo4j team has been making a lot of noise recently, announcing some huge 4.0 features including multi-tenancy, and sharding with fabric. But that is not all, there is so much more going on beneath the surface. So I thought I would take some time to put together a list of new features that you may not have spotted in Neo4j Desktop.First Graph ExperienceNew users are now greeted by a new database when they first install Neo4j Desktop. Instead of being presented with a blank screen and wondering where to go next, you will now see a sample database containing a graph of Movies and Actors.The new look Neo4j Desktop project viewClicking the Browser Guide link for the Sample database will open up the Neo4j Browser with a Browser Guide that will guide you through your first queries on this Neo4j database.Clicking a Browser Guide will automatically open it in Neo4j BrowserFilesYou may have noticed a new Files section in the screenshot above. Here you can drag and drop Cypher snippets or browser guides to be used within your project. For example, if you add a Cypher file, you will be given the option to open up the script in Neo4j Browser. This could be useful for seeding a new database with sample data or creating constraints.You can now add files that can be used within your projectChanges to Graph AppsThe new look Graph Apps pane with Launch Icons next to each Graph AppThe use of Graph Apps in Neo4j Desktop has changed quite a bit since the release of Desktop 1.2.6. Now, you don’t need to add a Graph App to a project. Instead, you launch Graph Apps straight from the Graph Apps sidebar pane. Just click the blue Open icon next to the App to launch it.The way that Graph Apps are launched has had a huge overhaul. Now, instead of having to add individual Graph Apps to a project, these are now treated as globally available. You can launch any Graph App by clicking the arrow next to the Open button and selecting the app or pressing Ctrl+K or Cmd+K and typing the name of the app.Graph App Gallery installed with Neo4j DesktopYou may have also noticed the top item in the previous screenshot. The Graph App Gallery is now included with every installation of Neo4j Desktop. All Neo4j Desktop users have access to a number of apps aimed at improving productivity, or visualising the data held in their database.If you have built a Graph App and would like it featured in the Graph App Gallery, feel free to get in touch and we will be happy to discuss it with you.Deep-Links to add a new GraphYou can now add your remote instance, Sandbox or Neo4j Aura databases to Neo4j Desktop using a deep-link. This means that you can now share a link to your new project (without the password of course) with you colleagues while chatting over Slack.neo4j-desktop://remote/add?url=myneo4jinstance&username=neo4j&name=ProductionCommand BarYou may have noticed the Magnifying Glass icon in the top right hand corner. Although the Command Bar (similar to macOS’ Spotlight) isn’t strictly a new feature, it has gone under the radar with many users (including myself). Along with the keyboard shortcut (ctrl+k on Windows & Linux or cmd+k on macOS), you can now get quick access by clicking the magnifying glass. From there you can launch Graph Apps, switch between projects or start another Graph.Open the action bar with ctrl+k on Windows & Linux or cmd+k on a MacActivation KeysA topic close to my heart is Activation Keys. You may already have entered an Activation Key for Neo4j Desktop or Neo4j Bloom but now you can utilise them in your own Graph Apps. Each key is tied to an app by its name in `package.json` and features a scope property.const context = await neo4jDesktopApi.getContext()const unlockedFeatures = context.activationKeys    .filter(key => key.featureName ===  @neo4j/neotel )    .map(key => key.scope)console.log(unlockedFeatures)These keys are currently issued by Neo4j, but we plan to roll these out so you can issue your own to customers as part of a checkout process. If you are interested in using Activation Keys to unlock features in your app, drop me a message and we will see what we can do.We’re here to listenThat’s far from a comprehensive list of everything we’ve been up to. If there are any features that you feel are missing feel free to reach out via the Community forum and we will see what we can do.Happy Developing!~ Adam(at neo4j.com);Apr 2, 2020;[]
https://medium.com/neo4j/run-cypher-without-leaving-your-ide-with-neo4j-vscode-extension-85e0cbac99f2;Adam CowleyFollowJan 12·5 min readRun Cypher Without Leaving Your IDE With Neo4j VSCode ExtensionTLDR: Manage Neo4j connections and run Cypher from VS Code by installing the new Neo4j VS Code extension.The main features of the plugin: Manage connections to Neo4j instances in the Sidebar, manage query parameters and run Cypher from a file or text selection.How did we get here?As a developer building Neo4j Applications in VS Code, I often switch between my code editor and other tools like Neo4j Browser when writing more complex Cypher statements. Constantly switching between the code editor and other windows to write and execute queries can take a toll on productivity. Not only does it take up a lot of time, but I also find it also disrupts my workflow and productivity.I started experimenting with custom VS Code extensions a few years ago but quickly got distracted by other things. The code sat dormant until recently when a colleague asked me what state it was in. That seeded an idea in my mind that others would really find this useful.I got talking to a few others, partly fuelled by frustration while working to a tight deadline before the holiday break, and partly motivated by the brilliant Prisma extension. I decided I had to pick this up when I returned to my desk in January.Introducing the Neo4j VS Code Extension 🎉As a first release, I’ll target a few pain points that I commonly experience myself:I often write Cypher queries in Neo4j Browser and then copy and paste the code into a Python, Node.js, or Typescript file.If I get something wrong in the Cypher statement, it can be difficult to identify and debug.Code highlighting packages for Cypher already exist but aren’t feature complete.I don’t like having too many windows open at once.The Neo4j VSCode Extension tackles these issues by providing a new Status bar menu that lists connections.A quick command will allow you to copy and paste the URL of your Neo4j instance to add a connection.Once you have added a connection, you can run a command to execute the currently highlighted text or the entire file contents against the database within a read or write transaction. The commands have a keybinding assigned to them, so you can even hit Ctrl+Alt+Space on Windows or Ctrl+Cmd+Space on Mac to execute the query.The extension is configured to register Cypher as a language in its own right while at the same time adding highlighting and auto-closing bracket pairs. I have also included a quick subset of commonly used Cypher code snippets, and I plan to add more over time:MATCH, MERGE, CREATE, DELETEnode pattern, relationship patterns — out, inWITH, RETURNSETInstallation and UsageYou can install the extension within the IDE by searching for Neo4j or the from the Visual Studio Marketplace. Once installed, you can access a list of database connections by clicking the Neo4j logo in the Activity Bar on the left-hand side.The Neo4j Status Bar, with database and Cypher parameters listed.Adding a new Database ConnectionYou can add a database connection in three ways, all available through the Command Palette.You can configure a new connection using the Neo4j: Add Connection command.Add Neo4j ConnectionTo quickly add a connection to your local server at neo4j://localhost:7687you can use the Neo4j: Add Local Connection command.Add Local Neo4j ConnectionYou can copy and paste the Connection URL directly from the Neo4j Aura console using the Neo4j: Add Aura Connection command.AuraDB ConsoleYou can switch your active database connection anytime using the Neo4j: Set Active Connection command or by clicking the connection URL in the status bar at the bottom of the screen.Add Neo4j Aura ConnectionRunning Cypher QueriesOnce an active connection is set, you can run a Cypher statement in a read transaction using the Neo4j: Run Cypher Statement in a Read Transaction command, or write to the database using the Neo4j: Run Cypher Statement in a Write Transaction command.Run StatementYou can also use the multi-cursor functionality to execute multiple statements simultaneously. Each result will open in a new window so no data is lost.Run Multiple StatementsIf you have a Cypher statement within a larger file, such as a JavaScript module, you can highlight the Cypher you want to execute and run the command. If you are missing any $parameters, the extension will prompt you for the missing values.Execute Highlighted StatementIf your query requires $parameters, you can set them using the Neo4j: Set Parameter command. If you no longer require the parameter, you can run the Neo4j: Remove Parameter command and select the key of the parameter you want to remove.Remove ParameterBugs, Feedback & SuggestionsI would love to hear what you think about the extension. If you have any questions, comments, or feature requests you can contact me via Twitter, Mastodon, LinkedIn, or open an Issue on the Github repository. Pull requests are also welcome!Repository and Marketplace ItemsGitHub - adam-cowley/neo4j-vscode: VS Code Extension ExperimentsThe Neo4j VS Code Extension allows you to manage connections and run Cypher without leaving VS Code. Cypher syntax…github.comNeo4j - Visual Studio MarketplaceThe Neo4j VS Code Extension allows you to manage connections and run Cypher without leaving VS Code. Cypher syntax…marketplace.visualstudio.comTo learn more about Neo4j, head to Neo4j GraphAcademy for free online, hands-on, self-paced courses.;Jan 12, 2023;[]
https://medium.com/neo4j/kickstart-a-chemical-graph-database-d9ee9779dfbe;Tom NijhofFollowMar 29·4 min readKickstart a chemical graph databaseI have spent some time scrapping and shaping PubChem data into a Neo4j graph database. The process took a lot of time, mainly downloading, and loading it into Neo4j. The whole process took weeks.If you want to have your own I will show you how to download mine and set it up in less than an hour (most of the time you’ll just have to wait).Photo by D koi on UnsplashThe process of how this dataset is created is described in the following blogs:Exploring Neodash for 197M chemical full-text graphIn a previous blog, I loaded 197M chemical names into a graph database. All of these are indexed with a full-text index…medium.comCombining 3 Biochemical Datasets in a Graph DatabaseThe open measurement graph will be used to find connections between different measurements in different experiments and…medium.comWhat do you get?The full database is a merge of 3 datasets, PubChem (compounds + synonyms), NCI60 (GI50), and ChEMBL (cell lines).It contains 6 nodes of interest:CompoundThis is related to a compound of PubChem. It has 1 property.pubChemCompId: The id within pubchem. So compound:cid162366967” links to https://pubchem.ncbi.nlm.nih.gov/compound/162366967.This number can be used with both PubChem RDF and PUG.SynonymA name found in the literature. This name can refer to zero, one, or more compounds. This helps find relations between natural language names and absolute compounds they are related to.Name: Natural language name. Can contain letters, spaces, numbers, and any other Unicode character.pubChemSynId: PubChem synonym id as used within the RDFCellLineThese are the ChEMBL cell lines. They hold a lot of information.Name: The name of the cell line.Uri: A unique URI for every element within the ChEMBL RDF.cellosaurusId: The id to connect it to the Cellosaurus dataset. This is one of the most extensive cell line datasets out there.MeasurementA measurement you can do within a biomedical experiment. Currently, only GI50 (the concentration needed for Growth Inhibition of 50%) is added.Name: Name of the measurement.Condition: A single condition of an experiment. A condition is part of an experiment. Examples are: an individual of the control group, a sample with drug A, or a sample with more CO2ExperimentA collection of multiple conditions all done at the same time with the same bias. Meaning we assume all uncontrolled variables are the same.Name: Name of the experiment.Overview of the graph designHow do download itWarning, you need 120 GB of free memory. The compressed file you download is already 30 GB. The uncompressed file is 30 GB. The database afterward is 60 GB.60 GB is only for temporary files, the other 60 is for the database.If you do this on an HDD hard disk it will be slow.If you load this into Neo4j desktop as a local database (like I do) it will scream and yell at you, just ignore this. We are pushing it far further than it is designed for, but it will still work.Download the fileGo to this Kaggle dataset and download the dump file. Unzip the file, then delete the zipped file. This part needs 60 GB but only takes 30 by the end of it.Create a databaseOpen the Neo4j desktop app, and click Reveal files in File Explorer”. Move the `.dump` you downloaded into this folder.Screenshot of Neo4J desktop, showing the `.dump` file in filesClick on the `…` behind the `.dump` file and click `Create new DBMS from dump`. This database is a dump from Neo4j V4, so your database also needs to be V4.x.x!Screenshot of the `create project` window, showing that version 4.4.18 is selectedIt will now create the database. This will take a long time, it might even say it has timed out. Do not believe this lie! In the background, it is still running.Every time you start it, it will time out. Just let it run and press start later again. The second time it will be started up directly.Screenshot of the `time out` error message. This message you can ignore, it will happen every time you start the databaseEvery time I start it up I get the timed-out error. After waiting 10 minutes and clicking start again the database, and with it, more than 200 million nodes, is ready.And you are done! Good luck and let me know what you build with it;Mar 29, 2023;[]
https://medium.com/neo4j/using-neo4j-graph-data-science-in-python-to-improve-machine-learning-models-c55a4e15f530;Tomaz BratanicFollowJul 6, 2022·13 min readUsing Neo4j Graph Data Science in Python to Improve Machine Learning ModelsUtilize graph algorithms in the Neo4j Graph Data Science library to extract graph-based features and improve the accuracy of machine learning modelsA wave of graph-based approaches to data science and machine learning is rising. We live in an era where the exponential growth of graph technology is predicted [1]. The ability to analyze data points through the context of their relationships enables more profound and accurate data exploration and predictions. To help you catch the rising wave of graph machine learning, I have prepared a simple demonstration where I show how using graph-based features can increase the accuracy of a machine learning model.We will be using an anonymized dataset from a P2P payment platform throughout this blog post. The graph schema of the dataset is:Graph schema of the P2P payment platform. Image by the author.The users are in the center of the graph. Throughout their stay on the P2P platform, they could have used multiple credit cards and various devices from multiple IPs. The main feature of the P2P payment platform is that users can send money to other users. Every transaction between users is represented as the P2P relationship, where the date and amount are described. There could be multiple transactions between a single pair of users going in both directions.A sample of P2P network. Image by the author.The sample visualization shows users (purple) that can have multiple IPs (blue), Cards (red), and Devices (orange) assigned to them. Users can make transactions with other users, which, as mentioned, is represented as the P2P relationship.Some of the users are labeled as fraud risks. Therefore, we can use the fraud risk label to train a supervised classification model to perform fraud detection. In this blog post, we will first use non-graphy features to train the classification model, and then in the second part, try to improve the model’s accuracy by including some graph-based features like the PageRank centrality.All the code is available as a Jupyter Notebook. Let’s dive right into the code!blogs/p2p-fraud.ipynb at master · tomasonjo/blogsJupyter notebooks that support my graph data science blog posts at https://bratanic-tomaz.medium.com/ …github.comPrepare the Neo4j environmentWe will be using Neo4j as the source of truth to train the ML model. Therefore, I suggest you download and install the Neo4j Desktop application if you want to follow along with the code examples.The dataset is available as a database dump. It is a variation of the database dump available on Neo4j’s product example GitHub to showcase fraud detection. I have added the fraud risk labels as described in the second part of the Exploring Fraud Detection series, so you don’t have to deal with it. You can download the updated database dump by clicking on this link.I wrote a post about restoring a database dump in Neo4j Desktop a while ago if you need some help. After you have restored the database dump, you will also need to install the Graph Data Science and APOC libraries. Make sure you are using version 2.0.0 of Graph Data Science or later.Install APOC and GDS plugins. Image by the author.Neo4j Graph Data Science Python clientThe Neo4j team released an official Python client for the Graph Data Science library alongside the recent upgrade of the library to version 2.0. Since the Python client is relatively new, I will dedicate a bit more time to it and explain how it works.First, you need to install the library. You can simply run the following command to install the latest deployed version of the Graph Data Science Python client.pip install graphdatascienceOnce you have installed the library, you need to input credentials to define a connection to the Neo4j database.In the example I’ve seen, the first thing one usually does is execute the print(gds.version()) line to verify that the connection is valid and the target database has the Graph Data Science library installed.Exploring the datasetWe will begin with simple data exploration. First, we will count the number of transactions by year from the database using the run_cypher method. The run_cypher method allows you to execute Cypher statements to retrieve data from the database and return a Pandas dataframe.ResultsCount of transactions per year. Image by the author.There were more than 50,000 transactions in 2017, with a slight drop to slightly less than 40,000 transactions in 2018. I would venture a guess that we don’t have all the transactions from 2019, as there are only 10,000 transactions available in the dataset.Our baseline classification model will contain only non-graph-based features. Therefore, we will begin by exploring various features like the number of devices, credit cards, and total and average incoming and outgoing amounts per user. We will stay clear of using the graph algorithm available in the Neo4j Graph Data Science library for now.We have counted the number of relationships a user has, along with some basic statistics around the incoming and outgoing amounts. First, we will evaluate how many users are labeled as fraud risks. Remember, since the run_cypher method returns a Pandas Dataframe, you can utilize all the typical Pandas Dataframe methods.ResultsAs is typical with the fraud detection scenario, the dataset is heavily imbalanced. One could say that we are searching for a needle in a haystack. Next, we will use the Pandas describemethod to evaluate value distributions.ResultsResults of the describe method. Image by the author.Some of the value distributions are missing from the above results, as they didn’t fit in a single image. You can always look at the full output in the accompanying Jupyter Notebook.Users have used 1.6 devices on average, with one outlier having used 65 devices. What’s a bit surprising is that the average number of credit cards used is almost four. I think that perhaps the high average of credit cards could be attributed to credit card renewals, and therefore a user needs to add more than one credit card. However, it’s still a higher average than I would expect.Both the total incoming and outgoing average amounts are around 1,000. Unfortunately, we don’t know the currency or if the transaction values have been normalized, so it is hard to evaluate absolute values. In addition, the median outgoing payment is only five, while the incoming median amount is 15. Of course, there are some outliers as always, as one user has sent over a million through the platform.Before we move on to training the classification model, we will also evaluate the correlation between features.ResultsCorrelation matrix. Image by the authorIt seems that none of the features correlate with the fraud risk label. As one would imagine, the number of transactions correlates with the total amount sent or received. The only other thing I find interesting is that the number of credit cards correlates with the number of IPs.Training a baseline classification modelNow it’s time to train a baseline classification model based on the non-graph-based features we pulled from the database. As part of the fraud detection use case, we will try to predict the fraud risk label. Since the dataset is heavily imbalanced, we will use an oversampling technique SMOTE on the training data. We will use Random Forest Classifier to keep things simple, as the scope of this post is not to select the best ML model and/or their hyper-parameters.I’ve prepared a function that will help us evaluate the model by visualizing the confusion matrix and the ROC curve.Now that we have the data and the code ready, we can go ahead and train the baseline classification model.The evaluate function will output confusion matrix, ROC curve, and the feature importance table.Baseline model confusion matrix. Image by the author.The baseline model features performed reasonably. As a result, it correctly assigned a fraud risk label to 50 percent of the actual fraud risks while misclassifying the other half. Around 13 percent are non-frauds wrongly classified as frauds. Remember, that is quite a considerable number, hence the heavy data imbalance.Baseline model ROC curve. Image by the author.The AUC score of the baseline model is 0.72. The higher the AUC score, the better the model can distinguish between positive and negative labels. Lastly, we will look at the feature importance of the model.Interestingly, the most important feature is the number of devices — not really what I would expect. I would instead think the that number of credit cards would have a higher impact. The following three important features are all tied to the count and the amount of the incoming transactions.Using graph-based features to increase the accuracy of the modelIn the second part of the post, we will use graph-based features to increase the performance of the classification model. Lately, graph neural networks and various node embedding models are gaining popularity. However, we will keep it simple in this post and not use any of the more complex graph algorithms. Instead, we will use more classical centrality and community detection algorithms to produce features that will increase the accuracy of the classification model.We have a couple of networks in our dataset. First, there is a direct P2P transaction network between users that we can employ to extract features that describe users.P2P transaction network. Image by the author.On the other hand, there are also indirect connections between users, where some users use the same device, IP, or credit card.Network of users and devices they used. Image by the author.The above image visualizes a network of users (purple) and devices (orange) they have used. For example, it might be that the left device used by a high number of users could be a public computer in a library or a coffee shop. It is also not unusual for family members to use the same device.In our example, we will use the P2P transaction network between users and indirect connections between users who share credit cards as the input to graph algorithms to extract predictive features.Before executing any graph algorithms, we have to project the Graph Data Science in-memory graph. We will be using the newly released Graph Data Science Python client to project an in-memory graph. The Python client mimics the Graph Data Science Cypher procedure and follows an almost identical syntax. It seems to me that the only difference is we don’t prefix the Cypher procedures with the CALLoperator as we would when, for example, executing graph algorithms in Neo4j Browser.We can use the following command to project User and Card nodes along with the HAS_CC and P2P relationships.For those of you who have experience with Graph Data Science, or read my previous blog post, the projection definition is, for the most part, straightforward. The only thing I haven’t used in a while is merging parallel P2P relationships into a single relationship during projection and summing their total amount. We merge parallel relationships and sum a specific property of the relationships using the following syntax:P2P:{           type:P2P,    properties:{              totalAmount:{             aggregation:SUM                   }            }    }You can observe the following visualization to understand better what the above syntax does.Merge parallel relationships and sum their properties. Image by the author.With the projected in-memory graph ready, we can go ahead and execute intended graph algorithms.Weakly connected componentsWe will begin by using the Weakly Connected components (WCC) algorithm. The WCC algorithm is used to find disconnected components or islands within the network.An example network where two weakly-connected components are present. Image by the author.All nodes in a single weakly connected component can reach other nodes in the component when we disregard the direction of the relationship. The above example visualizes a network where two components are present. There are no connections between the two components, so members of one component cannot reach the members of the other component.In our example, we will use the WCC algorithm to find components or islands of users who used the same credit card.An example network where four users used the same credit card. Image by the author.In this example, we have four users who used the same credit card. Therefore, the Weakly Connected component algorithm that considers both users and their credit cards will identify that this component contains four users.We will use the stream mode of the WCC algorithm using the GDS Python client, which will return a Pandas Dataframe. Any additional configuration parameters can be added as keyword arguments.We have used the nodeLabels parameter to specify which nodes the algorithm should consider, as well as the relationshipTypes parameter to define relationship types. Using the relationshipTypes parameter, we have defined the algorithm to consider the HAS_CC relationships and ignore the P2P relationships.As mentioned, the above statement returns a Pandas Dataframe that contains the internal node ids and the component ids.Results of the WCC algorithm. Image by the author.The output of the WCC algorithm will contain both the User and the Card nodes. Since we are only interested in User nodes, we must first retrieve the node labels using the gds.util.asNodes method and then filter on the node label.Lastly, we will define two features based on the WCC algorithm results. The componentSize feature will contain a value of the users in the component, while the part_of_community feature will indicate if the component has more than one member.PageRank centralityNext, we will use the PageRank centrality of the P2P transaction network as one of our features. PageRank algorithm is commonly used to find the most important or influential nodes in the network. The algorithm considers every relationship to be a vote of confidence or importance, and then the nodes deemed the most important by other important nodes rank the highest.Unlike the degree centrality, which only considers the number of incoming relationships, the PageRank algorithm also considers the importance of nodes pointing to it. A simple example is that being friends with the president of the country or a company gives you more influence than being friends with an intern. Unless that intern happens to be a family relative to the CEO.An example network where nodes are colored based on their PageRank score. Image by the author.In this visualization, nodes are colored based on their PageRank score, with the red color indicating the highest rank and the white color indicating the lowest score. For example, Captain America has the highest PageRank score. Not only does he have a lot of incoming relationships, but he also has connections with other important characters like Spider Man and Thor.You can execute the stream mode of the weighted variant of the PageRank algorithm using the following Python code.This code first executes the stream mode of the PageRank algorithm, which returns the results in the form of the Pandas Dataframe. Using the nodeLabels parameter, we specify that the algorithm should only consider User nodes. Additionally, we use the relationshipTypes parameter to use only the P2P relationships as input. Lastly, we merge the new PageRank score column to the graph_features dataframe.Closeness centralityThe last feature we will use is the Closeness centrality. The Closeness centrality algorithm evaluates how close a node is to all the other nodes in the network. Essentially, the algorithm results inform us which nodes can reach all the other nodes in the network the fastest.An example network where nodes are colored based on their Closeness centrality score. Image by the author.This visualization contains the same network I used for the PageRank centrality score. The only difference is that the node’s color depends on their Closeness centrality score. We can observe that the nodes in the center of the network have the highest Closeness centrality score, as they can reach all the other nodes the fastest.The syntax to execute the Closeness centrality and merge the results to the graph_features data frame is almost identical to the PageRank example.Combine baseline and graph featuresBefore we can train the new classification model, we have to combine the original dataframe that contains the baseline features with the graph_feature dataframe that includes the graph-based features.The original dataframe does not contain the internal node ids, so we must first extract the user ids from the node object column. Next, we can use the user id column to merge the baseline and the graph-based feature dataframes.Include the graph-based features in the classification modelNow we can go ahead and include both the baseline as well as the graph-based features to train the fraud detection classification model.First, we can take a look at the confusion matrix.Confusion matrix of the model that includes graph-based features. Image by the author.We can observe that the model correctly classified 79 percent of fraudsters, rather than the 50 percent with the baseline model. However, it also misclassifies fewer non-frauds as frauds. We can observe that the graph-based features helped improve the classification model accuracy.ROC curve of the model that includes graph-based features. Image by the author.We can also observe that the AUC score has risen from 0.72 to 0.92, which is a considerable increase.Finally, let’s evaluate the importance of the graph-based features.While the number of credit cards used by a user might be significant to classify the fraudsters accurately, a far more predictive feature in this dataset is looking at multiple users and how many used those credit cards. The PageRank and Closeness centrality also added a slight increase in the accuracy, although they are less predictive in this example than the Weakly Connected component size.ConclusionSometimes a more extensive dataset or more annotated labels can help you improve the machine learning model accuracy. Other times, you need to dig deeper into the dataset and extract more predictive features.If your datasets contain any relationships between data points, it is worth exploring if they can be used to extract predictive features to be used in a downstream machine learning task. We’ve used simple graph algorithms like centrality and community detection in this example, but you can also dabble with more complex ones like graph neural networks or node embedding models. I hope to find more scenarios where more complex algorithms come into play and then write about them.In the meantime, you can start exploring your datasets with the Neo4j Graph Data Science library and try to produce predictive graph-based features. As always, all the code is available on GitHub.References[1]https://www.techtarget.com/searchbusinessanalytics/news/252507769/Gartner-predicts-exponential-growth-of-graph-technology;Jul 6, 2022;[]
https://medium.com/neo4j/how-to-create-conditional-and-dynamic-queries-in-neo4j-bloom-bdf0a2984837;Dan FlavinFollowJun 30, 2021·12 min readHow to Create Conditional and Dynamic Queries in Neo4j BloomGetting more with less (data) out of Neo4j Bloom with a few simple techniquesLet’s focus on the chocolates we wantNote: This post was previously named Enabling BI tool-like functionality in Neo4j Bloom — I want to easily search based on this and/or that logic”Key points:Parameterized and chained customer Cypher statements in Neo4j Bloom allow for complicated Cypher query patterns that can improve a user’s experience and effectiveness.Bloom has an upper limit of 10,000 nodes retrieved for display. Parameterized Cypher-driven Bloom queries that keep that data retrieved under that limit to ensure you’re only working with the set of data you’re interested in. In short, embedded Cypher statements in Bloom allow push-down filtering query filtering in the database.Neo4j Bloom is a wonderful tool for navigating and visualizing a Neo4j graph without having to know the Cypher query language. This functionality is unique and a necessity for graph database end-users. Why?The variety and depth of connections between nodes and relationships that form paths in a graph often cannot be determined ahead of time. The Cypher query language can express a query graph pattern easily. Take the simple Cypher query below as an example:A graph is worth a thousand words that I haven’t read yet.”Cypher is an information-liberating language to use, but learning how to write a Cypher query, similar to learning SQL, can be limiting for some. Neo4j Bloom removes the need to know Cypher to explore a graph. Besides, it’s fun to be able to visually traverse the graph, similar to taking an unknown path through a forest. You never know what you might find. Take a quick look at the Bloom Features: Near Natural Language Search” video if you’re not familiar with the Bloom search and graph navigation capabilities.Bloom Near NLP SearchCompare using Bloom to freely explore data in a graph with classic business intelligence tools. BI tools work with known data structures and predetermined links that are instantiated at runtime (think SQL joins). The user stories for BI tools like Tableau and Neo4j Bloom address two different scenarios.BI tools provide functionality that Bloom was never designed to provide (think subtotals, charts, maps, etc.). A Neo4j graph database can appear as a RDBMS structure and queried using SQL using the Neo4j BI Connector, but that’s a different topic.Using the BI Connector to Query Neo4j with SQLGraphs and tables — SQL and Cypher!medium.comQuerying and traversing a graph in Bloom is done with idiomatic, near-natural language search patterns. This is an extremely useful and wonderfully graph-y thing, but there are scenarios where free-form pattern traversing of a graph could be made even better. The developers of Neo4j Bloom addressed this by enabling custom Cypher queries executed in Bloom via Search Phrases” and its newer cousin Scene Actions”. See the video below if you’re not familiar with Search Phrases.Bloom Search Phrase VideoThe core values for using Bloom Search Phrases IMHO are:Transparently provide users with a guided, parameterized, and potentially complicated pre-built Cypher query that users interact with via the search box. Users are guided to a specific area of the graph instead of having to start with a blank slate and then navigate and filter. An example would be a Search Phrase having a parameter that asks a user for an account id, which in turn drives a Cypher query with procedural logic to get all the things related to the account with the provided account id.Allow for AND ORand other chained declarative logical constructs, such as the CONTAINSstring function that is not provided by the Bloom Scene filtering functionality. An example would be to query for all the node:Personlabeled nodes where the node.sequence = 16 OR node.age >= 42.Filter data at the database level before it is ingested into the Bloom workspace. What is the hidden value of this? The first is minimizing the amount of work by the Neo4j database needed to retrieve the data into the Bloom Scene. Second, Bloom will retrieve a user-configurable upper limit of 10,000 nodes. This makes sense as Bloom is a visual tool and 10,000 nodes is a common tipping point where visualization becomes ineffective. All forest and no trees so to speak. A user could filter down the data after it is retrieved into Bloom, but what if the data you’re interested in never makes it into Bloom because it’s in node 10,001? Bloom will tell you when you’ve exceeded the set node limit. At least you know you need to pare down the data brought into Bloom.Example Time! Dynamic Search Based on If X and/or Y Kinda StuffThings most don’t realize you can do with Bloom👉 Run a Cypher Statement in Bloom 👈The building block for all that follows is one of my favorite Neo4j apoc procedures, apoc.cypher.run(). It’s a handy little function that will execute a Cypher query passed in as a string. Guess what? Bloom search phrases can dynamically build Cypher strings. The most basic example of using apoc.cypher.run() in Bloom would be to create a search phrase that inputs a Cypher query string and executes it:Bloom Search phrase definition paneA user would do the following in the search box to execute this search phrase:Start typing Q1 -” in the search box and tab to complete the search phrase.Once you have Q1 — Cypher Input: showing the search box, type a the free-form Cypher query, e.g. MATCH path=(:Person)-->().Then press <enter> to execute the query which will show something similar to:The Cypher query text entered in the search box as part of the search phrase is passed to the main query in the parameter $cypherText, which is then executed by apoc.cypher.run(). I’ve used this search phrase when I need a visualization that is too much for the Neo4j Browser to render, or want to use the enhanced Bloom functionality. Read the Where’s My Neo4j Cypher Query Results” post for more details on when this might occur.Where’s My Neo4j Cypher Query Results? 😠 ⚡️ ⁉️Why a Cypher query run in the Neo4j Browser may not return in a reasonable amount of time, what is happening, and what…medium.comThis example has a hardcoded LIMIT clause in the search phrase query. In practice the LIMIT value is usually a parameter. It might be prudent to set a limit to the number of nodes returned by the submitted dynamic Cypher that is equal to or lesser than the Node query limit” setting in Bloom or some other sensible metric. Bloom will stop retrieving data when the node query limit setting is reached, so why have the Neo4j database work on retrieving more data than Bloom will display? The techniques outlined below can be used to have users specify the node limit in a search phrase to match the changeable Bloom node query limit setting.👉 Search Phrase That Allows for Conditionals on Properties 👈Forming a query pattern in Bloom allows for flexible searching with exact matching values, but can be problematic for other types of operators. A generic search phrase can be used to guide a user through this functionality. The animation below shows this search phrase functionality using the ubiquitous Neo4j hello world” movies database.Finding all :Person nodes with property born > 1965The above example was done with a search phrase that chains search phrase parameters. Chaining parameters is where parameter values are used in other parameter definitions. Below is the Bloom search phrase definition for the above shown in the Bloom search phrase editing panel:The logic behind this search phrase is:1️⃣ Create Search Phrase. Enter search phrase text with the conditional parameters $label, $property, $condition and $value shown as a,b,c, and d in the above image.2️⃣ Add a user-friendly description.3️⃣ Main Cypher Query. The main Cypher query will be built concatenating the parameter values created in subsequent steps into a string to be run by apoc.cypher.run() The parameters are created in the search phrase defined in step 1️⃣:// Main Bloom Cypher queryWITH  MATCH (n:  + $label +  )   +   WHERE n.  + $property +       + $condition +      + $value +    RETURN n  AS qryStrCALL apoc.cypher.run(qryStr, {}) YIELD value AS nodesRETURN nodes4️⃣ Pick a Node Label. Get the $label parameter value (‘a’ in the image). Present the users with all the labels in the database using the database metadata call db.labels() that returns what labels exist in the graph, except for the _Bloom_Perspective_ label because it does not hold any user data:// Parameter: $label Data type: String values: Cypher QueryCALL db.labels() YIELD labelWHERE NOT label IN [ _Bloom_Perspective_, _Bloom_Scene_ ]RETURN label5️⃣ Present Only Numeric Properties As a Query Option. Get the $property parameter value (‘b’ in the image). Use the metadata database call db.schema.nodeTypeProperties() to return any property for the nodes that have the label defined in the $label parameter and are numeric values.// Parameter: $property data type: String values: Cypher QueryCALL db.schema.nodeTypeProperties() YIELD nodeType, propertyName, propertyTypesWHERE nodeType =  :`  + $label +  `   AND propertyTypes IN [[Long], [Double]]RETURN DISTINCT propertyNameThis example restricts the search to numeric values to reduce the code shown in this blog. The dynamic search…” example below and the corresponding blog github repo code has logic to handle conditionals for different data types. For example, this search phrase parameter Cypher does not deal with string query conditionals, such as STARTS WITH conditions.6️⃣ Present Conditional Operator. Pick a $condition parameter (‘c’ in the image). Only the valid conditionals for numerics are returned here.// Parameter: $condition data type: String values: Cypher QueryUNWIND [ = ,  <> ,  < ,  > ,  <= ,  >= ,  IS NULL ,  IS NOT NULL ] AS opXRETURN opXIS NOT NULL and IS NULL can be used to test a property’s existence it could be switched to use [NOT] exists(<node.property>) with a different search phrase query pattern.7️⃣ Enter A Value To Query On. Free-format $value to search for (‘d’ in the image). No Cypher is needed and the return value is String. The node properties to include in the main query is limited numeric values as picked up by the call db.schema.nodeTypeProperties() in step 5️⃣.➡️ the Last step is to save the query and run it using the Bloom search box.As you think about creating your own dynamic search phrases, keep in mind that properties in a Label Property Graph can be unique to each node or relationship. This means that two different nodes can have a property with the same name but different data types. It can be valid to have a node with the property born = 1997 and another with born = ‘St. Mare’, with or without the same label(s). If so, then maybe your use case would call for specifying the data type being queried.👉 Dynamic Labels and Search on Multiple Properties with AND/OR Condition 👈The next logical step is to expand on the above conditional logic example to include choosing a property, setting a conditional, then choosing AND/OR condition for a second property selection. This search phrase also presents the user with the proper conditional tests based on the data type of the property being tested.Person born property IS NULL and name CONTAINS ‘J’A few things to note on how this search phrase was constructed when you look at the code in the file srchPhrs_3_labelMultiProp.cypher in the github repo:Spaces are a delimiter for the Bloom search box, which makes completing parameter terms with spaces, such as IS NOT NULL, tricky for the user to navigate. The approach used here for parameters with spaces is to have them replaced with an underbar (e.g. IS_NOT_NULL). Personally, I think it’s easier to read the terms in the search box when clauses with spaces have underbars, such as IS_NOT_NULL and ENDS_WITH. The underbars for these phrases are replaced with spaces when the final string passed to apoc.cypher.run().Search phrase completion requires that each parameter have a value, and the conditional terms IS_NULL and IS_NOT_NULL do not have a target value. This is addressed by breaking the IS_ conditions into two parts. The first parameter can be IS_, which allows testing in the second parameter to present the user with NULL and NOT_NULL.The main query is a regular Cypher query, so string data types used as test values must be enclosed in quotes (the J in the animation above). This is different from the typeahead functionality in the Bloom search box.👉 Finding Multi-Label Nodes 👈A scenario that is occasionally asked for is to be able to search for nodes with multi-label combinations. To show this, the movies database has been enhanced to create labels on :Person and :Movie nodes as described in the github repo readme with the code in the 0_2_enhanceMovie.cypher file:// New labels to make things interesting  // A :Person labeled node can be:   //   Rich  //   Famous  //   Rich and famous  //   Neither rich or famous  //  // A :Movie labeled node can be:  //   Famous  //   Or notThe query that drives this search phrase does not return a single label option because Bloom already does this. It also does not allow for invalid label combinations a node with a :Person and :Movie label combination is not valid in the movie database and is excluded from the options presented to the user via the Cypher WHERE NOT clause.// Parameter $label type String output Cypher Query// Note: logic in the cypher query removes invalid label // combinations for the enhanced movies graphCALL db.labels() YIELD label as labelWHERE label <> _Bloom_Perspective_WITH  collect(label) as labelsWITH labels, size(labels) as nbrLabelsUNWIND apoc.coll.combinations(labels, 2, nbrLabels) AS labelCombosWITH labelCombos  // exclude invalid label combinationsWHERE NOT (Movie IN labelCombos AND Person IN labelCombos)  AND NOT (Movie IN labelCombos AND Rich IN labelCombos)WITH labelCombosWITH reduce(x = , lab in labelCombos | x + : + lab +  AND  ) AS whereClauseRETURN left(whereClause, size(whereClause) - 4) as whereClause👉 One Last Thing. Show the Graph Database Schema 👈It’s always useful to see what types of labels, relationships, and properties are in a graph, which can easily be done with db.visualize.schema() query in a search phrase:CALL db.schema.visualization() YIELD nodes, relationshipsUNWIND nodes as nodeWITH node, relationshipsWHERE any(lbl IN apoc.node.labels(node) WHERE NOT lbl IN [_Bloom_Perspective_, _Bloom_Scene_])RETURN collect(node) AS nodes, relationshipsResult of :Schema search phrase🤔 What Else Might Be Done?Allow search phrases that that work with alternative data structures, such as lists.Use aggregate functions to enable searching for minimum, maximum, averages, etc. of properties.Use predicate functions such as any() or all() for extended AND/OR query patterns.Hopefully, more examples could be added or suggested to the github repo for this post if anyone has other ideas.👩‍💻 Github / Setup If You Want to Play Along in Your Own Bloom EnvironmentThe Cypher statement files referenced in this blog can be found in the github repo:dfgitn4j/bloom-dynamic-cond-searchThis repository contains the supporting queries for the medium blog. The queries referenced in the blog are…github.comA few things need to be available and done to run the examples below in your own Bloom environment:Have access to Neo4j Bloom through Neo4j Desktop or a server install.Create the movies database running the :play movies gist in the Neo4j Browser, or running the Cypher in the file 0_1_creatMovieDB.cypher.Add new node label combinations by executing the Cypher in the file 0_2_enhanceMovie.cypher if you want to run the multi-label combination search phrase example.Import the perspective Bloom conditional and dynamic queries.json into Bloom by clicking on Import Perspective” from Bloom’s Perspective Gallery.💭 Tips and TechniquesA few tips about working with Bloom and the constructs presented in this blog:The Bloom search box is very aggressive looking for relevant search phrases and graph patterns as users type. Having unique wording in a search phrase definition and description reduces the number of search phrases being presented to the user.Consider adding aLIMIT clause to dynamic queries generated using the technique described in this post that matches Bloom’s Node query limit” parameter.Metadata commands such as the db.labels() used in search phrases will return Bloom Categories (labels in Neo4j graph database speak) that have not been included in the current Bloom Categories definition. Unwanted categories can be explicitly excluded in the search phrase Cypher.Case insensitivity does not work the same as with the Bloom case insensitive search option. Case insensitivity must be coded in the search phrase Cypher statements.Thank you for your time, and please email the author or add code and suggestions to the github repo for this blog if you have other useful search phrase patterns. I will be adding some in the future that are bouncing around in my when I have spare time and have functional consciousness” spot in my b̶r̶i̶a̶n brain.;Jun 30, 2021;[]
https://medium.com/neo4j/5-ways-to-explore-the-global-graph-celebration-day-attendee-graph-7bd2ed7768dc;William LyonFollowFeb 13, 2020·4 min read5 Ways To Explore The Global Graph Celebration Day Attendee GraphGet Involved with GGCD And Find Connections To The Community!The second annual Global Graph Celebration Day is only two months away! In this post, we want to show you how you can explore the GGCD Attendee Graph to find connections in the community. We have more in common than you think!What is Global Graph Celebration Day?To honor Leonhard Euler, the Swiss mathematician and inventor of graph theory, the Neo4j community and graph-enthusiasts all over the world gather and host events to celebrate the day of his birth, April 15th. Check out the blog post for information on how to find an event near you or host one yourself!Last year was the first official GGCD and there were more than 60 events on six continents!What’s The GGCD Attendee Graph?The data model for the GGCD Attendee GraphWhen you RSVP to a GGCD event you can choose to optionally share what technologies you use, what industry you work in, your interests, and more. This information is anonymized and stored in a public Neo4j Aura instance so that the graph community can see things we have in common, learn how others are using graphs and Neo4j, and discover new technologies and tools that we might be interested in.As part of Global Graph Celebration Day, we want you to explore the GGCD Attendee Graph and we’re going to feature the top-10 most-interesting queries, visualizations, or apps built around the attendee graph in one of our GGCD re-cap blog posts.Here are a few ways you can get started exploring this data:1) Query In Neo4j BrowserFor example, here’s the JavaScript graph of GGCD attendees so far!The data is available in a public Neo4j Aura instance that you can access here:https://88f0bc82.databases.neo4j.io with the username events and password eventsIn Neo4j Browser you can write Cypher queries to query the dataset and visualize the results. Use the CALL db.schema() command to see the data model which can give you an idea of the type of queries you can write.Here’s a Cypher query to get you started:MATCH (t:Tool)<-[:USES]-(p:Person)-[:INTERESTED_IN|:ATTENDING]->(o)WHERE t.name =  Javascript RETURN *Can you find the other attendees of the event you’re attending? What is the most common programming language used by the other attendees?2) Build A Graph VisualizationThis graph visualization of the GGCD Attendee Graph was created using the 3d-force-graph library. See it at https://globalgraphappreciationday.com/Whether with neovis.js, 3d-force-graph, or Neo4j Bloom graph visualization tools can help you make sense of and interpret graph data at a glance.For ideas on how to get started with creating visualizations with data from Neo4j, check out some of the popular blog posts on the Neo4j Developer Blog about graph visualization3) Query Using iPython NotebooksA sample iPython notebook pulling GGCD Attendee Graph data into a pandas DataFrame. Try it hereiPython notebooks allow us to combine data from Neo4j with popular Python data science tools like pandas, matplotlib, scikit-learn, NLTK, and more.We put together a starter notebook that shows how to use the Python Neo4j driver to query the GGCD attendee graph, load it into a pandas DataFrame and create a simple chart of the most common use cases using matplotlib. It also shows how to use the TextBlob NLP library to find keywords and entities in event descriptions:Try it here:https://colab.research.google.com/github/johnymontana/ggcd-samples/blob/master/notebooks/ggcd.ipynb4) Query Using GraphQLGraphQL Playground for the GGCD Attendee Graph GraphQL API. Try it here.There’s also a GraphQL API exposing this data which you can query at globalgraphcelebrationday.com/graphql This GraphQL API uses neo4j-graphql.js to easily build a GraphQL API backed by Neo4j.GraphQL makes it easy to express data fetching logic in a single request to the backend. You can learn more about GraphQL in the Neo4j developer guides.For example, here’s a query to find all events within a 10km radius and show some information about the interests of the attendees:{  Event(    filter: {      point_distance_lt: {        point: { latitude: 47.4979, longitude: 19.0402 }        distance: 10000      }    }  ) {    name    location    attendees {      name      works_in {        name      }      uses {        name      }    }  }}5) Build An AppEither using one of the Neo4j language drivers or the GraphQL API, build an application that queries Neo4j and shows the results. You can build an app that makes it easier to find friends with similar interests at the GGCD events or an application to find events near you!For examples of applications you build that use GraphQL, check out the GRANDstack starter UI implemented in React or Angular.Share What You’ve Built!Whatever you come up with, put it online (Github gist, google doc, app URL, github repo, etc) and submit your URL with a short description of what it is below.(Button will route to Google submission form);Feb 13, 2020;[]
https://medium.com/neo4j/announcing-neo4j-auradb-free-971ed09113c3;David AllenFollowNov 9, 2021·6 min readAnnouncing Neo4j AuraDB FreeWe’re excited to announce that everyone can now use Neo4j AuraDB Free, and get started without a credit card in a number of cloud regions. AuraDB Free is ideal for people who are getting started learning graph, prototyping, and doing early development.Get Started Now for Free!Thousands of users have been using Neo4j for years to help with connected data problems, where the relationships between data items matter as much as the items themselves. Our community has used the technology for COVID-19 tracking, building full-stack GraphQL apps, and even by investigative journalists in the recent bombshell Pandora Papers!Graph is a part of any well-rounded developer’s toolchain, so we want to make it as easy as possible for you to start your own projects. In this article, we’ll walk you through AuraDB Free so you can get started right away:Create your first databaseExplore the sample dataConnect to the database from your programExpand your graph horizonsTake your next stepsCreate Your First DatabaseCreating a free AuraDB instance is easy. First, go to the Neo4j Aura console and sign up for an account if you don’t already have one. You can either use a social login, or an email and password followed by verifying your password. Once your account is set up, create your first database as shown below.If you’re just learning about Neo4j, make sure to choose Learn about graphs with a movie dataset.” This is the easiest way to start with pre-loaded data so you can just explore and learn what Neo4j has to offer. If you’re already a Neo4j user, feel free to load your own dataset or start with a blank database.Once you click Create Free DB” you’ll be given the password for your instance. Be sure to save it! It will not be shown again, and Neo4j doesn’t store your password. Your database will now be provisioned, which will take a few minutes. Neo4j Aura will ask you a few questions to help set up your account, which includes a short video about what Neo4j is, and how native graph databases are different.When your database has a green dot by it and shows as running like this, you can use the Open” button to open Neo4j Browser.Explore the Sample DataIn my case, I chose the Movies” sample dataset, so I already have data loaded in! On the left hand side of Neo4j Browser you will see a guide that describes the dataset and has a number of sample Cypher queries that will let you explore and visualize the graph. All you have to do is click the queries in order to put them into the command window at the right. You can then click the Play” button in the command window to run the query.Looks like we’ve got some data and everything’s working well. On the left hand side of Neo4j Browser, you’ll find Next” and Previous” buttons to walk you through the pages of the start-up experience. In this guide, it takes you through the basics of what a graph is, and how to use the Cypher language to create and read data in graphs.Connect to the DatabaseNeo4j Browser lets you run any query you like, interactively against the database and visualize the results. But when you’re ready to move forward building your own application, you’ll need to connect a program to it. Neo4j offers drivers for many popular programming languages, which you can install quickly and get started.Back inside of the Aura console, let’s click on the database we created, and select the Connect” tab. Here, you’ll find easy copy/paste code samples for how to get started with most languages.I’ve already created a Replit for you of this sample code in Python, which you can go run directly. Behind that code link, you will find an IDE where you can click Edit in Workspace.” Check the connection details at the very bottom of the python file, and fill out the connection URI, the username (neo4j), and the password for your database. Right from there, you can run your first python program against Neo4j AuraDB.Here’s what that code looks like when it runs successfully inside of Replit.And remember:This sample program creates two people in the database named Alice and David, and a relationship between them. Once you’ve run the program, you can go back to Neo4j Browser and verify that the data is there with a simple Cypher query:Now that we’ve got a working program running against the database, it’s up to you to determine what’s next with your own data.Expand Your Graph HorizonsThe next thing most people will want to do is import their own dataset into AuraDB. The easiest way to do that is with the Cypher LOAD CSV command, which will take a CSV file from any public URL and break it into lines for you. The most common option is to host a CSV file on GitHub, S3, or similar source and load directly from there. You can then use the Cypher language to structure that data into your graph model.Here’s a specific example you can copy/paste into your AuraDB Free instance. The simplest graph is one that has two kinds of things, connected to one another. Here, we’ll load data on more than 11,000 world cities. You can run this query directly in your AuraDB Free instance:LOAD CSV WITH HEADERS FROMhttps://storage.googleapis.com/meetup-data/worldcities.csv AS lineMERGE (city:City { name: line.city })MERGE (country:Country { name: line.country })MERGE (city)-[:IN]->(country)What’s happening here? It’s simple!Loads the CSV from a public URL as the line” variable (each line in the file)Ensures a city exists with the value of the city” column on the lineEnsures a country exists with the value of the country” columnEnsures a relationship between that city and country is in the graphHere’s what the results look like, for all of the cities in Spain:For more information on how to do this with your data, check the Neo4j documentation which provides worked examples to use, and the Neo4j Community is always helpful as well.Take Your Next StepsBecause this is all cloud native, you can do everything we did here with zero software setup, installed software, or anything on your computer, right now. It starts with creating a new database on Neo4j AuraDB.Get Started Now for Free!We are very excited to see what our wonderful graph community does with AuraDB Free — whether it’s a hobby project, an internal application for your job, or investigative journalism.Happy graph hacking!;Nov 9, 2021;[]
https://medium.com/neo4j/neo4j-driver-best-practices-dfa70cf5a763;David AllenFollowFeb 16, 2022·7 min readNeo4j Driver Best PracticesNeo4j & Neo4j Aura support Python, JavaScript, Java, Go, and .NET. Additionally, community support is available for other languages such as PHP and Ruby. In this article, we’ll cover some best practices for using Neo4j drivers in your application. The code examples will all be in Python for simplicity, but the basic practices outlined apply to all languages supported by Neo4j.We will go from the simplest, to the more complex as the article goes on.Connect Using the neo4j+s:// scheme Whenever PossibleFor newer Neo4j versions after 4.0, neo4j+s://is usually your best bet. It works with secured single instance databases, and also with clusters. The +s means that certificate validation will be performed, and that your connection will be secure.Available URI connection schemes for driversThe bolt://connection scheme connects to only one machine, and will usually not work like you expect when connecting to Neo4j Aura or any other clustered setup.Use Very Recent Versions of Neo4j DriversParticularly for Neo4j Aura environments, Neo4j has been making big improvements in query performance and speed. If you have an older driver, make sure to upgrade it for best connectivity, at least to the 4.4 series of drivers.At the time of writing: If you are using Java or JavaScript, make sure to use driver ≥ 4.3.6. For Python & Go, use at least ≥ 4.3.4 for .NET ≥ 4.4.0. These releases include a server hints” feature that sends server keep-alive messages and keeps connections open and error free in a wider variety of network situations.Verify Connectivity Before Issuing QueriesAs soon as you create a driver instance, you should verify connectivity, like this:from neo4j import GraphDatabaseuri =  neo4j+s://my-aura-instance.databases.neo4j.io driver = GraphDatabase.driver(uri, auth=( neo4j ,  password ))driver.verify_connectivity()Normally, the Neo4j driver creates connections as needed and manages them in a pool by verifying connectivity at the very beginning, it forces the driver to create a connection at that moment. If the address, username, or password is wrong, it will fail immediately, and this is useful.Beginners will sometimes report errors in code that’s running a Cypher query, when in fact the error was that the connection wasn’t established until the cypher query was run, and it is in fact a connection error.Create One Driver Instance Once and Hold Onto ItDriver objects in Neo4j contain connection pools, and they are typically expensive to create — it may take up to a few seconds to establish all of the necessary connections and establish connectivity. As a result, your application should only create one driver instance per Neo4j DBMS, and hold on to that and use it for everything.This is very important in environments like AWS Lambda and other types of serverless cloud functions, where, if you create a driver instance every time your code runs, it will incur a performance penalty. In some environments (like AWS Lambda), you may also want to look into changing connectivity settings to reduce the number of connections created, to reduce cold startup time. Drivers are generally heavyweight objects that expect to be reused many times.Sessions, on the other hand, are cheap. Create and close as many of them as you like.Use Explicit Transaction FunctionsThe simplest API is sometimes not the best. Here is a common mistake people make that sometimes bites them later. You can do this with Neo4j drivers, which creates what’s called an Auto-commit Transaction”:with driver.session() as session:    session.run( CREATE (p:Person { name: $name }) , name= David )The trouble with this is that Neo4j drivers don’t parse your Cypher, and because Neo4j clusters use routed drivers, typically this kind of setup will always send all of your queries (both reads and writes) to the leader of your cluster. Effectively, you won’t be using 2/3rds of your 3-member cluster because all queries are going to the same machine.Another thing that’s less good about using session.run and Auto-commit transactions is that you lose control over the commit behavior. Sometimes you need to run multiple cypher queries which all commit at once or all fail (rather than committing one by one individually).You can not do that with autocommit.The better practice is to use an explicit transaction function”, like this:def add_person(self, name):    with driver.session() as session:        session.write_transaction(create_person_node, name)def create_person_node(tx, name):    return tx.run( CREATE (a:Person {name: $name}) , name=name)Two things to notice here:The Cypher work” was encapsulated in create_person_nodeWe called session.write_transaction which tells the driver that it’s a transaction containing write operations. When we use session.read_transaction for reads, the driver can spread the work out among cluster members.Use Query Parameters Wherever PossibleIn the previous example, we used a query parameter called $name to pass a variable into a Cypher query:def add_person(self, name):    with driver.session() as session:        session.write_transaction(self.create_person_node, name)def create_person_node(tx, name):    tx.run( CREATE (a:Person {name: $name}) , name=name)You should use these parameters wherever you’re substituting data into your queries, and never use string concatenation. For example, the following is poor practice:def create_person_node(tx, name):    tx.run( CREATE (a:Person {name: %s})  % name)Use of query parameters gives several advantages:Safer code protects against security flaws like Cypher Injection AttacksIt shows the database fewer query forms, allowing the database to compile fewer queries and make them run faster, rather than every new query being a new string the database hasn’t seen before.Process Database Results Within Your Transaction FunctionLet’s take a look at the following bit of code, and in particular the bold portion. We’re running a transaction to get Alice’s friends. Notice how instead of returning the Neo4j result object, the code is processing each record in the result into a new data structure called friends and returning that.from neo4j import GraphDatabaseuri =  neo4j://localhost:7687 driver = GraphDatabase.driver(uri, auth=( neo4j ,  password ))def get_friends_of(tx, name):    friends = []    result = tx.run(           MATCH (a:Person)-[:KNOWS]->(f)         WHERE a.name = $name         RETURN f.name AS friend           , name=name)    for record in result:        friends.append(record[ friend ])    return friendswith driver.session() as session:    friends = session.read_transaction(get_friends_of,  Alice )    for friend in friends:        print(friend)driver.close()In Neo4j drivers, generally the result that comes back from a transaction function isn’t available outside of the scope of the transaction function. So this code would fail:def get_friends_of(tx, name):    result = tx.run( MATCH (a:Person)-[:KNOWS]->(f)                            WHERE a.name = $name                            RETURN f.name AS friend , name=name)    return resultwith driver.session() as session:    results = session.read_transaction(get_friends_of,  Alice )    for record in result:        print(record[ friend ])The reason that it would fail is that the database cursor which fetches the results isn’t available outside of the scope of the transaction function get_friends_of.Understand Bookmarking and Causal Consistency If You See Inconsistent ReadsNeo4j drivers have a concept called bookmarks that allow you to read your own writes. Suppose you did two queries in a sequence:CREATE (p:Person { name: David })MATCH (p:Person) WHERE name = David RETURN count(p)You’ll expect to get the answer of 1 from the second query. But let’s think through how this actually happens in a real cluster:The first query succeeds when a majority of cluster members have acknowledged the writes. In an AuraDB instance, that might be 2 of the 3 cluster members have your writeIf the second query happens (by chance) to be routed to the third member (that doesn’t have the write yet) it could return the answer 0 because the writes haven’t caught up yet!Fortunately there’s an easy fix for this. If you use the same session object to do the read as you did for the write, it will always work as you expect. This is because the server returns a bookmark as of the write. The session object uses bookmark chaining” and all subsequent reads are done as of that bookmark in the database.Let’s distill this into super-simple best practices:If you are doing a sequence of operations and you need the later operations to be guaranteed to read the writes from earlier operations, simply reuse the same session object, and all of this is automatically taken care of for you, because sessions chain the bookmarks they receive from each query.If you need to coordinate multiple different programs (and program B needs to read program A’s writes) then program A must pass its write bookmark to the other program’s session, so that B can read the database’s state as at least of that bookmark to guarantee consistency to that moment in time.Most of the time you won’t need to access or worry about bookmarks, but they’re easy to access:my_bookmark = Nonewith driver.session() as session:    results = session.write_transaction(write_bunch_of_people)    my_bookmark = session.last_bookmark()with driver.session(bookmarks=[my_bookmark]) as session2:    # Do some reads guaranteed to include writes which    # happened above.For more information on why Neo4j works this way, read into Causal Consistency.SummaryIf you use these practices, your graph code will be safer, more secure, more performant, and also more portable between graph environments, whether you’re running on a Neo4j Desktop install, an AuraDB Free database, or scaling all the way up to large AuraDB Enterprise clusters.;Feb 16, 2022;[]
https://medium.com/neo4j/how-to-build-a-neo4j-application-with-node-js-63d3b7f671a2;Adam CowleyFollowDec 3, 2021·3 min readHow to Build a Neo4j Application with Node.jsAre you a Node.js developer tasked with learning Neo4j, or are you just interested in learning something new?If this sounds like you (or even if it doesn’t), then the Building Neo4j Applications with Node.js course on GraphAcademy is for you!This is a course I’ve been working hard on for the past month, and I’m happy to share it with you now.What is GraphAcademy?Neo4j GraphAcademy is our free, self-paced, hands-on online learning platform.We on the Neo4j DevRel team have been working hard to build a brand new website and course curriculum that provides a fun, engaging, and hands-on learning experience. You can read more about the changes that we made on the Neo4j Developer blog.About the CourseIn the course, you will learn all about the Neo4j JavaScript Driver by adding it into an existing project, and then modifying a set of services to interact with Neo4j.You will learn all about database sessions, read and write transactions, and see how to execute a Cypher query and handle results.At the end of the course, the accompanying UI will be populated with data held in a Neo4j Sandbox instance.AssumptionsWe assume that you have prior working knowledge of JavaScript, NPM, and the Node.js ecosystem. We also assume that you have some previous experience working with Neo4j.If you have no prior experience with Neo4j, you can follow the Beginners Learning Path. There are four courses designed to teach you the basics in approximately six hours.What You Will LearnThe course is split into three modules, which will guide you from complete beginner to expert.In the first module, Project Setup, you are guided through setting up the project and registering environment variables so that the API can communicate with the Neo4j Sandbox instance created during the enrollment process.In the second module, The Neo4j JavaScript Driver, you will learn all about the Neo4j Driver and how it should be used within an application. This includes installing the neo4j-driver dependency using npm, building a connection string, creating a Driver instance using the driver() method, and verifying that the credentials used to create the driver instance were correct.The third module, Interacting with Neo4j, teaches you about the Driver lifecycle — how to open new database sessions, execute read and write transactions, and how to consume the results. The module also teaches you how to handle potential errors thrown by the driver. Throughout this module, you will be challenged to modify the existing project and run tests to verify that the code has been written correctly.The final module allows you to practice the skills covered in the previous three modules by implementing the remaining features.At the end of the course, you will have a working API that serves data to the SPA included in the project. The API will allow you to register a new user and sign in, browse the movie catalog, rate movies, and create a list of favorite movies.The course is free of charge and there is no time limit to complete the course!Enroll NowYou can enroll now, for free, by registering for GraphAcademy and clicking Enroll Now on the Building Neo4j Applications with Node.js page.If you have any comments or feedback you can get in touch with me on Twitter or use the feedback widgets on each page.Good luck!;Dec 3, 2021;[]
https://medium.com/neo4j/eventing-graph-data-neo4j-rabbitmq-e16b91274fbc;Vinodh SubramanianFollowMar 31, 2019·5 min readEventing Graph DataWith Neo4j & RabbitMQGo — Neo4j — RabbitMQWhat is Eventing?Eventing is just notifying someone about something that happened which they are interested in” — too simple isn’t it. The kind of eventing which we are talking about is an asynchronous integration pattern for microservices. To be specific, it’s called the Data Event Exchange.This is a scenario where one or more microservices subscribe to data change events (change data capture - CDC) directly from a NoSQL store and notified upon data changes.Any interaction with the store which triggered a data change, like data being persisted, existing data being modified or deleted, will be notified. Events which are generated contain most preferably the updated data itself.What is a Trigger?Triggers can be considered a mechanism who does some action based on an event.So in a database, there are triggers which do the same as stated before. You can enable them to make database do something other than just accepting data changes passively. As are talking about eventing, we can make the database react to events as well.Enough of theories. Now let’s get to actual work.Our use case is, we’ll create a node called DB along with few properties of it, in Neo4j.// cypher query which creates the DB node with its propertiesCREATE (a:DB) SET a.id = 0, a.name =  Redis , a.type =  Cache  RETURN a.idA Go application which will receive the data change event from RabbitMQ triggered by Neo4j.//Event received from RabbitMQ by Go applicationReceived an event: { Labels :[ DB ], Properties :{ name : Redis , id : 0 , type : Cache }}Following is the steps to achieve this.Go application which creates data in Neo4jOn node creation, Neo4j will trigger an event to RabbitMQGo application, again, which will consume the same events from RabbitMQ.Go application to create a node in Neo4jWe have already done this in my previous post. If you’ve not read it please take a look into it. I’ve modified the code a bit. Just the cypher query which creates the node.Go method to execute a create node eventThe complete source code is available at createnode.goNeo4j to trigger event to RabbitMQHere comes the interesting part. How does trigger work?Triggers require two pieces of information:Condition (which event should the trigger fire on?)Action (what to do when the trigger fires?)Triggers are enabled in Neo4j through APOC(Awesome Procedures on Cypher), a clear and well-documented extension library.Go ahead and install the plugin to your graph as documented in the readme. And add the following configuration to neo4j.conf file.apoc.trigger.enabled=trueThis will enable the trigger mechanism. Let’s continue with our trigger.CALL apoc.trigger.add(createEvent,    UNWIND apoc.trigger.nodesByLabel({assignedLabels}, DB) AS node CALL rabbitmq.event(labels(node), node) return count(*) ,    { phase: after })// Trigger to be executed in our Neo4j GraphWait a minute… What is this now?This is how we write an apoc trigger. Let’s break this out.The first argument ‘createEvent’ is the name of the trigger. Which can be later used to check the status of the trigger or to disable it completely.Second is the cypher query which gets executed as part of this trigger. Let’s break it even furtherapoc.trigger.nodesByLabel({assignedLabels}, ‘DB’)” command let’s you find out all the nodes which got recently created or assigned by the label ‘DB’. Since it returns a list of nodes we use UNWIND” to iterate through them, to execute the proceeding cypher statement, with a variable name node”.CALL rabbitmq.event(labels(node), node) return count(*)”, a procedure call made to send out event information about the node which got created.The cypher statement returns the count of ‘DB’ nodes which got created and passed on.As per the date when this blog is written, Neo4j doesn’t support streaming data to RabbitMQ by default. For Kafka users, Neo4j has recently launched support for streaming events.Whereas for RabbitMQ folks, we should be hanging for bit more until there is support from Neo4j officially. But that doesn’t stop us from building what we want?We can achieve this feature via User Defined Procedures. For today’s task, I’ve quickly written some code to create a procedure which will receive the node details, labels and properties, and event it out to RabbitMQ Queue. Following two code snippets shows how it is done. They are self-explanatory.Neo4j user-defined procedure to receive node label and property details.Simple RabbitMQ connection. It’s not optimised for production deployment.Clone/Download the source code. Change the configuration of RabbitMQ if needed and build a jar. Place it in the Neo4j Plugin folder and restart your graph. That makes the procedure available to your Neo4j instance.The final argument { phase: ‘after’ }” of the trigger allows you to control when your trigger action fires. We can choose from 3 possible lifecycles of transactions, — before”, after”, or rollback”. Since we want our event to be triggered once the node got persisted, we use after”.Okay, go ahead and execute the apoc.trigger.add command in your Neo4j graph.Go application to receive the RabbitMQ eventsThis one is straight forward. I’ve used the HelloWorld tutorial by RabbitMQ for GoLang to do this. Made a bit of tweak to the code, queue name and queue declaration properties.Establishing a RabbitMQ connection in go application.The complete source code is available at receivecreateevents.goLet’s see them at workMake sure the Neo4j application and RabbitMQ service are up and running as per your configuration. Run the go application in two different terminalsgo run createnode.gogo run receivecreateevents.goIf everything went well. You should getIn createnode.go terminal-------------------------Eventing Graph Data - Neo4j & RabbitMQNode Created With ID : 0In receivecreateevents.go terminal----------------------------------[*] Waiting for events. To exit press CTRL+CReceived an event: { Labels :[ DB ], Properties :{ name : Reddis , id : 0 , type : Cache }}So that’s it for this post. Feel free to comment on the content.;Mar 31, 2019;[]
https://medium.com/neo4j/creating-charts-from-your-graphs-2f5b4e86fd6c;Adam CowleyFollowNov 18, 2020·5 min readCreating Charts from your GraphsOver the past few years I’ve spent a lot of time trying to define the difference between a Graph and a Chart. But I’m here today to muddy the waters once again.I’d like to introduce a Graph App that I’ve been working on for the past few weeks, simply called Charts. I’ve designed the app to be useful to both beginners to Neo4j or more experienced users who would like to make their graph-shaped data more understandable to the people around them.As the name suggests, Charts allows you to create sets of charts from the data held in a Neo4j Database. The Graph App will take the raw results from a Cypher query and convert them into the correct format for the applicable chart, meaning you only have to do a minimal amount of manipulation to the data within your query.For example, the Metric report will take the first value of the first row and display it in on screen, treating it as a number.The range of Bar reports (either horizontal or vertical orientation, and stacked or unstacked) all require a set of results with a key, x value and y value and are converted into the correct format by the underlying React component.Visual Query BuilderFor those that aren’t so familiar with the Cypher query language, or those that aren’t interested in writing code at all, the app includes a Visual Query Builder. The query builder inspects your graph structure and provides links to build up a graph pattern without writing a line of code.After selecting the starting label for the pattern (eg. Person or Movie), the right hand panel will display a list of relationships going into or out of that label. You can continue to click through the graph pattern until you have built up your pattern.As you build the query, the raw Cypher query will show up in the bottom left hand corner of the screen. You can copy and paste this into Neo4j Browser to preview the results as you go.Once you have built up the graph pattern, you can click the Where Tab on the right hand panel to add predicates to your where clause. You can select from a range of conditions including equals, starts with, ends with, greater than or less than, and if it’s a negative predicate, tick the NOT checkbox.As you add these, a set of :param statements will be appended to the top of the query — you should copy these one by one into Neo4j Browser before running your query.After you have built up the query, you can use the Return Tab to select which properties you would like to return. You can provide an alias to the field and also select from a number of aggregate functions including sum(), count(), and collect().You’ll end up with a generated cypher query similar to the following:MATCH (n1:Actor)-[r1:ACTED_IN]->(n2:Movie)-[r2:IN_GENRE]->(n3:Genre)WHERE (n1.name = $n1_name AND NOT r1.role = $r1_role)RETURN n2.title AS title, collect(n3.name)DashboardsDashboards are a way of grouping and displaying data. You can create as many dashboards as you need to group different pieces of information together.You can click Add Dashboard to create a new Dashboard, then click the Add Report button in the top right hand corner to start adding data to the dashboard.Each dashboard can then contain a number of Reports. Reports use the current Neo4j Connection to return data in a number of formats — signified by the report Type. These can range from simple displays like a Metric (a single figure) or a Table of results to more complex Chart types including Line, Bar and Radar charts.The Source of each report can either be a Raw Cypher Query that you type directly into the box, or a Query that has been built in the query builder.If the should be run on any database other than the default database (either the default as configured in neo4j.conf or as specified at login), you can define this in the Database field. You can also choose how many columns (between 1 and 4) the report spans visually in the dashboard.Help TabIf you get stuck at any point, you can click the Help link in the top right hand corner of the screen. This page provides a list of the available chart types and an example cypher query for populating the chart.Sharing Dashboards and ReportsAs you start to create queries, dashboards, and reports in the charts graph app, you may notice a couple of json files appear in the Files section of your current active project — graphpanel-dashboards.json and graphpanel-queries.json. As you make changes to queries or dashboards, the changes will be saved into these files ready for the next time you open the graph app.You can share these files with your friends and colleagues so they see the same view of the data as you have created.Technical InformationIf you’re not interested in the technical aspects of this graph app, you can skip straight to the Install Now section.The graph app is built using React, taking advantage of the use-neo4j hooks for React. This provides the login form you see when you first open the app, and the useSchema hook to build a picture of the current data model in the query builder. The useReadCypher hook is used by the report components to query the database.The @neode/querybuilder package is used to convert the query tree into a working Cypher query.The charts themselves are all provided by Nivo, a set of responsive dataviz components built on top of React and D3.If you are interested in building your own Graph App using React, you can use this repository to get started.Install Now…If you would like to use this graph app today, you can visit charts.graphapp.io to try the app or install it straight to Neo4j Desktop from install.graphapp.io.If you have any comments or feedback, feel free to get in touch on Twitter. You can also use the feedback form in the bottom right hand corner of the Graph App or create a GitHub issue in the repository.Happy Charting!;Nov 18, 2020;[]
https://medium.com/neo4j/will-it-graph-identifying-a-good-fit-for-graph-databases-part-3-9cd5913f5f95;Ljubica LazarevicFollowAug 12, 2021·4 min readWill It Graph? Identifying A Good Fit For Graph Databases — Part 3Graph Database as a General-Purpose DatabaseLast time, we tackled the differences between relational databases and graph databases as well as the good and poor fits for graph databases. This week, we’re going to discuss how you can use a native graph database as a general-purpose database. Also, check out the GraphStuff.FM podcast!Many applications — especially in enterprises — aren’t backed by a single database. Oftentimes you have to pick your own tech stack with many different systems working together. Think of a general-purpose database as your first and your main database, which you’re going to use as your source of truth as you’re building out your application.Ecosystem IntegrationThe first thing you should think about when determining which database would be a good fit for your general-purpose database is the ecosystem of framework and language integrations that exist for this database. With the tech stack that you have chosen for your application in mind, you need to ask: Is there an integration with this database and the technology that I’m interested in using? How easily can it be integrated into my tech stack? That is going to be one of the most limiting factors.So if we think about this for a Neo4j graph database, it’s important to understand what ecosystem of integrations exist for Neo4j. There is quite a good range of language drivers out there that you can use to integrate Neo4j into your application. So what languages can I use to integrate Neo4j into my application. There are also all sorts of tooling at the framework level that make it easier to integrate Neo4j into your architecture as a first database. These are things like the GraphQL integration for Neo4j, the Spring Data Neo4j project, the Django integration for Neo4j and connectors such as the Spark Connector, the Kafka connector, or the JDBC driver for Neo4j.Why Use Graph Database as a General-Purpose Database?Now that we’ve identified whether we have the means and mechanisms to connect up the database with our framework, the next question is: Why would I consider using a graph database as my general-purpose database?The really powerful thing about graph databases is speedy prototyping. If you want to ask your data how you can get away with murder in the quickest possible way, you don’t have to declare a schema. You don’t have to go through a protracted process to quickly start answering questions. The beauty of a graph database is, you just need to think about your data model and loosely think about how your entities are connected to each other. In a relatively short amount of time, you’ll have something that you can start loading data with. You get your data in, and it immediately captures your domain. So it can start answering questions straight away.It figures out what the schema is as you’re putting data into the database. And as you start querying, you’re getting feedback, the answers to your questions. You can pull a project together very quickly.And then, if you want to start looking at how you can optimize how your data is stored, the structure of your data in the database, you can now make changes relatively easily with the data as it stands in the database without necessarily having to get rid of everything and load it back in again. So this is a really powerful way of being able to rapidly prototype projects. You can answer questions quickly, and it gives you the edge of being able to get going as fast as possible.The idea of intuitiveness in a graph model is much more closely aligned with how we think of data, and really especially how we work with data in our applications. It makes that whole process so much faster. We spend a lot of time thinking about building API applications, and oftentimes the way that we interact with an API is really in the context of relationships. So it really makes sense to model, store and query data as a graph, as you’re building out my applications.ConclusionHopefully, we’ve got you all curious and keen to explore this new database paradigm. There are some options for you to check out, and what we would recommend as a starting point is to have a look at the Neo4j Sandbox.We’ve got a blank one that if you are feeling adventurous and want to go and build something based on looking at the various documentations. But for those of you who just want to have a look and have a gentle guided journey, we have examples of recommendations, fraud detection, etc. already built in there. What you’ll find in that is pre-canned data, as well as walkthrough guides. So it will show you the data model, it will show you the queries, and this is a nice way to dip your toe in the water, and just have a look at what’s going on.;Aug 12, 2021;[]
https://medium.com/neo4j/reactive-multi-tenancy-with-neo4j-4-0-and-sdn-rx-d8ae0754c35;Michael SimonsFollowJan 31, 2020·9 min readReactive multi-tenancy with Neo4j 4.0 and SDN/RXThree for the price of one: A movie rental storyDenise Jans on unsplashWe are going to address three topics in this post, and they all deal with new features in Neo4j 4.0 and surrounding ecosystem. We have:Multi-database support in Neo4j 4.0 (Enterprise Edition)Reactive Database, drivers and Spring Integration (All Editions)Role-based access controls (RBAC) (Enterprise Edition).And, as a bonus: Fabric a new way of querying multiple Neo4j databases together.In this post, I use the super secret password that we’ll use in testing Neo4j SDN-RX as well. The secret password is for the user neo4j will be secret.The business domainNeo4j offers the movie graph starter code sample through its browser interface, so we are going to use that. It’s an easy and quick way to generate some data. It contains a set of movies and people, and how those people interacted (acted, directed, etc.) with those movies.Now imagine two rental streaming services that use this data, Videos ‘R Us” and Video City”. Both are tenants of a bigger corporate company, Videokingdom”. Videokingdom runs a new Neo4j 4.0 Enterprise Edition instance and provides each tenant their own database. Both tenants use the movie streaming application provided by Videokingdom. This application provides a reactive, scalable REST-ful backend. We must make sure that each user of the backend is identified as a user of the specific tenant, and database interactions go to the correct backend.Preparing the databasesTo begin, you will need a locally available Neo4j 4.0 installation. You can either use the server version or use Neo4j Desktop. Navigate to the bin folder in the installation directory. Start the instance service as follows:./bin/neo4j startNow, bring up the Cypher Shell with:./bin/cypher-shell -u neo4j -p neo4jOn the initial start, it will ask for a new password. I chose secret . Next, create two databases::USE systemCREATE DATABASE `videos-are-us`CREATE DATABASE `video-city`The first command tells the Cypher shell to switch to the system database. The system database understands the CREATE DATABASE command.Next step, create a role movie_streaming_applicationCREATE ROLE movie_streaming_applicationIt is always a good idea to create a dedicated database user for a web application, so we will create that too:CREATE USER reactive_rental SET PASSWORD secret CHANGE NOT REQUIREDGRANT ROLE publisher TO reactive_rentalGRANT ROLE movie_streaming_application TO reactive_rentalpublisher is a built-in role that allows reads and writes of data but no changes to indexes or constraints.Imagine now that users of Video City are not allowed to see who reviewed a movie because… whatever reason a business had. In Neo4j 4.0, we can use fine grained access control for that. Let’s have a look at the data model to remind ourselves what changes to access we’re going to be making:graph data model for the movies databaseSo we’re going to prevent Video City’s database being able to traverse REVIEWED:DENY TRAVERSE    ON GRAPH `video-city`    RELATIONSHIPS REVIEWED    TO movie_streaming_applicationWhilst we are at it, we can also deny writes to the video-city database, at least for a movie_streaming_application :DENY WRITE ON GRAPH `video-city` TO movie_streaming_applicationThe next step is to get the movie dataset into both databases. You can do that in Neo4j browser via the :play movies command or use the movies.cypher script from SDN/RX. For that post here we stay in the terminal respectively in the command line and use the provided cypher file above. Leave the Cypher shell with :exit , download the above Cypher-Script and run the following two commands, adapting the path to the Cypher file as needed:./bin/cypher-shell -u neo4j -p secret -d videos-are-us \   -f movies.cypher./bin/cypher-shell -u neo4j -p secret -d video-city -f movies.cypherBoth databases for the rental companies are now set up and have data for us to work with. To verify that our access rules are working, try the following. Open the Cypher shell as user reactive_rental (use ./bin/cypher-shell -u reactive_rental -p secret ):use videos-are-usMATCH (m:Movie {title: Cloud Atlas})OPTIONAL MATCH (m) <- [:REVIEWED] - (p) RETURN m, p:use video-cityMATCH (m:Movie {title: Cloud Atlas})OPTIONAL MATCH (m) <- [:REVIEWED] - (p) RETURN m, pWhilst the first query gives us Cloud Atlas and Jessica Thompson, the same query in a different database that has the same initial dataset, gives us only the movie.Setting up the Spring Boot SDN/RX applicationFirst of all, go to the Spring Initializr. As we want to create a reactive, secured web application, you will need to add reactive and security as dependencies. Don’t add Neo4j here, we will add that dependency later on.Here is a link that selects all relevant dependencies on the initializer: multidatabase_movies. Click this and generate the project. Save it, open it in your favorite IDE. First of all, add Spring Data Neo4j⚡️RX as dependency in the pom.xml and make it look like this:<?xml version= 1.0  encoding= UTF-8 ?><project xmlns= http://maven.apache.org/POM/4.0.0  xmlns:xsi= http://www.w3.org/2001/XMLSchema-instance        xsi:schemaLocation= http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd >   <modelVersion>4.0.0</modelVersion>   <parent>      <groupId>org.springframework.boot</groupId>      <artifactId>spring-boot-starter-parent</artifactId>      <version>2.2.4.RELEASE</version>      <relativePath/>   </parent>   <groupId>ac.simons.neo4j.examples</groupId>   <artifactId>multidatabase-movies</artifactId>   <version>0.0.1-SNAPSHOT</version>   <properties>      <java.version>1.8</java.version>   </properties>   <dependencies>      <dependency>         <groupId>org.springframework.boot</groupId>         <artifactId>spring-boot-starter-security</artifactId>      </dependency>      <dependency>         <groupId>org.springframework.boot</groupId>         <artifactId>spring-boot-starter-webflux</artifactId>      </dependency>      <dependency>         <groupId>org.projectlombok</groupId>         <artifactId>lombok</artifactId>      </dependency>      <dependency>         <groupId>org.neo4j.springframework.data</groupId>         <artifactId>spring-data-neo4j-rx-spring-boot-starter</artifactId>         <version>1.0.0-beta03</version>      </dependency>   </dependencies>   <build>      <plugins>         <plugin>            <groupId>org.springframework.boot</groupId>            <artifactId>spring-boot-maven-plugin</artifactId>         </plugin>      </plugins>   </build></project>I have omitted the test dependencies for brevity. SDN/RX is a mapping framework, so we can define the following entities and corresponding repositoriesMovies…package ac.simons.neo4j.examples.multidatabase_movies.domainimport static org.neo4j.springframework.data.core.schema.Relationship.Direction.*import lombok.Getterimport java.util.Setimport org.neo4j.springframework.data.core.schema.Idimport org.neo4j.springframework.data.core.schema.Nodeimport org.neo4j.springframework.data.core.schema.Propertyimport org.neo4j.springframework.data.core.schema.Relationship@Node( Movie )@Getterpublic class MovieEntity {   @Id   private final String title   @Property( tagline )   private final String description   @Relationship(type =  ACTED_IN , direction = INCOMING)   private Set<PersonEntity> actors   @Relationship(type =  DIRECTED , direction = INCOMING)   private Set<PersonEntity> directors   @Relationship(type =  REVIEWED , direction = INCOMING)   private Set<PersonEntity> reviewers   public MovieEntity(String title, String description) {      this.title = title      this.description = description   }}SDN/RX repositories are 100% declarative, so we can define them as interfaces:package ac.simons.neo4j.examples.multidatabase_movies.domainimport reactor.core.publisher.Monoimport org.neo4j.springframework.data.repository.ReactiveNeo4jRepositorypublic interface MovieRepository extends ReactiveNeo4jRepository<MovieEntity, String> {   Mono<MovieEntity> findOneByTitle(String title)}…and Peoplepackage ac.simons.neo4j.examples.multidatabase_movies.domainimport lombok.Getterimport org.neo4j.springframework.data.core.schema.Idimport org.neo4j.springframework.data.core.schema.Node@Node( Person )@Getterpublic class PersonEntity {   @Id   private final String name   private final Long born   public PersonEntity(Long born, String name) {      this.born = born      this.name = name   }}As we don’t query the people directly, so we won’t define a repository for them. The reactive REST interface will be defined with a more or less standard Spring controller, using the MoviesRepository . That controller however returns reactive data types:package ac.simons.neo4j.examples.multidatabase_movies.webimport ac.simons.neo4j.examples.multidatabase_movies.domain.MovieEntityimport ac.simons.neo4j.examples.multidatabase_movies.domain.MovieRepositoryimport lombok.RequiredArgsConstructorimport reactor.core.publisher.Fluximport reactor.core.publisher.Monoimport org.springframework.http.MediaTypeimport org.springframework.web.bind.annotation.DeleteMappingimport org.springframework.web.bind.annotation.GetMappingimport org.springframework.web.bind.annotation.PathVariableimport org.springframework.web.bind.annotation.PutMappingimport org.springframework.web.bind.annotation.RequestBodyimport org.springframework.web.bind.annotation.RequestMappingimport org.springframework.web.bind.annotation.RequestParamimport org.springframework.web.bind.annotation.RestController@RestController@RequestMapping( /movies )@RequiredArgsConstructorpublic class MovieController {   private final MovieRepository movieRepository   @PutMapping   Mono<MovieEntity> createOrUpdateMovie(@RequestBody MovieEntity newMovie) {      return movieRepository.save(newMovie)   }   @GetMapping(value = {   ,  /  }, produces = MediaType.TEXT_EVENT_STREAM_VALUE)   Flux<MovieEntity> getMovies() {      return movieRepository         .findAll()   }   @GetMapping( /by-title )   Mono<MovieEntity> byTitle(@RequestParam String title) {      return movieRepository.findOneByTitle(title)   }   @DeleteMapping( /{id} )   Mono<Void> delete(@PathVariable String id) {      return movieRepository.deleteById(id)   }}To start the application itself, we also need a main class:package ac.simons.neo4j.examples.multidatabase_moviesimport org.springframework.boot.SpringApplicationimport org.springframework.boot.autoconfigure.SpringBootApplication@SpringBootApplicationpublic class MultidatabaseMoviesApplication {   public static void main(String[] args) {      SpringApplication.run(          MultidatabaseMoviesApplication.class, args      )   }}Which is basically a standard Spring Boot application.Configuration timeThe pure database connection is easy and can be done via properties or the environment. Here is the properties solution:org.neo4j.driver.uri=bolt://localhost:7687org.neo4j.driver.authentication.username=reactive_rentalorg.neo4j.driver.authentication.password=secretspring.jackson.default-property-inclusion=non_nullThe last line has nothing to do with SDN/RX, but prevents null fields in our JSON responses.For the security configuration, we have a bit more manual work to do. The comments at the Bean methods explains in detail what we are doing here.package ac.simons.neo4j.examples.multidatabase_moviesimport reactor.core.publisher.Monoimport java.util.Localeimport org.neo4j.springframework.data.core.DatabaseSelectionimport org.neo4j.springframework.data.core.ReactiveDatabaseSelectionProviderimport org.springframework.context.annotation.Beanimport org.springframework.context.annotation.Configurationimport org.springframework.security.config.web.server.ServerHttpSecurityimport org.springframework.security.core.Authenticationimport org.springframework.security.core.context.ReactiveSecurityContextHolderimport org.springframework.security.core.context.SecurityContextimport org.springframework.security.core.userdetails.MapReactiveUserDetailsServiceimport org.springframework.security.core.userdetails.Userimport org.springframework.security.core.userdetails.UserDetailsimport org.springframework.security.web.server.SecurityWebFilterChain@Configurationpublic class SecurityConfig {   /*    * This bean configures relevant features of Spring security:    * We want all requests to be authenticated and we dont use CSRF    * protection in this application. We support only basic auth.    */   @Bean   public SecurityWebFilterChain springSecurityFilterChain(      ServerHttpSecurity http   ) {      http         .authorizeExchange()         .anyExchange().authenticated()         .and()         .csrf().disable()         .httpBasic()      return http.build()   }   /*    * This creates two users: Michael that has a role of VIDEO-CITY,    * and Gerrit, who is a VIDEOS-ARE-US user.    */   @Bean   public MapReactiveUserDetailsService userDetailsService() {      UserDetails userA = User.withDefaultPasswordEncoder()         .username( Michael )         .password( secret )         .roles( VIDEO-CITY )         .build()      UserDetails userB = User.withDefaultPasswordEncoder()         .username( Gerrit )         .password( secret )         .roles( VIDEOS-ARE-US )         .build()      return new MapReactiveUserDetailsService(userA, userB)   }   /*    * This bean is part of SDN/RX. A DatabaseSelectionProvider     * can be used to determine the database to use.     * Here, we choose the reactive security context.    */   @Bean   ReactiveDatabaseSelectionProvider reactiveDatabaseSelectionProvider() {      return () -> ReactiveSecurityContextHolder.getContext()         .map(SecurityContext::getAuthentication)         .filter(Authentication::isAuthenticated)         .map(Authentication::getPrincipal)         .map(User.class::cast)         .flatMap(u -> Mono.justOrEmpty(u.getAuthorities().stream()            .map(r -> r.getAuthority().replace( ROLE_ ,   ).toLowerCase(Locale.ENGLISH))            .map(DatabaseSelection::byName)            .findFirst()))         .switchIfEmpty(Mono.just(DatabaseSelection.undecided()))   }}The idea of this database selection is the following: We use the authorities granted to the users to determine the tenant of Videokingdom. This is done in a bean of type ReactiveDatabaseSelectionProvider . We have an imperative variant for it as well, but hey… Reactive all the things!The reactive variant must of course return a reactive type as well. A Mono of DatabaseSelection . This fits nicely with the ReactiveSecurityContextHolder . This class is a part of Spring’s reactive security and gives you the authentication relevant at that point in time on which a reactive flow is executed. We check whether someone is authenticated and extract the roles assigned to the principal. These are the roles we defined when creating the user. Spring did prefix them with ROLE_ for internal Spring reasons.Putting all this together and starting the main application will give you the following output at some point:2020-01-29 21:27:04.256  INFO 2373 --- [           main] o.s.b.web.embedded.netty.NettyWebServer  : Netty started on port(s): 8080And then try the following CURL request:curl -u Michael:secret  http://localhost:8080/movies/by-title?title=Cloud%20Atlas curl -u Gerrit:secret  http://localhost:8080/movies/by-title?title=Cloud%20Atlas The first one returns:{   title :  Cloud Atlas ,   description :  Everything is connected ,   actors : [    {       name :  Halle Berry ,       born : 1966    },    {       name :  Tom Hanks ,       born : 1956    },    {       name :  Jim Broadbent ,       born : 1949    },    {       name :  Hugo Weaving ,       born : 1960    }  ],   directors : [    {       name :  Tom Tykwer ,       born : 1965    },    {       name :  Lana Wachowski ,       born : 1965    },    {       name :  Lilly Wachowski ,       born : 1967    }  ],   reviewers : []}While the second one returns:{   title :  Cloud Atlas ,   description :  Everything is connected ,  // Same actors and directors, but in addition, the reviewers   reviewers : [    {       name :  Jessica Thompson     }  ]}So we have an application in place that dynamically selects the tenant database based on the user role.We also can verify that only user Gerrit can create new movies. While the following command:curl -X  PUT  -u Gerrit:secret  http://localhost:8080/movies  \                    -H Content-Type: application/json charset=utf-8 \     -d ${   title :  Aeon Flux ,   description :  Reactive is the new cool }succeeds with:{ title : Aeon Flux , description : Reactive is the new cool }%Using Michael fails:curl -X  PUT  -u Michael:secret  http://localhost:8080/movies  \                    -H Content-Type: application/json charset=utf-8 \     -d ${   title :  Aeon Flux ,   description :  Reactive is the new cool }{   timestamp :  2020-01-29T20:39:43.913+0000 ,   path :  /movies ,   status : 500,   error :  Internal Server Error ,   message :  Write operations are not allowed for user reactive_rental with roles [movie_streaming_application,publisher]. ,   requestId :  ba7cc39b }I think role-based access controls in a database are a great way to secure things. The rental application might go away, replaced by another, along with different rules in it as well. The database is there to stay.You’ll find the whole project as a zip file here: multidatabase-movies.zip.But what about a complete view of all the movies?This is where Fabric comes into play.Fabric, introduced in Neo4j 4.0, is a way to store and retrieve data in multiple databases, whether they are on the same Neo4j DBMS or in multiple DBMSs, using a single Cypher query. Fabric achieves a number of desirable objectives:a unified view of local and distributed data, accessible via a single client connection and user sessionincreased scalability for read/write operations, data volume and concurrencypredictable response time for queries executed during normal operations, a failover, or other infrastructure changesHigh Availability and No Single Point of Failure for large data volume.This is only a small teaser, and we are only interested in the first point.Fabric is easy to set up, but you have to stop your Neo4j instance and open conf/neo4j.conf in your editor of choice. We’ll define a new Fabric database, named videokingdom, that will contain the two distributed graphs:fabric.database.name=videokingdomfabric.graph.0.uri=neo4j://localhost:7687fabric.graph.0.database=videos-are-usfabric.graph.0.name=moviesAfabric.graph.1.uri=neo4j://localhost:7687fabric.graph.1.database=video-cityfabric.graph.1.name=moviesBThe server is then restarted as usual, and you can use the Cypher shell again. Let’s get a list of all of the movies starting with ‘A’ held in both these databases. To do this, run the following query:./bin/cypher-shell -u neo4j -p secret -d videokingdomCALL {  USE videokingdom.moviesA  MATCH (movie:Movie)  WHERE movie.title =~ A.*  RETURN movie.title AS title   UNION  USE videokingdom.moviesB  MATCH (movie:Movie)  WHERE movie.title =~ A.*  RETURN movie.title AS title} RETURN title  ORDER BY title ASCThis gives us an overview over all movies starting with A”, including the newly created Aeon Flux.I hope that gives you some ideas to play around with. Thank you and happy coding.Thanks to the whole Neo4j engineering team for a fabulous Neo4j 4.0 release and my colleagues Ljubica Lazarevic, Jennifer Reif and Gerrit Meier for reviewing this article.;Jan 31, 2020;[]
https://medium.com/neo4j/welcome-to-the-neo4j-publication-d7e7cf7a8562;Neo4jFollowApr 5, 2018·2 min readWelcome to the Neo4j PublicationHello everyone,We, the developer relations team at Neo4j, are always looking into new ways to support the developer community. We are starting this publication around Neo4j related topics to not only share lessons learned and tips and tricks, but also encourage everyone of you to contribute and share as well.So if you have written a Neo4j or graph-related article on Medium in the past or are planning to publish one in the future, reach out to us (devrel@neo4j.com) and we can add you as a writer to this publication, and consider your stories for additions. Allowing you to educate a larger group of people.Please let us also know via comments what kind of content you’re interested in most, so we can provide the things you need.For weekly updates from our community check out This week in Neo4j” which we consider to publish here as well to have all the developer updates in one place.If you have quick questions, make sure to join the neo4j-users Slack and ask in one of the channels there.Happy building,Michael Hunger for the Neo4j team;Apr 5, 2018;[]
https://medium.com/neo4j/approximate-maximum-k-cut-with-neo4j-graph-data-science-5683518673ae;Nathan SmithFollowOct 1, 2021·6 min readApproximate Maximum K-Cut with Neo4j Graph Data ScienceCluster related products and separate conflicting entities with the newest algorithm in the Graph Data Science Library: Approximate Maximum K-cutPhoto by Matt Artz on UnsplashThe 1.7 release of Neo4j’s Graph Data Science Library contains some amazing features, like machine learning pipelines for link prediction, and the ability to query in-memory graphs in the graph catalog with Cypher. Amongst all that goodness is a new algorithm called Approximate Maximum K-cut.Approximate Maximum k-cut - Neo4j Graph Data ScienceThis section describes the Approximate Maximum k-cut algorithm in the Neo4j Graph Data Science library. A k-cut of a…neo4j.comThe approximate maximum k-cut algorithm will divide a graph into a number of partitions that you specify. The k” in the algorithm’s name stands for the number of partitions. The goal of the algorithm is to divide the graph in such a way that the sum of the weights of relationships that connect nodes in different partitions is as high as possible.At first, this struck me as a strange objective for an algorithm. I often think of the relationships in a graph as representing attraction, or similarity. Why would I want to separate things that are similar into different communities?However, there’s no reason graph relationships can only represent likes” or friends.” A relationship could also represent dislike,” with a weight property that represents the level of mutual antipathy.Imagine planning a party for guests who haven’t seen each other in a while. Old resentments and lack of recent practice in social interactions might create some awkward moments if you don’t plan the seating at the party’s two tables in a way that separates the most combustible combinations of personalities. Maximum k-cut can help.To follow the examples in this blog post, you can use a free Neo4j Sandbox. Choose a blank sandbox, because we will add our own data.Neo4j SandboxStart learning Neo4j quickly with a personal, accessible online graph database. Get started with built-in guides and…dev.neo4j.comIn the Neo4j Browser UI, run this script to create nodes representing party guests (People nodes) and their level of mutual dislike. Even though the relationships are directed, we’ll assume the same level of dislike goes both ways between the parties.CREATE(aaron:Person {name: Aaron}),(bridget:Person {name: Bridget}),(charles:Person {name: Charles}),(doug:Person {name: Doug}),(eric:Person {name: Eric}),(fiona:Person {name: Fiona}),(george:Person {name: George}),(heather:Person {name: Heather}),(aaron)-[:DISLIKES {strength:2}]->(bridget),(aaron)-[:DISLIKES {strength:1}]->(doug),(aaron)-[:DISLIKES {strength:1}]->(eric),(aaron)-[:DISLIKES {strength:1}]->(fiona),(aaron)-[:DISLIKES {strength:3}]->(george),(bridget)-[:DISLIKES {strength:3}]->(charles),(bridget)-[:DISLIKES {strength:3}]->(george),(doug)-[:DISLIKES {strength:3}]->(george),(eric)-[:DISLIKES {strength:1}]->(fiona),(eric)-[:DISLIKES {strength:1}]->(george),(eric)-[:DISLIKES {strength:4}]->(heather),(fiona)-[:DISLIKES {strength:5}]->(george)I visualized the resulting graph in Neo4j Bloom so that I see the strength property on the relationships as line weight.DISLIKES relationships among party guestsNow we’ll create the graph in Neo4j’s in-memory graph catalog.CALL gds.graph.create(  partyGraph,  Person,  {    DISLIKES: {      properties: [strength]    }  })Finally, we can use the approximate maximum k-cut algorithm to assign the guests to the two tables.CALL gds.alpha.maxkcut.stream(partyGraph, {k:2})YIELD nodeId, communityIdwith gds.util.asNode(nodeId) as guest, communityIdreturn guest.name as name, communityId as tableNumberorder by tableNumber, guest.name╒═════════╤═════════════╕│ name    │ tableNumber │╞═════════╪═════════════╡│ Bridget │0            │├─────────┼─────────────┤│ Doug    │0            │├─────────┼─────────────┤│ Eric    │0            │├─────────┼─────────────┤│ Fiona   │0            │├─────────┼─────────────┤│ Aaron   │1            │├─────────┼─────────────┤│ Charles │1            │├─────────┼─────────────┤│ George  │1            │├─────────┼─────────────┤│ Heather │1            │└─────────┴─────────────┘Our seating plan looks promising. There might be some minor tension between Eric and Fiona at table 0, and Aaron and George might create some sparks at table 1. There are no other feuding table-mates to worry about.The maximum k-cut algorithm has been used for mobile wireless communication planning in a way that reminds me of our dinner party problem. Imagine that instead of assigning table numbers to dinner guests so that incompatible guests don’t share the same table, you want to assign identifiers to cellular towers so that nearby towers’ reference frequencies don’t share the same ID, as described in a paper by Fairbother, Letchford, and Briggs.A paper by Gaur, Krishnamurti, and Kohli offers some other interesting examples of maximum k-cut in action. I experimented with a retail scenario they suggested. Imagine a bakery that wants to divide its menu into several sections. If items that are frequently purchased together could be clustered in the same menu section, it would increase opportunities for cross-selling.To explore this scenario, I loaded a bakery-purchases dataset from Kaggle into my Neo4j Sandbox using the steps below.In Neo4j Browser, I ran these commands to create unique constraints for the node types that will be in my data model.CREATE CONSTRAINT uniqueTransaction ON (t:Transaction) ASSERT t.ID IS UNIQUECREATE CONSTRAINT uniqueItem ON (i:Item) ASSERT i.name IS UNIQUEThen, I ran these commands to load the nodes and relationships.LOAD CSV WITH HEADERS FROM  https://raw.githubusercontent.com/smithna/datasets/main/bread_basket.csv  AS line MERGE (t:Transaction {ID:line.Transaction})LOAD CSV WITH HEADERS FROM  https://raw.githubusercontent.com/smithna/datasets/main/bread_basket.csv  AS lineMERGE (i:Item {name:line.Item})LOAD CSV WITH HEADERS FROM  https://raw.githubusercontent.com/smithna/datasets/main/bread_basket.csv  AS line MATCH (t:Transaction {ID:line.Transaction}),       (i:Item {name:line.Item}) MERGE (t)-[:CONTAINS]->(i)So far, I had loaded a group of Transaction nodes. Each transaction had a CONTAINS relationship leading to the Item nodes that were purchased in that transaction.Now, I wanted to know which items did not appear frequently together in the same transactions. It would be good to split these infrequently paired items up into separate sections of the menu.I counted the total number of transactions in the dataset with this query.match (t:Transaction) return count(t)╒══════════╕│ count(t) │╞══════════╡│9465      │└──────────┘Next, I created the GOES_WITH relationship between all possible pairs of items. The relationship has a pairedTransactions property that represent the number of transactions on which the items were both present, and a unpairedTransactions property that represents the number of transactions where one or both of the items were absent.Keep in mind that if n is the number of items on the menu, the number of pairs to be considered is n * (n-1)/2. That will be an expensive query for large values of n, but it’s fine for the size of our data.MATCH (i:Item)WITH collect(i) AS itemsWITH apoc.coll.combinations(items, 2, 2) AS pairsUNWIND pairs AS pairWITH pair[0] as i0, pair[1] as i1OPTIONAL MATCH (i0)<-[:CONTAINS]-(t:Transaction)-[:CONTAINS]->(i1)WITH i0, i1, count(t) AS pairedTransactionsCREATE (i0)-[:GOES_WITH {pairedTransactions:pairedTransactions, unpairedTransactions:9465-pairedTransactions}]->(i1)Now it was time to create the graph in the Neo4j graph catalog using the negative” unpairedTransactions weight property.CALL gds.graph.create(  bakeryGraph,  Item,  {    GOES_WITH: {      properties: [unpairedTransactions]    }  })I wanted to use the approximate maximum k-cut algorithm to push items with high unpairedTransactions values into separate menu categories. I chose to create 10 different categories. I used this query to modify the in-memory graph with the results of the algorithm run.CALL gds.alpha.maxkcut.mutate(  bakeryGraph,   {    mutateProperty: menuSection,     k:10,     relationshipWeightProperty:unpairedTransactions,    concurrency: 1,    randomSeed: 25  })YIELD cutCost, nodePropertiesWrittenI wrote the properties from the in-memory graph back to the persistent Neo4j graph.CALL gds.graph.writeNodeProperties(  bakeryGraph,   [menuSection]) YIELD propertiesWrittenThen, I could query the results and see which items ended up in the same menu section.MATCH (i:Item)<-[:CONTAINS]-(t:Transaction)RETURN i.menuSection as menuSection, i.name as item, count(t) as transactionCountORDER BY menuSection, transactionCount desc╒═════════════╤═══════════════════════════════╤══════════════════╕│ menuSection │ item                          │ transactionCount │╞═════════════╪═══════════════════════════════╪══════════════════╡│0            │ Soup                          │326               │├─────────────┼───────────────────────────────┼──────────────────┤│0            │ Toast                         │318               │├─────────────┼───────────────────────────────┼──────────────────┤│0            │ Truffles                      │192               │├─────────────┼───────────────────────────────┼──────────────────┤│0            │ Spanish Brunch                │172               │├─────────────┼───────────────────────────────┼──────────────────┤│0            │ Mineral water                 │134               │├─────────────┼───────────────────────────────┼──────────────────┤│0            │ Hearty & Seasonal             │100               │├─────────────┼───────────────────────────────┼──────────────────┤It looks like many of the most popular items ended up together in menu section 7.├─────────────┼───────────────────────────────┼──────────────────┤│7            │ Coffee                        │4528              │├─────────────┼───────────────────────────────┼──────────────────┤│7            │ Bread                         │3097              │├─────────────┼───────────────────────────────┼──────────────────┤│7            │ Tea                           │1350              │├─────────────┼───────────────────────────────┼──────────────────┤│7            │ Pastry                        │815               │├─────────────┼───────────────────────────────┼──────────────────┤│7            │ Sandwich                      │680               │├─────────────┼───────────────────────────────┼──────────────────┤Customers with a sweet tooth would enjoy section 1 of the menu.├─────────────┼───────────────────────────────┼──────────────────┤│1            │ Brownie                       │379               │├─────────────┼───────────────────────────────┼──────────────────┤│1            │ Alfajores                     │344               │├─────────────┼───────────────────────────────┼──────────────────┤│1            │ Coke                          │184               │├─────────────┼───────────────────────────────┼──────────────────┤Several of the non-food items ended up together in section 5.├─────────────┼───────────────────────────────┼──────────────────┤│5            │ Tshirt                        │21                │├─────────────┼───────────────────────────────┼──────────────────┤│5            │ Valentines card              │13                │├─────────────┼───────────────────────────────┼──────────────────┤│5            │ Kids biscuit                  │12                │├─────────────┼───────────────────────────────┼──────────────────┤│5            │ Postcard                      │10                │├─────────────┼───────────────────────────────┼──────────────────┤I hope these examples inspire you to have fun with approximate maximum k-cut as part of your data science toolkit.;Oct 1, 2021;[]
https://medium.com/neo4j/announcing-the-neo4j-graphql-library-beta-99ae8541bbe7;Daniel StarnsFollowApr 7, 2021·5 min readAnnouncing the Neo4j GraphQL Library Beta ReleaseHi all! Over the last year engineers inside Neo4j have been planning, building, and refining an official Neo4j GraphQL integration. Today, we’re excited to announce the beta release of the Neo4j GraphQL Library!Our JourneyIf you have experimented with Neo4j GraphQL in the past, you may know about neo4j-graphql.js. You may be asking yourself: What’s wrong with it? The answer is: nothing.However, as a Neo4j Labs” project, support was provided on a best effort” basis and many of the features and technical decisions were shaped without a clear roadmap.As a new Product Engineering project, we are now dedicated to the Neo4j GraphQL Library on a full-time, ongoing basis. One of the first things we did was rewrite neo4j-graphql.js in TypeScript. Leveraging TypeScript has massively improved the quality of the project, giving us an upperhand as the product evolves.Everything has been built on the foundations established by Neo4j Labs working on Neo4j GraphQL integrations for several years, so existing users should feel right at home with familiar fundamentals and concepts.Not only did we want to build a robust type-safe codebase but we wanted to position ourselves in line with our users and Neo4j’s goals. With many feature requests, and the anticipation to offer GraphQL as a service, we wanted a clear workbench to implement features that would satisfy everyones requirements, such as the new Auth Directive for adding authorization rules to your GraphQL API.Finally, we know that great support helps make a great product. With the new Official Neo4j Product stamp and the realignments to the implementation, developers and users can expect high-quality support plus continuous improvement.The TeamBuilding the new implementation was no one-man job! Here is the team:Daniel Starns (Engineer)Darrell Warde (Engineer)Oskar Hane (Team Lead)William Lyon (Developer Relations)Anurag Tandon (Product Manager)Special thanks to Michael Graham, Andreas Berger, Michael Hunger, and many more members of the community who contributed to the previous Neo4j GraphQL integrations and have helped us get to this stage.🍻Getting Started with the Neo4j GraphQL LibraryIn this section, I will take you through setting up a Node.js Neo4j GraphQL API application, using Neo4j Sandbox, mutating data, and then querying that data.First: Visit Neo4j Sandbox, sign up, and create a new Blank Sandbox project:Using Neo4j Sandbox means all you need is the ability to run some Node.js code.If you dont want to use Sandbox, you can download and install Neo4j Desktop here.Installing the Neo4j GraphQL LibraryIn a new folder, start a new npm project:npm init --yThen install dependencies:npm install @neo4j/graphql graphql apollo-server neo4j-driverCTRL + C, CTRL + VYou can pretty much copy and paste this example into your new project, although you will need to change the 3 references to connect to your Neo4j Sandbox instance. You can find these values in the Connection Details” tab of Neo4j Sandbox.YOUR_BOLT_URLYOUR_BOLT_USERYOUR_BOLT_PASSWORDRun the Servernode index.jsExecute the given copy paste job and navigate to http://localhost:4000 in your web browser:What you’re seeing is GraphQL Playground.Dissecting the Schema 🧐One of the goals of Neo4j GraphQL is that given a single set of GraphQL type definitions, we produce a fully functional GraphQL schema ready to be used in your application.Let’s take a closer look at the GraphQL type definitions you just copied:These GraphQL type definitions map to a Neo4j property graph model. If you were to represent these type definitions visually, using Arrows.app, it would look something like:Creating a MovieForrest Gump is a pretty cool film! Let’s add that into our database.Creating DirectorFirstly, lets create Person node Robert Zemeckis by executing this GraphQL mutation in GraphQL Playground:Creating MovieNow we have our director, lets create the Movie node and connect a relationship to the already created director node:Adding ActorsTom Hanks and Robin Wright starred in Forrest Gump, so let’s perform an update where we create and connect the actors to the movie:Adding a GenreNow we have our Movie and Persons connected together, all we are missing is a Genre:Reading The GraphNow all of our Movies, Genres, and People are inserted and connected together, we can perform a query to grab all the data:Note that we didn’t need to write any Cypher or other data-fetching code like GraphQL resolver functions to build our GraphQL API application. The Neo4j GraphQL Library took care of translating our arbitrary GraphQL operations to Cypher and handling the database request.What’s Next?To get the best idea of where to go next, visit the Docs to find out more & come chat with us on Discord. Below is a quick peek at some of the other exciting features that the new library offers.Auth DirectiveUse the brand new Auth directive to secure your GraphQL API using JSON Web Tokens (JWTs). Below is an example of only allowing the director to edit a movie:Type Definitions Intentionally SimplifiedOGMIn addition to the approach outlined above, you can also use Type Definitions to power an Object-Graph Mapper or OGM. Below is an example that uses nested mutations to create Movie, Persons, and Genre nodes at once:Life is like a box of chocolates. You never know what youre gonna get.~ Forrest GumpLearn More About Neo4j GraphQLThe Beta release of the Neo4j GraphQL Library is available today via npm. The 1.0 GA release of the library will be coming later in the quarter, so stay tuned for more updates. If you are a current user of the Labs neo4j-graphql.js library please consider migrating to the new official Neo4j GraphQL Library as neo4j-graphql.js will be sunset sometime this summer.We’ll be hosting a Neo4j GraphQL Community Call as part of Global Graph Celebration Day on April 15th where the team will dive into more detail about the Neo4j GraphQL Library and answer any questions. Register for an email reminder or see the detailed schedule at GlobalGraphCelebrationDay.comOther LinksGithubDiscordDocs;Apr 7, 2021;[]
https://medium.com/neo4j/summer-of-nodes-week-1-the-barbecue-ead98d441a91;Ljubica LazarevicFollowAug 3, 2020·7 min read*Update! Now with hints and solution!*Summer of Nodes: Week 1 — The BarbecueMissed the live stream? Catch up here!Missed the hints stream? Catch up here!Missed the solutions stream? Catch up here!Series playlistHello everybody!Summer of Nodes 2020 is now over. If you’ve not had a chance to look at the challenges, you can always have a go at your leisure:The socially distanced barbecue (this post)The online day outThe whodunitExploring the areaFirst week’s theme — the BarbecueNothing better marks summertime than a barbecue — it is a wonderful combination of food, amazing weather, and great company with friends and family. Whilst the current situation is making the latter a bit harder, it is still possible to share the occasion through social distancing and virtual means.Our first Summer of Nodes challenges are going to be around learning more about barbecues around the world and planning for that socially-distanced event!These challenges should take approximately an hour each, depending on experience. Of course, you are welcome to try both challenges!Beginner’s challenge: model barbecues around the world!IntroductionThere are many different ways one can barbecue — from the different cooking fuels in use, different types of cooking implements, down to cooking style (nice and quick, or low and slow), and what actually makes it onto the grill! There are many country and regional variations worldwide, and you can find out more in this wikipedia article: r.neo4j.com/wiki-bbqThis week’s challenge — based on the article, think about how you’d model all of the variety in a barbecue. The types of questions we might ask are:What is the most common type of barbecue?What countries tend to have vegetarian-friendly barbecues, based on the food?What type of food is the most popular?Is there a relationship between cooking surface, cooking type and fuel?Potential modelling tools and links:If you’re new to graph data modelling, you may find this guide helpful. You can also complete this free online training course.You can use any sensible modelling too, pen and paper are acceptable! Can’t think of one? We suggest you use Arrows. You can find out how to use it here.What we are looking for?Use these labels: Barbecue, Country, CookingType, Fuel, Food, Dietary, CookingSurface. Add details, e.g. appropriate relationship types and properties. An example value for a property is fine. No multiple nodes with the same label!Examples of what we’re looking/not looking for!Hints please!Ah, go on then!Make sure you use all of the node labels we have provided you! We asked you a set of questions further up. Think about how you might connect all of the node labels based on the questions we have asked you. You may find it easier to underline the corresponding labels ‘mentioned’ in the questions.Don’t forget to add details to help you answer the questions. For example, what property might you need to add to Barbecue so that you can determine what is the most common type?Alex also gave a quick demo of Arrows. You can watch it here.Experienced challenge: the socially distanced barbecueIntroductionAround the world we’re starting to see lockdown easing, which is allowing us to once again see our nearest and dearest. Of course, we need to be careful, and maintain social distancing.This week’s challenge — you will be provided with data to work with. This data contains the guests you’ve invited to your socially-distanced barbecue. The guests are individuals, or members from the same household, and so forth. You are to use the graph to figure out the optimal seating plan for all your guests, whilst respecting social distancing guidelines.The rules:There are a total of 6 tables, each with 6 places. Each place is 1 metre apart from each otherAll guests not from the same household must be at least 2 metres apart from each other. Guests from the same household may sit closer togetherA table must have no more than a maximum of 2 people from the same household sat thereA table must have members of at least two householdsThe data:You can get the CSV from here.What are we looking for?The query/ies you used to load and prepare your dataThe query you ran on this data to allocate guests to tablesThe output should be: guest, household, tableNumberYou do not have to use pure Cypher, you may use the standard Neo4j pluginsThe queries must be executable!Hints please!Of course, this was bit of a challenger, but a fun one at that. If we revisit the rules, we discover that we will need to:Think about what to do with households with more than 2 people, as they won’t be able to sit at the same table! Think about how you might pre-process the dataThink about what are the maximum possible numbers of guests to tables, i.e. 4 from 2 households, or 3 from 3 households, etc.Think about using a weighting system of some sort. Something that will allow you to determine when the maximum capacity for the table has been reached, that works for the scenarios mentioned aboveDon’t be shy to draw out some of your thinking on paper, this may help you determine a way to resolve the challenge!We will need to iteratively allocate guests to tables until they are all done. APOC has some tools to help us with that. For example, could apoc.periodic.commit() help us?SolutionsWe covered potential options for solutions in our live stream on 10th August. Watch the recap here.Beginners challengeThere were a number of ways this could be solved, and what we were looking for was:Using all of the node labels providedChoosing sensible relationship types, with no type repetitionChoosing sensible property key namesA model that could answer the provided questions in a reasonable wayHere is an example of what your data model could have looked like. Don’t worry if it’s not identical!You data model may have looked similar to thisExperienced challengeAgain, there were a number of ways this could be solved, and we received some very novel approaches!Let’s take a quick look at what our options were. In the diagram below, using the rules, we could have one of four seating options (where the same colour indicates members from the same household at the table):Seating plan options as per the rulesAn approach we found helpful to solve this conundrum was to think about applying weights. This was the following system used:Total weight allowed at a table = total distance / minimum distance separation. In this scenario the total weight is, 6 / 2 = 3Household weight = 0.5 + number of members in that household / 2. So for a 1-member household it’s 1, for 2 members it’s 1.5, etc.Use household weights to split out to sub households if they’re larger than 2Allocate households to tables based on these weightsWe also used the assumption around how internal node IDs are allocated, i.e. when splitting out a household, the ID won’t be consecutively before/after the original household.Based on these assumptions, the following set of queries were used to load and pre-process the data://load the householdersLOAD CSV WITH HEADERS FROMhttps://raw.githubusercontent.com/summer-of-nodes/2020/master/week1/bbq_households.csv AS rowCREATE (p:Person {name:row.Name})MERGE (h:Household {id:row.`Household Group`})ON CREATE SET h.weight = 1.0ON MATCH SET h.weight = h.weight+0.5CREATE (p)-[:IN_HOUSEHOLD]->(h)//split out the households that are bigger than 2 peopleCALL apoc.periodic.commit( MATCH (h:Household)WHERE h.weight>1.5 //get the households with more than 2 peopleWITH h limit 1MATCH (h)<-[r]-(p)WITH * limit 2MERGE (h2:Household {id:h.id + h.weight, weight:1.5})MERGE (p)-[:IN_HOUSEHOLD]->(h2)WITH h, rDELETE rWITH DISTINCT hSET h.weight = h.weight - 1.0RETURN count(*) , {limit:1})//create the tablesWITH range(1,6) as tablesUNWIND tables AS noCREATE (t:Table {number:no, weight:0})And this a potential query that can be used to allocate the seats://allocate seats!CALL apoc.periodic.commit( MATCH (h:Household) WHERE NOT (h)-->() WITH h ORDER BY h.weight DESC, id(h) LIMIT $limitMATCH (t:Table) WHERE t.weight+h.weight <= 3.0 WITH t, h order by t.number LIMIT 1 CREATE (h)-[:AT_TABLE]->(t)SET t.weight=t.weight+h.weightRETURN count(*) ,{limit:1})YIELD updatesWITH updates AS ignoreMeMATCH (p:Person)-->(h:Household)-->(t:Table)RETURN p.name AS guest, left(h.id,1) AS household, t.number AS tableNumber ORDER BY tableNumberThere are, of course, several things we could do to make this query more robust to dealing with fewer guests, better distribution, ensure non-consecutive IDs of split households, etc. Perhaps if you fancy an extension to the challenge, why not think about how you could make this flexible to more tables/places!We hope you enjoyed the challenges! Hungry for more? We’ll post the next set of challenges shortly!;Aug 3, 2020;[]
https://medium.com/neo4j/importing-data-into-neo4j-using-rxjs-ed017004bb25;Adam CowleyFollowSep 23, 2020·14 min readDart River-Te Awa Whakatipu, New Zealand shot by Alistair MacKenzie from unsplash.Importing Data into Neo4j using RxJSThis post has been inspired by a call I had with an old client in which we discussed ways to improve the efficiency of their imports into Neo4j. The client heavily uses Reactive programming to deal with streams of data into their application. The data is then split into new streams, which are transformed and then inserted into Neo4j.This inspired me to take a closer look at RxJS, and to see where it might be useful in for importing data into Neo4j.What is Reactive Programming?There are many out there that will no doubt describe Reactive programming in better terms than me. But to me, Reactive programming is a declarative paradigm in which streams (or observables) of data are manipulated asynchronously using a pipe” of one or more operators. There are implementations for Java, JavaScript and . NET amongst others — but being a {java|type}script guy I’ll be focussing on RxJS.Usage of Reactive programming and RxJS seems to be growing fast. It is supported by a large number of frameworks including NestJS, which I am a huge fan of, and is a major component of Angular.If we take a look at the example in the RxJS repository we can get a flavour of what we are in for:import { range } from  rxjs import { map, filter } from  rxjs/operators range(1, 200)  .pipe(    filter(x => x % 2 === 1),    map(x => x + x)  )  .subscribe(x => console.log(x))The RxJS library exports a number of observable creation methods (in this case range ) which return an observable. The observable itself produces a stream of results which can be manipulated using operators.In the example above, a stream of numbers between 1 and 200 is produced by the range function. Those numbers are sent through a pipe of operators:filter - Similar to filtering an array, the filter operator accepts a function that should return true or false. If the function returns false for that item, then the item will be omitted from the stream - in this case odd numbers are omitted.map - Similar to the map function on an array, each item will be transformed - in this case the number is doubled.The stream is then subscribed to using the subscribe method, where the callback function logs the value. The output would look something like this:24812etc. etc.RxJS for Neo4jThe Neo4j drivers have supported Reactive programming for a while now. The idea remains the same — you create a driver, session, then process the results.Take a basic example where we create a new node using a regular session and consume the results:const driver = neo4j.driver(bolt://localhost:7687, neo4j.auth.basic(neo4j, neo))const session = driver.session({ defaultAccessMode: neo4j.session.READ })session.run( `CREATE (p:Person {name: $name}) RETURN n` , { name: Adam})  // Manipulate the results  .then(res => res.records.map(row => row.get(n)))  .then(res => {    // Close the session    session.close()    return res  })  .then(output => console.log(output))There are a few minor changes — instead of calling then on a promise, we use the records method to return an observable, and use pipe instead of map to manipulate the results.const driver = neo4j.driver(bolt://localhost:7687, neo4j.auth.basic(neo4j, neo))// Call .rxSession() rather than .session()const session = driver.rxSession({ defaultAccessMode: neo4j.session.READ })session.run( `CREATE (p:Person {name: $name}) RETURN n` , { name: Adam})  // The records function returns an Observable which produces a stream of rows  .records()  // This stream can then be manipulated  .pipe(    // Close the session    concat(session.close()),    // Manipulate the results    map(row => row.get(n))  )  .subscribe(output => console.log(output))A working example — SkyshardFor this example, I’ll revisit a previous blog post for the fictional client SkyShard. SkyShard decided it was appropriate to split their data across many shards in order to improve read and write performance. The data is sharded by month with a two-day overlap, because the business rules dictate that a multi-hop flight can arrive in it’s destination up to two days after the initial departure.I will also be upgrading the code to TypeScript to take advantage of strong typing.If you are interested in the reasons behind the decision, head back and read the previous post. Otherwise, let’s write some code…Extract, Transform, LoadIn the post I built a simple ETL process using Node.js to read files from a CSV file, partition the rows based on the month, and then insert the data into the correct Neo4j Fabric database. This worked by processing the lines from a CSV file one-by-one, then appending them to an array based on their date.The code does the job but is far from perfect. The entire file needs to be processed upfront before any data can be inserted into Neo4j — and with over 320k rows in the file, this can take some time.We could improve the process by using RxJS to filter and manipulate the data as it is produced. To do this, we can:Produce an observable which produces a stream of rows.Modify each row to prepare it for insert into Neo4j.Use the flights departure date to extract the flights for a particular month (eg. January 2020).Insert the data into the correct shard.Producing an observableIf we take a look at the old code, we can see that the CSV file is read using the csv-parser library and added processed inside a Promise function. The promise only resolves when the end event is emitted, which means the whole file needs to be processed before the next step begins.const csv = require(csv-parser)const fs = require(fs)// ...return new Promise((resolve, reject) => {  fs.createReadStream(file)    .pipe(csv())    .on(data, async row => {      // Process row and add to correct array    })    .on(end, () => {      // Resolve the Promise      resolve(results)    })})The fs.createReadStream method returns an Event Emitter which is piped (not using RxJS yet) into the csv function which converts a Buffer into an Object that represents the row.RxJS exports a fromEvent function that will produce an observable object which will produce a new item when the specified event is emitted by an EventOmitter. A commonly used example in Ultimate Coursess RxJS Course is to listen for clicks on a DOM element:import { fromEvent } from rxjs/operatorsfromEvent(document.getElementById(my-button), click)We can do the same thing with our fs.createReadStream(file) call:import csv from csv-parserimport fs from fsimport { fromEvent } from rxjs// ...const readStreamEventEmitter = fs.createReadStream(file).pipe(csv())// Observable will emit when the entire file has been readconst readCsvFinished$: Observable<void> = fromEvent(readStreamEventEmitter, done)// Stream of objects representing CSV rowsconst flights$: Observable<object> = fromEvent(readStreamEventEmitter, data)Now when the data event is emitted, a new value will be sent to any subscriber of the flights$ stream.NOTE: I’ve added a $ suffix to the flights$ variable to denote that it is an Rx observable.Modify each item ready for importCurrently, the data isn’t in a format that we would like. To do this, we will pipe the observable through a map operator. In order to standardise the output, we can use a TypeScript interface to provide some guarantees about the payload:interface Flight {  origin: string  destination: string  departsAt: Date  price?: number}This will ensure that any Flight object passed through to any subscriber will have the minimal information we need to perform an update to Neo4j.Next, we can use the map operator imported from rxjs/operators to convert the row into an object that corresponds to the Flight interface.import { map, share } from rxjs/operators// ...const flights$: Observable<Flight> = fromEvent(readStreamEventEmitter, data)  .pipe(    // Convert the raw row into a Flight    map(row => {      const departsAt = new Date(row.time_hour)      return <Flight> {        origin: row.origin,        destination: row.dest,        departsAt,      }    ),    // multicast the values amongst subscribers     // without creating a new observable    share()  })Filter on departure dateNow that the observable will emit the correct values, we need to partition the data based on the shard. To do this, we can create a function that accepts a flights observable, filters based on arbitrary dates, and sends a write query to Neo4j.To do this, we can pipe the results through a filterimport { filter } from rxjs/operators// ...const writeToShard$ = (flights$: Observable<Flight>, start: Date, end: Date, shard: string) => {  return flights$.pipe(    // Filter Results by Date    filter((flight: Flight) => flight.departsAt >= start && flight.departsAt < end )  )}This will ensure that any flights passed to subsequent operators will be within a particular date range. This function provides an easy way insert data into the appropriate shard. For example, we can create streams for January, February, and March 2020 with the two-day overlap into the next month.const jan$ = writeToShard$(flights$, new Date(2020-01-01), new Date(2020-02-02), january2020)const feb$ = writeToShard$(flights$, new Date(2020-02-01), new Date(2020-03-02), february2020)const mar$ = writeToShard$(flights$, new Date(2020-03-01), new Date(2020-04-02), march2020)Insert the records into Neo4jNow that we have a stream with the correct data we can start to insert the data into Neo4j. The original import used an async function to insert batches of 1000:while ( data.length ) {  const batch = data.splice(0, batch_size)  await session.run(query, { batch })}We can create these batches using a buffer operator. There are a few options to choose from, but in this case the bufferCount or bufferTime operators may be applicable. bufferCount will collect the emitted values up to a certain number and emit the values as an array. bufferTime will collect the emitted values until the provided interval has passed.In this case, bufferCount makes the most sense but if you were working with streams of real-time data, it may make more sense to insert a new batch every few seconds or minute.import { filter, bufferCount } from rxjs/operators// ...const writeToShard$ = (flights$: Observable<Flight>, start: Date, end: Date, shard: string) => {  return flights$.pipe(    // Filter Results by Date    filter((flight: Flight) => flight.departsAt >= start && flight.departsAt < end ),    // Collect batches of 1000    bufferCount(1000)    // On the resulting batches, insert into Neo4j    // ...  )}The bufferCount operator will convert a stream of individual flights into an array of flights (or Flight[] in TypeScript terms).We can create a function to accept an array of flights and insert them into Neo4j using the RxSession. We’re not interested in the individual records, but instead the outcome of the query — namely the number of rows written and the time taken to insert the data. So we can create a new interface to represent this:interface Summary {  shard: string  writes: number  availableAfter: number}Now we can use the Neo4j driver to create a new session and run the query. Because we’re not interested in any rows, just the summary, we can call .consume() instead of .records().This will return an observable which omits the ResultSummary for the query. This can be used to get the update statistics for the query.Once the query result has been consume d, and the ResultStatement has been safely produced, we can use the concat operator to close the session and release any resources that it is currently holding. The concat operator will ensure that the session is closed before the result is emitted.// Import dependenciesimport neo4j, { ResultSummary } from neo4j-driverimport { concat, map } from rxjs/operators// Create one instance of the driver per applicationconst driver = neo4j.driver(bolt://localhost:7687, neo4j.auth.basic(neo4j, neo))// ...const writeFlightsToNeo4j = (batch: Flight[], shard: string): Observable<Summary> => {  const cypher = `    USE fabric.${shard}    UNWIND $batch AS row        MERGE (origin:Airport {code: row.origin})    MERGE (destination:Airport {code: row.dest})        MERGE (f:Flight {id: row.year +-+ row.month +-+ row.day +--+ row.flight})    SET f.price = coalesce(parseFloat(row.price), f.price)        MERGE (f)-[:ORIGIN]->(origin)    MERGE (f)-[:DESTINATION]->(destination)  `  const session = driver.rxSession({ defaultAccessMode: neo4j.session.WRITE })  return session.run(cypher, { batch })    .consume()    .pipe(      // Close the Session      concat(session.close()),      // Convert the neo4j ResultSummary into our Summary      map((result: ResultSummary) => (<Summary> {         shard,        writes: result.counters.updates().nodesCreated,         availableAfter: result.resultAvailableAfter.toNumber()       }))    )}To use this function within the stream, we can use the switchMap operator. By using this operator, the previous observable (ie. the one that produced the batch) is cancelled and replaced by the new observable which is produced by the function — in this case an observable which emits a Summary .import { filter, bufferCount, switchMap } from rxjs/operators// ...const writeToShard$ = (flights$: Observable<Flight>, start: Date, end: Date, shard: string) => {  return flights$.pipe(    // Filter Results by Date    filter((flight: Flight) => flight.departsAt >= start && flight.arrivesAt < end ),    // Collect batches of 1000    bufferCount(1000),    // On the resulting batches, insert into Neo4j    switchMap(batch => writeFlightsToNeo4j(batch, shard))  )}Subscribing to the subjectNothing will happen to this stream until it has at least one subscriber. We could call subscribe on each method call, but instead we can use the merge function exported by rxjs to combine the three observables into a single observable. This will mean we only need to create a single subscriber through which all results will be emitted.import { merge } from rxjsconst jan$ = writeToShard$(flights$, new Date(2020-01-01), new Date(2020-02-02), january2020)const feb$ = writeToShard$(flights$, new Date(2020-02-01), new Date(2020-03-02), february2020)const mar$ = writeToShard$(flights$, new Date(2020-03-01), new Date(2020-04-02), march2020)merge(jan$, feb$, mar$)  .subscribe({    next: (summary: Summary) => console.log(      wrote, summary.writes,       records to , summary.shard,       in, summary.availableAfter, ms    ),    error: (error: Error) => console.error(error),    complete: () => console.log(Completed import)  })Completing the streamRunning this code at the moment will produce a stream of results, but the complete function will not be called, and the process will not be exited. This is because we need to explicitly close the original stream. You may have noticed earlier that I created readCsvFinished$ which listened a done event but I hadnt used it:// Observable will emit when the entire file has been readconst readCsvFinished$: Observable<void> = fromEvent(readStreamEventEmitter, done)// Stream of objects representing CSV rowsconst flights$: Observable<object> = fromEvent(readStreamEventEmitter, data)We can use this observable to explicitly complete the flights$ — automatically unsubscribing all subscribers, closing the stream, and releasing all resources. To do this, we can use the takeUntil operator. This instructs the observable to emit results until another observable emits a value. In this case, when the done event is emitted by the File Reader, the file has finished being read.import { map, share, takeUntil } from rxjs/operatorsconst flights$: Observable<Flight> = fromEvent(readStreamEventEmitter, data)  .pipe(    // Convert the raw row into a Flight    map({ /* ... */),    // Emit values until the File Reader emits the  done  event    takeUntil(readCsvFinished$),    // multicast the values amongst subscribers     // without creating a new observable    share()  })Deadlocks and retryingThe benefit to this approach is that the rows are processed and batched as they are received which means that the overall import will be quicker. However, this may lead to what are known as deadlock errors in Neo4j.When two competing transactions attempt to modify the same record, the first will acquire a lock on the node until the transaction has been either committed or rolled back.If the second transaction cannot acquire the lock on the same node within a reasonable timeframe, a Neo.TransientError.Transaction.DeadlockDetected error will be thrown and the transaction will be rolled back.You may see something similar to the following error message in your neo4j.log or debug.log :2020-09-15 10:38:09.150+0000 ERROR Client triggered an unexpected error [Neo.DatabaseError.Statement.RemoteExecutionFailed]: Remote execution failed with code Neo.TransientError.Transaction.DeadlockDetected and message ForsetiClient[21] cant acquire ExclusiveLock{owner=ForsetiClient[23]} on NODE(2183), because holders of that lock are waiting for ForsetiClient[21].This is referred to as a transient error because it is temporary because that lock will only be held for a short time. There is retry logic built into the drivers for transient errors and a Maximum Transaction Retry Time can be configured but there may be cases where one or more of these errors leak into the application.This is particularly applicable in this use case, because of our simple model model:(:Airport)<-[:ORIGIN]-(:Flight)-[:DESTINATION]->(:Airport)When creating a relationship, a transaction will acquire a lock on the nodes at both ends of the relationship before creating the relationship. With busy airports like London Heathrow (LHR) or Frankfurt (FRA), these nodes are likely to be locked by many transactions.You may be able to avoid this problem by further filtering results to segregate airports within a batch to avoid concurrent transactions locking the same node, or limit the application to sending one write query at a time — but that is beyond the scope of this article.A rudimental way to handle these errors would be to use the retry operator. The retry operator retries the sequence of pipes for a predefined number of times before the error should be passed through to subscribers.Think of a mobile device with an intermittent signal — you may want to try a HTTP request a few times before forcing the user to re-submit a form.In our case, the error may be resolved in a few ms, so we could just try to run the query again:import { concat, map, retry } from rxjs/operatorsconst writeFlightsToNeo4j = (batch: Flight[], shard: string): Observable<Summary> => {  // ...  return session.run(cypher, { batch })    .consume()    .pipe(      // Close the Session      concat(session.close()),      // Convert the neo4j ResultSummary into our Summary      map(/* ... */),      // Retry 3 times before giving up      retry(3)    )}Bonus: Extracting airportsTo demonstrate how easy it can be to manipulate data, you can also create an Observer to get a list of airports with a few lines of code.The pluck operator will take a value from an emitted object — in this case the origin and the distinct operator will ensure that only distinct values are emitted.import { distinct, pluck } from rxjs/operatorsflights$.pipe(  pluck(destination),  distinct(),  bufferCount(50),   take(2)).subscribe(batch => console.log(batch))This will produce two distinct batches of 50 airports:[  EWR, IAH, LGA, JFK, MIA, BQN,  ATL, ORD, FLL, IAD, MCO, PBI,  TPA, LAX, SFO, DFW, BOS, LAS,  MSP, DTW, RSW, SJU, PHX, BWI,  CLT, BUF, DEN, SNA, MSY, SLC,  XNA, MKE, SEA, ROC, SYR, SRQ,  RDU, CMH, JAX, CHS, MEM, PIT,  SAN, DCA, CLE, STL, MYR, JAC,  MDW, HNL][  BNA, AUS, BTV, PHL, STT, EGE,  AVL, PWM, IND, SAV, CAK, HOU,  LGB, DAY, ALB, BDL, MHT, MSN,  GSO, CVG, BUR, RIC, GSP, GRR,  MCI, ORF, SAT, SDF, PDX, SJC,  OMA, CRW, OAK, SMF, TYS, PVD,  DSM, PSE, TUL, BHM, OKC, CAE,  HDN, BZN, MTJ, EYW, PSP, ACK,  BGR, ABQ]ConclusionReactive programming is a new paradigm for me, but I found it a fun way to manipulate streams of data. One benefit to this approach is that many streams can be combined into a single pipeline — you can consume events from many sources and update the database in real time.We can also merge events provided from different sources into a single stream — we can manage real-time streams coming in from other data sources. For example, a web-socket connection from another micro-service or events provided by message queues like Kafka or RabbitMQ.The huge number of operators available mean that you can take those streams and do some pretty powerful things to the data.On the downside, there are a lot of operators to remember and those operators aren’t always logically named. They can be hard to find if you don’t know specifically what to google.I spent a long time searching for batching operators before someone pointed me in the direction of buffer - which, in my mind, meant something completely different.The RxJS documentation is also scattered, which can be frustrating but I’m sure that will be fixed over time.The Learn RxJS website that I have linked to in this post is a good place to look if you know the operator name. Personally, I also learned the basics from Ultimate Courses, but I’m sure there are also many free resources around too.A full code example is available on Github in the rx-etl folder. If you have any comments, questions, or spot anything that could be improved, feel free to reach out. Im @adamcowley or you can raise an issue or open a PR in the repository.For more content like this, join us on the Neo4j Twitch Channel.;Sep 23, 2020;[]
https://medium.com/neo4j/graph-visualization-of-panama-papers-data-in-neo4j-9c08ca17039c;William LyonFollowDec 3, 2018·7 min readGraph Visualization of Panama Papers Data In Neo4jRevisiting ICIJ’s Offshore Leaks In The Face Of The Latest Deutsche Bank ScandalThe Panama Papers / Offshore Leaks Neo4j database data model. The database contains offshore legal entities and connections to them.News last week that the headquarters of Deutsche Bank was raided in a money laundering investigation as a result of the Panama Papers investigation has rekindled interest in the public Neo4j dataset that ICIJ released as part of the Offshore Leaks database. In this post, we take a look at how we can explore the Panama Papers data in Neo4j, in the context of the recent Deutsche Bank investigation.First, we’ll see how we can access the dataset in Neo4j, using either Neo4j Desktop or Neo4j Sandbox. Then we take a look at some simple Cypher queries we can use to search for offshore companies connected to Deutsche Bank, and how to visualize the results, both in Neo4j Browser and Neo4j Bloom. Finally, we learn to use virtual nodes and relationships to simplify the graph visualizations.We’ve previously seen some great examples of how to analyze the Offshore Leaks database in Neo4j, so if you’re not familiar with the dataset I’d suggest you first give some of those a quick read. They’re quite interesting, showing how to create geospatial data visualizations, even showing the shortest path from Rex Tillerson to the Queen of England in the dataset.To follow along, the first step is to access the Offshore Leaks Neo4j database. This database can either be downloaded from the ICIJ website as Neo4j Desktop For ICIJ” or accessed via Neo4j Sandbox without any download.Neo4j Desktop For ICIJThe ICIJ has made the Offshore Leaks database (which includes Panama Papers and Paradise Papers) available as a downloadable Neo4j database.The database includes an interactive Neo4j Browser Guide that embeds images, database queries, and text to walk you through exploring the dataset. You can see a screencast here of how it all works.Neo4j SandboxNeo4j Sandbox allows you to spin up Neo4j instances on demand hosted in the cloud. You can choose from several different sandbox use cases” that include datasets and guides to walk you through querying and visualizing the data.Connection credentials for a Neo4j Sandbox instance.After signing in to Neo4j Sandbox you’ll see Panama Papers by ICIJ” as one of the sandbox use cases to choose from. Choose Launch Sandbox” and after a few seconds, your personal Neo4j instance loaded with the Panama Papers dataset will be available and ready to start querying.Previous posts have shown how to explore the data in Neo4j Browser using Cypher, so instead of rehashing those techniques, we’ll jump straight into visualizing the data using Neo4j Bloom.Graph Visualization With BloomNeo4j Bloom is a graph exploration application for visually interacting with graph data. Bloom is designed to enable exploration of the graph without writing Cypher. Bloom is available through Neo4j Desktop, but you’ll need an access key to install it. You can request an activation key here. Once you have your activation key it can be installed the way other Graph Apps are installed in Neo4j Browser.Neo4j Bloom is one of many Graph Apps” that can be installed via Neo4j Desktop.Once Bloom is installed in Neo4j Desktop it will automatically connect to the active Neo4j database. If you’re using Neo4j Sandbox you’ll need to create a remote graph” using the connection credentials specific to your private Neo4j sandbox instance.Bloom uses natural search phrases to query the graph. Here we use a natural search phrase to search for all Entity nodes that contain Deutsche Bank” in their name. We can then expand out from these nodes to find all connected Entities, Officers, Intermediaries, etc:Querying the graph in Neo4j Bloom using a natural search phrase.In Bloom we can also zoom in and inspect the properties of nodes and relationships. Here we inspect one of the Deutsche Bank matches, Deutsche Bank (Cayman) Limited”. We can see that this entity serves as an officer of several other offshore legal entities, and also served as the intermediary” (a sort of go-between for creating an offshore legal entity)for several offshore entities:Inspecting a specific Deutsche Bank offshore legal entity.Let’s take a step back from Bloom and see how we can query the dataset using Cypher in Neo4j Browser, using for context some of the recent reports about the current Deutsche Bank investigation.Querying With CypherRecent reporting of the latest Deutsche Bank raids has identified a handful of specific Deutsche Bank subsidiaries that were allegedly involved in the money laundering scheme at the center of the most recent investigations. One of these is Regula Limited”, an offshore legal entity registered in the Bahamas and British Virgin islands that was connected to Deutsche Bank, as a wholly owned subsidiary.We can search for Regula Limited and all the companies it is connected to in the Panama Papers dataset with this Cypher query in Neo4j Browser:MATCH path=(regula:Officer)-->()WHERE toUpper(regula.name) CONTAINS  REGULA LIMITED RETURN pathSearching for Regula Limited” and associated connections in the Panama Papers dataset.One observation we can quickly make is that there are many Regula Limited” nodes, connected by SAME_NAME_AS relationships. The reason for these seemingly duplicated nodes is that the data came from several sources and the ICIJ was not able to determine with absolute certainty that these are in fact the same legal entities. Obviously, they share a name but given the data available, the ICIJ could not make the assumption that they are the same legal entity. However, for our purposes of exploring the data today we’d like to make the assumption that any legal entity with the name Regular Limited” is referring to the same legal entity.Cases like this, where we want to collapse a group of nodes into a single node, including the relationships of the original nodes, are an example of graph simplification. The idea of graph simplification is that we have some internal representation of the data in the database, but we want to present a simplified representation of the data to the user for visualization. One way to accomplish this simplification is through the use of virtual nodes and relationships.Graph Simplification With Virtual Nodes And RelationshipsVirtual nodes and relationships are graph objects that do not exist in the database, but can be constructed at query time to be used by tools like Neo4j Browser or Bloom for visualization. We can create virtual nodes using the APOC procedure library and the functions apoc.create.vNode and apoc.create.vRelationshipHere’s an example showing how we can collapse all Regula Limited” nodes into a single node, including all relationships, using virtual nodes and relationships:MATCH (off:Officer) WHERE toUpper(off.name) CONTAINS  REGULA LIMITED WITH off AS collapsed LIMIT 1WITH apoc.create.vNode([ Regula ], collapsed {.*}) AS collapsedMATCH (off)-[col1]->(e:Entity) WHERE toUpper(off.name) CONTAINS  REGULA LIMITED WITH collapsed, off, e,   apoc.create.vRelationship(collapsed, type(col1), {}, e) AS r1MATCH (e)<-[r2]-(o2) WHERE (o2:Officer OR o2:Intermediary)   AND NOT toUpper(o2.name) CONTAINS  REGULA LIMITED RETURN collapsed, e, r1, r2, o2Running this query in Neo4j Browser gives us a simplified view of the data. Now we can more easily see the connections to Regula Limited”:Visualizing Deutsche Bank subsidiary, Regula Limited, as a single node in the graph using virtual nodes and relationships.Virtual Nodes and Relationships In Bloom With Search PhrasesCreating a Search Phrase in Bloom to use virtual nodes and relationships in our graph visualization.We saw how to simplify our graph visualization with virtual nodes and relationships in Neo4j Browser, but how can we use the same technique in Bloom? To create virtual nodes and relationships in Bloom we will make use of a feature in Bloom called Search Phrases.Search Phrases allow us to define a parameterized Cypher query that can be used as part of the natural language search in Bloom, with user provided values for the parameters at search time. For example, we will create a search phraseCollapse Officer named $officer into label $labelthat maps to the Cypher query we used above for creating virtual nodes and relationships for Regular Limited. But because we parameterize it, the search phrase can be used to collapse any entity with inputs specified by the user. Here’s how we use it in Bloom:Using a search phrase in Bloom to collapse all Regula Limited nodes into a single node in the visualization.Here are some resources to help explore the data further:ResourcesLearn more about Bloom features in the docs. You can also register your interest in Bloom here.The Offshore Leaks Database by ICJI;Dec 3, 2018;[]
https://medium.com/neo4j/whats-cooking-part-2-what-can-i-make-with-these-ingredients-7df9dc129993;Mark NeedhamFollowJan 28, 2019·4 min readWhat’s cooking? Part 2: What can I make with these ingredients?Last week my colleague Lju started off a series of posts showing how to import the BBC goodfood dataset into Neo4j. Lju also shows how to query the dataset to find recipes written by the same author.In this post we’re going to continue from where Lju left off and explore the dataset a bit more. Imagine that we have some chillis and want to find some recipes that will make use of those chillis.We could write the following query, which finds recipes containing chilli, and then returns the name of the recipe along with its list of ingredients:MATCH (r:Recipe)WHERE (r)-[:CONTAINS_INGREDIENT]->(:Ingredient {name:  chilli })RETURN r.name AS recipe,        [(r)-[:CONTAINS_INGREDIENT]->(i) | i.name]        AS ingredientsIf we run that query, we’ll see this output:Recipes containing chilliI like the look of the beef skewers!But what about if we want to find recipes that contain more than one ingredient, for example chilli and prawn?One approach would be to update our WHERE clause to find all recipes that contain those ingredients:MATCH (r:Recipe)WHERE (r)-[:CONTAINS_INGREDIENT]->(:Ingredient {name:  chilli })AND   (r)-[:CONTAINS_INGREDIENT]->(:Ingredient {name:  prawn })RETURN r.name AS recipe,        [(r)-[:CONTAINS_INGREDIENT]->(i) | i.name]        AS ingredientsLIMIT 20If we run that query we’ll see this output:Recipes containing chilli and prawnsThat works well if we know how many ingredients we want to search for, but not so well if we want to search for a dynamic number of ingredients. To search for a variable number of ingredients we’ll need to use the all predicate function.Let’s move away from chillis and prawns, and this time find recipes that contain eggs, onions, and milk. We’ll then order the resulting recipes by their number of ingredients in ascending order::param ingredients =>   [ egg ,  onion ,  milk ]MATCH (r:Recipe)WHERE all(i in $ingredients WHERE exists(  (r)-[:CONTAINS_INGREDIENT]->(:Ingredient {name: i})))RETURN r.name AS recipe,        [(r)-[:CONTAINS_INGREDIENT]->(i) | i.name]        AS ingredientsORDER BY size(ingredients)LIMIT 20If we run that query we’ll see this output:Recipes containing eggs, onions, and milkUnfortunately I’m allergic to most of the dishes so far, so I want to try and find something that I can eat.We’ll add a parameter containing allergens to go with our ingredients parameter.We can then use the none predicate function to make sure that we don’t return any recipes containing the allergens::param allergens =>   [ egg ,  milk ]:param ingredients => [ coconut milk ,  rice ]MATCH (r:Recipe)WHERE all(i in $ingredients WHERE exists(  (r)-[:CONTAINS_INGREDIENT]->(:Ingredient {name: i})))AND none(i in $allergens WHERE exists(  (r)-[:CONTAINS_INGREDIENT]->(:Ingredient {name: i})))RETURN r.name AS recipe,        [(r)-[:CONTAINS_INGREDIENT]->(i) | i.name]        AS ingredientsORDER BY size(ingredients)LIMIT 20Recipes containing coconut milk and rice, but excluding egg and milkThere are still a bunch of tasty looking recipes returned here, even though we’ve taken out two key ingredients.In this post we’ve learnt how to use three of Cypher’s predicate functions: none(), all(), and exists().There are two others that we didn’t look at: any(), which can be used to find if a recipe contains any of a collection of ingredients, and single(), which can be used to find recipes that contain exactly one of a collection of ingredients.For now though, I’ll hand over to Lju for part 3 of the series!;Jan 28, 2019;[]
https://medium.com/neo4j/neo4j-query-log-analyzer-and-db-analyzer-update-for-neo4j-4-0-with-new-features-181081a464a7;Kees VegterFollowJan 30, 2020·4 min readNeo4j Query Log Analyzer and DB Analyzer updated for Neo4j 4.0With the release of Neo4j 4.0 my ‘Graph Apps’ for the Neo4j Desktop needed an update. The new Neo4j version, is a very big update with a lot of new features. An important feature is multi-database where you can run more than one graph database on a Neo4j Server.You can install both apps into Neo4j desktop from: https://install.graphapp.ioQuery Log Analyzer 1.0.1The first version of this tool as described here was able to analyze an uploaded query.log file. I also gave a presentation at NODES 2019NODES 2019 presentationWith this new version the following new features are added:Multi-databaseThe Neo4j version 4 capability to run multiple databases leads to extra information in the query log file. You can now see in the query log on which database the query was executed. This will now be displayed in the tool:This query is executed 27 times on database, neo4jDatabase connectionIn this version of the Query Log Analyzer, a connection to database is made which makes the following functions available:Explain PlanCurrent QueriesQuery StatsExplain PlanWhen you have a Query displayed (less than 10000 characters) in the tool you can start a Explain Plan” of this query by pressing the following icon.After clicking, a Query Details screen will be shown with the Query Plan in it.You can zoom in zoom out on the plan, the current selected step Procedure Call is shown at the right. When you click Next, you jump to the next Query Planning step.With the color of the ‘Step’ you can see if the step is expensive or not, so things like All Nodes Scan or Full Label Scan are in red. The Explain Plan will be executed on the database where the query was executed on.Current QueriesWith this function it is possible to show the current running queries on the database. In the figure below you see a screenshot when working with version 4 of the Neo4j database. Per database you can list the current running queries.Here is a connection to a version 4 database which runs two databases, neo4j and base1Query StatsSince version 3.5.4 the database collects the query statistics for the last 8192 invocations and keeps that in memory. This is a great way to see what the latest load was on the server. If you want to fine tune your queries you have here the query times in micro seconds (the query log uses milliseconds). This also works when the query logging to file is switched off.When you click on the timeline icon, you can see the Invocation Time Line for this query.Neo4j Database Analyzer 1.0.1The main change in this tool is that can handle multiple databases on a Neo4j server version 4. You will now have a tab per database. The functionality of the Neo4j Database Analyzer is described in the introduction of this tool.The tool is connected to a Neo4j version 4 database, where two databases, neo4j and base1 are running.ResourcesThe source code for the Query Log Analyzer is on Github at kvegter/query-analyzer-app. There you can read the documentation and report issues.The source code for the Neo4j db Analyzer is on Github at kvegter/dbreportapp. There you can read the documentation and report issues.If you have questions regarding the query performance, you can always head to the #help-cypher channel on the Neo4j Users Slack or on the neo4j community.;Jan 30, 2020;[]
https://medium.com/neo4j/a-timely-update-to-the-bolt-protocol-aa5c8dbdefb8;Florent BivilleFollowNov 3, 2022·5 min readA Timely Update to the Bolt ProtocolThe Persistence of Memory by Salvador DalíOur main mission in the Drivers team is to offer an idiomatic and consistent developer experience, regardless of the target programming language and Neo4j deployment topology.Consistency manifests itself in the concepts and API the various drivers expose, but also in the way they behave.The main way we ensure this is through our shared suite of acceptance tests, also known as TestKit.To that end, when my colleague Rouven worked on introducing date and time-related tests to TestKit he noticed something strange going on.Neo4j History and Bolt Protocol 101Before diving into the core of the issue, let us recap what purpose the Bolt protocol serves.Skip this and the next section if you are already familiar with the protocol.Historically, Neo4j started as a JVM-only embedded database.Indeed, it would only run co-located to your application, sharing the same environment (shared heap, shared garbage collection cycles, …).Around Neo4j 1.0 (ca. 2010), a REST API was introduced and Neo4j could be deployed as a standalone server with the default port 7474 that you all know and love.Clustering appeared shortly after.A few years later (ca. 2015 — early 2016), Neo4j 3.0 came out and its brand new binary application protocol appeared (with the default port 7687). The Bolt protocol was born, as well as the Bolt server component and a set of official drivers for Java, Python, and JavaScript implementing it (.NET and Go came along a bit later.)In a nutshell, the Bolt protocol specifies how clients and servers interact via specific Bolt messages, exchanging data following the PackStream format.You can learn more about it here.DateTime* StructuresAmong these data structures, you can find DateTime and DateTimeZoneId.They both encode a localized point in time, defined in seconds and nanoseconds.They can be:Cypher query parametersResults of Cypher queries when invoking one of the temporal functionsReturned nodes’ and/or relationships’ date/time propertiesThey differ by the way they localize the point in time:DateTime specifies the offset from UTC in seconds.DateTimeZoneId specifies the localization with a timezone name.Let’s illustrate how they work.Take 1970–01–01T02:15:00.000000042+01:00 as a DateTime.The corresponding UTC time is 1970–01–01T01:15:00.000000042Z (Z denotes UTC).The number of seconds since the Unix epoch is 1 hour and 15 minutes, i.e. 4500 seconds.The offset is 1h, i.e. 3600 seconds.The localized number of seconds is 4500+3600, i.e. 8100 seconds.The resulting DateTime will therefore be as follows:{    seconds: 8100    nanoseconds: 42,    tz_offset_seconds: 3600}Let’s do the same with DateTimeZoneId and 1970–01–01T02:15:00.000000042[Europe/Paris].The UTC offset for this timezone at that point in time is +1 hour.Therefore, the UTC time is 1970–01–01T01:15:00.000000042Z.From there, the same computations as above occur and the resulting DateTimeZoneId is as follows:{   seconds: 8100   nanoseconds: 42,   tz_id: Europe/Paris”}Back to SwedenRouven noticed a problem with the following point in time: 1980–09–28T02:30:00[Europe/Stockholm], i.e. September 28, 1980 at 02:30 AM in the Europe/Stockholm timezone.You can try it in a program by running the following Cypher query RETURN datetime ( 1980–09–28T02:30:00[Europe/Stockholm] )and extracting the results.Did something happen on September 28, 1980 in Sweden that could cause an issue?(Sweden — it has to be said — has some form in this area, as Jon Skeet famously pointed out.)Here I Come to Save the Day(light)!The answer is yes!In 1980, Sweden started to implement Daylight Saving Time (DST), also known as Summer Time.DST consists of shifting the clock at different times of the year to adjust waking hours to daylight hours.The clock is usually advanced one hour in winter, thus creating a gap.For example, a country could decide to implement time shifts at 2 AM: after 1:59:59 AM, the clock moves forward to 3:00:00 AM — 2:15:00 AM for instance does not occur.The clock is usually set backward by 1 hour in summer, thus creating an overlap.Following the same example, after 2:59:59 AM, the clock is set back to 2:00:00 AM — 2:15:00 AM e.g. occurs twice with different UTC offsets.The Ambiguity of DateTimeZoneIdLet’s try to convert 1980–09–28T02:30:00[Europe/Stockholm] to a DateTimeZoneId as represented in the Bolt protocol.First, we need to determine the corresponding UTC time.On that specific day, the clock was set back to 2:00:00 AM after the first occurring hour of 2:59:59 AM.Therefore, there was two 2:30:00 AM with different UTC offsets.Since this time represents an overlap, we cannot know which offset to use!To make matters worse, most languages will silently resolve this datetime.The following Go program, when run on my machine, will print an offset of 1 hour.This will print: The offset is: 3600s.The Go API time.Date explicitly documents:Date returns a time that is correct in one of the two zones involved in the transition, but it does not guarantee which.The following Python program shows that the programmer has a little more control over the localization process (notice the second localize call and its parameter is_dst):Overall, the ambiguity remains and may cause bugs further down the line.Time for a FixThe root cause of this issue is that the seconds field of DateTimeZoneId includes the offset we sometimes cannot resolve.One way to resolve the ambiguity is to encode the seconds field of DateTimeZoneId as UTC time (as well as the seconds field of DateTime’s for consistency’s sake).Indeed, UTC time is monotonic (it only ever grows) and is therefore non-ambiguous.Going back to the dreaded September 28, 1980 at 2:30 AM in the Europe/Stockholm timezone, the seconds field of UTCDateTimeZoneId (the UTC-encoded replacement of DateTimeZoneId) will either be:338949000 seconds, i.e. 1980–09–28T02:30:00+02:00Or 338952600 seconds, i.e. 1980–09–28T02:30:00+01:00No more ambiguity — problem solved!Correction TimelineThe UTC-aware structures are available in all the Neo4j 5 releases.They are also available in any Neo4j 4.4 release following 4.4.12 (included), if the driver requests it and the server accepts the request.That way, older drivers connecting to newer servers or vice-versa will continue to work with the existing datetime structures, thus not causing any disruption.;Nov 3, 2022;[]
https://medium.com/neo4j/neo4j-go-driver-is-now-generally-available-307159e623ed;Florent BivilleFollowDec 14, 2020·2 min readNeo4j Go Driver is now Generally AvailableThe official Neo4j Go Driver 4.x is now out of beta!Gopher artwork by Ashley McNamara (CC BY-NC-SA 4.0)As you might have heard in my last joint stream with Eric Solender on GoGM, the 4.2.0 version of the Go Driver for Neo4j has been released this week and supports the following Neo4j versions:3.5, which is a Long Term Version4.04.14.2neo4j/neo4j-go-driverAligned with Neo4j database version 4.2. Driver supports Neo4j databases of versions 3.5, 4.0, 4.1 and 4.2. Added…github.comThis release means that the Go Driver is finally aligned with all the other official Neo4j drivers.From now on, every stable release of the Go Driver will be coordinated with the other official driver releases, hurray!Our examples have already been updated, such as the unmissable Movies repository for Go.neo4j-examples/movies-golang-boltMovie Example App and back-end with the Neo4j GO driver…github.comIf you want to get started, follow the previous link or just copy and adapt these few snippets.First, you need to initialize your project and add the 4.x driver dependency:go mod init github.com/<org>/<name>go get github.com/neo4j/neo4j-go-driver/v4Once you’ve done that, you’re ready to write your first program and run your first queries:Line 11 include the credentials of a ready-to-use Neo4j Sandbox instance. The Neo4j Sandbox is a very convenient way to get started with Neo4j in a few clicks. If you like managed services, we also got you covered for production with Neo4j Aura!Lines 21 to 25 instantiate the (usually singleton) Driver so that we can open new sessions and run queries against our configured Neo4j instance.Once the session is opened (and closed later via defer) on lines 26 and 27, we can finally run a query via a read-only transaction function from lines 28 to 49, in order to fetch all the actor names playing in the legendary Matrix” movie.And that is all that is needed to get started!Happy Coding,Florent;Dec 14, 2020;[]
https://medium.com/neo4j/tips-and-tricks-in-neo4j-bloom-41e4b3b1cc8f;Ljubica LazarevicFollowJan 9, 2020·7 min readTips and tricks in Neo4j BloomLooking at some Bloom patterns to help explore your dataIntroductionNeo4j Bloom is a graph visualisation, exploration and communication tool. It uses near-natural language querying without reliance on user knowledge of Cypher.Neo4j Bloom only works with the Enterprise Edition of the database. Bloom is available by license, and also through the Startup Program.Getting started in BloomWhilst this post is targeted more at users who have had some experience with Bloom, it may be helpful to those just starting out with Bloom.If you’re completely new to Neo4j Bloom, I would recommend you explore these very useful resources:You might want to start out with the Bloom Sandbox.You will want to check out the excellent User Interface Guide.You may want to watch the Bloom video series.You can find the Bloom documentation here.The Neo4j Community board also has a dedicated Bloom category.Introducing the Bloom patternsA reasonably common question I’ve encountered with Bloom has been along the lines of the best way to ask a question. Many Bloom search phrases tend to be very similar, depending on the type of question. The goal of the patterns presented is to try and categorise these into groups. These patterns should help quickly identify how to ask questions in the form of Bloom phrases. The following patterns we’ll cover are:Specific pathShortest pathNode pathsMore than one ofExtended more than one ofWe’re now going to go over these Bloom patterns to assist exploring your data. We’ll have a look at how we get information of interest back using near natural language, how we can explore our data based on our domain knowledge of it. We’ll also flag any ‘be aware’ situations with the pattern, such as unexpected behaviours. I also covered these patterns during my GraphConnect ’18 talk if you prefer the video version.The dataWe will be using a cut-down data set from Kaggle which has data from the Summer and Winter Olympics between 1896 and 2016.If you’d like to follow through this post, you can import the data running this set of Cypher queries in Neo4j Browser. You will need to have multi-line statements enabled. If you’re not sure how to do that, you can find more information here.One thing to be aware of — the data set does include athlete initials, middle names, etc. so you will see verbose names in some of the examples.Our data modelPattern #1 — Getting a specific pathWhy use this pattern?This is a great example of being able to ask a question in near natural language in Bloom, powered by a sensible data mode. We use this pattern when we want to retrieve all of the information from a specific start point. For example we may be asking a question from an anchor point.Question 1 — Which Olympic games did athlete Helen Glover compete in?Helen Rachel M. Glover part of Team participated in GamesThere are a few interesting observations to point out with our Bloom search phrase:You don’t need to tell Bloom that Helen Glover is an Athlete. For databases with less than a thousand nodes, Bloom will scan the properties and try to find which nodes match the string you’ve provided. For larger databases, Bloom will have a look at what indexes are in use, and use those to auto-populate lists with suggested names, and try and match the provided string.Bloom will take your relationship types and turn them in to user-friendly alternatives. We did not need to put in PART_OF — Bloom will take relationship types, convert them to lower case and split out words accordingly (e.g. replacing underscores with spaces). This not only makes the user experience of searching for data more friendly, but places that important emphasis on a good data model.It’s also worth noting that Bloom will also fill the gap for you. This means that you don’t always have to specify all the categories and relationships, in fact, you can miss either all the categories or all the relationships. For example, the following will both do the same thing:Helen Rachel M. Glover Team GamesHelen Rachel M. Glover part of participated in GamesPattern #2 — Showing the shortest pathWhy use this pattern?This pattern is very useful for starting to understand the shortest path between two nodes.Question 2 — What’s the shortest path connecting athletes Helen Glover and Serena Williams?To answer this question requires a couple of steps:Bring back the Helen Glover and Serena Williams nodes, we can do this by running these two phrases:Helen Rachel M. GloverSerena Jamika Williams2. Select both nodes and then right-click for the path->shortest path:A word of warning: The shortest path returned may not necessarily be the one you were expecting. In this example, there could be a number of things that would result in the shortest path of equal length, such as common countries, etc. The shortest path function will only return one of the shortest paths, not all of them.Pattern #3 — Paths between nodesWhy use this pattern?This pattern is very useful for revealing potentially different paths of a certain pattern between set start an end points. For example, we encountered that we may get interesting/unexpected shortest paths returned in the previous example. Here, if we know paths of interest, we can force the pattern to get responses more in-line with our expectations.Question 3 — What Olympic games link athletes Helen Glover and Serena Williams?Helen Rachel M. Glover part of Team participated in Games participated in Team part of Serena Jamika Williamsor, allowing Bloom to fill the gaps for us:Helen Rachel M. Glover Team Games Team Serena Jamika WilliamsPattern #4 — More than one type ofWhy use this pattern?This pattern is useful for when you’re looking for more than one instance of an element. The way we approach this in Bloom is repeat the item we’re looking for more than one of around the ‘pivot’ point of our question.Question 4— Which cities have held the Olympic games more than once?Games held in City held in GamesOrGames City GamesIn this example, we want to know where there’s more than one of Games, so we use City as a pivot point, and repeat Games either side of it.Please bear in mind this is a reduced data set, so not all cities which have hosted multiple Olympics (e.g. Athens) will be displayed.Pattern #5 — Extended more than one type ofWhy use this pattern?Much like the previous pattern, we want to find more than one instance of an element. However, the element in question isn’t always conveniently connected to our chosen ‘pivot’ point. We still use the same principles from the previous pattern, and our query will look very much like a palindrome!Question 5— Which athletes have won more than one gold medal?Medal type Gold won Team part of Athlete part of Team won Medal type GoldFor those of you who have not encountered it, we’re now calling properties on the nodes, here we’re calling the type property on Medal. This allows us to specifically only query Gold medals. As this property will not have an index on it (there are only three variants), it will not be automatically picked up by Bloom so we need to specifically query for it. Of course, you can query node properties with indexes this way as well! You will notice that we mirror the phrase around our pivot point, which is Athlete.Some words of warning:Nodes with more than one path (relationship) will be revisited.You may notice only 327 Athlete nodes returned, and this figure should be a lot higher. This is because to improve performance, Bloom will place (configurable) internal limits on the amount of data returned, which may cause result sent truncation. Whilst Bloom is an excellent tool to examine samples of data, it cannot guarantee returning all matching results.SummaryIn this post we’ve explored some categorisations of Bloom phrases and examples to assist your data exploration.We’ve also covered some of the mechanics of how Bloom works, and some scenarios to be aware of.;Jan 9, 2020;[]
https://medium.com/neo4j/whats-new-in-neo4j-spatial-features-586d69cda8d0;David AllenFollowMay 24, 2018·4 min readWhat’s New in Neo4j 3.4: Spatial FeaturesNeo4j 3.4.0 was recently released, and it has some big new features in it. While there are good roundups of what’s in this release overall, I wanted to focus in on one of my favorites, the spatial features and go a bit deeper with some technical examples.In this article we’ll cover how to create points with neo4j 3.4, how to compute distances between points on the globe, and then go one step further — by themselves those are good features, combined with other things you could already do with neo4j, it gets even better.First though, we’ll need some data with latitude and longitude so we can place some points on a map.Getting Started With DataWe’ll need some data to show how this works.Here’s a picture of what the source data looks like:And here’s the cypher to load it. If you’re new to neo4j, you can find documentation on how LOAD CSV works here.This loads some simple data on around 7,000 world cities, complete with latitude and longitude into a simple graph of cities, countries, and provinces. The spatial part of this load process is simply the location” attribute of the city. Note that we create a spacial point by passing an object with latitude and longitude to the point” function, taking care to make sure that the data type we’re passing in is a float and not a string.LOAD CSV WITH HEADERS FROM https://simplemaps.com/static/data/world-cities/basic/simplemaps-worldcities-basic.csv as line CREATE (c:City {   name: coalesce(line.city, ),   name_ascii: coalesce(line.city_ascii, ),   location: point({ latitude: toFloat(line.lat),        longitude: toFloat(line.lng) }),   population: coalesce(line.pop, -1)   })MERGE (country:Country {   name: coalesce(line.country, ),   iso2: coalesce(line.iso2, ),   iso3: coalesce(line.iso3, )})MERGE (province:Province {   name: coalesce(line.province, )})CREATE (c)-[:IN]->(province)CREATE (c)-[:IN]->(country)MERGE (province)-[:IN]->(country)return count(c)Here’s a picture of what a resulting snippet of the graph looks like, connecting cities, provinces, and countries. In this case, we’re just looking at West Yorkshire in the UK.Cities, provinces, and countriesPoint DataNow we have a totally new datatype in our graph, the point” attribute on the City label, which is a spatial point.They look like this:This contains about the information you’d expect, an x and a y coordinate on the globe. The srid” bit is to refer to a spatial reference. If you’re not familiar with geospatial data prior, there are many different coordinate reference systems” (CRSs). The default one that many people are used to is called WGS 84, and that’s what you get by default.Coordinate Reference SystemsIf your data uses a different CRS, the only difference in the load would be passing a crs attribute to the point function, such as in this example:point({x: 2.3, y: 4.5, crs: cartesian})For supported CRSs and further examples, consult the documentation.Distance Between PointsNow that we have points on the globe, we can compute distance between them. Let’s use our city data to find out which cities in the UK are closest to London.MATCH (c1:City { name:  London  })-[:IN]->(:Country { name:  United Kingdom  })<-[:IN]-(c2:City) WHERE c2.name <>  London  WITH c1, distance(c1.location, c2.location) as dist, c2 RETURN c1.name, dist, c2.name ORDER BY dist ASC LIMIT 10In bold, we compute the distance by simply calling the distance function on two spatial point types. The results are quoted in meters. Our results below tell us that Luton is the closest city to London, about 47km away from London. These distances are computed as the crow flies”. Google maps has Luton about 54km from London, but you can’t drive straight there and it’s possible the latitude and longitude are marked from slightly different points, but generally this result checks out.Technical details about how distance is computed, what the result data type is under the various CRSs, and other important details can be found in the documentation.Using Distances as WeightsLet’s now compute all-pairs distances within the UK by going through all the cities, computing their distances, and adding edges.MATCH (c1:City)-[:IN]->(:Country { name:  United Kingdom  })<-[:IN]-(c2:City)WHERE id(c1) < id(c2)CREATE (c1)-[r:PATH { distance: distance(c1.location, c2.location) }]->(c2)RETURN count(r)This query creates something like a distance atlas in our graph map” like the picture below.This can now be used as a great jumping off point for weighted paths through the graph, when using APOC graph algorithms.What you do with the new features is up to you! But let us know how you’re coming along, tweet to @neo4j and show us what you’re doing with it!;May 24, 2018;[]
https://medium.com/neo4j/load-testing-neo4j-procedures-with-apache-jmeter-caff4c0d9d5a;Nicolas MervaillieFollowNov 5, 2019·3 min readLoad Testing Neo4j procedures with Apache JMeterApache JMeter is a popular, open source application to load test functionality and measure performance. It started off with the intent of testing Web Applications but its widespread use and availability of reports and visualizers saw it expand to supporting the testing of various protocols including databases via JDBC.JMeter would be very suitable to test Neo4j procedures as well — often, procedures are not load tested early enough and there aren’t very straightforward ways of testing them for performance. So to celebrate the first ever Global GraphHack 2019, we extended JMeter to provide support for the Bolt protocol and the execution of Cypher queries, and hence, procedures.This extension has been integrated in the JMeter codebase and will be available in the 5.2 release. In the meantime, you can test it using a release candidate.The JMeter Bolt protocol supportThe new bolt protocol support adds 2 new elements to the JMeter UI: the Bolt connection configuration and the Bolt Sampler.Database connection configurationThe first thing to do is to configure Neo4j connection parameters.This configuration is done through the Bolt Connection Configuration, which you just need to add at the root of your test plan. It will automatically handle the database driver initialization and make sure it is cleaned up after the test.The Neo4j connection configurationAt the moment, the configuration is fairly basic and you cannot configure additional parameters. But the driver defaults are quite good out of the box and allow you to run a reasonable benchmark.SamplerOnce the configuration part is done, we can now define what our actual load test will do. In JMeter this is done through samplers, and in our Neo4j case, with Bolt Request samplers. Each sampler contains a Cypher query, along with its parameters.The Bolt request configurationThe parameters have to be provided in JSON format, where the keys must match the query parameters. The JSON values in the example above are JMeter variables, and in this case, are dynamically replaced with the values coming from a CSV dataset. Other forms of JMeter variable substitution are also supported, such as User Defined Variables.We strongly recommend using query parameters, as they allow the database to cache the query execution plans and get better performance.There is also an additional option Record Query Results”, in case you’d like to store/display the data returned by the query. By default, this option is disabled to minimize memory usage, and avoid iterating through the entire result set.Note that a sampler has to be nested within a thread group that controls the concurrency of the test. In this example, we have JMeter spawn 20 threads that will execute our query in parallel.Running the test planOnce the sampler(s) are ready, we can add a couple of reporting elements to gather statistics at runtime, and start the load test.In the screenshot below, while our test running, we can monitor the number of queries executed, the response times, error rates, etc.Live performance results while the load test is runningLoad testing best practicesIn this blog post we went through a very simple example. If you are going to do more advanced testing, make sure to follow JMeter best practices such as running the test in CLI mode, disabling the View Results Tree” listener, or do proper validation on query results.Make sure to check them out on the JMeter website https://jmeter.apache.org/usermanual/best-practices.htmlWhat’s nextThe bolt support will be available in JMeter 5.2, which will be released soon.The example presented here is available at Github, feel free to use it as a bootstrap for your tests.We welcome feedback and suggestions, and hope you find this useful. Happy load testing! 🚀 🚀 🚀;Nov 5, 2019;[]
https://medium.com/neo4j/whats-new-in-neo4j-graphql-java-d3e9abd0ed34;Andreas BergerFollowNov 12, 2021·4 min readWhat’s New in Neo4j-GraphQL-JavaNew features in version 1.3 and 1.4: from Spring-Boot DGS integration to Type-safe APIs, better pagination and alignment with neo4j/graphql.jsThere have been many changes in the last two releases, about which I would like to give some insights here.Photo by George Rosema on UnsplashIntegration with Spring-Boot DGSWith version 1.4 of Neo4j GraphQL Java, we have enabled an easy integration into Spring Boot DGS applications. This integration makes it possible to use the features of DGS in conjunction with schema augmented by Neo4j GraphQL Java.ExampleCreate a schema which should be augmented by the librarySchema to be augmented by neo4j GraphQL Java2. Create a schema to extend the Neo4j schema by some fields that should be provided through the DGS applicationSchema to be used by DGS applicationAs you can see, the Movie type is extended by some additional fields. New query methods can be added as well.3. Creating a combined graphql-schema by using some of the extension points provided by the DSG library:GraphQLConfiguration.ktIn the postConstruct method, a SchemaBuilder is created, to augment the simple neo4j.graphql schemaThe registry method ensures, that DSG uses the same TypeDefinitionRegistry as the one used by the SchemaBuilderThe codeRegistry method is called to register the handlers for all those fields that should be handled by Neo4j GraphQL Java. In the example for the field Movie:title .The runtimeWiring method ensures, that all required scalars and interfaces can be handled correctly4. Extend the remaining fields that are not covered by Neo4j GraphQL Java with some custom logic:Extend augmented schema by custom logicThat’s it. Now the schema can be queried to retrieve combined data from the Neo4j database as well as from the DGS application:Request retrieving combined dataGenerating a Type-Safe Model to Query the APIIn the previous example, we have shown how to combine neo4j-graphql-java with a Spring boot DGS application. DGS also provides a way to generate classes at build time from a grapqhl schema, which can be used to call the GraphQL API in a type-safe manner.In order to be able to use this feature for the combined schema, we have created a Maven plugin that can be used to generate the fully augmented schema at build time:Maven plugin configurationIt is important that the schemaConfig settings are identical to the one used to configure the Schemabuilder.Now type-safe projections can be used like in this example:Type-safe projectionComplete ExampleA complete example is available in our GitHub repository.neo4j-graphql-java/examples/dgs-spring-boot at master · neo4j-graphql/neo4j-graphql-javaIn the Neo4jConfiguration a DataFetchingInterceptor is created, which will be bound to all fields augmented by the…github.comAdditional New FeaturesHaving Default Values for Paging and SortingDefault values for paging and sorting can now be defined in the source schema:Defaults for paging and sorting (old style)Now when the following query is triggered:Query, using default sorting and pagingA list of movies where each movie contains the first 3 actors sorted by descending name is returned.API-Alignment with neo4j/graphqlOverviewWe have started to align the API of the augmented schema with the official JavaScript version. This is an ongoing process that still requires a lot of work (you can follow the progress at GitHub). The goal is to make the augmented GraphQL schema compatible with the one generated by the JavaScript version.To make the transition for our users as smooth as possible, any changes to the schema augmentation can be adjusted via a configuration where the default values are set to be compatible with older versions of neo4j GraphQL Java.Using Scalars for Temporal TypesWe had some issues with the previous implementation when using temporal types. These issues were resolved by the introduction of scalar temporal types, wich can be activated via useTemporalScalars = true.This means that time values are only transmitted as ISO strings, which leaves no room for interpretation. Also for filtering it is now clear about which point in time a comparison should take place.Separate Sorting and Paging into Own Input TypesQuery options can now be configured to be handled as input types (via queryOptionStyle = INPUT_TYPE). Default values can also be defined for paging and sorting at this new input types:Defaults for paging and sorting (new style)When this query is triggered:Query, using default sortingA list of genres sorted by descending name is returned.Further ImprovementsUsage of neo4j-cypher-dslIn version 1.3, the entire Cypher generation mechanism was migrated to neo4j-cypher-dsl. This allows us to pretty print the generated Cypher statements for our test cases. Furthermore, we can store the values for parameters directly in the DSL model and extract them easily.Better TestingOur integration tests have been extended so that they now also test the GraphQL layer. Thus, our test framework also serves as documentation, in which all test-relevant information such asgraphql-schematest-data (data within the neo4 database)graphql-requestgenerated cypher queryand graphql-responsecan be accessed via an nicely rendered Ascii-doc file. Here is one example from a GitHub issue:FeedbackI would love if you’d try it out and provide feedback in the form of GitHub issues or via GitHub discussionsIssues · neo4j-graphql/neo4j-graphql-javaNew issue Have a question about this project? Sign up for a free GitHub account to open an issue and contact its…github.comDiscussions · neo4j-graphql/neo4j-graphql-javaPure JVM translation for GraphQL queries and mutations to Neo4js Cypher - Discussions ·…github.com;Nov 12, 2021;[]
https://medium.com/neo4j/the-graph-algorithms-playground-and-graph-data-science-library-69575a0fb329;Mark NeedhamFollowApr 2, 2020·5 min readThe Graph Algorithms Playground and Graph Data Science LibraryNEuler, the Graph Algorithms Playground, now supports the Graph Data Science LibraryJust under a year ago we released NEuler, the Graph Algorithms Playground, which made it easy for users to learn how to use the Graph Algorithms Library.Its successor, the Graph Data Science Library, was recently released, which meant that NEuler needed to be updated to use that instead.I’m happy to announce that as of version 0.1.16, NEuler is Graph Data Science Library ready. It has also been renamed to be the Graph Data Science Playground!This version is only supported by Neo4j Desktop versions 1.2.5 and higher, so you’ll need to update that as well.If you had an older version of Neuler installed, you might need to uninstall it first before you install this version due to the renaming.How do I install it?First you should have Neo4j Desktop installed.Once you’ve done that, create a project called ‘Neuler GDS’ and create and start a database (version 3.5.x for the Graph Data Science Library!):How to setup a database in Neo4j DesktopOnce you’ve done that it’s time to install the Graph Data Science Library. Select Add Plugin, and then you’ll see the following screen:Install Graph Data Science LibraryClick on the install button for the Graph Data Science Library, and make sure you’ve also installed APOC if you haven’t done that already!Once you’ve done that, select the little arrow on the Open button underneath the database, and launch the Graph Apps Gallery.Open the Graph Apps GalleryWe’ll then see the following screen, from where we can install the Graph Algorithms Playground:The Graph Apps GalleryThis only works on Mac and Windows at the moment. If like me, you’re using Linux, you’ll need to paste https://bit.ly/install-neuler into the ‘Install Graph Application’ form and then click on the Install button. That always works.Install the Graph Algorithms PlaygroundOnce we’ve done that we should see the Graph Data Science Playground under the Graph Apps menu, and it’ll also be available via the Open button as well.Let’s now launch that and see what it looks like.Exploring the Graph Data Science PlaygroundOnce you launch the Graph Data Science Playground you’ll be faced with this screen, which describes the categories of algorithms available in the app.The Graph Data Science PlaygrounIf you want to learn more about the intricacies of the algorithms in each of these categories, this is the part of the post where I shamelessly plugin the Graph Algorithms Book that Amy Hodler and I have been working on.O’Reilly Graph Algorithms BookYou’ll can download your complimentary copy of this book by going to neo4j.com/graph-algorithms-book. But be quick because the complimentary copy is only available until the middle of April 2020.Loading sample graphsRight, back to NEuler. The best way to understand graph algorithms is to play around with sample datasets that we know well and inspect the results that the algorithms return.To help with that we’ve added a section of the app from which you can load sample graphs:NEuler with the Sample Graphs” sectionAs we can see in the screenshot above, we have three sample datasets to play with. We use the Game of Thrones dataset in the initial NEuler blog post, so let’s use the Twitter one this time! If we click on the Load button under that dataset, we’ll see the following screen:Load Twitter into Neo4jNote that the import script uses the APOC Library, so make sure you’ve got that installed.Running an algorithmOne of the simplest algorithms is Degree Centrality, which on this dataset indicates the accounts that are followed by users. This is the default algorithm under the Centrality category, and we can have this algorithm to run on a specified Label and Relationship type configuration via the following form:Degree CentralityThis configuration of the algorithm will return the number of users that follow an account, and if we click on the Run button we’ll see the following screen:Results of Degree CentralityWe can also download a screenshot of the results by clicking on the screenshot icon. For example, below we can see a screenshot of the chart view of the results of running this algorithm:Chart of Degree Centrality resultsShow me the codeIf we want to try these algorithms out on our own, the ‘Code’ tab shows the queries and parameters that can be used in the Neo4j Browser to achieve this:The code behind that runs the Degree Centrality algorithmWe can either copy the parameters and procedure call onto the clipboard using the Copy to clipboard button, or we can generate a Neo4j Browser guide.Neo4j Browser GuideOn the screenshot below we can see the contents of the guide created for the example used in this blog post:In SummaryWe hope you like using this app to explore the Graph Data Science Library. Enjoy playing around with the app and let us know in the comments if you like it. You can also share screenshots of the algorithm results on your data on Twitter, with the tags #Neo4j and #Neuler.If you have questions regarding your Neo4j experience, you can always head to the Neo4j Community Forum.Don’t forget to grab the free copy of our O’Reilly book about Graph Algorithms on Apache Spark and Neo4j”;Apr 2, 2020;[]
https://medium.com/neo4j/building-a-graphql-based-cms-with-the-grandstack-781ec083b240;"CristinaFollowMar 18, 2021·7 min readBuilding a GraphQL-Based CMS with the GRANDstackWe migrated Neo4j’s Graph Examples Portal (AKA the Graph Gist Portal) from Ruby on Rails to GRANDstack (GraphQL, React, Apollo, Neo4j Database). Here is how we did it.It was an exciting project, as I love the combination of React and GraphQL and a graph database.GRANDstack.ioAbout the PortalThe Portal is a self-serve content management system (CMS) for Neo4j Graph Examples. Graph Examples, also called GraphGists” are small, self-contained descriptions of graph use cases, including real data setup and queries to demonstrate concepts specific to their domain.You can find it live and running here at portal.graphgist.orgGraph Example PortalThe ArchitectureThe portal is a relatively lightweight content management system. Although authors interact with the portal’s web app to upload and edit their content, and the admins use the portal to review and approve the content.The vast majority of users interact with graph examples either via the Neo4j Desktop Graph App or on Neo4j.com. In short, the majority of readers of the graph examples are not even aware of the portal.The Portal’s ArchitectureThe Domain ModelNodes:User : Represents the authenticated users of the portal. They can add and edit content and rate existing content via their Person node.GraphGist : The published Graph Examples visible on Neo4j.comGraphGistCandidate : The unapproved content submitted by users.Use Case , Challenge , Industry : Categories assigned to GraphGist and GraphGistCandidate to be able to place related content into thematic buckets.Relationships:IS_PERSON : The association between a User and the Person who writes the content.WROTE: The association between a Person and their GraphGistCandidateIS_VERSION : The association between the GraphGistCandidate and the final” GraphGistRATES: The relationship between a User and a GraphGist rated by the userFOR_INDUSTRY, FOR_USE_CASE, FOR_CHALLENGE: The relationship between GraphGist and their thematic categoryDomain with Users, Assets(Graph Example) and different CategoriesExample: Graph Examples and Related Use Cases and IndustriesHere is an example for a few graph examples in the Sports and Recreation Industry” and a number of Use Cases.Example: Graph Examples and Related Use Cases and IndustriesSummary of FeaturesAnonymous Users: Consume ContentAnonymous users can:View graph examples (the content) on Neo4j Browser as interactive guides, rendered to slide format from the sourceView graph examples on the Neo4j Wordpress Site powered by the GraphQL APISearch and browse graph examples on the portal itself in the React AppCreate accounts and become Authenticated usersAuthenticated Users: Create ContentAuthenticated users can:Create and edit graph example candidates to preview inlineSave their work before submitting for approvalAssign their candidates to industries, use cases, and challengesCreate new graph example candidate by editing an approved graph exampleAdministrator Users: Administer ContentAdministrator users can:View and edit graph candidatesPromote candidates to graph examplesAssign graph examples to industries, use cases, and challengesCreate and manage industries, use cases, and challengesHandle notifying admins there is new content to review and approve via automated emailsAdministrators can edit the Graph Example’s metadata, including the summary, image, associated Use Cases, Industries, and Challenges.After the administrator is happy with the content, the admin can approve the candidate, promoting it to be able to be seen and searched by the public.The CodeThe code for the CMS portal is all available on GitHub, feel free to use it for yourselves or to learn from it:neo4j-contrib/graphgist-portal-v3github.comGRANDstack ImplementationWhy Migrate to GRANDstack?Although the Rails implementation was fine and had been serving us for five years, the portal would be more manageable by migrating to an actively supported stack (Javascript, GraphQL, React) and was due for a database upgrade. The old portal’s data was transferred into a fully managed 4.x Neo4j Aura database.GRANDstack brings the combination of React, GraphQL, and Neo4j to the table. One can easily set up a new app with little to no previous experience with Cypher (the Neo4j query language), as GRANDstack takes care of GraphQL-to-Cypher translations through neo4j-graphql-js.The GRANDstack starter allows developers to easily get started with:npx create-grandstack-appWhich walks you through some config and even allows to infer the schema from the data in a database.All you really need to get started is to define a simple data schema of GraphQL types. Once the schema is defined, you can manipulate all data via the auto-generated GraphQL API!No migrations and no need to deal with exposing API methods manually with a lot of boilerplate code!A GraphQL Query RequestHere is how a request coming from a GraphQL query is translated into Cypher and executed against the database to be returned to the caller.GraphQL Query translation and executionImplementation: Browse Graph ExamplesEither by finding the portal via a search engine or being referred to it by a friend, anonymous users can browse, search, and view individual graph examples on the portal:Looking at a single Graph Example as an unauthenticated user via the React Frontend. See is live: Chemicals In CosmeticsThe schema type for the GraphGist entity shown on that view is quite large but some of that is due to legacy field management.Code Example: GraphGistInterfaceBut as soon as you have the schema defined and made known to the library it generates all the enriched GraphQL types and API queries and mutations for you that you can explore and immediately e.g. in GraphiQLQuerying with the GraphQL API for information about a Graph ExampleView a List of Graph Examples, Filtered by Use CaseAfter viewing a single Graph Example, visitors may want to browse Graph Examples in the same Industry or Use Case. We could have done this with some of the advanced filter options, but it was easier to just use the @cypher directive to implement a custom query.See the Code: graphGistsByCategoryAgain immediately usable in the API.Querying with the GraphQL API for a list if Graph Examples belonging to a particular Use CaseHere is the equivalent result as Nodes and Relationships in Neo4j Browser.A similar query in the database — what Graph Examples are associated with the UseCase Data Analysis”?Rate Published Graph ExamplesWhile browsing a Graph Example on the portal, a visitor might decide to give the item a star rating. If unauthenticated, the visitor will have to log in or create an account to use the rating feature.Upon rating a Graph Example, a RATES relationship is created between the User and the Graph Example. (GraphQL Schema, GraphQL Mutation)Create Candidate ContentAfter browsing and rating existing Graph Examples, a user may decide to create their own content which is initially a candidate. After the Graph Example Candidate is created, the Administrator users receive an email alerting them of the update, and decide whether or not to approve the content.Authenticated users can create new candidate Graph Examples with just some asciidoc. Creating a candidate will trigger an email to the admins that a new candidate has been created. See it live: Start yours nowAuthenticated users can also add one or more Industries, Use Cases, or Challenges to their ContentThe type for the GraphGistCandidate adds a few more fields and relationships from Asset to allow for the content-management aspects of the publishing flow.See the code: GraphGistCandidateAuthenticated users can also create candidate Graph Examples vis the API.Here is a GraphGistCandidate pointing to the live version of an GraphGist with it’s Author, their User and the Industry.Upon the creation of a candidate, a final Graph Example is also created, but in a draft state that is not visible to unauthenticated users.GRANDStack ResourcesThis blog post was just a brief overview of a very utilitarian GRANDStack application. Feel encouraged to build your own GRANDStack application with the resources below, and stay on the lookout for the next version of GRANDStack.Fullstack GraphQL Applications with GRANDStack - Essential ExcerptsApplication development is never an easy task. GRANDstack simplifies the development process by combining a variety of…neo4j.comBuild Fullstack GraphQL Applications With Ease | GRANDstackBuild Fullstack GraphQL Applications With Ease| GRANDstack
Build Fullstack GraphQL Applications With Easegrandstack.ioBuild a Neo4J & GraphQL APIIn this course we will learn some of the basics of getting data into and out of Neo4J, as well as touch on some of the…egghead.ioAbout the AuthorsCristina and Alisson work at The SilverLogic, a software development company based in Boca Raton.Alisson is a software engineer at The SilverLogic. Passionate about plants, he is the driving force behind Que Planta, a GraphQL-based social network for plants.";Mar 18, 2021;[]
https://medium.com/neo4j/youtube-videos-analysis-with-neo4j-and-graphaware-nlp-91ee388584fa;Christophe WillemsenFollowJul 27, 2018·5 min readYoutube videos analysis with Neo4j and GraphAware NLPNeo4j Youtube ChannelAt GraphAware, one of our Graph based solutions is the Knowledge Platform, an Intelligent Insight Engine built atop Neo4j.In order to provide to our customers the ability to unlock hidden insights from new forms of data, we decided to start an R&D phase for video analysis.For this blog post we will analyse the Neo4j Youtube channel video transcripts, extract some insights and show what type of business value such analysis can bring.Part I : Getting the dataYoutube display of English Transcription of this talk at GraphConnect LondonYoutube offers the ability to download the transcription of the videos, when available. Fetching this data can be done in multiple ways, like connecting to the Google APIs with your preferred client.I have chosen to use a much easier solution, using youtube-dl . This python tool offers you the ability to download those informations and much more from Youtube and other video providers like Vimeo.For the installation, you can just follow the simple step described in the youtube-dl Github repository.You can now download all the transcripts available in a single command :mkdir transcripts && cd transcriptsyoutube-dl --all-subs --skip-download https://www.youtube.com/channel/UCvze3hU6OZBkB1vkhH2lH9QThe skip-download option avoids the tool to download the actual video, which could take a serious amount of time and bandwith.List of WebVTT files downloadedThe previous operation will create one .vtt file per video containing the transcription.The WebVTT format stands for Web Video Text Tracks, you can find more information on Wikipedia, but the content of a file looks like this :Recommendations with Neo4j ( Michal Bachman ), first lines of the transcription filePart II : Importing into Neo4jThe advantage of using a Graph database such as Neo4j is the flexibility to model your data for serving the needs of your applications. A general rule is to define first the questions you want to ask to the graph and apply a model that will fit for it.A model here could be one Video node per full transcription, which can serve the use case of providing recommendation on the overall content of the video. But what if we would want the recommendations to change over time, along with the evolution of the content of the video ? Looking at the .vtt file we can see that we have the potential to serve such use case by storing a node by sentence frame for example.So, let’s do it !First, you will need to have Neo4j installed along with the GraphAware NLP plugins, I will not explain the full installation process of them as it is pretty well covered in the repository documentationThe NLP plugin comes with a handy .vtt files parser as well as utilities for loading files from a directory for processing :Which will create the following data model for all videos :Model in Neo4j of a Youtube Video along with transcription nodes by sentence framesPart III : Apply Natural Language ProcessingThe next step is to apply Natural Language Processing on the data, the following Cypher commands will do the following :Create an NLP pipeline that determines the analysis you want to apply on the textual data, such as lemmatization, named entity extraction, etcUse a Machine Learning algorithm to extract the keywords of the captions, keywords are words or small sentences ( 2 or 3 words ) that best describe a particular captionThe next gist is the full list of commands, as you can see it is pretty easy to extract insights from text with very few lines of Cypher :Part IV : Extract InsightsWe can run a couple of queries to extract some meaningful insights from the dataWhat companies are mentioned in the videos ?What about persons then ?What are the top 20 keywords describing captions ?Part V : Very cool queriesA nice query we can do, is to show the evolution of keywords, minute by minute while the content evolves :Based atop this, we can provide content based recommendation, but this time the recommendation candidates would evolve minute by minute with the voice of the speaker :Part VI : SEO Optimization of the video descriptions (idea)If I would have stored as well the descriptions of the videos that we can also retrieve with youtube-dl, we can achieve SEO optimization with the following flow :We summarize the content of the transcripts ( for this it would be better to have the annotation on the full transcript rather than per sentence frame or caption )We compare the keywords appearing in the topX relevant sentences with the keywords extracted from the video descriptionWe find the topX sentences that contain keywords not appearing in the description and suggest them to the SEO experts for evaluationWith our plugins, you can easily perform summarization with the following query :And retrieve the top X sentences (here on an intro to Neo4j and Cypher video) :ConclusionThe combination of Network Science together with Natural Language Processing offers amazing possibilities, add to that the ease of creating a domain specific language atop Neo4j’s Cypher and the development of such applications become a joy.GraphAware Intelligent Insight EngineOur Enterprise Insight Engine goes further, applying Natural Language Processing, Machine Learning and Indexing on structured and unstructured data, offering enterprises the ability to find and use data at the right moment for the right decisions, be it for empowering your employees with historical knowledge ( aka make every employee your best employee ) or for analysts that do not have time to know where and in which format the information they need reside.Don’t hesitate to get in touch with us if you want to know more.If you are attending GraphConnect NYC this September, we are delivering a full-day training on Graph-Based Natural Language Understanding with Neo4j. Sign up while there are still some free seats !I hope you enjoyed this blog post, do not hesitate to share and contact with me on Twitter if you have some questions.Thanks for reading !;Jul 27, 2018;[]
https://medium.com/neo4j/neo4j-graphql-aggregations-f460c92698c9;Daniel StarnsFollowNov 23, 2021·3 min readNeo4j GraphQL AggregationsRecently we released Neo4j GraphQL 2.4.0 that contained some really cool features, but the coolest, in my opinion, is the autogenerated aggregation queries.What Do You Mean by Aggregations?We use the term aggregations to encapsulate the retrieval of statistical information about the nodes, relationships, and their properties in your graph. Statistical information may be answering questions such as:What is the longest movie title?What is the latest movie?Who directed more than 3 movies?What movie has an average rating greater than 10?What is the average age of all the actors in a movie?What is the highest rating of a movie?Prior to our 2.4.0 release, users would have to employ our Cypher Directive, and where they would have to manually write Cypher to answer such questions. Now, with our latest release, the autogenerated aggregation queries remove the need to write any custom logic at all!How Do I Get Started?Lets first establish what our Graph will look like. For this we will use the ‘Movie Graph’ located on the right side of Neo4j Browser:This can be expressed with Neo4j GraphQL Type Definitions like so:You can visit our Documentation Getting Started Guide to learn how to use the above Type Definitions to autogenerate all the queries in this blog.Top-Level AggregationsFirstly let’s start at the top. We consider Top-Level to be the queries at the root of your GraphQL API. Those queries would be, for example movies, people, etc. The point is it’s always considered a node from the root. In our new release, we added aggregate queries to this level. Below are some examples using those new queries plus you can learn more about top-level Aggregations here.What is the longest movie title?GraphQL Query:Response:What is the latest movie?GraphQL Query:Response:Where AggregationsThese aggregations are those that are inside the already existing where argument. Below are some examples using those new queries plus you can learn more about Where Aggregations here.Who directed more than 3 movies?GraphQL Query:Response:What movie has an average rating greater than 10?GraphQL Query:Response:Field AggregationsFinally, these aggregations are across relationships. You use these aggregations nested inside a node, just like how you would select the actors of a movie, you can now also aggregate the actors of a movie. Below are some examples using Field Aggregations plus you can learn more about them here.What is the average age of all the actors in a movie?GraphQL Query:Response:What is the highest rating of a movie?GraphQL Query:Response:Aggregating with the OGMWe also exposed the aggregation queries using the OGM. Here is an example aggregating the title fields on a movie:Thats It.Thanks for taking the time to learn more about Neo4j GraphQL aggregations. We are looking forward to hearing your feedback. Resources:NPM: https://www.npmjs.com/package/@neo4j/graphqlGithub: https://github.com/neo4j/graphqlDocumentation: https://neo4j.com/docs/graphql-manual/current/Discord: https://discord.gg/neo4j;Nov 23, 2021;[]
https://medium.com/neo4j/loading-graph-data-for-an-object-graph-mapper-or-graphql-5103b1a8b66e;Michael HungerFollowAug 10, 2018·7 min readLoading Graph Data for An Object Graph Mapper or GraphQLAn interesting way to load complex data structures from a graph into objects is using nested pattern comprehensions in Cypher, Neo4j’s query language.The approach I want to describe today, originates from experiences that gathered and improved over the years while working on Spring Data Neo4j and Neo4j-OGM. The same approach is used in the neo4j-graphql integration, the Neo4j-PHP-OGM and in py2neo’s OGM, the Neode Javascript OGM and probably in some more.Today we focus on loading a specific slice of data into an object network using a mapping description. I wrote an earlier article on efficiently writing data to the graph in batches.As the example dataset we’ll use our usual supect, the built-in movie database that you can create via the :play moviescommand in the Neo4j-Browser by executing the huge data creation statement on the 2nd slide.Object Graph MappingTo map graph data to objects in your stack of choice, you usually need some kind of description or mapping.You could also load the graph data directly, so each node and relationship has an 1:1 representation as an object, which sometimes, is the best solution. Then you could just use the appropriate bolt driver to load node and relationship-objects and either wrap or convert them into objects of your choice as needed.At other times either constraints from your domain or use-case require different kinds of mappings. Those can then either described in mapping information that is provided externally, like a graphql-schema, or a JSON or XML mapping-description. Or you can derive it from the class data structures for the target objects, which can again contain additional annotations for type or name mapping or specific projections.graphql schema with directivestype Movie {    title: ID!    released: Int    tagline: String    actors: [Person] @relation(name: ACTED_IN , direction:IN)    director: Person @relation(name: DIRECTED , direction:IN)}Spring Data Neo4j Movie class with annotations@NodeEntitypublic class Movie {    @Id    @GeneratedValue	private Long id    @Indexed	private String title	private int released	private String tagline	@Relationship(type =  ACTED_IN , direction = INCOMING)	private List<Role> roles}Based on that mapping information and your starting points, you then load your root entities, for example a set of users or movies and then related objects to a determined depth or all the way down.Often you need to specify a depth, because otherwise due to the connectedness of the graph you would pull the whole database over the wire and into memory. Sometimes your subgraphs are isolated, then it’s possible to load until you reach the end.Another important bit is what to load.Imagine a movie with with its cast, directors, generes, but potentially millions of ratings. Then in most cases you don’t want to load all these ratings, just the movie and its few related entities. Therefore it would be useful, if either the default mapping expressed that (i.e. leaves off the ratings) or you could control what parts to load or skip at loading time.Also you might not want to load all properties from the database, e.g. imagine the movie node also contained the full script or poster images, which are rarely needed on in the loaded entity and could be fetched on demand.Different SolutionsOne approach, that was used in early versions of Neo4j-OGM was just to MATCH an arbitrary variable length path patternto the required depth and then return all paths. That requires to transport much more data over the wire, which you then have to deconstruct and sort out along the path and hydrate your objects as needed. Also you cannot selectively load fields for your objects.MATCH (m:Movie) WHERE ...MATCH path = (m)-[*..5]-()RETURN pathTo match only selected parts of your object graph that wouldn’t work as you have only limited control over which relationship-types and directions to load. And it get’s trickier deeper down esp. when relationship-types and directions potentially overlap.MATCH (m:Movie) WHERE ...MATCH path = (m)-[:ACTED_IN|:DIRECTED|:HAS_GENRE*..5]-()RETURN pathThe direct approach, would find the root entity and then MATCH related entities, collect each of them into lists and return the compound representation. Here is an example.MATCH (m:Movie) WHERE ...OPTIONAL MATCH (m)<-[:ACTED_IN]-(a)WITH m, collect(a) as actorsOPTIONAL MATCH (m)<-[:DIRECTED]-(d)WITH m, actors, collect(d) as directorsOPTIONAL MATCH (m)-[:IN_GENRE]->(g)RETURN m, actors, directors, collect(g) as genresThis only works well(ish) on the first level, for nested matches it gets quite difficult to correctly match and collect only the entities you want. Also all those matches and collects put additional strain on the query engine, especially for large results.Pattern Comprehensions for the rescue.Pattern Comprehensions and Map ProjectionsI wrote an introductory blog post a few years ago, but here is another quick explaination.What are pattern comprehensions?A pattern comprehension is a subquery-like expression added to Cypher by Andrés Taylor to allow for some of the nested query capabilities of GraphQL.Similar in syntax to list comprehensions, they allow you to define a pattern (even with new variable declarations), an optional WHERE filter clause and a result projection expression.That projection expression can be anything, a node or relationship, a nested map or list or a scalar value. The result of such an pattern comprehension is always a list of the projection results.Here are some examples:Example 1 - attributes of related entitiesMATCH (m:Movie)RETURN m, [(m)<-[:ACTED_IN]-(a:Person) | a.name] as actorsResult 1╒══════════════════════╤══════════════════════════════════════════╕│ m.title              │ actors                                   │╞══════════════════════╪══════════════════════════════════════════╡│ The Matrix Reloaded  │[ Laurence Fishburne , Hugo Weaving , Car │├──────────────────────┼──────────────────────────────────────────┤│ The Devils Advocate │[ Al Pacino , Charlize Theron , Keanu Ree │├──────────────────────┼──────────────────────────────────────────┤│ As Good as It Gets   │[ Helen Hunt , Jack Nicholson , Cuba Good │Example 2 - Filter RelatedMATCH (m:Movie)RETURN m,     [(m)<-[:ACTED_IN]-(a:Person) WHERE a.born > 1975 | a] as actorsExample 3 - Filter and Map expression with several AttributesMATCH (m:Movie)RETURN m, [(m)<-[:ACTED_IN]-(a:Person)            WHERE a.name STARTS WITH T            | {name: a.name, id:id(a), label:Actor}] as actorsResult 3╒════════════════╤═════════════════════════════════════════════════╕│ m.title        │ actors                                          │╞════════════════╪═════════════════════════════════════════════════╡│ The Matrix     │[]                                               │├────────────────┼─────────────────────────────────────────────────┤│ A Few Good Men │[{ name : Tom Cruise , id :921, label : Actor }] │├────────────────┼─────────────────────────────────────────────────┤│ Youve Got Mail│[{ name : Tom Hanks , id :976, label : Actor }]  │While this might seem equivalent to a MATCH and collect, the nice thing is that this is an expression, so it can be used wherever expressions are allowed. And as the pattern comprehension’s projection is again an expression, you can also nest them.Another cool feature that was added at the same time, were map projections.Those allow you to take any map-like entity — nodes, relationships and maps, and use a subscript in curly braces to extract eitherindividual attributes like .name,all attributes with .* oradd additional entries, like label:Actor or total: count(*).Map Projection ExampleMATCH (movie:Movie)<-[:ACTED_IN]-(p:Person)RETURN movie { .title, .released, cast: collect(p.name)} as dataResult╒═══════════════════════════════════════════════════════════════╕│ data                                                          │╞═══════════════════════════════════════════════════════════════╡│{ title : What Dreams May Come , cast :[ Robin Williams ,      ││ Annabella Sciorra , Cuba Gooding Jr. , Werner Herzog ,        ││ Max von Sydow ], released :1998}                              │├───────────────────────────────────────────────────────────────┤│{ title : Somethings Gotta Give , cast :[ Jack Nicholson ,    ││ Keanu Reeves , Diane Keaton ], released :2003}                │├───────────────────────────────────────────────────────────────┤│{ title : Johnny Mnemonic , cast :[ Takeshi Kitano ,           ││ Keanu Reeves , Ice-T , Dina Meyer ], released :1995}          │This approach can be used to only extract the attributes that you actually want to load for the mapping and skip all that are either irrelevant or huge. Both of which can also be fetched later on demand.How to Use these Concepts for Data LoadingGiven that pattern comprehensions act like subqueries, and that their expressions, can either be map projections or again pattern comprehensions, we can nest them as needed to achieve our goals. The result of such an expression is basically a nested document, that then can be traversed on the client-side to hydrate your tree of objects, while making sure to not create duplicate instances.To identify nodes (and relationships) uniquely you can either use an id-field as indicated by the mapping, or the built-in Neo4j ids (which you shouldn’t expose to other systems).What would this look like for our Movie example?Imagine we wanted to load only title, released year and actors, directors (with their other movies) and genres for each movie we fetch.Then our generated statement (driven by the mapping information) would look something like this:MATCH (m:Movie) WHERE ...RETURN m { id: id(m), .title, .released,         actors: [(m)<-[:ACTED_IN]-(a) | a {.id, .name, .born } ],         directors: [(m)<-[:DIRECTED]-(d) | d { .id, .name, .born,            movies: [(d)-[:DIRECTED]->(m2) WHERE m2 <> m | m2                               { id: id(m), .title, .released} ]}],         genres: [(m)-[:IN_GENRE]->(g) | g { .id, .name }] } as dataComplex Data Loading ExpressionThis pattern can now be combined with filters when loading our root objects, e.gfor a single entity by id,multiple ones by any index lookup operation (text or ranges)a pattern matching operation, oreven all of the entities.Additionally even at the nested pattern comprehensions we can use additional filter, that is something we use quite a lot in the neo4j-graphql integrations for field arguments.This was probably a lot to take it, so make sure to try out and understand the individual parts before putting them together. And then go forth and load your graph data effectively. These tools are useful not just for folks building object graph mappers but for everyone loading specific slices of graph data.Having your query return a JSON-like document makes it often easy to consume in many stacks.ReferencesEfficiently batching updatesCypher Query LanguageSpring Data Neo4j and Neo4j-OGMGrandStackneo4j-graphqlNeo4j-PHP-OGMpy2neo’s OGMNeode Javascript OGM;Aug 10, 2018;[]
https://medium.com/neo4j/visualizing-graphs-in-3d-with-webgl-9adaaff6fe43;Michael HungerFollowJul 23, 2018·6 min readVisualizing Graphs in 3D with WebGLWhile looking for efficient graph visualization libraries for large scale rendering, I came across 3d-force-graph, a really neat wrapper around three.js for graph visualization. Check out that repository after reading this, they have many more examples and settings to explore.I was already aware of and impressed by three.js, especially the 3d and WebGL capabilities. I had seen and done some graph visualization using it years ago, but 3d-force-graph packages three.js nicely with a Graph API and adds useful options to quickly get good looking results.If you want to try the different approaches directly, I put them into a GitHub Repository with live examples using the RawGit Service to serve HTML pages and assets directly from the repository.Update: I recently did a twitch session exploring the library live and also using its React component in a small Neo4j Query & Visualization App. Enjoy!DatasetsI started by using the Game of Thrones interaction graph, that my colleague Will Lyon wrote about here and which you can create yourself by running :play got in your Neo4j Browser. The GitHub repository uses the demo.neo4jlabs.com server which hosts a Game of Thrones dataset under the readonly user/password/database: gameofthrones”.Besides the basic graph of characters, it also has interaction weights” on the relationships and the guide adds some additonal metrics like a pageRank property that we want to use in the visualization.To take it to a larger dataset, you can explore the ICIJ’s Paradise Papers database, which is available as a Neo4j Sandbox, using this visualization.But let’s get started.Accessing the DataAs I outlined in the first article of this series, Neo4j’s Cypher over the binary Bolt protocol, can pull up to 1M records per second over the wire, enough to feed our graph visualization interactively with enough data.In our experiments, we use again the Neo4j Javasscript Driver to query the graph for data and render it in the 3d-graph.Please note that the JS driver uses a custom Number object, which we have to turn into JS integers with value.toNumber() and if you’re sending integer values use neo4j.int(value).Thee following, interactive pages load 5000 relationships from your graph at bolt://localhost, you might need to set auth (default is user: neo4j, password: test)Basic LoadingBasic loading here we’re just using the node ids because that is the fastest way to extract the graph structure.Load relationship listMATCH (n)-->(m) RETURN id(n) as source, id(m) as target LIMIT $limitWe load that data using the Javasscript driver as demonstrated before and transform it into a data structure in gDatathat 3d-force-graph understands.const driver = neo4j.driver( bolt://localhost , neo4j.auth.basic( neo4j ,  test ))const session = driver.session({database: neo4j })const start = new Date()session  .run(MATCH (n)-->(m) RETURN id(n) as source, id(m) as target        LIMIT $limit, {limit: neo4j.int(5000)})  .then(function (result) {We need one array with links from a source id to a target id. And one array for nodes containing our data to render. In the initial example that will just be the ids. We compute those id’s from the unique Set of source and target ids of our relationship-list.const links = result.records.map(r =>    { return {source:r.get(source).toNumber(),              target:r.get(target).toNumber()}})session.close()console.log(links.length+  links loaded  +(new Date()-start)+  ms. )const ids = new Set()links.forEach(l => {ids.add(l.source)ids.add(l.target)})const gData = { nodes: Array.from(ids).map(id => {return {id:id}}),                links: links}Then we just need to use that to initialize our 3d-graph.ForceGraph3D()(document.getElementById(3d-graph)).graphData(gData)The full code example can be seen and read here:Render Example | CodeIncremental LoadingWith incremental loading, we can add each row from the query result to the graph as it arrives. Especially with large results, that improves the responsiveness of the visualization and the user immediately sees feedback that they can interact with. Also it’s fun to watch the new nodes and relationships popping into existence.The main difference is that we now create the 3d-graph upfront and then update its graphData with each arriving row, adding our new entries using the Array spread operator.result.records.forEach(r => {   const { nodes, links } = Graph.graphData()   const link={source:r.get(source).toNumber(),                target:r.get(target).toNumber()}   Graph.graphData({        nodes: [...nodes, { id:link.source }, { id: link.target}],        links: [...links, link]    })})This is best shown in action, so test it out below.Render Example | CodeColor and CaptionOf course a bland graph excites no one, so let’s some color and information. That requires us to provide that info for the nodes, so besides the id, we also provide a label for coloring and a caption for each node. color by label and text caption on hoverMATCH (n)-->(m)RETURN   { id: id(n), label:head(labels(n)), caption:n.name } as source,  { id: id(m), label:head(labels(m)), caption:m.name } as targetLIMIT $limitFortunately 3d-force-graph comes with some hand option for auto-coloring based on an attribute, so we don’t have to handle colors or palettes manually. For the caption we just provide text to render in the nodeLabel callback and for feedback we also change the cursor while hovering over a node.const Graph = ForceGraph3D()(elem)              .graphData(gData)              .nodeAutoColorBy(label)              .nodeLabel(node => `${node.label}: ${node.caption}`)              .onNodeHover(node =>                        elem.style.cursor = node ? pointer : null)Render Example | CodeThis time, we use the Paradise Papers as an example as it has more different labels (five) than the Game of Thrones interaction graph. Something that becomes quickly apparent in the 3d visualization are the clusters that form around officers (people in management roles for shell companies) and intermediaries (law firms and banks creating shell companies).Weights for Node and Relationship SizesAfter running a graph algorithm like PageRank on our data, the nodes get a rank score as a weight. The relationships of the GoT graph already had interaction weights, both of which we can now render in addition to colors and captions.We also want to color relationships by type. We use log(weight) for relationships as they would grow too thick otherwise.So we return then as additional information from our graph query.MATCH (n)-[r]->(m)RETURN { id: id(n), label:head(labels(n)), caption:n.name,          size:n.pagerank } as source,       { id: id(m), label:head(labels(m)), caption:m.name,          size:m.pagerank } as target,       { weight:log(r.weight), type:type(r)} as relLIMIT $limitWe use the size attribute for nodes with the nodeVal method and the weight on relationships with the linkWidthmethods. For coloring the relationship-type we call linkAutoColorBy.const Graph = ForceGraph3D()(elem)              .graphData(gData)              .nodeAutoColorBy(label)              .nodeVal(size)              .linkAutoColorBy(type)              .linkWidth(weight)              .nodeLabel(node => `${node.label}: ${node.caption}`)              .onNodeHover(node =>                       elem.style.cursor = node ? pointer : null)Render Example | CodeParticles & Cluster ColoringIf we also ran a community detection or clustering algorithm like Louvain, we can use the cluster information for coloring nodes and relationships instead, making immediately visible to which cluster an entity belongs. So we return the louvain property as community (or cluster) entry for nodes and for relationships return the  smaller  community number as the one the relationship belongs to.MATCH (n)-[r]->(m)RETURN { id: id(n), label:head(labels(n)), community:n.louvain,          caption:n.name, size:n.pagerank } as source,       { id: id(m), label:head(labels(m)), community:n.louvain,          caption:m.name, size:m.pagerank } as target,       { weight:r.weight, type:type(r),          community:case            when n.community < m.community then n.community            else m.community end} as relLIMIT $limitAs an additional, fancy feature we replace the solid relationships with a particle stream instead, making them less obstructive.In this case we use the original weight property, (not log(weight)) as it represents the number of interactions, i.e. the number of particles traveling between the two characters.const Graph = ForceGraph3D()(elem)              .graphData(gData)              .nodeAutoColorBy(community)              .nodeVal(size)              .linkAutoColorBy(community)              .linkWidth(0)              // number of particles              .linkDirectionalParticles(weight)               .linkDirectionalParticleSpeed(0.001) // slow down              .nodeLabel(node => `${node.label}: ${node.caption}`)              .onNodeHover(node =>                       elem.style.cursor = node ? pointer : null)Render Example | CodeThat’s it for today from me.There are many more options in 3d-force-graph, please check it out, play around and let me know in the comments what you came up with.Big thanks to its author, Vasco Asturiano for creating such a great and easy to use library.;Jul 23, 2018;[]
https://medium.com/neo4j/using-a-graph-recommendation-algorithm-for-predicting-chemical-cell-interaction-679da18f52b8;Tom NijhofFollowNov 8, 2022·8 min readUsing a Graph Recommendation Algorithm for Predicting Chemical—Cell InteractionRecommendation algorithms are often written with a user-product relationship in mind. Which user bought what product” or Who liked this movie”. From this data, a prediction is made about an unknown user-product relation.However, this same algorithm can predict reactions between a chemical and a specific cell line.In this blog, I will implement a Collaborative filtering algorithm in a graph database. The focus will be on how to implement it. While a basic validation is done, it is not the focus of this blog.The DataThe dataset used is the NCI60 dataset. In a previous blog, I went into detail on how I created the full graph, here is just a recap.The NCI60 dataset has the GI50 measurement. This is the concentration of a chemical to have a 50% Growth Inhibition of a cell line.Concentration is given in the logarithmic scale, which means a GI50 of -5 means the concentration is 10–5 or 0.0001The GraphThe graph I made previously holds the experiments, their conditions, their measurements, and all variables used. This is done with the idea it can be extended later with different kinds of experiments.This graph shows 2 measurements, NSC123127 on NCI-H23, and NSC 19893 on A498In this blog, I only care about 2 nodes, (chemical) compounds and cell lines, and 1 relationship, GI50. This means the whole graph can be simplified to this.simplified graphI will use the simplified graph to explain most of the logic behind the algorithm, but the queries are written for the complete graph.Collaborative FilteringCollaborative filtering works on the premise that If 2 people/chemicals agree 10 out of 10 times, they will most likely also agree the 11th time”. Moshanin’s GIF explains it very clearly.Moshanin GIF made for WikipediaThe prediction consists of 2 steps:Finding similar users/chemicalsVoting between these similar ones on missing linkFinding Similar ChemicalsThe relation between a chemical and a cell is non-binary. This means we need to take numbers into account to calculate the ‘difference score’.To solve this we take the difference for each shared cell and average this.Where Dab is the difference between chemicals a and bNab is the list of all cells lines that chemicals a and b have in commonGI50ai is the concentration of chemical a, needed for a 50% Growth Inhibition of cell line iFor example, we have 2 chemicals with 3 cell lines in common.To score the difference between these chemicals, we take the GI50 values (on the relationships) and we solve the formula:⅓ * (|-4.8 — -4.9| + |-4.5 — -5.0| + |-4.4 — -5.3|) =0.5VotingWe search for the 25 chemicals with the lowest difference and make sure all of these have GI50 value with the cell line of interest.Next up is the voting, many complex methods can be used. However, a simple method can already be effective we take the average.Implementing ItAll code can be found here. I ran a Pandas implementation next to it to double-check my work.There will be a small difference between Pandas and Cypher implementations because not all measurements are in the graph (see detailed blog for why).Given the full query is a lot, I will build it up step by step.GI50 Per Cell LineThe full graph shown earlier is a bit too complex for our needs. So we want 1 value between every cell line and chemical. If we find multiple values, we average them.MATCH (org_chem:Synonym {pubChemSynId: 176dde90cc9dd83eed129de11b203b03”})MATCH (gi50:Measurement {name: GI50”})MATCH (cell:CellLine)<-[:USES]-(c:Condition)-[:USES]->(org_chem)MATCH (c)-[m:MEASURES]->(gi50)WITH DISTINCT cell, avg(toFloat(m.value)) as values1RETURN DISTINCT cell.name, values1This results in an average GI50 value for 74 cell lines that are tested with chemical NSC 19893.{‘cell.name’: ‘NCI-H23’, ‘values1’: -4.914398316498304}{‘cell.name’: ‘NCI-H226’, ‘values1’: -3.5967323114653547}…{‘cell.name’: ‘A 172’, ‘values1’: -5.0341}{‘cell.name’: ‘U87’, ‘values1’: -5.5868}Compare 2 ChemicalsIf we repeat the previous step on a second chemical (chemical B) we can calculate the difference. Here we are going to make use of the fact we already know what cell lines are connected to chemical A.// Match to chemical AMATCH (org_chem:Synonym {pubChemSynId: 176dde90cc9dd83eed129de11b203b03”})MATCH (gi50:Measurement {name: GI50”})// Find all cell lines connected to chemical AMATCH (cell:CellLine)<-[:USES]-(c:Condition)-[:USES]->(org_chem)MATCH (c)-[m:MEASURES]->(gi50)// If multiple GI50 values are know for chemical A to a cell line// Take the the averageWITH DISTINCT cell, avg(toFloat(m.value)) as values1, gi50// Match chemical BMATCH (chem:Synonym {pubChemSynId: 1d75798754df81c782e805287ff7ef83”})// Find what cell lines are connected to chemical A, also connect to chemical BMATCH (cell)<-[:USES]-(c:Condition)-[:USES]->(chem)MATCH (c)-[m2:MEASURES]->(gi50)// If multiple GI50 values are know for chemical B to a cell line// Take the the average// Then take the difference with chemical AWITH DISTINCT cell as cell2, abs(avg(toFloat(m2.value)) — values1) as difference, chem// Each row has 1 cell with difference of GI50 between chemical A and B// Average all differencesRETURN DISTINCT chem.name, avg(difference) as dist, count(difference) as num_cellsThis results in a difference of ~0.53652 with 59 cell lines in common.If we do the same with Pandas we get a difference score of ~0.53651 with also 59 cell lines in common. This shows they are close enough to each other.Compare to All ChemicalsThis query is fine if I want to compare 2 known chemicals, but I want to find the most similar chemicals of all known chemicals. To do this we remove the second chemical match and search for every chemical that shares a cell line with chemical A. To do this efficiently a call {sub query} is used. This way the subquery is called per cell line, speeding up the total query.The rest is still the same but now a difference is given for every chemical to chemical A.MATCH (org_chem:Synonym {pubChemSynId: 176dde90cc9dd83eed129de11b203b03”})MATCH (gi50:Measurement {name: GI50”})MATCH (cell:CellLine)<-[:USES]-(c:Condition)-[:USES]->(org_chem)MATCH (c)-[m:MEASURES]->(gi50)WITH DISTINCT cell, avg(toFloat(m.value)) as values1, org_chem, gi50CALL {WITH cell, gi50, values1MATCH (cell)<-[:USES]-(c:Condition)-[:USES]->(chem:Synonym)MATCH (c)-[m2:MEASURES]->(gi50)RETURN abs(avg(toFloat(m2.value)) — values1) as distance, chem}RETURN DISTINCT chem.name, avg(distance) as avg_dist, count(distance) as num_cell ORDER BY avg_dist limit 25The top result will not shock you, ‘nsc19893’ looks most like ‘nsc19893’.{‘chem.name’: ‘nsc19893’, ‘avg_dist’: 3.4386907735687276e-15, ‘num_cell’: 74}{‘chem.name’: ‘nsc-684405’, ‘avg_dist’: 0.3018711839192636, ‘num_cell’: 60}{‘chem.name’: ‘nsc-361605’, ‘avg_dist’: 0.3177947681672626, ‘num_cell’: 49}{‘chem.name’: ‘nsc-628083’, ‘avg_dist’: 0.33545731366684184, ‘num_cell’: 60}{‘chem.name’: ‘nsc-618093’, ‘avg_dist’: 0.3451951788719919, ‘num_cell’: 60}{‘chem.name’: ‘nsc-613493’, ‘avg_dist’: 0.35390246003660175, ‘num_cell’: 50}{‘chem.name’: ‘nsc628537’, ‘avg_dist’: 0.3781293960020346, ‘num_cell’: 46}{‘chem.name’: ‘nsc-785594’, ‘avg_dist’: 0.3781573653571885, ‘num_cell’: 60}{‘chem.name’: ‘nsc-628081’, ‘avg_dist’: 0.39269238426318265, ‘num_cell’: 60}{‘chem.name’: ‘nsc-625429’, ‘avg_dist’: 0.4004440031716282, ‘num_cell’: 48}If we look up the chemical structure of NSC 19893 and NSC 684405 we see something interesting. The full structure of NSC 19893 can be found with NSC 684405. And the same is true for NSC 361605 and NSC 628083. Meaning the top 3 (that I checked) all have the same molecular structure as the chemical we compare against.Left: nsc19893 right: nsc-684405Making a PredictionTo make a prediction 3 updates need to happen:1) Remove the cell line of interest from the collaborative filteringLet’s say we want to predict the GI50 of chemical NSC 19893 on cell line HCT-15. We do not want to take HCT-15 into account with collaborative filtering. This is because I want to use this interaction as validation.2) Only select chemicals with a known reaction to the cell line of interestIf a chemical does not have a known GI50 with the cell line of interest, it cannot help us make a prediction. So we ignore it.3) Remove NSC 19893 from the predicted chemicalsPredicting yourself is just stupid.// To still have a limit for the chemical we wrap the chemical matching in a call sub queryCALL{MATCH (the_cell:CellLine {name: HCT-15”})MATCH (org_chem:Synonym {pubChemSynId: 176dde90cc9dd83eed129de11b203b03”})MATCH (gi50:Measurement {name: GI50”})MATCH (cell:CellLine)<-[:USES]-(c:Condition)-[:USES]->(org_chem)WHERE cell <> the_cell // 1)MATCH (c)-[m:MEASURES]->(gi50)WITH DISTINCT cell, avg(toFloat(m.value)) as values1, org_chem, gi50, the_cellCALL {WITH cell, gi50, values1, org_chem, the_cellMATCH (cell)<-[:USES]-(c:Condition)-[:USES]->(chem:Synonym)WHERE exists((chem)<-[:USES]-(:Condition)-[:USES]->(the_cell)) AND chem <> org_chem // 2) & 3)MATCH (c)-[m2:MEASURES]->(gi50)RETURN abs(avg(toFloat(m2.value)) — values1) as difference, chem}WITH DISTINCT chem, avg(difference) as avg_diff, count(difference) as num_cellRETURN chem, avg_diff, num_cell ORDER BY avg_diff limit 25}// Get all reactions to cell line of interest of the 25 most similar chemicalsMATCH (the_cell)<-[:USES]-(c:Condition)-[:USES]->(chem)MATCH (c)-[m3:MEASURES]->(gi50)// Take the average of the 25 GI50// We also return all values just for some analysisWITH DISTINCT chem, avg(toFloat(m3.value)) as avg_valuesRETURN avg(avg_values) as prediction, collect(avg_values) as all_valuesThis gives us{‘prediction’: -4.637486125715776, ‘all_values’: [-4.727294017094016, -4.668238738738738, -4.538984496124033, -4.522523228346455, -4.5537044642857145, -4.714266037735851, -4.592720720720721, -4.5019374407582955, -4.436698214285715, -4.610763461538461, -4.541042553191488, -4.65542857142857, -4.67053893129771, -4.45142962962963, -4.965887826086957, -4.737043181818183, -4.601286440677966, -4.758097727272728, -4.699171428571429, -4.734895238095239, -4.684224, -4.787217241379309, -4.521838461538462, -4.59928888888889, -4.66263220338983]}ValidationComplete validation is not the goal of this blog. However, I do want to give an idea that there is merit in the idea.NSC 19893 on cell line HCT-15 has 1812 GI50 measurements, with an average of -5.24.The 25 chemicals close to NSC 19893 predicted an average of -4.64.A difference of -0.6 is not a lot if we look at all GI50 on cell line HCT-15, these go from -8 to -4 (see histogram 1). A value of -4 is special given it is often the higher concentration tested. If a concentration of -4 did NOT result in a 50% Growth Inhibition, -4 is noted down. This means most of these have a true GI50 greater than -4 (see histogram 2 for -4 excluded).None of our top 25 predictions had -4, which is a good sign. Our predictions were in a much smaller range than all HCT-15 GI50 (see histograms 2 & 3).Histogram 1: All HCT-15 valuesHistogram 2: All HCT-15 values excluding -4Histogram 3: 25 predictions;Nov 8, 2022;[]
https://medium.com/neo4j/lets-go-with-neo4j-4f153e343128;Vinodh SubramanianFollowMar 23, 2019·3 min readLet’s Go with Neo4jSetting up and using the Neo4j Go DriverIt’s been a while since the official Neo4j driver is available for Go. So I thought of giving it a try.I’m used to working with windows but recently I started using Mac. And got used to problems while installing new software.Though installing the Neo4j application is a straight forward job, establishing the connectivity from my Go project to Neo4j DB was tedious. There are a lot of things to be done to make it work. I found it quite difficult installing these drivers, other essential libraries and bridging the gap between them. So I’ve broken down the process and made it easy.I guess you should have already installed Neo4j and Go in your Mac. If not please check Neo4j and Go installations for Mac OS.The Go driver is built on top of the C Connector, Seabolt. So we need to have a cgo development environment.Let’s get started with our work !!!Pkg-ConfigLinux provides pkg-config as part of the package management system. But, yeah you are right, Mac os doesnt. It’s simple to install though.brew install pkg-configSeaboltClone the Seabolt source from github. To build the project you’ll be needing a few libraries as well.brew install cmakebrew install opensslCreate an environment variable named OPEN_SSL_ROOT_DIR. that points to the openssl library installation path (default is /usr/local/opt/openssl)I keep all my vendor libraries in /Users/username/libraries. I’ve cloned the Seabolt source to the same path. You can proceed with your preferred directory.cd /Users/username/libraries/seabolt-1.7./make_debug.shAdd to the pathvim ~/.bash_profileAdd the following two variables to the profileexport PKG_CONFIG_PATH = /Users/username/libraries/seabolt-1.7/build/dist/share/pkgconfigexport DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/Users/username/libraries/seabolt-1.7/build/dist/libYou can make sure if the Seabolt library has been added to the pkg-config.pkg-config --list-allIf you can see seabolt17-static seabolt17-static — The C Connector Library for Neo4j in the list of libraries, you are good to go.Neo4j golang driverInstall the Neo4j golang driver.go get github.com/neo4j/neo4j-go-driver/neo4jWe are done with all the requirements.Let’s check whether our efforts have paid offStart your Neo4j desktop application and get the bolt URL, username and password for your graph DB.I’ve created a Graph named neo4j”, password as password” and it can be accessed in bolt://localhost:7687” URL.Create a file neo4jgolang.go. Add the following two code snippets to it. The code is self-explanatory.The main function for go projectThe function which creates driver, session and transaction to execute the Cypher queryI assume you’ve followed all the steps above. Now simply go ahead and execute your codego run neo4jgolang.goIf all goes well you should be able to see the following response in the terminal.Welcome to world of GraphsGreetings from graph : hello, world, from node 0”In Neo4j browser, you will be able to see a Greeting node created with a message as hello world”So that’s it for this post. I’ve covered the basic and essential procedure to make your go project backed by Neo4j.Hope you like it!!!;Mar 23, 2019;[]
https://medium.com/neo4j/tap-into-hidden-connections-translating-your-relational-data-to-graph-d3a2591d4026;Jennifer ReifFollowJun 13, 2018·8 min readTap into Hidden Connections — Translating Your Relational Data to Graph*Last updated 4/11/2019Graph databases are a relatively new space in the technology industry. However, that doesn’t stop many companies big and small from jumping in. There are many cases to be made for one path or another, but one thing can be demonstrated in using graph databases — the importance of and meaning hidden in relationships. Graph databases tap into how pieces of data connect to one another, showing even the most remote connections between seemingly random pieces of data.Perhaps you have read or heard about graph databases. Maybe you have even learned a little bit about them. But where do you go from there? How do you start taking real-life data from one source and put it into a graph to realize the value hidden within?Today, I’d like to walk you through one way to translate your data from a relational database into a Neo4j graph database. While there are plenty of other types of data sources (like NoSQL), this post will simply focus on going from relational to graph. Hopefully, in future posts, I can dive into moving other types of sources into graph and other ways to translate relational data to graph, as well.Neo4j ETL ToolThe Neo4j ETL (Extract-Tranform-Load) tool was built to allow developers an easy way to load relational data structures into a graph. It includes a 3-step process that allows the user to specify a relational database via a JDBC setup, then edit the data model mapping that the tool creates for graph, and finally import all of the data itself to Neo4j.The tool is easy enough to access and use. There are two ways to interact with the tool — via command line or via Neo4j Desktop application. For working with the command line, you can download the GitHub project and run each step at a command prompt.Add the Neo4j ETL ApplicationFor working with the user interface version in Neo4j Desktop, you need to add this key in Neo4j Desktop — https://r.neo4j.com/neo4j-etl-app. This is demonstrated in the screenshot below.Once this is complete, you will have access to the ETL application. Now, we need to add it to your graph applications.First, go to your Projects menu within Neo4j Desktop and choose/create a project to work within. Then, click on the Add Application under the first banner on the right menu pane. When the popup displays to show you the applications you can add, choose the ETL App by clicking the Add button. The screenshots below step through this process visually.Now we are ready to launch the application and start transforming data from relational to graph! To load the application, just click on the newly-added application in your project.Select a Project to Work WithThis will open a new window with the main screen of the application, as shown below. You will need to choose the project to work within. This will tell the application where the Neo4j database is that you want to load your data into. To the right, you can see the list of possible databases in that project that you can use. Since no connections show up on the left, we don’t have any relational databases defined in our tool, so we will click ADD CONNECTION to set one up.Setup RDBMS ConnectionThis screen is where we set up a relational database connection. The tool allows most types of relational databases with an JDBC Driver, including MySQL, PostgreSQL, Oracle, Cassandra, DB2, SQL Server, Derby, and others. While MySQL and PostgreSQL databases are built-in to the tool, all other databases can be easily set up by specifying a driver file. We will walk through each step below.For this example, I will be using a PostgreSQL relational database with the Northwind dataset loaded. If you do not already have a sample database/dataset to work with, you can follow along with this post by downloading PostgreSQL and running the shell script pulled from a GitHub repository for the Northwind dataset for Postgres.By choosing the database type from the Type dropdown field first, some of the other fields will automatically populate for you and save a bit of hassle. For this example, you will choose the postgresql option. Notice that the port and connection URL are already correct for that choice.We can now fill in the rest of the details, as needed. You can put whatever you like in the Connection Name field, but the Database field must match the name you gave your relational database. If you are using another database outside of Postgres or MySQL, be sure to choose a driver file for the tool to use for connecting.*Note: if you set up a username and password for your relational database, you must enter those in the appropriate fields. However, if you did not specify a username and/or password for your relational database, you can leave one or both of the fields blank.Once all the details are filled in, click the Test and Save Connection button at the bottom of the pane. You should see a blue message bar appear at the top to say your connection was saved successfully. A red bar with an error message will appear if one of the fields is not correct or it cannot find the relational database.Retrieve RDBMS MetadataOk, now we have our relational database connection set up. Next, we need to map from relational to graph by choosing our relational database connection that we are loading from in the left list, and then selecting our graph database we are writing to in the list on the right. Then you can click the Start Mapping button.You will see a blue message bar at the top of your screen if the step was successful or a red one if the step failed. Since ours was successful, our Start Mapping button inactivates, and the Next button activates to proceed to the next step. If the step fails, you can click the SEE LOGS button at the bottom to debug the process.The Data in Mapping.jsonThis is where the mapping.json file is created with an outline of the mapping between the databases. This is an important part of the process because you can make changes to this file if you decide you want to leave out particular tables or fields in the migration.To look at the file, pull up your main Neo4j Desktop window, go to your project, then click Manage on the Neo4j database where you will migrate the data (in this example, it is named ETL_db). Click the Open Folder button in the header pane and go into the import folder.There, you should see a postgresql_northwind_mapping.json file (or some similarly-named file). You can open this and edit if there are any changes you want to make before going to the next step. For this example, I will leave the mapping.json file without any changes.Adapt the MappingGoing back to the ETL application window, you can click the Next button at the bottom right, which will take you to a data model screen. You can move the nodes and relationships in the diagram on the right side of the screen so that you can better view each of them.Here, you can change names of nodes, relationships, and even property (field) names and data types. I do think we can choose more appropriate relationship types, changing them from generic nouns to more specific verbs.For instance, I changed the relationship from Order node to Product from ORDER_DETAILS” to INCLUDES” because an order includes products (i.e. line items).Data ImportOnce you are satisfied, you can click the SAVE MAPPING button and then click the NEXT button at the bottom right. This takes us to the final screen, which is where we choose our import method. There are two options, and both are explained below.Bulk Import — fast, bulk-import of large data sets, database must be offline to runOnline Import — runs Cypher via BOLT connection, database can be online and runningThe key thing to note on the import method is whether your Neo4j database is running/online or not.Finally, you can click the Import Data button and see the output display at the bottom of the screen from the behind-the-scenes commands that get automatically executed. When it completes, a blue message bar will appear at the top of the window to say that the import was successful (screenshot below).Check Imported DataTo verify everything loaded and to start working with your data in Neo4j, we can look at the data model and run queries from Neo4j Browser or an application connected to your database.For this example, we will go back to our main Neo4j Desktop window, go to Manage on the ETL_db database, then click Open Browser. Executing the CALL db.schema() on the Browser command line will display the graph data model for you.CALL db.schema()Notice that the model does not contain the Customer Demographics node connecting to Customer node. This is because the Postgres data set used did not have any data in the customer demographics csv file.From here, you can run queries, update data, and investigate the data to gather value and insight from structuring your connected data as a graph. The screenshot below shows an initial query (under Favorites -> Sample Scripts -> Basic Queries -> Get some data) to retrieve and display some customers, shippers, orders, and products in the data set.If you have any questions, do not hesitate to reach out on our neo4j-users Slack community under the #neo4j-etl channel. As always, happy learning! :)ResourcesETL Latest Release blog postNeo4j ETL Command Line ToolDownload Neo4jNeo4j SandboxDeveloper Guide;Jun 13, 2018;[]
https://medium.com/neo4j/create-a-data-marvel-part-6-developing-more-entities-be5aeab1817a;Jennifer ReifFollowJan 16, 2019·8 min readCreate a Data Marvel — Part 6: Developing More Entities*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)After last week’s Part 5, we have the beginnings of our domain classes. We developed the Character domain, along with its repository interface and a controller to manage request/response. This week, we will continue that momentum and create classes for another 4 areas of our graph data model — creator, event, series, and story.We will see how our Character classes gave us a foundation that simplifies building the next set of classes. Let’s get started!Organizing classesBefore we dive right into code, we should do a bit of housekeeping. As you might imagine, when we build out all 6 of our data model entities with 3 (or more) classes each, we will end up with quite a long list of classes in our demo folder. To help organize this, we can add subfolders for each entity to drop each set of classes.Starting with our existing Character classes, we can right-mouse click on the demo folder (src->main->java->com->example->demo) and choose New, then Package.Type in the name character, then click OK. Now we need to move our existing classes to the new folder. Select all three classes (Character, CharacterController, CharacterRepo), then drag and drop them on the new character folder. A popup box will appear to ensure you want to refactor all references and paths on the classes. Ensure the boxes are checked and the folder path looks correct, then click Refactor.Once this is complete, you should now see a structure like in the image below.Let’s go ahead and create the folders for each of the other classes. You can follow the same steps we executed above, but name each of the packages as follows: creator, comicissue, event, series, story. If you completed those steps, you should now see a structure like this.Domain classesThe Character classes that we coded last time gave us the foundation for the annotations, structure, and some fields that we can use across each of our other entities. Because of this, the other non-central entities (creator, event, series, and story) are able to be coded very easily and quickly.The Creator classNow that we have our folders set up, we are ready to start coding our Creator classes. We need to right-mouse click on our new creator package and choose New->Java Class. Name it Creator and click OK. Again, IntelliJ generates the class path and the empty class declaration for us, so we can fill in the gaps.Just like we did with the Character class, we add our Lombok annotations and our fields to this. The code for the class is below. We will review each component in the next paragraph.package com.example.demo.creatorimport ...@Data@NoArgsConstructor@RequiredArgsConstructor@NodeEntitypublic class Creator {  @Id @GeneratedValue  private Long neoId  @NonNull  private Long id  @NonNull  private String name, resourceURI}The @Data annotation handles much of the boilerplate for the standard POJOs, supplying the getters, setters, and equals+hashcode+toString methods. The next Lombok annotations @NoArgsConstructor and @RequiredArgsConstructor generate a constructor with no arguments and a constructor with one argument for each field required to be @NonNull, respectively. We also need to annotate the class with @NodeEntity to let Spring Data Neo4j (SDN) know that this is a domain class for a node in our graph database.Within our Creator class declaration, we set up additional annotations on each of our fields. The @Id and @GeneratedValue annotations show which field is the id (neoId) and that the value will not be user-defined. This field is the internal unique id that Neo4j creates for each node and relationship. Though it is rarely referenced, it is part of the entity in the database.The next field (id) is the id coming from the Marvel system, so it’s the API id. We used a Lombok @NonNull annotation here to handle a null-check and ensure the value is not null. The final line has the String fields we felt most meaningful to this entity from the API. These fields also have the non-null annotation to ensure values are not missing.Our Creator class is completed! It’s time to add our repository interface and our controller class to it.The repository interfaceWe need the repository interface for the data access layer. To start, right-mouse click on the creator package folder again and choose New->Java Class. Name it CreatorRepo, then be sure to choose the Kind as Interface before clicking OK.Since this class mirrors the CharacterRepo we coded last time, we will skip to the full code. All we need to do is extend the interface from the Neo4jRepository.package com.example.demo.creatorimport org.springframework.data.neo4j.repository.Neo4jRepositorypublic interface CreatorRepo extends Neo4jRepository<Creator, Long> {}The controller classTo wrap up our Creator classes, we need to add the controller class that will handle requests and responses between the user interface and data layers. This class carries the bulk of the logic.This class’s code is similar to ourCharacterController. A brief explanation follows the code.package com.example.demo.creatorimport ...@RestController@RequestMapping(/creators”)public class CreatorController {  private final CreatorRepo repo  public CreatorController(CreatorRepo repo) { this.repo = repo }  @GetMapping  public Iterable<Creator> getAllCreators() {     return repo.findAll()  }}Remember that the @RestController annotation eliminates the need to put @ResponseBody on each request handling method, and @RequestMapping maps requests to a controller method for a certain path (in this case, /creators).Inside the class declaration, we have a local member variable and a constructor for our CreatorRepo to inject the repository into our controller. The @GetMapping annotation tells us this will be a GET method and precedes the method declaration for getAllCreators() that accesses the repository, executes the provided findAll() method, and returns multiple Creator entities (Iterable of type Creator).Remaining classesFollowing these steps above, we can push on through the rest of the classes easily. No new or different code is needed to build out the other entities. The code for each of these will be listed, followed by a final folder structure at the end for reference and verification. See you at the finish line! :)Event:package com.example.demo.eventimport ...@Data@NoArgsConstructor@RequiredArgsConstructor@NodeEntitypublic class Event {  @Id @GeneratedValue  private Long neoId  @NonNull  private Long id  @NonNull  private String name, resourceURI}EventRepo:package com.example.demo.eventimport org.springframework.data.neo4j.repository.Neo4jRepositorypublic interface EventRepo extends Neo4jRepository<Event, Long> {}EventController:package com.example.demo.eventimport ...@RestController@RequestMapping(/events”)public class EventController {  private final EventRepo repo    public EventController(EventRepo repo) { this.repo = repo }  @GetMapping  public Iterable<Event> getAllEvent() {     return repo.findAll()  }}Series:package com.example.demo.seriesimport ...@Data@NoArgsConstructor@RequiredArgsConstructor@NodeEntitypublic class Series {  @Id @GeneratedValue  private Long neoId  @NonNull  private Long id  @NonNull  private String name, resourceURI, thumbnail  @NonNull  private Integer startYear, endYear}SeriesRepo:package com.example.demo.seriesimport org.springframework.data.neo4j.repository.Neo4jRepositorypublic interface SeriesRepo extends Neo4jRepository<Series, Long> {}SeriesController:package com.example.demo.seriesimport ...@RestController@RequestMapping(/series”)public class SeriesController {  private final SeriesRepo repo  public SeriesController(SeriesRepo repo) { this.repo = repo }  @GetMapping  public Iterable<Series> getAllSeries() {    return repo.findAll()  }}Story:package com.example.demo.storyimport ...@Data@NoArgsConstructor@RequiredArgsConstructor@NodeEntitypublic class Story {    @Id @GeneratedValue    private Long neoId    @NonNull    private Long id    @NonNull    private String name, resourceURI, type}StoryRepo:package com.example.demo.storyimport org.springframework.data.neo4j.repository.Neo4jRepositorypublic interface StoryRepo extends Neo4jRepository<Story, Long> {}StoryController:package com.example.demo.storyimport …@RestController@RequestMapping(/stories”)public class StoryController {  private final StoryRepo repo  public StoryController(StoryRepo repo) { this.repo = repo }  public Iterable<Story> getAllStories() {    return repo.findAll()  }}Wrapping upWhew! We made it. That wraps up code for our domain classes for 5 out of 6 entities. That’s correct, we still have one more set of classes to cover for the ComicIssue. The comic issue node is the center for all of our relationship connections to the other nodes, so the code will require more depth and explanation.What I LearnedMost of this code mirrored what we did in the last post with our Character classes. This made it really simple to code the Creator, Event, Series, and Story classes quickly and efficiently and reduce explanation.Let’s look at the key takeaways here.We didn’t need any additional fluff in our code, and Spring keeps the overhead low, so we minimize repetitive code. In about 5min, we could whip up the rest of these classes and have them function. It makes it incredibly helpful in our short development process (and for later live-coding times!).None of our entities had any extraneous or unusual logic or code needed. We could have added more methods or entrypoints to access different entity aspects, but our goal was to keep it simple and clean. This made the code nearly copy across our 5 non-central entities.Next StepsWe now have the bulk of our supporting code developed and are ready to work through the final piece that connects them all. Stay tuned for the next post where we will code the ComicIssue classes and show how to tie it all together!ResourcesFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j GuideProject Lombok docsPrevious parts of this blog series: Part 1, Part 2, Part 3, Part 4, Part 5;Jan 16, 2019;[]
https://medium.com/neo4j/fetching-large-amount-of-data-using-the-neo4j-reactive-driver-the-bloom-case-4c3bcd0f4194;Angeliki KomianouFollowJun 28, 2021·7 min readFetching Large Amounts of Data Using the Neo4j Reactive Driver: The Bloom CaseBloom is a graph exploration tool which displays data in graph format. It’s a Javascript application (React.js based) and uses the Neo4j Javascript driver in order to interact with a Neo4j database.Neo4j Bloom - Graph Visualization and CollaborationBloom gives graph novices and experts the ability to visually investigate and explore their graph data from different…neo4j.comThe users are able to search for graph patterns using a generated vocabulary by the components of their databases (labels, relationships, properties or values) or create their own custom queries. They, also, have access to a very large amount of graph patterns which can lead to a huge amount of results.The complexity of the queries and the size of the available data in combination with the limitation of the client resources requires restrictions on the size of the response and the number of the displayed nodes. So fetching results has always been a challenge for Bloom.For that reason, Bloom can display a range between 100 to 10,000 unique nodes and their relationships. The size of a Cypher query response is limited to 10,000 records (rows), as well.The question is how large the response should be in order to get the maximum available number of unique nodes in a very large space of results.For example, the user searches for Person — Movie which can be translated in Cypher to MATCH (n:Person)-[r]-(m:Movie) RETURN n, r, m. The selected maximum number of unique nodes, from the Bloom settings, is 10000.Bloom search for Person — MovieIn the below image you can see a visual representation of the response. Each column represents a record. Each unique node has a unique color. The multiple relationship types between a Person and the Movie (for example, a Person can be an ACTOR in a Movie and the same Person can be the PRODUCER of the same Movie) can create many duplicates for the nodes.The number of the records to be parsed, in order to get the specified number of unique nodes is not a priori known by the response (if you don’t collect DISTINCT lists), as many records can include duplicates of the same nodes.After filtering all the records, at the example query, there is a need to fetch 26000 records in order to get the ideal number of 10000 unique nodes. The problem comes up when the size of the records is too large to be filtered at once by the client (for example 100K rows), or even when the server has to parse a huge amount of records which may not be needed. At the same time Bloom needs a common way to fetch results for the pattern search and the custom Cypher queries created by the users.As mentioned above, you are not able to know a priori which is the ‘right’ limit in order to get exactly 10000 unique nodes (26K rows in this example) and set it as LIMIT in your query. Even if you knew that, a large LIMIT could still cause performance issues at the client.The question is how can you set the number of the records in your query in order to get a size of response which can be handled by the client and at the same time calculate efficiently the maximum number of unique nodes.Solution 1: Chunk ApproachAt Bloom versions earlier than 1.6.0 we developed a pagination method using SKIP/LIMIT combination. The same method is used in the Bloom version 1.6.0 and later, as well, but only for the older 3.5.x Neo4j databases.Bloom divided the response in chunks using multiple queries until the desired number of unique nodes was reached or when all the records were consumed.Chunk approachSo, despite fetching all the rows that match the pattern of the Cypher query calling a simple transaction,Simple transactionusing the chunk approach we divide the response size calling different transactions with a specified SKIP and LIMIT.Chunk approach transactionsThe Chunk Approach enables the client to manage the size of the response and calculate the maximum number of unique nodes efficiently, but the generation of multiple queries and the use of SKIP adds more complexity to the code and raises the execution time. And the server still needs to process the data to be skipped time and again.Solution 2: Reactive DriversIn version 4.0 Neo4j database’s reactive drivers were introduced. You can imagine them as a gate between the client and the server in which you control the flow of data and adjust it accordingly. The results return to the client as quickly as they are generated by the server. Reactive drivers are used in Bloom since the 1.6.0 version.The Neo4j JavaScript Driver Manual v4.3 - Neo4j JavaScript Driver ManualThis is the manual for Neo4j JavaScript Driver version 4.3, authored by the Neo4j Team. This manual covers the…neo4j.comBloom uses the Reactive API of the Neo4j Javascript driver which is an extension of ReactiveX, so it would be good to get familiarised with reactive programming and streams of data. A reactive session should be established, using rxSession(), in order to use reactive programming API provided by the neo4j driver.ReactiveXEdit descriptionreactivex.ioThe basic components of a reactive session are the Publisher, which emits data (records()), the Subscriber, which watches the flow of the data and the Processor which is between publisher and the subscriber and represents the state of values processing.The basic operators we use are bufferCount(<buffer_size>) and takeUntil(<notifier>) by ReactiveX — RxJS (reactive extensions library for javascript). Using bufferCount(<buffer_size>) you can define the size of results you can accept each time.Technically, the values of the stream of data are packed in arrays with a specified size at bufferCount(<buffer_size>). This procedure continues until the notifier defined at takeUntil(<notifier>) emits a value or when all the possible results are consumed by the server.Transaction using reactive drivers in BloomAt the Subscriber level, when the desired number of unique nodes is reached, Bloom forces the notifier in order to emit a value and the transaction is completed declaratively. In case there are not more results the transaction is completed automatically.Completion of transaction using reactive driversThis technique can be used for any data processing you need in the client side using data size limitations.In the example code below you can find a simple way for calculating unique nodes using reactive drivers (in combination with bufferSize() and takeUntil() operators), in the client side.import neo4j from neo4j-driverimport { Subject } from rxjsimport { bufferCount, takeUntil } from rxjs/operatorsimport { concat, flatten, uniqBy, slice } from lodashconst MAXIMUM_UNIQUE_NODES = 10000const BUFFER_SIZE = 10000let uniqueNodes = []const driver = neo4j.driver(  neo4j://localhost,  neo4j.auth.basic(neo4j, password))const query = `MATCH (n:Person)-->(p:Movie)                RETURN id(n) as idn, id(p) as idp`const rxSession = driver.rxSession({ database: myDatabase })const notifier = new Subject()const emitNotifier = () => {  notifier.next()  notifier.complete()}const filterUniqueNodes = records => {  const newNodes = records.map(d =>     [d.toObject()[idn].toInt(), d.toObject()[idp].toInt()])  // filter out the new unique nodes  const newUniqueNodes = uniqBy(flatten(newNodes))  // compare with the existing unique nodes and update the    uniqueNodes  uniqueNodes = uniqBy(concat(newUniqueNodes, uniqueNodes))  if (uniqueNodes.length === MAXIMUM_UNIQUE_NODES){    emitNotifier()  }  if (uniqueNodes.length > MAXIMUM_UNIQUE_NODES) {    uniqueNodes = slice(uniqueNodes, 0, MAXIMUM_UNIQUE_NODES)    emitNotifier()  }}export const fetchResultsUsingReactiveDrivers = () =>  rxSession.readTransaction(tx => tx    .run(query, {})    .records()    .pipe(       bufferCount(BUFFER_SIZE),       takeUntil(notifier)    ))    .subscribe({      next: records => {        // here you can add your own function for manipulating the incoming data        filterUniqueNodes(records)      },      complete: () => {        console.log(completed, uniqueNodes)      },      error: error => {        console.error(error)      }    })Another advantage of the use of the reactive drivers is that gives the flexibility to the client to request data with specified size until they are totally consumed or terminate the transaction when needed. There are, also, a plethora of operators provided by RxJS that you can use and customise your Neo4j transactions to the needs of your application.For a more in-depth explanation of the reactive driver model provided by Neo4j 4.x drivers, Greg Woods presents the details in this session:So we were really happy and impressed how well we could change our data-fetching code to be more efficient and making use of the reactive dataflow to only fetch as much as we needed without overloading client or server.You can see the results of this work in our latest Neo4j Bloom release 1.7.0 which also brings a lot of other great features.Visual Tour - Neo4j BloomThis chapter presents a visual overview of the UI of Neo4j Bloom. The sidebar contains a set of drawers where you can…neo4j.com;Jun 28, 2021;[]
https://medium.com/neo4j/new-graphacademy-course-cypher-aggregations-4f62f333b4b8;Elaine RosenbergFollowFeb 8·2 min readNew GraphAcademy Course: Cypher AggregationsWe have just published the Cypher Aggregations course in our Cypher Learning path.GraphAcademy Cypher Aggregations Course PageThis course will take about 4 hours to complete and includes content, videos, checks your understanding questions about the content, and hands-on challenges so you can practice your newly-learned skills. This course uses the Recommendations sandbox for your queries.In this course, you will learn what aggregation is in Cypher and how it behaves at runtime. You will see and run example code that uses the most widely-used aggregation functions in Cypher.collect() is the most widely-used function for aggregation in CypherWhat you will learn:You will profile and learn how aggregation works at runtime using:collect()count()Grouping keysList functions like Pattern comprehensions”Using pattern comprehension to create lists2. You will gain experience working with list functions:Functions that return a single valueFunctions that return listsElement type transformationsList selection predicatesList comprehensionTesting if a single list element satisfies the predicate3. You will gain experience using some useful aggregating functions:sum()avg()/stddev()min()/max()percentileCont()/percentileDisc()Using percentileCont()We hope you enjoy your learning experience with this new GraphAcademy course.Take the Cypher Aggregations course with Neo4j GraphAcademyIf you find yourself stuck at any stage then our friendly community will be happy to help. You can reach out for help…graphacademy.neo4j.comIt will take you on the path for Neo4j Certification, which also earns you this cool t-shirt.Neo4j Certification T-ShirtElaine Rosenberg from GraphAcademy;Feb 8, 2023;[]
https://medium.com/neo4j/announcing-the-release-of-neo4j-graphql-library-2-0-0-b8e536777112;Darrell WardeFollowAug 10, 2021·4 min readAnnouncing the Release of Neo4j GraphQL Library 2.0.0What’s new in version 2?Today, we’re excited to announce the stable release of the Neo4j GraphQL Library 2.0.0! 🎉 This has been in the pipeline pretty much since the day of the 1.0.0 release, and culminates not only the work on some amazing new features, but several learnings from that first release.Thank you to everyone who has tried out the 2.0.0 prereleases, engaged with the RFC process or contributed to the project during this period — we wouldn’t be where we are now if it wasn’t for your involvement. I would like to especially thank Nick Sethi (@litewarp) for his work on cursor-based pagination — it’s a great feature which I’m sure will see a lot of use.Here’s a summary of what’s new with links to any relevant documentation:Relationship propertiesCursor-based paginationCount queriesImprovements to the developer experience with union relationship fieldsMore comprehensive validation of type definitionsInternal refactoring and bug fixesBefore diving into the detail of the new features in this release, I’d like to talk a bit about the design decisions which guided their development.Connections, Edges, and NodesWhen thinking about how to add support for relationship properties, we had a few goals in mind:Low friction to add relationship properties to existing type definitionsEasy to understand how type definitions map onto the generated schemaFamiliar constructs used for the representation of relationship propertiesOn the familiarity side of things, our minds turned to Relay and the GraphQL Cursor Connections Specification, with its concept of edges and nodes, almost perfectly mapping over to Neo4j’s relationships and nodes. When digging out the following sentence from the specification, we knew we were on the right track for our relationship properties problem:Edge types must have fields named node and cursor. They may have additional fields related to the edge, as the schema designer sees fit.By adding fields to the edge types, we could represent relationship properties where they conceptually belong, on the traversal between two nodes. Running with this specification would also open the door to cursor-based pagination in the future, a feature which had been suggested, so it seemed like a no brainer to take this approach.Whilst this release doesn’t cover Object Identification needed to make the library fully Relay compliant, discussions around this topic are underway and it’s definitely something we want to implement.What’s New?Relationship PropertiesThe original goal of this release, the Neo4j GraphQL Library now has support for relationship properties. Relationships are the special sauce” of Neo4j which add a real richness to your graph datasets, which combined with the accessibility of GraphQL, will enable users to easily build applications on top of highly complex, interconnected data.Adding relationship properties into your existing @neo4j/graphql type definitions is easy — just add a new interface containing your properties and point to it in your existing @relationship directive:Cursor-Based PaginationCursor-based pagination if often the preferred method of pagination when building infinitely scrolling applications, and with today’s release, it will be automatically available for all relationship fields in your schema.To get started with cursor-based pagination, you query the connection field of a relationship, passing in the cursor string which you want to start navigation after, for example, getting the actors of a movie 10 at a time:Count QueriesCursor-based pagination doesn’t tickle your fancy? We realize that page-based pagination wasn’t so easy in the 1.x releases, with a lack of any count queries making it difficult to calculate how many pages are available in total. So, we’ve added count queries to the root Query type for all defined nodes, which will allow you to better paginate with pages.These accept the same where arguments which you’re familiar with, for example, getting a count of movies which Robin Williams has acted in:What’s Changed?Goodbye Skip”, Hello Offset”Since we released version 1.0.0 of the Neo4j GraphQL Library, Apollo Client 3.0 was released. In this release, Apollo Client has become a bit more opinionated, favouring offset and limit over skip and limit for pagination. Acknowledging that Apollo Client is by far the most popular GraphQL client library, we have updated the page-based pagination arguments in the Neo4j GraphQL Library to align with this change.Changing Union Input TypesFrom day one, union relationship fields have been a lot more popular than we’d anticipated! As such, we wanted to take the opportunity of this release to restructure input types for a better developer experience using unions.You can read about these changes in our migration guide.What Are You Waiting for?Does all of the above sound good to you? Not only does the 2.0.0 release contain all of these great features, but also a great deal of non-functional improvements which will improve your experience with the library.Being a major release, there are several breaking changes which will need to be addressed as you upgrade. We have written a handy migration guide to point of these out, and you can find us in the #graphql channel on Discord if you have any questions.If you want to see these features in action, Will Lyon (@lyonwj) and I will be taking a look at the new release of the library in a livestream today (10th August 2021) — tune in at 16:00 UTC!Also, keep an eye out for a blog from Dan Starns (@danstarns1) in the coming days, in which he will go through an example of how you can take advantage of the features in today’s release.;Aug 10, 2021;[]
https://medium.com/neo4j/zendesk-to-neo4j-integration-2c5ddba16767;Dana CanzanoFollowAug 14, 2018·8 min readZendesk to Neo4j IntegrationBetter control over you reporting needsZendesk, one of the leading customer service and engagement platforms, provides built in reporting functionality which will satisfy some reporting metrics. However at some point you are simply going to want more, whether it be entirely unique to your organization or something that the default reporting interface can not satisfy.All hope should not be lost for they also provide a well documented and robust REST API . The REST APi returns all its results in JSON syntax.The following details how to use the REST API to export data from Zendesk into Neo4 primarily using APOC through the use of apoc.periodic.commit and apoc.load.jsonParamsThe data model we will construct in Neo4j will beZendesk Graph ModelWhereby the ZendeskTicket is the center of the model represented by the green node (circle).Each ZendeskTicket has a relationship to the ZendeskUser, represented by the yellow node, who opened the ZendeskTicket as well as the agent who will solve the ZendeskTicket.Each ZendeskUser has a relationship to the ZendeskOrg, represented by the pink node, the user is a member of.Additionally, each ZendeskUser has a relationship to any KnowledgeBase Sections, represented by the grey node, the user has choosen to [Follows].The remaining labels, namely Quarter represented by the blue node and Hour represented by the magenta node are created from the imported data and are used to facilitate reporting.Finally the Import node, represented by the red node serves as meta-information to support pagination through the Zendesk API. This will be described in further detail.As to usage of the Zendesk API the following needs to be be understoodAuthenticationIn the Cypher examples provided below, the calls to the Zendesk REST API will authenticate using a OAUTH token. Generating a token and its use is described at the Zendesk APIRate LimitsDepending on your Zendesk subscription type, you are limited to the number of REST requests per minute. The Cypher to export the data assume the lowest request rate and to accommodate this you will see references to call apoc.util.sleep(x) which will cause the Cypher to sleep/pause for x millisecondsPaginationBy default, most Zendesk Rest endpoints will return at most 100 results in a single call. As such the Cypher script will paginate and request the next 100 results until all results have been returned. This is primarily accomplished through the Neo4j meta-data node with label :Import.Used ToolsThe example below has been tested and implemented with Neo4j 3.4.0 and APOC 3.4.0.1 though would expect this to work with most any APOC supported Neo4j version.For our Neo4j-Support Zendesk environment this will result in almost 10k nodes, and supporting indexes, with properties and associated relationships on the order of 7 minutes to complete on a empty graph.Part of the slowness is due to the Rate Limits Zendesk imposes and thus the Cypher purposely slows itself down through 3 calls to apoc.util.sleep(60000), thus sleeping for 3 minutes in total.PreparationThe script will create the following indexes namely for the benefit of MERGE statements.create index on :ZendeskOrg(id)create index on :ZendeskUser(id)create index on :ZendeskTicket(id)create index on :ZendeskTicket(initial_severity)create index on :Hour(hour)create index on :Quarter(quarter)Finally, the script will create 4 Import nodes which are used to support pagination.Zendesk API UsageDocumenting input and output fields and sample JSON return stringZendesk Organizations.Zendesk UsersZendesk Incremental Tickets ExportZendesk KB Section and SubscriptionFor each type of object to be extracted from Zendesk the Cypher is constructed such that we first define the :Import node for the object, representing the starting ‘page’ in Zendesk to request and then will perform a apoc.load.jsonParams within a apoc.periodic.commit which will thus allow us to paginate. For example the following Cypher is used to load Zendesk OrganizationsIn all Cypher example below, references to https://neotechnology.zendesk.com/api/v2/ will need to be replaced with your Zendesk domain. We make this easy by defining an url” parameter for it. Same goes for the :param zd_identifier: Basic ZGFuYSTlnVlVDNnQ=” OAUTH Zendesk token:param url:  https://neotechnology.zendesk.com/api/v2/ :param zd_identifier:  Basic ZGFuYSTlnVlVDNnQ= Importing OrganizationsMERGE (import:Import {id: 1})SET import.page = $url + organizations.json?page=1,     import.token = $zd_identifierCALL apoc.periodic.commit( match (import:Import {id:1}) CALL apoc.load.jsonParams(import.page,{Authorization: import.token},null)  YIELD value as orgs  WITH orgs, orgs.next_page as next_page  UNWIND orgs.organizations as oneorg  UNWIND oneorg.organization_fields as orgf  MERGE (n:ZendeskOrg {id:oneorg.id})   SET n.name=oneorg.name,       n.url=oneorg.url,       n.created=oneorg.created_at,       n.domain_name=oneorg.domain_names WITH next_page, CASE WHEN next_page is null then 0 ELSE 1 END AS count FOREACH(_ in CASE WHEN count = 0 THEN [] ELSE [1] END |     MERGE (import:Import {id:1})        SET import.page = next_page ) RETURN count, null)In the above example the first statement sets the starting page for the Zendesk API to page=1 for requesting Organizations.The statement withinapoc.periodic.commit is responsible for allowing the Zendesk pagination to work. In this case we are evaluating the returned JSON parameter named next_page and if it is set we update the Import node to reflect the next page from the API to fetch.Importing UsersThe same process occurs for loading Users as the Cypher is:MERGE (import:Import {id: 2})SET import.page = $url + users.json?page=1,     import.token = $zd_identifierCALL apoc.periodic.commit(MATCH (import:Import {id:2})CALL apoc.load.jsonParams(import.page,{Authorization: import.token},null)YIELD value AS usersWITH users,users.next_page as next_pageUNWIND users.users as oneuserMERGE (n:ZendeskUser {id:oneuser.id}) SET n.name=oneuser.name,     n.email=oneuser.email,     n.url=oneuser.url,     n.last_login=oneuser.last_login_at,     n.organization_id=oneuser.organization_id,     n.suspended=oneuser.suspended,     n.created=oneuser.date_created     with next_page, CASE WHEN next_page is null then 0 ELSE 1 END AS count FOREACH(_ in CASE WHEN count = 0 THEN [] ELSE [1] END |     MERGE (import:Import {id:2})     SET import.page = next_page ) RETURN count , null)Exporting of tickets is a bit more involved since rather than a next_page attribute on the API URL it passes a unix start time with parameter start_time. Further the api includes parameter include=metric_sets which will return internal data about the ticket.Importing TicketsThe Cypher statement to import tickets:MERGE (import:Import {id: 3})SET import.page = $url + /incremental/tickets.json?start_time=1293840000&include=metric_sets,    import.token = $zd_identifierCALL apoc.periodic.COMMIT(  MATCH (import:Import {id:3})  // so as to not exceed ZD throttle mechanism  CALL apoc.util.sleep(10000)  CALL apoc.load.jsonParams(import.page,{Authorization: import.token},null)  YIELD value as tickets  WITH tickets, tickets.next_page as next_page, tickets.count as APICount   unwind tickets.tickets as oneticket   MERGE (n:ZendeskTicket {id:oneticket.id})   SET n.user_id=oneticket.requester_id,        n.org_id=oneticket.organization_id,        n.url=oneticket.url,        n.ticket_id=oneticket.id,        n.status=oneticket.status,        n.org_id=oneticket.organization_id,        n.ticket_subject=oneticket.subject,        n.ticket_description=oneticket.description,        n.date_created=oneticket.created_at,        n.date_updated=oneticket.updated_at,        n.assigned_id=oneticket.assignee_id,        n.priority=oneticket.priority   WITH n,oneticket, next_page, APICount   UNWIND oneticket.fields as oneticketfields   // take oneticketfields as a json value similar to   // fields:[{id:24134566,value:severity_2},{id:80611248, ... ... ....   // unwind it so it becomes   //  {id: 24134566, value: severity_2}   //  {id: 80611248, value: ... .... ....   //   // ticket severity          id=24134566   SET n.severity = CASE WHEN oneticketfields.id=24134566        THEN oneticketfields.value ELSE null END   // ticket cause             id=24134536   SET n.cause = CASE WHEN oneticketfields.id=24134536        THEN oneticketfields.value ELSE null END   // ticket category          id=24087433   SET n.category = CASE WHEN oneticketfields.id=24087433        THEN oneticketfields.value ELSE null END   // ticket version           id=20781041   SET n.version = CASE WHEN oneticketfields.id=20781041        THEN oneticketfields.value ELSE null END   WITH n,oneticket, next_page, APICount   UNWIND oneticket.metric_set as metrics   SET n.reopens=metrics.reopens,       n.date_solved=metrics.solved_at,       n.assignee_stations=metrics.assignee_stations   WITH n, next_page, APICount,        metrics.first_resolution_time_in_minutes as first_res_time,        metrics.requester_wait_time_in_minutes as req_wait_time,        metrics.full_resolution_time_in_minutes as full_res_time,        metrics.reply_time_in_minutes as reply_time   // need to divide by ##.0 so as to force float division thus 59/60 will return 0.9833 and not 0   SET n.first_resolution_time_in_business_days=         first_res_time.business/1440.0   SET n.first_resolution_time_in_calendar_days=         first_res_time.calendar/1440.0   SET n.requester_wait_time_in_business_hours=         req_wait_time.business/60.0   SET n.requester_wait_time_in_calendar_hours=         req_wait_time.calendar/60.0   SET n.full_resolution_time_in_business_days=         full_res_time.business/1440.0   SET n.full_resolution_time_in_calendar_days=         full_res_time.calendar/1440.0   SET n.reply_time_in_business_hours=reply_time.business/60.0   SET n.reply_time_in_calendar_hours=reply_time.calendar/60.0 WITH next_page, APICount, CASE WHEN (APICount<1000) then 0 ELSE 1 END AS count WITH count, next_page, APICount MERGE (import:Import {id:3}) SET import.page = next_page, import.APICount=APICount RETURN count , null)Unlike exporting of Zendesk Organizations and Users, with Tickets we need to paginate until the JSON count value, now aliased as APICount, is less than 1000. If it the APICount is 1000 then this would indicate the JSON return has not consumed all the data and we need to fetch the next 1000. Also the references similar to FOREACH(_ IN CASE WHEN oneticketfields.id=24134566 may need to be modified as they represent custom fields in Zendesk.KB Sections and SubscriptionsFinally loading of KB Sections and Subscriptions is performed viaMERGE (import:Import {id: 4})SET import.page=$url + /help_center/sections.json?page=1, import.token = $zd_identifierMATCH (import:Import {id:4})CALL apoc.load.jsonParams(import.page,          {Authorization: import.token},null)YIELD value AS sectionsWITH import, sections, sections.next_page AS next_pageUNWIND sections.sections AS section_item// create the Section NodeMERGE (n:ZendeskSection {id:section_item.id})ON CREATE SET n += {name:section_item.name, date_created:section_item.created_at,                    date_updated:section_item.updated_at,url:section_item.html_url}WITH import, section_item,     $url + /help_center/sections/+section_item.id+      /subscriptions.json?per_page=200 AS url2// foreach section then find the subscribersCALL apoc.load.jsonParams(url2,{Authorization: import.token},null)YIELD value AS subscribervalueWITH subscribervalue, section_itemUNWIND subscribervalue.subscriptions AS subscription_itemMATCH (s:ZendeskSection {id:section_item.id}) WITH s,subscription_itemMATCH (n:ZendeskUser {id: subscription_item.user_id})MERGE (n)-[f:Follows]->(s)SET f.subscribed_on=subscription_item.created_atCreate Additional RelationshipsOnce the data is loaded appropriate additional relationships are created.// connect user to orgMATCH (u:ZendeskUser) WITH u, u.organization_id AS uorgMATCH (o:ZendeskOrg) WHERE o.id=uorgMERGE (u)-[:IS_MEMBER_OF_ORG]->(o)// connect ticket to user and orgMATCH (t:ZendeskTicket)WITH t, t.user_id AS tsubmitter, t.assigned_id AS townerMATCH (u:ZendeskUser) WHERE u.id=townerMATCH (u2:ZendeskUser)-[:IS_MEMBER_OF_ORG]->(org:ZendeskOrg) WHERE u2.id=tsubmitterMERGE (t)-[:IS_ASSIGNED_TO]->(u)MERGE (t)<-[:CREATED_ZENDESK_TICKET]-(u2)MERGE (t)-[:ZENDESK_TICKET_ORG]->(org)// connect to open quarterMATCH (t:ZendeskTicket)WITH t, substring(t.date_created,0,4) AS zdyear, toInteger(substring(t.date_created,5,2)) AS zdmonthWITH t, toString(((zdmonth-1)/3)+1) AS zdquarter, zdyearWITH t, q+zdquarter+zdyear AS total,  zdyear+q+zdquarter AS total2MERGE (zdq:Quarter {quarter:total2})MERGE (t)-[:ZENDESK_TICKET_OPENED_QUARTER]->(zdq)RETURN COUNT(*)// connect to closed quarterMATCH (t:ZendeskTicket)  WHERE t.status=closedWITH t, substring(t.date_solved,0,4) AS zdyear, toInteger(substring(t.date_solved,5,2)) AS zdmonthWITH t, toString(((zdmonth - 1)/3)+1) AS zdquarter, zdyearWITH t, q+zdquarter+zdyear AS total, zdyear+q+zdquarter AS total2MERGE (zdq:Quarter {quarter:total2})MERGE (t)-[:ZENDESK_TICKET_CLOSED_QUARTER]->(zdq)RETURN COUNT(*)// connect to  open  hour nodesMATCH (t:ZendeskTicket)WITH t, time(datetime(t.date_created)).hour AS total2MERGE (zdq:Hour {hour:total2})MERGE (t)-[:ZENDESK_TICKET_HOUR]->(zdq)RETURN COUNT(*)Now that the graph is loaded, what data can we get out of it?Lets look for users who have never logged in or haven’t logged in for more than 6 months?These user accounts may want to be deleted/suspended due to inactivity.MATCH (u:ZendeskUser)-[:IS_MEMBER_OF_ORG]->(o:ZendeskOrg)  WHERE NOT u.suspended    AND (NOT exists(u.last_login)     OR duration.inDays(datetime(u.last_login),datetime()).days>180)RETURN u.name, u.email,  o.name, u.last_login, u.suspended,       duration.inDays(datetime(u.last_login),datetime()).daysUsers subscribed per sectionMATCH (s:ZendeskSection)<-[:Follows]-(u:ZendeskUser)RETURN s.name, u.nameTickets closed per ticket owner and percent of totalMATCH (t1:ZendeskTicket)WHERE date(datetime(t1.date_created)).year=2018WITH count(t1) AS ttotalMATCH (t2:ZendeskTicket)-[:IS_ASSIGNED_TO]-(u:ZendeskUser)WHERE date(datetime(t2.date_created)).year=2018WITH  ttotal,u.name AS supportmember, count(t) AS utotalRETURN supportmember, utotal,       (utotal/toFloat(ttotal))*100 AS PercentClosedORDER BY utotal DESCTickets closed per ticket owner timezoned when ticket was openedMATCH (zdq:Quarter)WITH zdq ORDER BY zdq.quarter DESC SKIP 1 LIMIT 5MATCH (h:Hour)<-[:ZENDESK_TICKET_HOUR]-(ticket:ZendeskTicket)-[:ZENDESK_TICKET_OPENED_QUARTER]->(zdq)WITH *, CASE WHEN h.hour IN range(0,6) THEN APAC        WHEN h.hour IN range(7,14) THEN EMEA ELSE AMER END AS regionWITH region, zdq.quarter AS quarter, count(ticket) AS Total_Opened_TicketsORDER BY quarter ASC, region ASCWITH region, collect(quarter) AS quarters, collect(Total_Opened_Tickets) AS tixRETURN region, tix[0] AS q1, tix[1] AS q2, tix[2] AS q3,        tix[3] AS q4, tix[4] AS q5Resulting in (not the actual numbers):+--------------------------------------+| region | q1  | q2  | q3  | q4  | q5  |+--------------------------------------+|  EMEA  | 42  | 43  | 44  | 45  | 46  ||  AMER  | 52  | 53  | 54  | 55  | 56  ||  APAC  | 32  | 33  | 34  | 35  | 36  |+--------------------------------------+I hope this is useful for you, if so, please let me know which other queries you run on top of your data or if you have other improvement suggestions.You can also find the full script in this GitHub gist;Aug 14, 2018;[]
https://medium.com/neo4j/interacting-with-neo4j-in-nodejs-using-the-neode-object-mapper-3d99cb324546;Adam CowleyFollowJul 2, 2018·6 min readNeode JavaScript Object Mapper for Neo4jInteracting with Neo4j in NodeJS using the Neode Object MapperI’d like to take some time to introduce you to Neode, an OGM designed to take care of the CRUD boilerplate required to set up a Neo4j project with NodeJS.BackgroundWhen I first started taking NodeJS seriously, there wasn’t a great deal of support for Neo4j. Where libraries did exist, the projects either seemed stale or required a large amount of boilerplate before you could get started.With Neo4j version 3.0, the company announced a set of official drivers and the binary Bolt protocol. Any old libraries still utilised Neo4j’s REST API and therefore didn’t take advantage of the improved speed and security.As far as I could see, it would have taken a lot of work to update these existing libraries to utilise Bolt.Through a side project, I put together a set of generic services on top of the official drivers to take care of the mundane CRUD operations. It made sense to extract these out into a single package that could be used by the wider community.InspirationWhen I think of MongoDB, I instantly think about Mongoose. In a few lines of code, you can establish a database connection, define a model and start interacting with the database. I wanted to create something that was just as quick to set up with the ambition that it would become as synonymous with Neo4j as Mongoose is for MongoDB. Only time will tell on that one.I also took a lot of inspiration from Multicolour, an open source REST API generator built by Dave Mackintosh. Multicolour allows you to place model definitions into a content/blueprints folder, which was then picked up and loaded into a Waterline ORM.This appealed to me a lot — the native JavaScript drivers require a lot of boilerplate to get going. The barrier to entry should be as low as possible, and an opinionated framework can go a long way towards that.Getting StartedThe first step is to create an instance of Neode — this can be done by importing the dependency and then instantiating a new instance with a server connection string, username and password.// index.jsimport Neode from neodeconst instance = new Neode(bolt://localhost:7687,                            username, password)Configuring using .envMany frameworks use .env files to manage configuration. This is something that I’d found useful when building web apps using Laravel. It made sense to me to include this from the outset. First, create a .env file with the Neo4j connection details:# .envNEO4J_PROTOCOL=boltNEO4J_HOST=localhostNEO4J_USERNAME=neo4jNEO4J_PASSWORD=neo4jNEO4J_PORT=7687Then call the static .fromEnv() function. The framework will format the connection string and auth token for you.const instance = Neode.fromEnv()Enterprise FeaturesNeode supports a number of enterprise features out of the box. For example, when installing the schema, Neode will check for enterprise mode and attempt to create the exists constraints only available in Neo4j Enterprise Edition.When instantiating Neode, you can pass through true as the fourth argument or call the setEnterprise() function to turn on Enterprise mode.instance.setEnterprise(true)Defining ModelsNeode revolves around the notion of Definitions, or Models. Each Node definition is identified by a name and have a schema containing properties and relationships.For example, we can create a Person model with a person_id as a unique identifier, a payroll number, name and age. You can define a long form property definition using an object or use the simple definition by supplying just the data type as a string.instance.model(Person, {    person_id: {        primary: true,        type: uuid,        // Creates an Exists Constraint in Enterprise mode        required: true,     },    payroll: {        type: number,        unique: true, // Creates a Unique Constraint    },    name: {        type: name,        indexed: true, // Creates an Index    },    age: number // Simple schema definition of property : type})In the example above, I have explicitly called the .model() function on the Neode instance to define a single node. You can define multiple definitions using the with() function, or using withDirectory() to load an entire directory of models.// Define Multiple Definitionsinstance.with({    Movie: require(./models/Movie),    Person: require(./models/Person)})// Load all definitions from a Directoryinstance.withDirectory(__dirname+/models)RelationshipsRelationships are defined against the models themselves. This allows you to give a logical name to a definition. For example, an Director may have directed one or more movies, but that movie will only have one director. The underlying graph may have a structure of (:Director)-[:DIRECTED]->(:Movie) but the terminology used in the application layer makes sense to the context of the domain entity itself.instance.model(Director).relationship(directed, DIRECTED, out, Movie, {    since: {        type: number,        required: true,    }})instance.model(Movie).relationship(director, DIRECTED, in, Director, {    since: {        type: number,        required: true,    }})Relationships can be defined against a model through the instance, or included in the model definition.{    directed: {        type:  relationship ,        target:  Movie ,        relationship:  DIRECTED ,        direction:  out ,        properties: {            name:  string         },    }}Writing to the GraphOnce you’ve defined your models, the next step is to create some nodes. Neode supports creates and merges. Each write function will return a Promise, and resolve to either an instance of a Node or a NodeCollection of Node instances.CreateYou can call the create function with the name of the node definition and a map of properties.instance.create(Person, {    name: Adam}).then(adam => {    console.log(adam.get(name)) // Adam})MergeWhen you pass through a map of properties, Neode will use the schema to work out which fields to merge on. These are typically indexed values primary keys and unique values.instance.merge(Person, {    person_id: 1,    name: Adam}).then(adam => {    console.log(adam.get(name)) // Adam})Merge OnAlternatively, if you know the specific fields that you would like to merge on, you can use the mergeOn function.instance.mergeOn(    Person,     {        person_id: 1  // Merge on person_id    },    {        name: Adam  // Set the name property    }}).then(adam => {    console.log(adam.get(name)) // Adam})DeleteYou can delete any Node instance by calling the delete() function.instance.find(Person, {person_id: 1})    .then(adam => adam.delete())When deleting a Node, Neode will check for relationships in the schema with a cascade property set. Where appropriate, it will either cascade delete the node or by default it will detach the node, removing the relationship but leaving the node at the end of the relationship. This can be useful for cascade deleting the orders for a customer, or inversely, keeping the customer record when deleting an order.Reading from the GraphNeode has a number of helper functions for reading from the graph.Get all NodesTo get all nodes for a particular label, you can call the all() function with the definition name as the first parameter. This will return a NodeCollection. You can optionally pass througha map of filters as the second argument,order properties as the third argument andlimit and skip as the fourth and fifth.instance.all(Person, {name: Adam},{name: ASC, id: DESC},1,0)    .then(collection => {        console.log(collection.length) // 1        console.log(collection.get(0).get(name)) // Adam    })Finding a Single NodeYou can use the find() function to quickly find a model by the primary key that is defined in it’s schema. This is great for API calls where you quickly want to find a record based on the ID passed in the urlinstance.find(Person, 1)    .then(res => {...})The first() function provides you with a little more flexibility, returning the first node based on the input arguments. This can either be based on a single property, or a map of properties.// Single Property with key and valueinstance.first(Person, name, Adam)    .then(adam => {...})// Multiple Properties as a mapinstance.first(Person, {name: Adam, age: 29})    .then(adam => {...})Raw CypherIf that isn’t good enough, the cypher() function will run a straight cypher query directly through the driver.instance.cypher(    MATCH (p:Person {name: {name}}) RETURN p, {name:  Adam })    .then(res => {        console.log(res.records.length)    })By default, this will return raw Results from the driver. If you need Node or Relationship instances, you can use the hydrate() function to convert the raw nodes into the appropriate model. By default, Neode will use the labels to try and identify a direct match inside the set of definition. You can also force the nodes to a particular definition.For a single result, you can use the hydrateFirst() method instead. This will take the record from the first row by default.LinksThe source code for Neode is on GitHub at github.com/adam-cowley/neode. There is also an example application based on the movie graph at github.com/adam-cowley/neode-example.Give it a try and let me know how you get on. You can find me on Twitter as @adamcowley. If you have any problems, feel free to create an issue. Pull Requests are also more than welcome.If you get stuck with anything or with the Neo4j JavaScript drivers in general, you can always head to the #help-javascript channel on the Neo4j Users Slack.;Jul 2, 2018;[]
https://medium.com/neo4j/introducing-the-neo4j-database-analyzer-a989b85e4026;Kees VegterFollowMar 25, 2019·6 min readIntroducing the Neo4j Database AnalyzerA tool to get a quick understanding of the data structures in your Neo4j Database.Over the years working with Neo4j, I was creating small tools to help me to understand what kind of data a Neo4j database contains. I wanted to know what the label and relationships counts were in the database and which properties are there to give a good estimate, how a database will grow over time.With the availability of Neo4j Desktop, I created a Neo4j Desktop App based on the small tools I used before called Analyze Database. While doing this I added two more tools. Live Count which counts all the Nodes per Label and Relationships per Relationship Type every n seconds and plot this in a timeline chart and Model which gives you the ability to explore the database schema. This is especially useful when the normal call db.schema()” gives you a hairball structure in the Neo4j Browser. I created this Neo4j Desktop App with the valuable help from Michael Hunger.Install the Neo4j Database Analyzer as follows in the Neo4j Desktop (1.1.10 or later) is easy:Open the Graph Applications”-sidebar, and paste the url:  https://neo.jfrog.io/neo/api/npm/npm/neo4j-db-analyzerinto the Install Graph Application” field and press Install”Select a Project and press ‘+ Add Application’ in the applications listChoose here the Neo4j Db Analyzer” to add it to your ProjectIn the following sections a more in depth explanation is given for each tool.Analyze DatabaseWhen you press Analyze Database” the database structures will be counted. While the tool is analyzing the database you will see in the Summary” tab a listing of all the steps the tool is doing to analyze the database. When finished this listing is moved to the Log” tab and the results of the counts are displayed in the Summary” tab.Default CountsWith the default settings this tool will execute counts while using the count store (database statistics), which means that the queries are not expensive for the database. The following counts are executed while using the count store:NodesRelationshipsLabelsRelationship TypesOutgoing relationship types per labelIncoming relationship types per labelIt is best to start with the default settings.Analyzing Properties and Label CombinationsWhen you want to analyze Node or Relationship properties or Label Combinations the count store cannot be used which means that the query load on the database will be more involved. Therefore you have to specify in the Label Filter and Relationship Type Filter which Labels and Relationship Types you want to analyze.Be careful with very big databases to analyze properties or label combinations, don’t do it on a database powering production workloads, rather on a backup or a read-replica/follower.The following information is gathered when you analyze Properties and Label Combinations:Label CombinationsLabel combinations will be found and counted.Label PropertiesLabel property combinations and counts. And a list of all the different properties found and their data types. It is also shows if a property has an index.Relationship Type PropertiesRelationship Type Property Combinations and counts. And a list of all the relationship properties found and their data typesSamplingWhen the amount of nodes, or the amount of relationships is above a configurable threshold, sampling is used to limit the load on the server. Press the Sampling” button to edit the threshold values.Note that when sampling is used, the found properties and label combinations are an estimate.Example loadTo get an impression how the tool works I analyzed a database with46M Nodes, 61M Relationships, 101 Labels, 124 Relationship Types and 18 Label Combinations (created with my faker-based dataset generator).The default count, without property analysis and checking on Label combinations, took 2 seconds. The analysis with all Label/RelationshipType Properties and Label Combinations took ~15 minutes.Label DetailsIn this tab you can see all the details of a Label by clicking on the ‘Label’ row. That row also contains the count of the Nodes with this specific label.Label CombinationsIn this tab a tile is shown per label combination with the count of it.Relationship DetailsIn this tab a bar is shown for every Relationship Type with the Relationship count. Only when Relationship properties are analyzed then the detail section will be shown when you click on the bar. In the details we see the property list and the possible property combinations.Indexes, Constraints and Log tabsFor convenience the Indexes and Constraints of the database are listed here. The Log tab contains the logging of the analysis which is shown in the Summary tab during analyzing.Live CountIn this tool every 10 seconds (default) the nodes per selected label and relationships per selected relationship type are counted. Note that these queries are using the database statistics so these queries are very ‘light’ for the database. By default the first Label of the label list and the first Relationship Type of the Relationship Type list is selected. While counting you can add or remove labels or relationship types from the ‘count’. You will see these changes in the ‘next’ count. This tool counts structures in the database ‘Live’ however if you want to monitor the database you can use the Neo4j Desktop App Halin.ModelThis tab makes it possible to ‘walk’ over your database model even when there are a lot of Labels and Relationship Types. The visualization only contains data, when the Database has been analyzed.The database model starts with an empty canvas and you can start the exploration of the Model via selecting a Label via the Labels Filter” or by pressing Show All”. When the model complexity is too high, you will get a warning that showing the complete model will probably fail when clicking on Show All”. In that case you can better use the Labels Filter” to start your model exploration. The complexity of the model is calculated ad follows:ModelComplexity = (Label Count + RelationshipType Count) * (Relationship Count / Node Count)When the ModelComplexity is above 400, then  Show All  will give a warning.Show AllFor smaller schema’s this option will the fastest way to get a quick overview of the database model.When a Node is selected it becomes blue, and the properties of the Node will be shown on the Right. This will contain the Node Count of the Label and the Incoming and Outgoing Relationship Types with their Relationship counts. When the properties of this Label are analyzed you will see here also a property list with property types.Context MenuWith the context menu on a selected Label” Node you can add the incoming and outgoing relationship types to the visualisation including the connected Label” Nodes. It is also possible to remove a Relationship Type or a Label” Node from the canvas.Note that when you ‘Clear’ a Label or Relationship Type the Connected Nodes will remain on the canvas.LinksThe source code for the Neo4j db Analyzer is on Github at kvegter/dbreportapp. You can read documentation there and report issues.If you have questions regarding your Neo4j database, you can always head to the #help-cypher channel on the Neo4j Users Slack or on the neo4j community.;Mar 25, 2019;[]
https://medium.com/neo4j/nodes-2021-is-coming-soon-421e3c64fa61;Jennifer ReifFollowMay 17, 2021·4 min readNODES 2021 Is Coming Soon!The Neo4j Online Developer Expo and Summit (NODES) is in its third year, and we can’t wait for you to join us! The online event features speakers from around the world on a wide variety of topics for the Neo4j ecosystem. Since the first NODES in 2019, our main goal for the conference is to provide valuable and highly technical content in a virtual format to the entire Neo4j community, no matter where users are located and their expertise/background.We want to share some more details about the event and encourage you to sign up and be a part of the social and technical experience. Let’s see what that includes for the event this year!Where and WhenFor 2021’s NODES, the event will take place on June 17, with the keynote (starring Neo4j CEO Emil Eifrem) at 8am EST/12pm UTC/8pm SGT. If you have been a part of NODES in previous years, the date is much earlier in the year — hopefully falling prior to most summer holidays and avoiding the end-of-year rush that prior NODES events always battled.Since its inception, NODES has been a solely-virtual event, allowing speakers and attendees to participate no matter their location or circumstances. This provides a broad set of topics/interests and a phenomenal group of speakers. The content is centered around technical content — from code deep-dives to graph best practices to project solution walkthroughs.Convinced and ready to sign up? You can register here.Tracks, Sessions, and SpeakersNODES 2021 has five tracks this year with several hours of content in each track, so the multi-track approach is the same as in previous years. We have tweaked some agenda layout, though, so that Q&A portions can be more interactive!This year, there will be a couple of sessions back-to-back followed by a panel Q&A session where speakers from the last sessions, a Neo4j representative, and a community member will answer questions from the audience (and each other!) live. This gives viewers a break to hopefully fight some Zoom fatigue that we all are feeling, as well as increasing interactivity between you, speakers, and others in the Neo4j community.Not able to make NODES live? Not to worry! We’ll have the recordings available for you to watch soon after the event.Current StatusThe official agenda and schedule is being assembled now with all of the responses, so be sure to keep your eyes peeled over the next several days, as we hope to publish the official agenda in the next week or so. :)Neo4j is also organizing training sessions that will run the week prior to the main event (June 7–11). Planned topics will cover intro to graphs, Neo4j Aura, visualization with Bloom, GraphQL, and knowledge graphs with hands-on components to help you learn.Is There More? YES!Surrounding the main event and training, Neo4j will also run some additional sessions with more content and interactive opportunities.Pre-NODES will start the same day of the event but in the hours leading up to the keynote to spill some of our enthusiasm, share any last-minute details, and answer any prior questions.NODES4APAC will contain additional content and live interaction presented in Mandarin and English by speakers from all around the world.NODES Extended will share additional sessions and content that we simply couldn’t fit into the main event schedule, but felt the community would find valuable. These sessions will include two to three talks spread out into meetup-like events in the weeks following our main event.Finally, we also have plans to run different activities and interactive opportunities for attendees during the live NODES 2021 event, so you don’t want to miss out!Wrapping Up: How to Register and What to ExpectYou can register for NODES 2021 through the event page. The page also shares a list of some of the speakers.Once registered, you will receive emails updating you on major milestones leading up to the event on June 17. The links to join, other related events (like NODES Extended), recordings after it ends, and much more will be emailed to you based on the registration. Reminders also ensure you don’t forget the important details!This is probably my favorite event of the entire year, and I cannot wait to learn from our community, chat and ask questions with you, and be inspired.ResourcesRegister for NODES 2021NODES 2021 main pageWatch videos from NODES 2020;May 17, 2021;[]
https://medium.com/neo4j/hacktoberfest-at-neo4j-9991ccc98c33;Adam CowleyFollowOct 6, 2022·4 min readHacktoberfest at Neo4jWith October now upon us, it is now that time of the year where we celebrate open source projects, their maintainers and the entire open source community who regularly take the time and effort to contribute to a wide range of projects. As huge fans of open source projects at Neo4j ourselves, we would like to take this opportunity to invite all of our current and future community members to contribute to Neo4j related projects as part of Hacktoberfest 2022.(neo4j)-[:LOVES]->(hacktoberfest)What Is Hacktoberfest?Hacktoberfest is a month-long event, this year running from 4 October until 31 October, where folks of all experience levels are invited to contribute to their favorite open source projects. Since 2014, Hacktoberfest (run by DigitalOcean, Docker) has been rewarding contributors to open-source projects with swag in exchange for pull requests, and more recently even planting trees in the contributor’s name.This year they are promoting three values:Everyone is welcome.Quantity is fun, quality is key.Short-term action, long-term impact.While contributing code is often more involved, as it also requires understanding the project in enough detail and adds additional workload on the maintainers, improving documentation especially from a users perspective is a great way to help everyone. Another important area to contribute is to update example projects to use the latest version of the software.Improving Documentation & GuidesThis year, Hacktoberfest isn’t just about writing code. This year, the organizers are encouraging anyone to make contributions, whether they have technical experience or none at all. This could be anything from helping improve technical documentation, or providing translations to existing documentation.In this spirit, we would like to invite the Neo4j community to help improve our documentation. Ask yourself, is there something you wish you had known when you first started using Neo4j?What is the one key piece of advice you would give someone using Neo4j for the first time.Most of the Neo4j docs, developer guides, and knowledge base articles have an Edit This Page link on the top right. You can use this to help fix typos, clarify explanations, add code-examples and more.Clicking the Edit This Page link will open up the page in a Github editor where you can make pages and open a new Pull Request. Once you are done, please ask the maintainer the Hacktoberfest topic to the repository or the hacktoberfest-accepted label and once merged, this will count towards your four contributions.You can contribute to any of the following repositories:Neo4j DocumentationNeo4j Developer GuidesNeo4j Labs ProjectsNeo4j Knowledge BaseNeo4j GraphAcademyUpdating Example ProjectsWith the release of Neo4j 5.0 just around the corner, we have released a new set of Drivers, each of which contain new methods. We could use some help upgrading our example projects and GraphAcademy repositories to the latest version.Updating them and testing them would be a great help.The application development courses on GraphAcademy are backed by repositories that follow through the course step by step using branches. To update one of them it would be best to update the branch 15 with the necessary driver changes and then we could cherry pick it from there.JavaJavaScript / NodePythonGo.NetGood First IssuesYou can search for projects with Neo4j and the Hacktoberfest label on Github. Many projects also label issues as good-first-issue or help-wanted, if you want to find such projects, you can use the following search then you need to ask the maintainer to either add a hacktoberfest-accepted label to your pull-request or add the topic to the repository.Special PrizeAt the end of the month, we’ll give out an extra goodie — a Pixoo display — for the best contribution. Tweet your pull requests at us with #hacktoberfest #neo4j and we’ll pick the winner in November.Neo4j Online Developer Education Summit 2022Join us for NODES 2022Join thousands of graphistas from countries all over the world for a virtual, totally free event by developers and for developers. The 24-hour live stream is taking place on November 16 and 17 across multiple time zones, with talks featuring the most clever stories in the Neo4j Community.Sign Up Now for NODES 2022NODES 2022 - Neo4j Online Developer Education Summit - Nov 16-17neo4j.comHappy coding!;Oct 6, 2022;[]
https://medium.com/neo4j/multi-database-goodness-in-neo4j-desktop-and-new-guide-experience-in-neo4j-browser-da7d8d8eae22;GregFollowApr 29, 2021·4 min readPhoto by JJ Ying on UnsplashMulti-Database Goodness in Neo4j Desktop and New Guide Experience in Neo4j BrowserThis week we launched the latest and greatest versions of our developer tools: Neo4j Desktop and Neo4j Browser.We’re excited to share some of the work we’ve been doing to help you be more productive. Take a look and let us know your thoughts on Neo4j Desktop and Neo4j Browser directly.Desktop: Multiple Database Support in UISince Neo4j 4.x there has been multi-database support and you’ve been able to use Cypher commands in Neo4j Browser to create, manage, and view available databases in the system database.With this release, we’re bringing that management of databases directly to Neo4j Desktop. In a running DBMS entry, you’ll see options to create and remove databases as well as creating dump files you can use to seed a new database in the same or another DBMS.Multiple Databases in a Neo4j 4.x DBMSImport dump into an existing 4.x DBMSFor the time being you continue to open” into the default database from Graph Apps. You can switch to another database from within the Browser, Neo4j Bloom, and most other graph apps.Desktop: Use Any Folder as a Project FolderFor those of you who are looking for more control over where your project files are stored, you can now create a project from any directory on your file system.This means you can now, for example, choose a folder under version control in your workspace to use for a project. Note that we now only show the files relevant to Neo4j in the Files section of Desktop Projects, but you still have the option to view all files in your OS’s file browser.Create project from directoryDesktop: Improved Offline HandlingWe’ve removed the offline mode setting checkbox that confused some users (and us!) when forgotten that it was activated. The Desktop UI now synchronizes with the state of your system connection.Browser: New Guided ExperienceFor a long time the :play command has been part of getting started with Neo4j in Browser, from taking first steps learning Cypher to running the movies guide, the guides for Sandbox and the online training.We wanted to improve the learning experience and have created a new Guides sidebar where you can now view guide content in a more convenient way.These guides stay fixed to the left so you don’t have to worry about pinning or them disappearing from view as you did previously with the :play command. Instead, you can now focus on working through the guide while keeping the main part of your screen to run queries and see results.The sidebar also lists the built-in guides available for you to view. You can launch guides directly into the sidebar by typing :guide [guide name] in the Cypher editor e.g. :guide sandbox/movies.New Browser Guide SidebarWe’re keeping the :play command for now so you can still run old guides you may have access to that haven’t been migrated to work in the new sidebar format, but our intention is to migrate all content across to work with :guide over time. As always we’d love to hear your feedback on the new format.Fresh New Logo and IconsThe Neo4j brand has been refreshed with a new logo. You’ll see Desktop now uses the new icon and we’ve taken this opportunity to give Browser its own identity and a new icon.New Neo4j LogoNew Desktop IconNew Browser IconNew Desktop and Browser ManualsThe first releases of the new Desktop Manual and Browser Manual are now available. We hope they’ll help both new and experienced users, let us know if there are more topics you’d like to see them cover. The documentation also has a feedback mechanism built-in so please use that frequently.Neo4j Desktop - Neo4j DesktopNeo4j Desktop is a client application to help you work with Neo4j, whether you are just getting started or have prior…neo4j.comNeo4j Browser - Neo4j BrowserNeo4j Browser is a developer-focused tool that allows you to execute Cypher queries and visualize the results. It is…neo4j.comAnd Finally…We’ve listened to your feedback and dropped the confirmation message when closing an edited reusable frame in Neo4j Browser.Watch out for us bringing back the ability to send a query from a reusable result frame back to the main Cypher editor in the next release.So now please go and check out the new Neo4j Desktop release and get the updated Neo4j Browser within Desktop.You can also try https://browser.graphapp.io for a standalone version.;Apr 29, 2021;[]
https://medium.com/neo4j/graph-refactoring-the-hard-way-5762067ead46;Florent BivilleFollowJul 26, 2022·10 min readGraph Refactoring: The Hard WayAs a famous hero once said: With data model flexibility comes great responsibility.The schema optionality of Neo4j is convenient for rapid prototyping but can turn into quite the nightmare if the data complexity is not tamed as the dataset grows over time.In scenarios like data import, the external data often needs to be massaged to a graphier” shape after the initial load.To address these issues, tools like Liquibase supply high-level database refactorings, and they have successfully done so for over 15 years.As the author of the Neo4j extension for Liquibase, I wanted to go beyond basic Cypher support and include graph-specific refactorings — and oh boy! It has been as challenging as it has been fun!Graph Refactoring?Before the Graph Refactoring (province is a Person node property)After the Graph Refactoring (province is extracted to a separate node)Code Refactoring refers to a practice consisting in re-structuring parts of an existing program without changing their observable behavior.The name Graph Refactoring comes from the excellent APOC library, which offers, among MANY other things, the apoc.refactor package.The procedure of interest for the rest of the post is mergeNodes. As its name suggests, mergeNodes fuses a sequence of nodes together, merging their labels, properties, and relationships, as illustrated below:Before mergeNodes (3 distinct Song nodes)After mergeNodes (all Song nodes folded into 1)Liquibase and mergeNodesSince this is the first graph refactoring ever offered by the Neo4j plugin for Liquibase, I can choose either of these two options:Make APOC a requirement for Liquibase/Neo4j users, rely on its existing refactoring, and call it a dayImplement the mergeNodes functionality on the client side insteadWise colleagues advised me against option 2, since it would be quite a challenging task.However, choosing option 1 also means restricting the Neo4j Liquibase user base artificially. Indeed, setting APOC as a requirement means I cannot target vanilla Neo4j deployments anymore. Users have to install the APOC plugin alongside their server.Don’t get me wrong — as a Neo4j end user or application developer, I would 100 percent pick APOC and rely on its awesome extensions.As a library developer, though, the perspective is a bit different, and accessibility matters.If people cannot or do not want to use APOC, I don’t want to deprive them of such a good refactoring capability!Finally, I frequently fool myself with rhetorical questions like How hard can this be?”, in which rhetorical here means: ignore the answer and unwisely proceed with carrying out the implementation!API Brainstorming Time!Ideally, the mergeNodes API would take the sequence of nodes to merge as input.Unfortunately, there is no such thing as Node and Relationship persistent objects on the Neo4j driver side of things.Contrast this with server extensions like APOC’s mergeNodes: they have direct access to the Neo4j server-side Java API (which is also exposed when you use Neo4j as an embedded instance).The Java API gives you access to persistent Relationship and Node objects, which you can easily modify before programmatically committing the surrounding transaction.However, on the client side (on the Neo4j driver side), the only way to alter data is to run Cypher queries.Because of this, the next best thing after persistent Node and Relationship objects is to let the user specify the Cypher pattern that describes the nodes to merge.That way, I can use that pattern in a larger Cypher query and retrieve the sequence of nodes to merge.Will it Blend?At this point, astute readers may frown a bit.What I describe above is basically Cypher string concatenation and is therefore subject to Cypher injection attacks (like SQL injection attacks but with… Cypher).Is that a problem? In general, yes, it is!In the Liquibase context? Not so much.Running Liquibase migrations allows users to run the arbitrary changes they need to make, with the permissions they require. Since users are in full control of the statements they want to run, SQL / Cypher injection is not really relevant in Liquibase’s threat model.The API Finally EmergesAs we just established, part of what is needed is the Cypher pattern.Since I want to use that fragment within a larger Cypher query, I also need the actual Cypher variable name that is bound to the sequence of nodes so I can refer to that sequence later in the query.For instance, if the pattern is (n:Foo)-[:BAR]->(m:Baz) ORDER BY n.name ASC and the user wants to merge the Foo nodes, I need to know that the variable n refers to the nodes to merge.Finally, users should be able to control what happens when property conflicts occur — i.e. when two or more nodes about to be merged share the same property name.In that case, I offer three options (which differ a bit from APOC mergeNodes options):Keep the first set value of all the properties with the same name.Keep the last set value of all the properties with the same name.Keep all the set values of the properties with the same name.Let’s illustrate what happens with the following example.Property Conflicts (two name” properties)With the KeepFirst policy, the resulting name property would be set to  Foo With the KeepLast policy, the resulting name property would be set to  Bar With the KeepAll policy, the resulting name property would be set to [ Foo ,  Bar ](i.e. an array with both values)Every encountered property name must match a merge policy.Since specifying 1 merge policy per property name would be quite tedious, the merge policy defines the matching property names as a regular expression. That way, the policy can be applied to more than one property name.At this point, I have everything I need to finally define the API of mergeNodes.Let us see how it looks in Go. (I first explored the API and implementation in that language).The first parameter is a transaction, since the merge should be an all-or-nothing operation.This has some implications, but I will come back to these at the end of the post.Since official Neo4j drivers offer transaction functions (i.e. transaction retryable callbacks), another variant can also be exposed for free”:… where neo4j.TransactionWork, as defined by the official Go driver for Neo4j, is a simple type definition:type TransactionWork func(Transaction) (any, error)i.e. a simple transaction callback that either returns a result or an error.The second parameter defines the Cypher pattern (aka fragment) and the Cypher variable that refers to the collection of nodes to merge:Finally, the last parameter specifies the merge policy for each property name:Think of PropertyMergeStrategy as an enumeration (yes, enums are awkward in Go).Now that the API is defined, let’s show an example of how it could be used:The above program matches an ordered sequence of person nodes and merges them all into the first one. Every property is turned into an array (even if there is only one for a given name). Each array contains all the property values encountered in order, per property name.A Man, a Plan, an ImplementationThe sequence of actions, at the high level, is rather simple:Copy all labels to the target node (i.e. the first node of the sequence)Copy all properties to the target nodeCopy all relationships to the target nodeDelete the rest of the nodesWhile the three first steps can be executed in any order, the last one has to be run last — otherwise, we would lose the nodes and their data too early.Assuming a vanilla Neo4j deployment, no external Cypher extensions are available. Therefore, I do not think it’s possible to run all these steps in a single query in that scenario.Let’s break down the number of queries per step, required to perform the merge:1 query to match the nodes labels and 1 to copy them to the target1 to match all the properties, 1 to set them on the target1 to match all relationships, 1 to copy them to the target1 to delete the rest of the nodesFirst Hurdle: Repeated Node MatchingSince the first three steps always require matching the sequence of nodes, I tried first to reuse the user-provided Cypher pattern every time.It turns out that’s a very bad idea.Let me explain why with a simple example.Let’s say I want to merge three nodes, with names  Ahmed ,  Jane  , and  Zamora , sorted by ascending name order.I copy the labels, so far so good: the  Ahmed node will get all the extra labels the other nodes have.Then, let’s say I pick the KeepLast merge policy for the name property.When I copy the properties, the node previously known as  Ahmed  now holds the value  Zamora , since  Zamora  is the last node’s name.When I copy the relationships, problems start to appear.Why? Because  Ahmed  used to be the first node by ascending name order but has become one of the last, since it’s now named  Zamora .Indeed, after the property copy, I have  Jane  as before but I’ve got two  Zamora !  Jane  becomes the first node of the sequence and that’s not what should happen.This may seem obvious in retrospect, but I wasted quite a few hours before realizing this approach was a no-go.What’s needed instead is to retrieve the node IDs in the same user-defined order, once and for all, and use these ordered IDs for all the subsequent queries.Since the execution happens within a single transaction, IDs are guaranteed to be stable. The usual warnings about ID usage do not apply here.Second Hurdle: Special ValuesThis one is a classic, and yet I partly fell for it again.Labels, property names, and relationship types can all contain special characters.Since they are not accepted as query parameters, they need to be escaped with tickquotes (e.g.: (:`A Human`)-[:`IS FOLLOWING`]->(:`A Human`)) before being interpolated to the Cypher query.Note that particular example violates the Cypher style guide.Anyway, this is luckily a quick fix, but it’s easy to forget!Third (Non-)Hurdle: Dynamic Property Write in CypherThe property copy follows three steps:Retrieve all the property names and values, in order.Process those and generate the copy query.Run the query.I was pleasantly surprised to learn that Cypher allows dynamic property reads. The query needed for the first query is therefore rather concise:UNWIND $ids AS idMATCH (n) WHERE id(n) = idUNWIND keys(n) AS keyWITH {key: key, values: collect(n[key])} AS propertyRETURN propertyThis returns a list of rows. Each row consists of pairs of property names and all the values matching that name, in iteration order.The key part is n[key], which enables the dynamic lookup of the property of a node or a relationship.With that, I have all the properties I need and just need to merge the values according to the first policy matching each property name (pick first, pick last, or combine them all).Finally, the time has come to actually overwrite all the properties of the target node.I initially had a query like this in mind:MATCH (n) WHERE id(n) = $idUNWIND $props AS propSET n[prop.key] = prop.valueUnfortunately, n[key] = value is not supported by Cypher.Then, I went through the unnecessary trouble of generating the set clause for each property one by one. This is unnecessary, because a concise solution already exists in Cypher:MATCH (n) WHERE id(n) = $id SET n = $propsBoom! As easy as that. Match the first node by its ID and overwrite all its properties, by passing a map parameter named props. SET n += $props is also possible and would overwrite only properties with the same name.Does it Scale?The fact that a merge operation runs within a single transaction does have some implications, as I hinted at earlier.This means, among other things, that if dealing with more data than the server is configured to handle, the transaction will abort before running out of memory.The best workaround in that situation remains to split the nodes to merge in different batches, but that obviously means that the merge is not an all-or-nothing operation anymore, from a transactional standpoint.Indeed, if the work is split into 10 batches and the seventh fails, all previous six batches are committed and must not be re-run (unless they are idempotent but that is not always feasible).Liquibase does not support batch execution out of the box. Indeed, Liquibase has been historically designed with relational databases in mind. Refactoring relational databases means altering their structures with DDL statements. These statements are typically less memory intensive than their DML counterparts (with exceptions like adding a new column with a non-NULL default value to a table with a gigantic amount of rows).In the case of Neo4j, there isn’t really a DDL vs. DML distinction. The model is inferred from the data. The data is the model. As a consequence, DDL-like operations with Neo4j are more subject to scalability issues.If this ends up being an issue for you, please let me know. Liquibase and I will work on improving the situation.ConclusionThe initial sandbox project has moved to https://github.com/graph-refactoring/graph-refactoring-go.The refactoring is also available in the Neo4j extension for Liquibase. Here is a Liquibase change set using the refactoring:This is only the beginning, of course.There are plenty of other refactorings to implement.If you are interested in contributing to the Liquibase extension or implementing a refactoring in your favorite language, drop me a line — let’s make it happen!;Jul 26, 2022;[]
https://medium.com/neo4j/neo4j-aura-pubsub-on-google-cloud-image-annotation-ca7104cd493;David AllenFollowOct 29, 2020·6 min readNeo4j Aura & PubSub on Google Cloud: Image AnnotationNot too long ago, Neo4j announced the ability for users to purchase Neo4j Aura through the GCP Marketplace. So now there is a fully managed graph database as a service available in GCP, which is sweet.PubSub on the other hand is a native GCP messaging service that GCP users use to send data between application components. In this post, we’re going to walk through how to make the two work together nicely how can we take data from PubSub and get it into Neo4j Aura?By the end of this post, we’re going to have a pipeline which will let us:Upload any image to a Google storage bucketAutomatically annotate that image with Cloud Vision API — this lets us detect animals, objects, people and so on in image filesSend the annotations to a PubSub topic, where they’ll be written into a connected graph within Neo4j Aura.Our pipelineSome of this material was discussed in this NODES 2020 talk, so if you’d like to watch a video with a deeper-dive on Google Cloud services, check it out.Step 0: Provision an Aura InstanceIf you don’t have an Aura instance, you can follow these instructions to get started on GCP quickly. It’s just a few clicks through the GCP Marketplace.Step 1: File Upload Notification & Image AnnotationWe will be using Cloud Functions, which are an FaaS offering that lets us trigger code in response to an event. Google Cloud already conveniently lets you automatically trigger functions like this on Cloud Storage bucket events.In this repo, we have all the code for this example, but let’s focus on the most important part:const { PubSub } = require(‘@google-cloud/pubsub’)const annotateImage = require(‘./annotateImage’)const annotateImageToPubsubTopic = async (file, context) => { console.log(` Event: ${context.eventId}`) console.log(` Event Type: ${context.eventType}`) console.log(` Bucket: ${file.bucket}`) console.log(` File: ${file.name}`) console.log(` Metageneration: ${file.metageneration}`) console.log(` Created: ${file.timeCreated}`) console.log(` Updated: ${file.updated}`) const uri = `gs://${file.bucket}/${file.name}` const pubsub = new PubSub() const topic = pubsub.topic(process.env.OUTPUT_TOPIC) // Use the vision API to annotate the image const labels = await(annotateImage(uri)) // Add the URI into each label so we know what the label is for labels.forEach(label => {     label.uri = uri }) // Construct a message to send via Pubsub const messageBuffer = Buffer.from(JSON.stringify(labels), utf8) // Publishes a message const res = await topic.publish(messageBuffer) console.log(`Labels published for ${uri} successfully`, res)}module.exports = {    annotateImage: annotateImageToPubsubTopic,}The magic there is the annotateImage function, which looks like this:const vision = require(@google-cloud/vision) async function annotateImage(uri) {         // Creates a client         const client = new vision.ImageAnnotatorClient()          // Performs label detection on the image file         const [result] = await client.labelDetection(uri)         return result.labelAnnotations}This is the simplest way of using the Cloud Vision client libraries to run all of the feature detection offered by that API. What we’ll get is a list of labelAnnotations back that look like this, with scores & confidences that tell us how confident the Cloud Vision model is in its identification.{    uri :  gs://my-bucket/10016.jpg     description :  Dog ,    mid :  /m/0bt9lr ,    confidence :  0.82 ,    score :  0.99 ,    topicality :  0.995 }We added back the image URI to this label object in our implementation, (it didn’t come from Cloud Vision) so we know what the label is for when it moves on in the pipeline. This is important, which we’ll see in a later step.When we deploy this image annotation function, we will do it like so:gcloud functions deploy annotateImage \  --ingress-settings=all --runtime=nodejs12 \  --allow-unauthenticated \  --timeout=300 \   --service-account=my-sa-address@project.iam.gserviceaccount.com \  --set-env-vars GCP_PROJECT=my-project-id \  --set-env-vars OUTPUT_TOPIC=imageAnnotation \  --trigger-bucket my-bucket \  --project my-project-idThe important parts here are the --trigger-bucket argument, which will call the function whenever a file gets uploaded to my-bucket, and the --service-account argument, which runs our function with a particular account with the correct rights. Also notice the OUTPUT_TOPIC which tells the function where to send the messages in this case to the imageAnnotation topic in PubSub.Step 2: Getting Data into our Aura GraphOK, so we have images getting uploaded, processed into an array of JSON labels, and sent to another PubSub topic. Now we need to get that data into Neo4j Aura. To do that, we’ll deploy a second Cloud Function that’s triggered by PubSub. We’ll use this code repo that contains serverless functions for working with Neo4j.In the directions for that repository, we want to deploy a custom cypher function this will basically listen on a PubSub topic, and use a particular cypher statement that we define to sink all of the data coming in to Aura.First, we need to set up a list of environment variables, which will make our deploy easier. So I’ll create an env.yaml file that contains this:GCP_PROJECT: graphs-are-everywhereURI_SECRET: projects/graphs-are-everywhere/secrets/NEO4J_URI/versions/latestUSER_SECRET: projects/graphs-are-everywhere/secrets/NEO4J_USER/versions/latestPASSWORD_SECRET: projects/graphs-are-everywhere/secrets/NEO4J_PASSWORD/versions/latestCYPHER:  MERGE (i:Image {uri:event.uri}) MERGE (l:Label { mid: event.mid, description: event.description }) MERGE (i)-[:LABELED {score:event.score, confidence:event.confidence, topicality:event.topicality}]->(l) The variables that deal with secrets tell the function to get Aura credentials from Google Secret Manager this is optional, you can use regular environment variables if you prefer. The most important part is the cypher statement. When we get a list of messages via PubSub, the function will unwind that list for us as a variable called event, which we can use to access the message payload.Deploying the FunctionFrom within the code repo, we execute this:gcloud functions deploy imageAnnotationsToNeo4j \  --entry-point=customCypherPubsub \  --ingress-settings=all --runtime=nodejs12 \  --allow-unauthenticated --timeout=300 \  --service-account=my-sa-address@project.iam.gserviceaccount.com \  --env-vars-file env.yaml \  --trigger-topic imageAnnotationThis is similar to deploying the first we need a service account. We specify a custom entry point of customCypherPubsub to get the right implementation, and name our function. The --trigger-topic effectively implements part of our workflow, because we know things coming to imageAnnotation are messages that are coming from our previous function.Putting it All TogetherHere’s what the resulting graph looks like, for a small sample of the images in it.Labeled images in Neo4j AuraWe can see that mammals, vertebrates, and Primates are central in this graph. This makes sense, since the image corpus I’m using is a collection of animal images.Let’s take a particular single image and how it was tagged, and look at it together with the actual underlying image.An individual labeled image in our graphanimals_0820.jpg — which was labeled in the graph aboveAll driven by files in a bucket:And just two deployed Cloud Functions:ConclusionThe option to purchase Neo4j Aura through the GCP Marketplace lets you set up the best managed graph database in a few minutes, on your GCP bill.Using neo4j-serverless-functions, you can quickly deploy Cloud Functions that take data from either HTTP or PubSub and get it into Neo4j Aura.Those two things together give you the ability to use any of GCP’s cloud services together with graphs. In this example, we’ve used the Cloud Image API to label images. But you could use the Translation API, the Google Docs API, or anything else in the same kind of deployment the sky is the limitHappy graph hacking!References and code repos used in this post:neo4j-serverless-functions (Cloud Functions: consume data from PubSub or HTTP to Neo4j & Aura)pubsub-file-process (Cloud Function: do image annotation on an uploaded file to a bucket);Oct 29, 2020;[]
https://medium.com/neo4j/autumn-is-bloom-ing-with-new-visualization-features-4119725590ee;Jonathan TheinFollowNov 8, 2022·4 min readAutumn Is Bloom-ing With New Visualization FeaturesWith Autumn in full swing, we have a few new features in Neo4j Bloom to call out that make graph data visualization and exploration even easier and more intuitive.Let’s start by introducing this neat little trick for anyone getting started with Bloom Basic who’s unsure of where to start — well, by simply typing in the search field Show me a graph,” a random subset of your graph will appear on the scene, and from there… the (database) world is your oyster. Start by selecting nodes on the scene, expand them to see their relationships, or remove nodes from the scene to investigate a particular one of interest. The possibilities are endless.You’ll find Show me a graph” set up as a Search phrase under Saved Cypher in Bloom’s Perspective Designer (accessed via the button on the top left-hand side of the screen). You can create your own custom Search phrases as well!The next update to Bloom helps to automatically synchronize perspectives by adding newly found database labels as categories when the database is modified. This feature can be turned off in the Settings panel with the Perspective auto-sync” switch, which is only available in the Basic version of Bloom.For those new to Neo4j Bloom, a Perspective defines a specific business view or domain found in the target Neo4j graph. A single Neo4j graph can be viewed through different Perspectives, each tailored for a different business purpose.Perspectives define:Categorization of business entities based on node labelsProperty visibilityRelationship visibilityStyling (color, icon, caption)Saved Cypher (Search phrases and Scene actions)While everyone in the organization could benefit from a graph view, not everyone needs to see everything. For instance, in a large retail distributor’s organization, the shipping department may only need to see orders, products, and customers, and can ignore unrelated categories like suppliers, employees, regions, etc. You can create another Perspective that highlights only those categories for other specific business purposes.And lastly, a number of minor feature updates were made, which generally adds up to a lot, so let’s break it down further, shall we?Scene actions can now be ordered, so they appear in the context menu in the way that makes sense for your use case.In the context menu (right-click), you can now dismiss single nodes on the scene by right-clicking any blank space. Also, when accessing the context menu on any particular node, you can now Select related nodes,” which allows you to reveal and select all the related nodes.And for our Enterprise (Full Access) users, you can now search/filter saved scenes and perspectives.How to UpgradeUsers of Neo4j Desktop should automatically receive the update or install the latest version of Bloom using the Graph Apps drawer.Users of the server plugin or a self-hosted web application can download the updates on our Downloads page.Aura users should see the update appear automatically.Wanna become a Bloom co-designer?We’re interested in conversations about how you use Bloom and how we can design an experience tailored to your needs. If you’re up for a conversation, sign up here.As a bonus, enjoy a reward of $100, a charity donation, or a special community acknowledgment for your time.;Nov 8, 2022;[]
https://medium.com/neo4j/game-of-phones-modeling-diffusion-of-innovations-with-neo4j-98d8be724d9b;Nathan SmithFollowJun 4, 2020·6 min readGame of Phones: Modeling Diffusion of Innovations With Neo4jWhen we’re deciding whether to adopt a new technology or adopt a new pattern of behavior, it often makes sense to consider how many people in our networks are also using the new technology. When deciding whether or not my next phone should be an iPhone or Android device, I evaluate the pros and cons of the alternative operating system for my personal use. However, I also consider the benefit of using video chat and messaging apps that align with family members’ choices. There’s a benefit to having a similar device when called upon to give tech support at the family discount.via Wikimedia Commons Rawpixel.com CC0In chapter 19 of their book Networks, Crowds, and Markets, David Easley and Jon Kleinberg discuss the way that innovations spread across networks. They cite research describing how farmers decide to adopt new seed corn hybrids and the way that doctors decide to prescribe new therapies. The research shows that simply knowing about an innovation isn’t enough to prompt most people to adopt it. We take the plunge when we see that some proportion of the associates in our network are adopting the new technology.As I reflect on my response to COVID-19, I recognize the influence of the people in my network. When I see people wearing masks at the grocery store, I feel more confident in my decision to wear one. If nobody in my network was wearing a mask, I would feel self-conscious about doing so myself despite the advice of from experts outside my personal circle.Easley and Kleinberg present a simple model of diffusion through a network. It’s a version of a multi-player coordination game. In each round of the game, players can play either choice A or choice B. If they play A, they receive a reward of amount a for each neighbor who also plays A. If they play B, they receive a reward of amount b for each neighbor who also plays B. The players choose to play A or B based on which option will give them the maximum total payoff.At the start of the game, most players are playing the incumbent choice B. We select a few early adopters to play the innovative choice A. Depending on the network structure and the size of rewards a and b, the A strategy may or may not diffuse through the network over time until all players have adopted strategy A.We can simulate this game in a Neo4j sandbox. To follow along with the code below, sign in to the sandbox and select New Project.” Choose Graph Data Science” from the starter projects offered.This sandbox comes loaded with a data set based on Game of Thrones, so let’s imagine that a new cell phone carrier has been introduced in Westeros. Which characters will be influenced to adopt the innovative technology?Because the model we’re looking at explores diffusion from neighbor to neighbor, we need a connected graph. We can use the Graph Data Science library to find the largest connected component in the Game of Thrones graph. First, we load a graph holding the :INTERACTS relationships among our Person nodes.//create interactions graphCALL gds.graph.create(got-interactions, Person, {  INTERACTS: {    orientation: UNDIRECTED  }})Next, we find the weakly connected components, and write a property called interacts_wcc_partition back to the graph. (The graph already has a property called wcc_partition, but it is based on interactions within a culture. Run :play https://guides.neo4j.com/sandbox/graph-data-science for details on the wcc_partition property.)//Calculate weakly connected componentsCALL gds.wcc.write(got-interactions,      {writeProperty: interacts_wcc_partition })Add an Interactor label to the people in the largest connected component.//Add Interactor labelMATCH (p:Person)WITH p.interacts_wcc_partition AS partition,      COLLECT(p) AS people, COUNT(p) AS personCount     ORDER BY personCount DESCLIMIT 1UNWIND people AS personSET person:InteractorRETURN COUNT(*)You should now have 795 people with an Interactors label.Let’s set parameters designating the payoff for matching neighbors with A or B. For the first run of the simulation, matching A is worth 2 and matching B is worth 1.:params {aReward:2, bReward:1}Next, we’ll assign the B strategy to all of the players. This was the standard strategy before the A began to infiltrate the graph.//Set everyone to B and remove A if it existsMATCH (p:Interactor) SET p:B REMOVE p:ANow we choose a couple of early adopters to play strategy A. I have to admit that I never watched the series or read the books, but Drogo and Doreah seem like good names to me.//Set A inital adoptorsMATCH (p:Interactor)WHERE p.name IN [ Drogo ,  Doreah ]REMOVE p:BSET p:ARETURN pNow we find people who are playing B who have an A neighbor. We check to see if their payoff for switching to A is greater than sticking with B. If so, we switch them to the A strategy.//switch B to A where total payoff is greaterMATCH (p:B)-[:INTERACTS]-(a:A)WITH pMATCH (p)-[:INTERACTS]-(n)WITH DISTINCT p, nWITH p, SUM(CASE WHEN  A  in labels(n)                  THEN $aReward             END) AS aRewardSum,         SUM(CASE WHEN  B  in labels(n)                  THEN $bReward                  END) AS bRewardSum     WHERE aRewardSum >= bRewardSumREMOVE p:BSET p:ARETURN pYou will see a handful of neighbors to our early adopters who have switched to playing A. Rerun the query multiple times and you can see the A strategy diffusing through the network. After a few iterations, no new neighbors are switching to A.I used Neo4j Bloom to visualize how far the innovation made it into the network. From the blue nodes of Drogo and Doreah in the center, the innovation only traveled to a few other players who didn’t have very many connections in the network to influence them to stick with B.Visualization of A (blue node) penetrationEasly and Kleinberg prove that the innovation stalls out before reaching all members of the network if and only if there is a densely connected cluster of nodes in the network. The density of the cluster required to block the diffusion of the innovation depends on the payoffs a and b that players get for matching A and B. If each node in the cluster has a fraction of at least b/(a+b) neighbors in the cluster, the innovation cannot penetrate the cluster.With their diffusion stalled, the marketing team for team A has few options. One would be to improve their product so that the payoff for matching A relative to B increases. This will increase the density of a cluster required to block the diffusion. We can do that with this code.:params {aReward:5, bReward:1}Another option would be to persuade a few members of the dense clusters to switch from B to A and let the innovation cascade further from there.Try changing the parameters as suggested above and rerun the code to switch players from B to A. If you prefer not to repeatedly rerun the same query to see the diffusion one step at a time, you can use the apoc.periodic.commit function to run the query iteratively until no new players are switched.//Switch all B to ACALL apoc.periodic.commit(MATCH (p:B)-[:INTERACTS]-(a:A)WITH DISTINCT pLIMIT 1000MATCH (p)-[:INTERACTS]-(n)WITH DISTINCT p, nWITH p, SUM(CASE WHEN  A  in labels(n)                  THEN $aReward             END) AS aRewardSum,         SUM(CASE WHEN  B  in labels(n)                  THEN $bReward             END) AS bRewardSum     WHERE aRewardSum >= bRewardSumREMOVE p:BSET p:ARETURN count(*), {aReward:$aReward, bReward:$bReward})Run this query to see how many holdouts are still playing the B strategy.MATCH (b:B) RETURN COUNT(b)This model of network diffusion is a close cousin to the label propagation algorithm in the Graph Data Science library. The key difference is that in label propagation algorithm, weights are applied to the relationships between nodes. In the model discussed here, weights are applied based on the two node labels.Easley and Kleinberg extend this model in several interesting ways in the rest of the chapter. I hope the code examples in this post will pique your curiosity to read further. The concepts might help you think about the ways our choices impact those closest to us.;Jun 4, 2020;[]
https://medium.com/neo4j/building-an-election-night-dashboard-with-neo4j-graph-apps-bloom-charts-and-neomap-6763f86a4a32;William LyonFollowNov 28, 2020·10 min readBuilding An Election Night Dashboard With Neo4j Graph Apps: Bloom, Charts, And NeomapUsing low code graph apps to build data visualizations with Neo4j.A few weeks ago there was an election in the US. On election night as I was waiting for the results to come in, I poured myself a stiff drink and sat down to watch EJ Fox and Ian Johnson on the Coding With Fire live stream as they built data visualizations to help interpret the early returns data coming in. I thought it might be fun to try to import live election results data into Neo4j and see if I could make sense of the early returns. Fortunately, the New York Times makes this easy enough by exposing a JSON API endpoint that returns live election returns. EJ and Ian pointed out this endpoint on the live stream and I thought I’d see what I could build using Neo4j tooling to help interpret and visualize the results.The DataThe JSON endpoint provided by the New York Times is here. This endpoint returns a JSON object that includes election returns at the county level for each candidate.We can make use of the apoc.load.json procedure in the APOC Neo4j standard library to import this data into Neo4j by pulling this data directly from the endpoint and writing to Neo4j.CALL apoc.load.json( http://static01.nyt.com/elections-assets/2020/data/api/2020-11-03/national-map-page/national/president.json ) YIELD valueUNWIND value.data.races AS raceMERGE (s:State {name: race.state_name})SET s.electoral_votes = race.electoral_votes,s.result = race.result,s.id = race.state_id,s.votes =race.votes,s.absentee_votes = race.absentee_votesWITH race,s UNWIND race.counties AS countyMERGE (c:County {fips: county.fips})SET c.name = county.name,    c.trump = county.results.trumpd + county.results_absentee.trumpd,    c.biden = county.results.bidenj + county.results_absentee.bidenj,    c.expected_votes = county.tot_exp_vote,    c.trump_lead = c.trump > c.biden,    c.biden_lead = c.biden > c.trump,    c.votes_2016 = county.votes2016,    c.margin_2016 = county.margin2016MERGE (c)-[:IN_STATE]->(s)WITH null AS foobarMATCH (s:State {result:  winner })<-[:IN_STATE]-(c:County)WITH s, sum(c.trump) AS trump, sum(c.biden) as bidenSET s.trump_winner = trump > biden,    s.biden_winner = biden > trumpThis Cypher script imports the data into Neo4j using a simple graph model of counties and states:(:County)-[:IN_STATE]->(:State)where we store properties such as the number of electoral votes on the State nodes, and votes by candidate on each County node.Periodic Data UpdatesSince this data is coming in live on election night and updated frequently we want to make sure that we’re regularly refreshing the data imported in Neo4j. We’ll make use of another APOC procedure to do this: apoc.periodic.schedule which allows us to schedule background jobs using Cypher. Lets create a background job to update our data every 5 minutes:CALL apoc.periodic.schedule(election-import, statement, 300)Here statement is the import statement above. We can make sure our background job is scheduled and running by listing all our background jobs:CALL apoc.periodic.list()Now that we’re importing and updating live election result data, it’s time to start interpreting the results.Data Visualization with Neo4j BloomFirst, we’ll use Neo4j Bloom to explore the data using an interactive graph visualization. Bloom is a super powerful graph data visualization tool for exploring data in Neo4j. Bloom allows us to express complex graph patterns to visualize using simple natural language search, meaning we don’t need to write any Cypher to visualize our data. WebGL acceleration enables us to visualize and work with large graphs. Bloom is available in Neo4j Desktop so we just need to select Neo4j Bloom” under the Open in” dropdown for our database in Neo4j Desktop. You can learn more about Neo4j Bloom here.Putting Things in PerspectiveGraph visualizations in Bloom can be styled using a perspective. The perspective is a configuration that defines a certain business view or domain that defines how nodes and relationships should appear in the visualization. You can read more about perspectives in the Neo4j Bloom documentation. We’ll make use of rule-based styling in our perspective to bind node size and color to our data to help convey election result details.Rule-Based StylingStyling node sizeRule-based styling allows us to bind node and relationship size and colors to our data. Let’s style county nodes based on population size so that counties with larger population sizes appear as larger nodes.Note that we need to specify the min and max values when creating a rule-based style configuration. We can run the following Cypher query to inspect our data and return the relevant min and max values:MATCH (c:County) RETURN min(c.expected_votes), max(c.expected_votes)Next, let’s style state size according to the number of electoral votes allocated to each state. This means that states with more electoral votes (or more significance) will appear larger.Styling node colorSimilarly, we can also use rule-based styling to bind the color of the nodes in our visualization to our data. A typical use case here is to color nodes according to the result of a community detection or clustering algorithm. In this case, we will add a rule to color County and State nodes blue in the case that the Democratic party candidate is leading and red for the jurisdictions where the Republican candidate is in the lead.Now, as we explore the graph we can make use of the rule-based styles to quickly see the most significant jurisdictions and which way the early election returns are leaning in those jurisdictions.Hierarchical LayoutA force-directed layout is typically used in graph visualizations. This layout often results in groups of nodes that are more connected than others being grouped together. This allows us to visually interpret node clusters. However, sometimes the data we are working with is inherently hierarchical, such as a reporting hierarchy within an organization or in our case, a hierarchy of states and counties. Neo4j Bloom allows us to choose between force directed and hierarchical layouts when rendering the visualization.And zoomed out the hierarchical layout even looks a bit like a Christmas Tree 🎄Building Dashboards with Neo4j ChartsNeo4j Bloom is great for interactive graph visualizations, however, sometimes the answer to our question is best expressed in tabular form which can be visualized as charts, instead of graphs. Fortunately, Adam Cowley has recently released the Neo4j Charts graph app for Neo4j Desktop that allows us to build dashboards of charts to help interpret our graph data. For a full overview of the features of the Charts graph app see this blog post. First, we’ll install the Charts graph app using the Graph App Gallery in Neo4j Desktop, or we can also visit install.graphapp.ioAfter installing and opening the Charts graph app, let’s get started creating a dashboard and adding reports to it. For each report that we add to the dashboard, we’ll need to include a Cypher query that fetches the data from Neo4j necessary to render the report chart. Let’s start off with a simple metric”, showing a single value for the report, in this case, the number of electoral votes assigned to each candidate. Our Cypher queries for this report match on each state then sum the number of electoral votes where the given candidate has been declared the winner:MATCH (s:State) WHERE s.biden_winnerRETURN SUM(s.electoral_votes)andMATCH (s:State) WHERE s.trump_winnerRETURN SUM(s.electoral_votes)And now let’s add a metric report to our dashboard using this Cypher query.Next, let’s find counties that have higher than expected voter turnout. To do this we’ll use the number of expected votes” included in the NYT dataset and the number of votes reported so far per county to calculate a metric showing where the number of votes cast exceeds the expected number. Our query will return both the number of expected votes and total votes per county, ordered by the percentage of expected votes exceeded.MATCH (s:State)<-[:IN_STATE]-(c:County) WITH c.expected_votes AS expected_votes, c.biden + c.trump AS total_votes, c.name +  ,   + s.id AS countyWITH toFloat(total_votes) / expected_votes AS turnout, county, expected_votes, total_votesWITH * where turnout IS NOT NULLRETURN  expected votes  AS key, expected_votes AS value, county AS index ORDER BY turnout DESC LIMIT 10UNION MATCH (s:State)<-[:IN_STATE]-(c:County) WITH c.expected_votes AS expected_votes, c.biden + c.trump AS total_votes, c.name +  ,   + s.id AS countyWITH toFloat(total_votes) / expected_votes AS turnout, county, expected_votes, total_votesWITH * where turnout IS NOT NULLRETURN  total votes  AS key, total_votes AS value, county AS index ORDER BY turnout DESC LIMIT 10We’ll add this query to our dashboard by creating a stacked bar chart report.Next, we want to determine which counties voted most overwhelmingly for a specific candidate. To do this we’ll query for the counties with the highest percentage of votes for Biden:MATCH (s:State)<-[:IN_STATE]-(c:County) WHERE c.biden > 0 AND c.trump > 0WITH (toFloat(c.biden) / (c.biden + c.trump)) AS pct_biden, c.name +  ,   + s.id AS county RETURN *  ORDER BY pct_biden DESC LIMIT 10And the same for Trump:MATCH (s:State)<-[:IN_STATE]-(c:County) WHERE c.biden > 0 AND c.trump > 0WITH (toFloat(c.trump) / (c.biden + c.trump)) AS pct_trump, c.name +  ,   + s.id AS county RETURN *  ORDER BY pct_trump DESC LIMIT 10Now, let’s add these queries to the dashboard using a line chart report:If we view our dashboard now we’ll see all our report visualizations together.We can add additional reports taking advantage of more complex charts such as radar charts, bump charts, and more. What interesting questions can you think of that can best be explained with these types of charts?Visualizing Geospatial Data with NeomapOur election results dashboard wouldn’t be complete without a map visualization. To create our map visualization we’ll use another graph app for Neo4j Desktop called Neomap. Neomap was created by Estelle Scifo and allows us to visualize geospatial data from Neo4j. You can learn more about Neomap in this introductory blog post and an example of visualizing routes with Neomap. We’ll install Neomap using the Graph App Gallery in Neo4j Desktop or by visiting install.graphapp.ioFirst, however, we need to add some geospatial feature data to our database. There are a few options here: we could add polygon geometries for each state or county to create a choropleth map, but let’s keep things simple and create a single marker for each county and color the marker according to the party of the candidate leading the jurisdiction (using the convention of red for Republican and blue for Democratic).For these markers, we’ll need the latitude and longitude of the centroid of each county. I found a dataset from the National Weather Service that includes the centroid of each county in the US.This data is available as a shapefile, which is a binary format for geospatial data. We’ll need to convert this to a format such as geojson or CSV which makes it easier to import into Neo4j. To do this we could use a tool like GDAL which can convert geospatial data between just about every format, but today we’ll use an online tool called Mapshaper to convert this shapefile data to CSV. We’ll open the shapefile in Mapshaper, select export to CSV and now we have our county centroid data in CSV format.Now, let’s use Neo4j’s LOAD CSV functionality to import this data into our dataset. First, lets make sure we can read the CSV file and see what data were working with:LOAD CSV WITH HEADERS FROM  file:///counties.csv  AS rowRETURN row LIMIT 1{   FE_AREA :  se ,   TIME_ZONE :  E ,   FIPS :  23029 ,   COUNTYNAME :  Washington ,   STATE :  ME ,   LON :  -67.6361 ,   CWA :  CAR ,   LAT :  45.0363 }We’ll use the FIPS property to look up our County nodes and add a Point property to each county which represents the latitude and longitude of the centroid of each county.LOAD CSV WITH HEADERS FROM  file:///counties.csv  AS rowMATCH (c:County {fips: row.FIPS})SET c.location = Point({latitude: toFloat(row.LAT), longitude: toFloat(row.LON)})Now that we have our geospatial data for each county, in Neomap we’ll add a layer to visualize the county data. With Neomap we can define a custom Cypher query returning the data we want to visualize, so we’ll filter for counties where the Democratic candidate is leading and color the markers in that layer blue (repeating the process for red makers for the Republican candidate)MATCH (c:County) WHERE EXISTS(c.location) AND c.biden_leadRETURN c.location.latitude AS latitude, c.location.longitude AS longitudeWhen both layers are added we can see the distribution of each party’s leading candidate across counties in the US. As a next step we might want to scale the size of each county marker according to population or perhaps render a polygon of the bounds of each county instead of a single marker, but this is a good start for our dashboard.The DashboardPutting everything together we now have an interactive live-updating election night dashboard showing graph data expressed in charts, graph visualization, and map view.;Nov 28, 2020;[]
https://medium.com/neo4j/nodes-2019-recap-8e48a6327dca;Ljubica LazarevicFollowNov 8, 2019·7 min readOne month on — our NODES experienceMusings about Neo4j’s first online developer conferenceIn the beginning…A long time ago (at the start of the year, anyway!) we wanted to explore something new and novel. Something that would enable our wonderful community to show to the world what amazing Neo4j projects they’ve been working on.Whilst in-person conferences are a great way to do this, (and we do this via GraphTour and GraphConnect), it’s not always easy for everyone to travel to attend or speak. After some deliberation, we decided that doing an online conference would be a great way to do this — not only do we remove the challenges of travel, but we still get the opportunity to interact as a group as well.And soon enough, NODES 2019 was born!The journey continued …We were really keen to have an interactive event — not just a conference, but have many ways for our active community members to get involved. As well as the the summit itself, we also included:A Global GraphHack — a spin on the usual events run at GraphConnect, these would be a worldwide affair.The Hunger Games — a live, 5-minute challenge to answer questions at the end of each session (if you paid attention), with prizes to be won.In addition, our community was actively organizing viewing parties around the world.Call for papersThe start of June marked the call for papers, along with the announcement of NODES2019 to the world. The original plan was to have three tracks in total, two standard-length talk tracks, with a third lightning talks track. Little did we realize the number of high-quality, fascinating proposals we would receive. The participation and level was so high that we decided to increase the total number of talk tracks to five, totaling over 50 sessions for the day.We also had many excellent proposed questions for the Hunger Games which would go on to make the day itself more engaging, with attendees eager to take part in each round.A couple of bumps in the roadIt wasn’t completely smooth, and due to a couple of challenges, we decided that we needed to change over to a different conferencing platform to meet some specific needs. So we switched from Crowdcast, which is more suitable for single track events, to BigMarker which met more of our requirements. Being so close to the 10th October wasn’t ideal, but all involved were able to adjust accordingly.Unfortunately, a couple of speakers also had to pull out weeks before the event. Again, our great community was able to be ready and able to step up and ensure we had a packed schedule full of excellent talks to enjoy.Global GraphHack kick-offPrevious GraphHack events took place before or after GraphConnect. These are always popular events, and we were keen to have something for NODES. Given the global reach of the online summit, we wanted something equally fitting for the GraphHack. Thus the Global GraphHack was born. Teams could be made up of individuals, or groups either friends or colleagues, or virtually across the Neo4j community.With the theme announced at the start of September (Extending the Graph Ecosystem), participants had a month to build their projects. As for the prizes, there were some fabulous ones up for grabs. The teams were competing for flights and hotels to GraphConnect 2020, along with other exclusive perks.Screenshots of the winning projects in actionThe close of the competition saw 17 projects submitted. We were truly impressed with the range of submissions. What was equally impressive was the number of participants new on their graph journey taking part, alongside seasoned graphistas. These projects really fit the soul of the competition, which was to expand the realm of graphs to the whole community.Such was the level of submissions, it was a difficult task to decide who the ultimate winners would be. We are grateful to all who took part in the voting, and to our judges who had a tough time scoring each submission. Ultimately, we had to pick winners from these fine submissions, and they were:neomap: a Neo4j Desktop application to visualize nodes with geographic attributes on a mapMeetup Mixer: an educational tool to teach people at a Meetup about graphs and CypherNeo4j Jmeter load testing: a custom procedure that incorporates native bolt protocol support, along with example load test plansWhat is inspirational, is the different levels of experience the winners had. Anybody can win a GraphHack with a great idea, irrespective of ability!On the dayIt was just before 7am (CEST), on the 10th October when the team set off towards Malmö, Sweden to do the final preparations for the online summit. Neo4j was born in Malmö and is still developed close to its place of birth. These days the Malmö office is our main engineering base with London coming 2nd. Half of the Developer Relations team headed off to the recording studio to set up, whilst the other half dropped into the office to say our hellos and pick up supplies.Images from the studioAs the time drew near, we all reconvened to the recording studio. At the location, we were in the throes of the final set up to start the conference for 2pm WEST. With moderators allocated to tracks, and everybody raring to go, the summit started with Emil Eifrem’s keynote.Following a short break, all five tracks began! The spirit of attendee interaction was alive and well, and as a moderator of one of the tracks I really enjoyed seeing the interesting questions being asked, as well as the involved discussions taking place during the talks.The day lasted for us till 11pm in the studio and we were back at our offsite place after midnight and properly tired but very happy.Thank you all for making it an active and engaging day!Viewing partiesIt just wouldn’t have been the same without a few viewing parties happening around the world. Following from the success of the Graph Day parties, the Neo4j Community delivered, hosting a number of public and private NODES2019 viewing parties.Some of the public viewing parties hosted worldwideWe loved the pictures coming in of the viewing parties, and it was fantastic to see the active participation in the session chat channels.We loved your viewing party pictures!The Hunger GamesOf course, we couldn’t avoid talking about… The Hunger Games! We saw fantastic participation in the end of talk questions which we hoped not only brought an enjoyable element to the day, but also an opportunity to test knowledge gained. Up for grabs were 20 x $50 vouchers for swag from the GraphGear store. We could certainly tell it was getting competitive, and we loved the enthusiasm. We look forward to round two in the future.It’s a wrap!In the end, we had over 4,600 registered attendees and over 50 talks with 60 speakers from 15 countries, covering topics from spatial, complete solutions to software analytics and construction applications. It was truly a ground-breaking day. We are so glad that you could be a part of it with us.Where are all these wonderful talks?I’m glad you asked! You can catch up on all of the videos in a variety of ways:You can get the slides as well as the videos from the schedule if you’re logged inAlternatively you can have a YouTube video bingeWe’d love to hear your feedback!Do let us know your thoughts. Either comment here, or drop us a line on devrel@neo4j.com. Tweet us your favorite talks @neo4j tagged #NODES2019.What’s next?It was an amazing event and we enjoyed every minute of it! It would not have been the success it was without you being a part of it. Thank you!Keep your eyes peeled for NODES2020.In the meantime, GraphConnect 2020 is just around the corner. We would love to hear your Neo4j story, get your paper proposals in.The closing date for the CfP is 15th November 2019, so don’t be a late submitter!Share Your Graph Story With The World At GraphConnect 2020!Call for talk proposals is now open — seeking developer stories about all things graph!medium.comDon’t forget that we do have a speaker program as well for those of you who want to share your Neo4j story around the world.;Nov 8, 2019;[]
https://medium.com/neo4j/handling-authentication-and-identity-with-neo4j-and-auth0-c279a36c3643;Adam CowleyFollowApr 15, 2022·7 min readHandling Authentication and Identity With Neo4j and Auth0We’re heavy users of Auth0 at Neo4j, with many of our products and services using Auth0 for authentication. For example, when logging into Neo4j Aura or Neo4j Sandbox, you are greeted with a customized Auth0 login screen.The customized Auth0 login form used by Neo4j AuraSo when it came to redeveloping Neo4j GraphAcademy, it made sense for me to use the same services. The GraphAcademy site itself is built in TypeScript, using Express.js and backed by a Neo4j Aura database for saving course information and user enrollment information.Adding Authentication is EasyThe Auth0 service itself is pretty straightforward. I added authentication to the site within minutes using the express-openid-connect library by following the quickstart example in the Auth0 management console.const { auth } = require(express-openid-connect)app.use(  auth({    issuerBaseURL: https://YOUR_DOMAIN,    baseURL: https://YOUR_APPLICATION_ROOT_URL,    clientID: YOUR_CLIENT_ID,    secret: LONG_RANDOM_STRING  }))Because the authentication is handled by Auth0 and JWT tokens, I didn’t necessarily need to store any information in my Neo4j instance until the user becomes active by enrolling. At the point where a user enrolls in a course, I could just decode the JWT token provided by Auth0 to get the sub (subject in JWT terms, or the unique ID of the user).Here is an abridged version of the enrollment route handler:import { requiresAuth } from express-openid-connect// An oidc object containing the user is added to `req` by express-openid-connectrouter.post(/:course/enrol,   requiresAuth(),   async (req, res, next) => {    try {      const user = req.oidc.user      // Create the (:User)-[:HAS_ENROLMENT]->(:Enrolment)-[:FOR_COURSE]->(c) pattern in Neo4j      const enrolment = await enrolInCourse(req.params.course, user)      // Redirect the user to the first lesson      res.redirect(enrolment.next.link)    }    catch(e) {      next(e)    }  })So far so good!Multiple Identities, Oops.As the site got more popular, some users were complaining that they couldn’t see their enrollments. Looking at the data, I could see that a number of User accounts had the same email address. Looking at the data, I spotted something curious:neo4j$ MATCH (u:User) WHERE u.email = adam@neo4j.com RETURN u.email, u.sub| u.email        │ u.sub                               │| adam@neo4j.com │ google-oauth2|113046196349780988147 │| adam@neo4j.com │ auth0|625587d086c092006f19966d      │It turns out that it is possible to register twice with the same email. If you take a look at the sub, it is formed of two parts separated by a pipe (|), the authentication method (eg. google-oauth2 for Continue with Google) and the unique user ID provided by the service.Rightly or wrongly, when using a different authentication method, Auth0 treats these as separate entities with a different sub and makes no attempt to reconcile the profiles. That’s fair enough, but something worth noting.So it falls upon us to handle these cases. Should we reconcile these on our end? Or should the same email with different authentication methods be treated as different people? After all, anyone could sign up with my email address and their own password. I spoke to a few people within the company on how I should manage these cases and got as many opinions back.Identity and Access Management (IAM) in Neo4jAs a former member of the Neo4j Professional Services Team, I’ve worked on a few Identity and Access Management (IAM) and Entity Resolution (ER) projects.A structure of Nodes and Relationships is a great way to handle complex authentication and authorisation data. In the past, I have imported complex Active Directory data into Neo4j for analysis and created complex trees of Users belonging to Groups that can have individual privileges granted or denied, and even inherited from the groups they belong to.Identity & Access ManagementTo verify an accurate identity, the system needs to traverse through a highly interconnected dataset that is…neo4j.comThere must be a simple solution to this problem, I thought.Handling Multiple IdentitiesThere’s a certain level of serendipity involved when working with graphs and that always make me happy I chose to build applications on top of a graph.The data model used on GraphAcademy is pretty simple. Here is an abstraction of the User management part of the Graph. When a user enrolls in a Course, a node is created with an :Enrolment label, which provides a connection between the User and the Course they have enrolled in.(:User)-[:HAS_ENROLMENT]->(:Enrolment)-[:FOR_COURSE]->(:Course)The User node has a Unique ID generated by Cypher’s randomUuid() function, with the sub and email provided by Auth0. The id and sub fields both have unique constraints applied to them to ensure an identity is not duplicated, and also to enable faster lookups at query time.MATCH (u:User {sub: $sub})-[:HAS_ENROLMENT]->(e:Enrolment)-[:FOR_COURSE]->(c:Course)RETURN c {  .*,  completed: e:CompletedEnrolment,  modules: [ (c)-[:HAS_MODULE]->(m) | m {     .*,     completed: exists( (e)-[:COMPLETED_MODULE]->(m) ),    lessons: [ (m)-[:HAS_LESSON]->(l) | l {       .*,      completed: exists( (e)-[:COMPLETED_LESSON]->(l) ),    } ] } ], }The above query gets all enrollments for the course and returns a map projection including all properties (.*) for the Course node, checks for the existence of a :CompletedEnrolment label on the node to signify that the user has completed the course, and then uses list comprehensions to get information about the modules and lessons within the course.If you want to learn more about the techniques used in the query above, you can enroll in the Cypher Intermediate Queries course on GraphAcademy.Alias AccountsThere are a few ways to handle this problem. You could store an array of subs on the original User node, maybe create an (:Email) node with and link both users to that? But I’m a fan of keeping it simple.Instead, I went for the approach of creating a :HAS_ALIASrelationship between the two nodes with the same email.Links between Google and Auth ProfilesThis takes a few lines of Cypher to generate:// Get all users ordered by their creation dateMATCH (u:User)WITH uORDER BY u.createdAt ASC// Generate a list of users grouped by their emailWITH u.email AS email, collect(u) AS users// Where duplicate users have this email addressWHERE size(users) > 1// Get the first created user and a list of all othersWITH head(users) AS head, tail(users) AS tail// Merge a HAS_ALIAS relationship between the twoFOREACH (u in tail | MERGE (head)-[:HAS_ALIAS]-(u))Utilizing the Good Old *0..1 TechniqueThe moment of serendipity here came when I noticed that I could just use what I like to call the *0..1 technique to unify the user’s enrollments at query time.The Star-zero-dot-dot-one technique? I don’t say it aloud too often.Variable-length paths are what Graphs are designed to deal with. In Cypher, you can represent a variable-length path using an asterisk (*) and lower and upper limits. For example, if I use the pattern (start)-[:NEXT*2..5]->(end) , Neo4j will expand the :NEXT relationships in an outgoing direction from 2 to 5 hops and provide one row for each end node at the end of the path.By setting the lower bound to 0, you are including the start node in the result set. The below query will start with the user with the supplied sub property (think the current logged in user). Then, from that node or any node 1 degree away through the :HAS_ALIAS relationship, follow the :HAS_ENROLMENT relationship to find their enrollments.So all we have to do is to adjust our pattern for course information a bit at the beginning and call it a day.MATCH (:User {sub: $sub})-[:HAS_ALIAS*0..1]-(u)-[:HAS_ENROLMENT]->(e:Enrolment)-[:FOR_COURSE]->(c:Course)This way, if I enroll in one course with my Twitter account and another with my Github account, they will both appear within the same result set as long as there is a relationship.No drastic changes to the data model and only approximately 20 more characters were added to my existing query.Email VerificationI mentioned earlier that anybody could sign up with an email address and password. When you create an account with Auth0, you can force email verification.Verify Emails using Auth0When using an email address supplied by a user, it is important to verify the user has access to that email. This is…auth0.comOnce the email is verified, it is safe to create this relationship between the two users. All enrolments related to both user nodes will be immediately available to the user currently logged into the site.Free, Hands-on Courses With Neo4j GraphAcademyIf you are interested in learning more about Neo4j, check out the Beginners learning path to learn everything you need to know to be successful with Neo4j. We also offer App Development courses for Developers and Data Scientists.Free, Self-Paced, Hands-on Online TrainingExpert Training All courses have been developed by seasoned Neo4j Professionals with years of experience.Our aim is to…graphacademy.neo4j.com;Apr 15, 2022;[]
https://medium.com/neo4j/how-to-painlessly-unite-art-with-java-javascript-and-graphs-or-the-story-behind-creating-an-e22fc4daa4e5;AlexTavgenFollowOct 10, 2018·12 min readHow to Painlessly Unite Art with Java, JavaScript, and Graphs or The Story Behind Creating an Interactive Theatre Production for the EV100 CelebrationsThis year, a theatre production series called Tale of the Century was launched in Estonia. Throughout the year, 22 local theatres presented their interpretations of the past hundred years of Estonian history to the audiences. In the draw, the Russian Theatre was assigned the topic of the future of Estonia.Each one of us has their own ideas about what the future might look like — what we’re afraid of and what we dream of. However, we didn’t want to create a play that would just tell the audience how the narrow circle of people at the theatre sees the future. That’s why we posed this question to the people who are actually going to live in the future — that is, the children and youths of Estonia, aged 3–19. We received responses both in Russian and in Estonian from cities and villages in all corners of the country. There were hundreds of them and we’re extremely grateful for all of them.Each response we received was unique, but when going through the collected materials, we noticed some clear tendencies and patterns. An immense number of possible future worlds, both attractive and horrific, took shape from the children’s answers. However, there is something that unites these worlds.The story takes us to the year 2118. Estonia is under a protective dome. We have learned a lot, including how to extend human life span. The main characters are an elderly couple called Linda and Timo. A hundred years ago, as kids, they had fantasies about the future in which they now live. In fact, they are the very children who helped create this play. During the play, these characters are dropped into different versions of Estonia in the year 2118 based on how the audience votes.Of course, the performance does not begin when the curtain goes up, but much earlier. After processing all the responses, we identified the main possible paths for the future according to the children — from the info-technological to the ecological state. Since the future is not predefined but depends on the choices that we all make, the possible paths for the future were divided into short stories that were linked by the overarching story of the main characters and their family.Artyom Gareev (the director of the play) demonstrates the main ideas on a miniature mock-up.The stage design resembles a construction toy that can take any shape and perform any function in the hands of children. It’s also shaped like a graph — a special abstract mathematical object that allows us to describe and model many phenomena of the real world, like road networks, organizational models of businesses, the Internet, or the variety of choices that people make — choices that can be turned into different plotlines.Decorations developmentWhen represented on a graph, the possible plot developments form a complex and multi-branched structure.Utopia often leads to dystopia and the road to hell is paved with good intentions. The scenes are bound together by the logic of dramaturgy. Taking into account all the possible options for the future that were inspired by the children’s fantasies, we had a lot of story lines. It was very important not to repeat any of them during a performance. For example, if the audience votes for a utopia, it will lead to a dystopia, or if the audience rejects the utopia, they need to make another choice. We have had situations where the votes have been almost equally divided and the final decision has depended on 2–3 votes.Scene related to the Ecological DystopiaGraph representation of storylinesBesides the dramaturgic task, there were multiple other technical challenges: the sophisticated controller systems that drive hundreds of LEDs on the stage, the artfully created costumes for each version of the future by one of the best Estonian theatre designers Rosita Raud (this is a challenge mainly for the actors who have to change their clothes fast when the voting has ended), and of course, the voting system that drives the story line and is built upon a graph model. This is what I’d like to tell you about in more detail.All music was live played by MODULSHTEINMODULSHTEINIn order to make it easier to understand, let’s enter a performanceWe are in the multicultural story line. All languages and genders are mixed together. Everything is accepted. The performance segment begins with a news bulletin that explains the context of the following scene to the audience in a 5 languages mix.Ironically, Jelena Solomina is a real host on a national television and a well-known public figure.In the multicultural future, everything is fun. Semenova jr. organizes a meeting with her parents, where she introduces her boyfriend who is wearing a mixture of Arabic and ballet clothes. After introductions have been made, everybody goes to a partyAt the end of the scene, voting opens. The main question is whether Estonia will go down this road or not. And if yes…The utopia will turn into a dystopia. All freedoms are curtailed and new restrictions are introduced. Suddenly, special forces shut the party down and sort people into groups. Some people are welcomed and go to the right into the bright future, while others have to take their clothes off and go to the left into the execution machine.The Technical SolutionHere are the requirements that we set down for our system:- The system should collect votes, show the results in real time, and make decisions up to that.- The system should select story lines depending on the voting results, open new votes, and switch between story line.- The system should communicate with the audience using natural language.- The system should be administered by the assistant director behind the scenes, with full statistics about ongoing states and votes.In fact, this was another actor called EMA (Mother” in Estonian), an artificial intelligence that has become a reality in the future. EMA leads the audience through the plot, manifesting itself in the different versions of the future. It also launches the voting, counts and presents the results, and moves the play along the plotlines based on the results of the voting. All of EMA was written from scratch in the 1.5 months before the premiere. Since the plotlines, various customizations, and graph transitions were constantly evolving as the play was taking shape according to the decisions of the team of scriptwriters and the director, it was not possible to start with the development earlier.The auditorium could seat 600 people, so people had to be able to vote using their phones and we needed to be able to receive and process all these votes within less than a minute. The option of developing a mobile app was immediately discarded, because downloading and installing an application takes extra effort — no one is going to download your app if you’re not Facebook or Google. In addition, supporting different types and models of phones would have required separate development, which was unrealistic, since we had only 1.5 months and one developer. Moreover, the mobile client would have had to be in two languages ​​– Estonian and Russian. Therefore, we needed a web application that was optimized for mobile phones.Another web application was needed as an administrative backend for controlling the system: starting/stopping, activating rounds of voting, monitoring the state of ongoing voting, and so on.And thirdly, and most importantly, we needed to coordinate the interaction of EMA with the audience.Here, it is necessary to introduce the concept of the super” — something that was new to me. The super” is the front curtain that goes down and hides the stage. It is reflective and allows you to display various projections, like news releases (which we’ll come back to later) or the EMA screen that displays information about the voting results and the choices made by the audience.So we have a complex graph, a voting system, and three frontend systems, which need to interact with the viewers and the administrator (Svetlana Shushina, an incredible assistant director).GraphLet’s start with the graph. The data structure — in this case, different versions of Estonia’s future and the possible transitions between the scenes depending on the voting results — had to be stored somewhere. Standard relational databases are not well suited for storing graph data for a number of reasons. For us, the essential requirements were flexible data storage, storage of state transitions, links to audio files depending on the current state, and so on and so forth.The obvious choice was Neo4j, the leading graph database management software in the world. Seven of the top ten technology companies in the world use it. It offers mature technology, speed, convenience, and an excellent community, which all really leave no other choice for this kind of solutions.We modelled the scenarios built by the scriptwriters as a graph in Neo4j. Here is an example of what the plot twists and turns look like from inside Neo4j.Being a native system for graph representation, the advantages that Neo4j provides are flexibility and the absence of a fixed structure. This made it easy to change and connect story lines and to add new versions of audio.Piece of our graph representationBackendWhen choosing the technology for the server side, we hesitated between Python and Java. Eventually, we chose Java because of its speed and reliability while having clear requirements, and also because of Spring Boot, which supports Neo4j out of the box.The server side was implemented in Spring Boot. The server collects votes, switches from the super” to the following states when the voting is over, accepts commands from the administrator, provides statistics, and so on. It also handles user registration and dispatches voting data (the opening or closing of a vote and the available options) to the frontend clients.Then we came to the most remarkable part of the task. The general idea was that the component that is displayed on the super” — EMA — had to be able to talk to the audience. It had to prepare the audience for voting, announce the voting options, start and end voting, announce the results, and depending on the audience’s choice, suggest the options for the next vote, or give an introduction for the next scene. This meant that the audio had to be dynamic and prepared on the fly. Considering that the number of options is enormous, the duration of each audio file is different, and synchronization is important (for example, voting should open only after the announcement Voting is open!”), this was not a trivial task at all.So the system ended up having two control centers.The first one is the server — it stores the current state of the plot, provides statistics, handles user registration, collects votes, and makes sure that each person can give only one vote.The second one is the super”, which starts and ends the voting, shows the progress and the results of the voting, and so on.FrontEndThe super” was implemented in Vue.js. Initially, the plan was to dynamically build the audio using the Google Speech API, but we couldn’t guarantee that there would be no network latency. Therefore, another option was chosen and we recorded the audio files individually using Google Speech.In total, we had 55 audio clips. The data about which audio files should be played in what state was stored on the graph together with the states. Since we were dealing with JavaScript, we strung the audio files and command calls on top of each other like grapes. Thus, we did not depend on the fact that the audio files were of various lengths and ended up with a fairly universal solution.Initially, the mobile client website was written in VueJS, but in testing we realized that there was an incompatibility issue that affected some mobile phone models. We tried to solve the problem with Babel transpilers, but as multiple settings were added the volume of the page grew, and even though the web app started working on some phones, it broke on other ones. So finally, the mobile client was rewritten in Vanilla JS. As a result, we ended up with a distributed system.The final volume of the system (the JAR file) was 146 megabytes. The telecommunication company Telia provided us with a virtual server at the symbolic price of 1 euro per month, for which we’re very grateful, and amazing quality of service.First live run test were at Playtech office which turned to be pretty funny.First live run at Playtech officeThe pace of work accelerated before the premiere. The expectations were raised even higher by the fact that the president of Estonia was going to attend the premiere. One last bug was detected half an hour before the premiere and there was no time for testing the fix, so during the first performances, testing of system behavior in case of all the possible future plotlines was done in parallel on the developer’s local machine behind the scenes, so that it would be possible to prepare an action plan in advance if any issues were to arise.President of Estonia Kersti Kaljulaid(in the right) with Margus Alliksaar and Philipp LossThere is one interesting fact that I would like to highlight. We were showing the names of the people who voted in real time on the voting screen on the front curtain. We used Vue’s animation template for this, and during the very first performance, somebody in the audience entered a killer symbol” as his name upon registration, which broke the animation and display of the Vue templates. Subsequently, we abandoned this solution(showing names in animated transitions) and switched to good old jQuery. VueJS is used only for super” for templating, audio managing and switching states.The performance received a lot of positive reviews in the press. Some of them:Estonian:Millised me, eestlased, oleme?See, et Aivar Mäe teab, kus on Kanuti Gildi saal, ning käib seal etendusi vaatamas, on juba suur asi. Nii tõdes…kultuur.postimees.eeMari Lill: „Üks asi on rääkida vene keelt tänaval, hoopis teine asi teha seda teatrilaval. „Mari, Mari, Mari...,  skandeeris Vene teatri lavastuse „Tuleb/Ei tule. Eesti 100 aasta pärast  trupp neljapäevase…www.ohtuleht.eeRussianНаше завтра в зеркалах утопии и антиутопии Будет/Не будет. Эстония через сто лет . Русский театр и объединение   Audiokinetica . Постановка движется от развилки…rus.postimees.ee Будет / Не будет : нет ничего опаснее будущегоКак известно, при жеребьевке тем для спектаклей, связанных со столетием Эстонской Республики, Русскому театру сильно…rus.postimees.eeРусский театр покажет спектакль  Будет/Не будет  на фестивале Draama в ТартуБолее двух десятков лет театры со всей Эстонии собираются на целую неделю в университетском городе. В основную…rus.err.eeОткрытие сезона в Русском театре - театральное событие годаВ этом году театральный сезон в Русском театре Эстонии начался неожиданно рано - 23 августа. Вниманию публики была…m.ru.sputnik-news.eeAt the Drama Festival in Tartu, the standing ovation lasted almost 5 minutes.It was an incredible adventure due to the crazy energy and dedication of the people involved in the project. I take my hat off to all the people who participated in the creation of this play.Director — Artjom GareevComposer, Producer– Aleksandr ZedeljovVisual — Aljona MovkoCostumes —Rosita RaudLights — Anton AndrejukChoreography — Olga PrivisPlaywright: Karin Lamson, Mari-Liis Lill, Jelena Chicherina, Laura KalleVideo — Nikolay AlhazovProgramming and IT solution— Aleksandr Tavgen, Anna AgafonovaLED lights — Aleksander SprohgisAnimations — Martin YakushMusic — ModulshteinMarten Altrov — Clarinet, Bass clarinetAleksej Semenihhin– Samples, Sound EffectsAleksandr Zedeljov— guitar, synths, abletonVocals— Anna DydynaDirector Assistant Svetlana ShushinaActors:Natalja Dymchenko, Aleksandr Zhilenko, Daniil Zandberg, Dmitrij Kordas, Ekaterina Kordas, Aleksandr Kuchmezov, Viktor Marvin, Natalja Murina, Jelena Tarassenko, Eduard TeeSergej Furmanjuk, Leonid Shevcov, Jelena JakovlevaRussian Theatre Studio: Deniss Volkov, Polina Grinjova, Nina Zagvozdkina, Anastassija Koleda, Natasha Kristensen, Anastassija Masalova, Sandra Minosjan, Sofia Mihaljova, Katrin Mägi, Mihhail Pashuk, Katrin Seljugina, Kristina Sorokina, Sofia Strömberg, Ilja Sutt.Tech Writer — Sigrid Maasen;Oct 10, 2018;[]
https://medium.com/neo4j/5-tips-tricks-for-fast-batched-updates-of-graph-structures-with-neo4j-and-cypher-73c7f693c8cc;Michael HungerFollowAug 11, 2017·6 min read5 Tips & Tricks for Fast Batched Updates of Graph Structures with Neo4j and CypherThis post was originally published on my blog, but I think that a wider audience can benefit from it.When you’re writing a lot of data from your application or library to the graph, you want to be efficient. The following tips have been widely used in libraries for object-graph mapping, like Spring Data Neo4j or the PHP-OGM.Inefficient SolutionsThese approaches are not very efficient:hard coding literal values instead of using parameterssending a single query / transaction per individual updatesending many single queries within a single transaction with individual updatesgenerating large, complex statements (hundreds of lines) and sending one of them per transaction and updatesending in HUGE (millions) of updates in a single transaction, will cause out-of-memory issuesBetter ApproachYou want small enough queries, that are constant in their shape (for caching) and are using parameters.Each query can update from a single property to a whole subgraph (100 nodes) but has to be the same in overall structure for caching.UNWIND to the RescueTo achieve that you just prefix your regular single-update-query” with an UNWIND that turns a batch of data (up to 10k or 50k entries)into individual rows, which contain the information for each of the (more or less complex) updates.You send in a $batch parameter (up to 10k-50k) of data (hopefully a delta) as a list of maps, which are then applied in a compact query, which is also properly compiled and cached, as it has a fixed structure.Overall Syntax Structure{batch: [{row1},{row2},{row3},...10k]}UNWIND $batch as row// now perform updates with the data in each  row  mapExamplesCreate node with properties{batch: [{name: Alice ,age:32},{name: Bob ,age:42}]}UNWIND $batch as rowCREATE (n:Label)SET n += rowMERGE node with properties{batch: [{id: alice@example.com ,properties:{name: Alice ,age:32}},{id: bob@example.com ,properties:{name: Bob ,age:42}}]}UNWIND $batch as rowMERGE (n:Label {id: row.id})(ON CREATE) SET n += row.propertiesNode lookup and MERGE/CREATE relationship between with properties{batch: [{from: alice@example.com ,to: bob@example.com ,properties:{since:2012}},{from: alice@example.com ,to: charlie@example.com ,properties:{since:2016}}]}UNWIND $batch as rowMATCH (from:Label {id: row.from})MATCH (to:Label {id: row.to})CREATE/MERGE (from)-[rel:KNOWS]->(to)(ON CREATE) SET rel += row.propertiesLookup by id, or even list of idsThis is useful for parent-child trees.Here we’re passing a single property created.Alternatively you could pass in no properties or a map of properties to be set/updated.{batch: [{from:123,to:[44,12,128],created: 2016-01-13 }, {from:34,to:[23,35,2983],created: 2016-01-15 },...]UNWIND $batch as rowMATCH (from) WHERE id(from) = row.fromMATCH (to) WHERE id(from) IN row.to // list of idsCREATE/MERGE (from)-[rel:FOO]->(to)SET rel.created = row.createdFaster, Better, Further: All the tricksThere are some more tricks.You can also send in a map where the keys are node- or relationship-ids (converted to as strings) that’s more compact and faster too for the id lookup.Update of existing nodes by id{ batch : [{ 1 :334, 2 :222,3:3840, ... 100k}]}WITH $batch as data, [k in keys($batch) | toInteger(k)] as idsMATCH (n) WHERE id(n) IN ids// single property valueSET n.count = data[toString(id(n))]// or override all propertiesSET n = data[toString(id(n))]// or add all propertiesSET n += data[toString(id(n))]Update of existing relationships by id{ batch : [{ 1 :334, 2 :222,3:3840, ... 100k}]}WITH $batch as data, [k in keys($batch) | toInteger(k)] as idsMATCH ()-[rel]->() WHERE id(rel) IN idsSET rel.foo = data[toString(id(rel))] // single propertySET rel= data[toString(id(rel))] // all propertiesConditional Data CreationSometimes you want to create data dynamically based on inputs, e.g. a node with a certain label.As cypher currently has no conditional WHEN or IF clause, and case when is just an expression, you have to use a trick I came up with many years ago.Fortunately there is FOREACH which is meant to iterate over a list of items and execute updateoperations for each of them.Fortunately a list of 0 or 1 elements can serve as a conditional of false and true, i.e. no iteration or one iteration.General idea:...FOREACH (_ IN CASE WHEN predicate THEN [true] ELSE [] END |... update operations ....)Note that the true value in that list could be anything, 42,   , null etc. as long as it is any single value so that we have a non-empty list.You can achieve something similar with a RANGE(1, CASE WHEN predicate THEN 1 ELSE 0 END) which will yield an empty list when the predicate is false.Or if you fancy filter then you can use: filter(_ IN [1] WHERE predicate).Here is a concrete example:LOAD CSV FROM {url} AS rowMATCH (o:Organization {name:row.org})FOREACH (_ IN case when row.type = Person then [1] else [] end|   MERGE (p:Person {name:row.name})   CREATE (p)-[:WORKS_FOR]->(o))FOREACH (_ IN case when row.type = Agency then [1] else [] end|   MERGE (a:Agency {name:row.name})   CREATE (a)-[:WORKS_FOR]->(o))Note that identifiers created within FOREACH are not accessible from the outside, you would have to re-match the value later on, or you have to move all your update operations into the foreach.Utilizing APOC ProceduresThe APOC procedure library comes with a lot of useful procedures that can help you here, I want to highlight 3 of them:create nodes / relationships with dynamic labels and propertiesbatched transactions / iteration of updatesfunctions for creating and manipulating maps to be set as propertiesCreating Nodes and Relationships dynamicallyWith apoc.create.node and apoc.create.relationship you can have dynamically computed node-labels and relationship-types as well as any map of properties.labels is a string arrayproperties is just a mapUNWIND $batch as rowCALL apoc.create.node(row.labels, row.properties) yield nodeRETURN count(*)There are also procedures in apoc.create.* for setting/updating/removing properties and labels with dynamic string keys.UNWIND $batch as rowMATCH (from) WHERE id(n) = row.fromMATCH (to:Label) where to.key = row.toCALL apoc.create.relationship(from, row.type, row.properties, to) yield relRETURN count(*)Batched TransactionsAs mentioned at the beginning huge transactions are a problem, you can update a million records with around 2G — 4G of heap but it gets difficult with larger volumes.My biggest volume per single transaction was about 10M nodes / relationships with 32G heap.That’s where apoc.periodic.iterate comes in.The idea is simple: You have two Cypher statements, the first statement provides the data to operate on and can produce a huge (many millions) stream of data (nodes, relationships, scalar values, etc.).The second statement does the actual update work, it is called for each item, but a new transaction is created only for each batch (e.g. 10k) of items.So for example your first statement returns 5 million nodes to update, with a computed value. The inner statement is executed once for each of those 5 M nodes.If your batch size is 10k then that happens in batches of 10k statements per transaction.There is a new variant of this, which you can enable with iterateList:true that adds an automatic UNWIND before the second statement, so it executes only one inner statement per transaction.If your updates are independent of each other (think creation of nodes or updates of properties, or updates of independent subgraphs), then you can run this procedure with a parallel:trueoption which will use all your CPUs.For example if you want to compute a score of many rated items and update this property in a batched fashion, this is what you would do:call apoc.periodic.iterate(MATCH (n:User)-[r1:LIKES]->(thing)<-[r2:RATED]-(m:User) WHERE id(n)<id(m) RETURN thing, avg( r1.rating + r2.rating ) as score,SET thing.score = score, {batchSize:10000, parallel:true, iterateList:true})Creating / Updating Maps dynamicallyWhile lists can be created and processed quite easily in Cypher with range, collect, unwind, reduce, extract, filter, size etc, maps have more limited means esp. for creation and modification.The apoc.map.* package comes with a number of functions that make your life easier:Creating Maps from other data:RETURN apoc.map.fromPairs([[ alice ,38],[ bob ,42],...​])// {alice:38, bob: 42, ...}RETURN apoc.map.fromLists([ alice , bob ,...],[38,42])// {alice:38, bob: 42, ...}// groups nodes, relationships, maps by key, good for quick lookups by that keyRETURN apoc.map.groupBy([{name: alice ,gender: female },{name: bob ,gender: male }], gender )// {female:{name: alice ,gender: female }, male:{name: bob ,gender: male }}RETURN apoc.map.groupByMulti([{name: alice ,gender: female },{name: bob ,gender: male },{name: Jane ,gender: female }], gender )// {female:[{name: alice ,gender: female },{name: jane ,gender: female }], male:[{name: bob ,gender: male }]}Updating MapsRETURN apoc.map.merge({alice: 38},{bob:42})// {alice:38, bob: 42}RETURN apoc.map.setKey({alice:38}, bob ,42)// {alice:38, bob: 42}RETURN apoc.map.removeKey({alice:38, bob: 42}, alice )// {bob: 42}RETURN apoc.map.removeKey({alice:38, bob: 42},[ alice , bob , charlie ])// {}// remove the given keys and values, good for data from load-csv/json/jdbc/xmlRETURN apoc.map.clean({name:  Alice , ssn:2324434, age: n/a , location:  },[ ssn ],[ n/a ,  ])// {name: Alice }ConclusionI used these approaches successfully for high volume update operations, and also in implementation of object graph mappers for bulk updates.Of course you can combine these variants for more complex operations.If you try them out and are successful, please let me know.If you have any other tricks that helped you to achieve more write throughput with Cypher, please let me know too and I’ll update this post.Follow me on Twitter and here on Medium for more tips like this.;Aug 11, 2017;[]
https://medium.com/neo4j/cypher-sleuthing-dealing-with-dates-part-2-629af84bbb26;Jennifer ReifFollowApr 26, 2021·10 min readCypher Sleuthing: Dealing with Dates, Part 2*Latest version with Neo4j Browser duration format changes is available at https://jmhreif.com/blog/cypher-sleuthing-dates-part2/My previous part 1 post on this topic introduced Cypher dates and translated a few other date formats to the Cypher-supported ISO 8601 format. If you read that, then this post is the next step with Cypher dates that covers durations and more. If you haven’t seen part 1, feel free to catch up — though this part 2 doesn’t necessarily require reading part 1. :)We will continue a bit of the last post by showing how to truncate dates and times. Then we will look at the next area of built-in Cypher date functionality — time distance measurements and difference calculations between dates with durations. We will discuss and see examples of how durations in Cypher work — calculating durations from single dates, adding or subtracting durations from dates, and calculating the distance between 2 dates or times.You can follow along by launching a blank sandbox (free) and copying the Cypher into the browser or tweaking and running the queries for your own data set. Time to dive in!Date TruncationIf you read the Cypher documentation on truncating dates, the wording is accurate but kinda makes my head spin. So let me try to translate it.Cypher manual text: A temporal instant value can be created by truncating another temporal instant value at the nearest preceding point in time at a specified component boundary (namely, a truncation unit). A temporal instant value created in this way will have all components which are less significant than the specified truncation unit set to their default values.My version: You can trim (truncate) a temporal value at a specific point, which sets any smaller units to default values. For instance, truncating the date 2021–04–21 to the year means the year value will be preserved, and the month and day values will be defaulted to 01, returning a result of 2021–01–01.This can be helpful if you want to default a search to generalize all dates by year or month. Note that if you want to completely shorten the date to the desired component (e.g. 2021–04–21 to just 2021), you can do that by accessing the component of the value. This was briefly covered in Part 1 of this series and will be handled more thoroughly in another post. Let’s look at a few examples of truncating dates.Each of the Neo4j temporal instants (Date, Time, Datetime, LocalDatetime, LocalTime) can use the .truncate() at the end. Just as with the last post, we will stick with the more common Date, Time, and Datetime values. While LocalDatetime and LocalTime are valid and supported, there are very few cases that truly require the use of local, and it is more complex to work with.Example 1: Truncating a datetime at the yearWITH datetime.truncate(‘year’,datetime()) as truncatedDatetimeMATCH (b:BlogPost)WHERE b.publishedDatetime >= truncatedDatetimeRETURN b.publishedDatetime, truncatedDatetimeThe above example looks for all blog posts published in 2021 (on or after 2021–01–01).Example 2: Truncating a date at the monthWITH date.truncate(‘month’,date()) as truncatedDateMATCH (p:Project)WHERE p.expectedEndDate > truncatedDateRETURN p.expectedEndDate, truncatedDateOur example above is searching for projects that are not yet completed — have an expected end date after April 1, 2021.Example 3: Truncating a date at the hourWITH time.truncate(‘hour’,time(‘09:30:52–06:00’)) as truncatedTimeMATCH (p:Presentation)WHERE time.truncate(‘hour’,p.time) = truncatedTimeRETURN time.truncate(‘hour’,p.time), truncatedTimeExample 3 above queries for any presentations that are going on during the 9am hour. This could be useful during a conference when you want to see which sessions are going on during a particular hour.We have seen how we can trim dates to create generic dates for use cases like starting points in searches. Now we will transition over to working with lengths of time using durations.Cypher DurationA duration in Cypher is the amount of time between two temporal instants. Whether we are using the duration itself to capture a length of time or calculating distance between points in time, these values are incredibly useful for time measurements. There are 2 different ways to state durations, listed as follows:Unit-based amounts (literal P and/or T + numeric value + component id) — e.g. P1Y3M10D or {hours: 24}.Date and Time instants (literal P and/or T + date and/or time value) — e.g. P20210419 or PT090000.Let’s look at some examples.Example 1: Getting a duration value (using unit-based amount)MERGE (p:Person)-[r:BOOKED]->(v:Vacation)SET v.length = duration(‘P5D’)RETURN v.length as vacationDurationSetting the length for a person’s vacation. We could use this query for lodging planning, out-of-office emails, vacation activity scheduling, or other use cases.Example 2: Set duration as frequency for medicine dosageMERGE (d:Dose)-[r:PRESCRIBED_AMOUNT]->(m:Medicine)SET d.frequency = duration(‘PT4H’)RETURN d.frequencyThe example above uses this to calculate how often someone can take a prescribed medication. Now, you might say wait, why did it calculate as seconds instead of hours and minutes”? We will explain this in further detail after a couple more examples. For now, know that hour and minute durations are converted to seconds.Example 3: Calculate a specific date as a durationMATCH (:Employee)-[rel:ASSIGNED]-(:Project)RETURN rel.startDate as date, duration(‘P’+ rel.startDate) as durationNOTE: Date format cannot include timezone value. A helpful memory key is that a length of time isn’t based on geographic location — only the distance between 2 dates or times. Also, date must be preceded by a literal ‘P’ and time must be preceded by a literal T.If we divide the resulting 24,254 months by 12 (to find the number of years), we get 2021.16667. This tells us the number of months from year 0 that the date 2021–02–15 is. While this probably isn’t super useful for this use case, it might be more helpful in science fields to precisely date artifacts or geography.Example 3: Calculate specific time as a durationMATCH (d:Dose)MERGE (p:Person)-[r:TAKES]->(d)SET d.dose1Time = duration(‘PT093000’)RETURN d.dose1TimeThe example above tells us exactly how long after midnight a person took a medication. This could be incredibly critical for determining how close together doses are, as well as for tracking a strict schedule. Again, we see that our duration has been converted to seconds. Before we get too much further, let’s talk a bit about the conversion of hours and minutes into seconds. This has to do with precision in calculations, as explained in the next paragraph.Duration PrecisionOne thing to note is that there are very specific rules that duration follows for calculations. Durations are stored as months, days, seconds, nanoseconds. This is because some components of time can vary — hours in a day (due to daylight savings time), days in a month (28, 29, 30, 31), etc. This is the reason you might notice hour and minute calculations diverting to the more precise and consistent second values.This is why our earlier queries (listed again below) were converted to seconds. In the first query for medicine dose frequency, 4 hours is converted to seconds because larger time values cannot be assumed as 100% consistent at all times. This could actually be a life-and-death situation if dose frequency was not calculated properly.The same scenario exists for the second query. If we took the dose at 9:30AM on a regular day or on a day that changed to/from Daylight Savings Time, this time could be off. It is more precise to calculate the seconds from midnight (34,200 seconds / 60 seconds in minute / 60 minutes in hour = 9.5 hours from midnight). This ensures no incorrect storage values and that users can calculate the time based on their circumstances.MERGE (d:Dose)-[r:PRESCRIBED_AMOUNT]->(m:Medicine)SET d.frequency = duration(‘PT4H’)RETURN d.frequencyMERGE (p:Person)-[r:TAKES]->(d:Dose)SET d.dose1Time = duration(‘PT093000’)RETURN d.dose1TimeWe will see how to convert these small, precise values into more meaningful amounts (like hours) in the next post. For now, we will simply deal with the math of turning durations into precise amounts and finding the difference between 2 date values.Calculations with Dates and DurationsThere are a few ways to go about using durations besides for the plain amounts we saw in the last section — for instance, adding or subtracting temporal amounts and calculating the difference between 2 dates. I would guess that these are probably the most common usages for durations. We will get some examples below of each, starting with adding or subtracting durations from dates.Example 1: Subtract 2.5 months from end date to calculate start dateMATCH (p:Project)SET p.expectedStartDate = p.expectedEndDate — duration(‘P2.5M’)RETURN p.expectedEndDate, p.expectedStartDateIn the example above, we are using a scheduled end date and an estimated duration of the project to calculate the project start date. I find that adding and subtracting lengths of time can be used for many different uses.Example 2: Add 30 hours to start datetime to calculate end datetimeMATCH (c:Conference)SET c.endDatetime = c.startDatetime + duration({hours: 30})RETURN c.startDatetime, c.endDatetimeAbove, we are using duration addition to calculate the end date/time for our conference. If we know what time the event plans to start and know the length of content we have, then this tells us what time the event can end.Example 3: Calculate when to take the next medicine doseMATCH (d:Dose)SET d.dose2Time = d.dose1Time + d.frequencyRETURN d.dose2TimeNOTE: we could do a quick manual calculation (48,600 / 60 seconds in minute / 60 minutes in hour = 13.5 hours after midnight), which would be around 1:30PM on a standard day. We’ll see how to do this with Cypher in another post, though.Based on the dose taken in a previous query and the frequency we can take the medication, the above query calculates the time (in seconds) of our next dosage.Calculate Difference Between 2 Dates with duration.betweenNow let’s calculate the difference between 2 dates. We will need to use duration.between to compare two dates and find the difference.Example 1: Calculate duration between project start and end datesMATCH (:Employee)-[rel:ASSIGNED]-(p:Project)RETURN rel.startDate as assigned, p.expectedEndDate as expectedCompletion, duration.between(rel.startDate, p.expectedEndDate) as lengthAssignedOur query above tells us how long an employee has been assigned to a project. This could be useful for determining resource usage or the number of hours someone has worked on something.Example 2: Calculate amount of time currently spent on projectMATCH (:Employee)-[rel:ASSIGNED]-(p:Project)RETURN rel.startDate as assigned, duration.between(rel.startDate, date()) as timeSpentUsing the query above, we can know how long our project has been going on. This tells us how much time has passed, and as with the previous query, can help us understand how many resources have been used on a project at a point in time. We will do that calculation in the next post! :)Example 3: Calculate duration between differing datetime valuesMATCH (b:BlogPost)RETURN date(‘2021–03–22’) as started, b.publishedDatetime as published, duration.between(date(‘2021–03–22’), b.publishedDatetime) as amountOfTimeSpentIn our above query, we can find out how long it took to write a blog post. Again, this could be used to determine average time consumption for a person or for planning time needed on future posts. Because the time and timezone exist on the publishedDatetime and not on the start date we set, the duration second and millisecond values are a little odd-looking, but we’ll see how to format that better in the next post!Wrapping Up!We saw how to use Cypher’s duration to measure distance in time — whether starting from a length like 2 days, adding or subtracting an amount from a date or time, or finding the difference between 2 dates/times. While some of the amounts returned converted to more precise (and maybe less meaningful) values, we found that this occurs on purpose to ensure consistent amounts that can be relied upon no matter the time of year, geographic location, or the time changes observed.In the next post, we will cover formatting and date components in more detail. We will see how to transform these precise values into more meaningful values, as well as how to access components of full date/time values and translate other temporal amounts into different units (i.e. 120 seconds into 2 minutes, or 72 hours into 3 days). Tune in next time and happy coding!ResourcesCypher manual: DurationsCypher manual: Create durations from units or from dates and timesCypher manual: Compute duration from 2 temporal instantsNeo4j sandbox: Test out date/time on a free instanceBlog post: Part 1 of Cypher Sleuthing with Dates;Apr 26, 2021;[]
https://medium.com/neo4j/connect-to-neo4j-with-php-e10e24afedff;Ghlen NagelsFollowJul 7, 2021·4 min readConnect to Neo4j with PHPLeverage the power of Neo4j from your PHP applications with the new Neo4j PHP clientPHP has been lacking a fully-featured and actively maintained client for a few years. As the maintainer of the new library, I am happy to announce the🎉 Neo4j PHP client!🎉The client features:Multiple, easily configurable driversAn intuitive APIExtensible architectureDesigned, built, and tested under close supervision with the official Neo4j driver teamValidated with testkitFully typed with psalmSet up your drivers/clientFirst off: Let’s install it with composer!Installation scriptA standard Neo4j deployment understands two protocols: HTTP and Bolt. All drivers have the same API and are determined by the scheme of the URI. The HTTP and Bolt schemes each act on these protocols. The Neo4j scheme enables auto-routing of queries when connecting to a cluster of Neo4j nodes over Bolt.Driver creation examplesThis library goes one step further and consolidates all drivers in a single object: a client. A client simply contains a set of drivers with an alias and routes all queries to the default alias if you don’t provide one as a parameter.Client creation exampleRunning queriesThere are three ways of running queries using the library:Transaction functionsAuto committed queriesUnmanaged transactionsUsing transaction functionsWhen working with high availability solutions such as a cluster or Neo4j Aura, transaction functions are preferred. Transient errors are likely to occur in this environment. The driver will automatically retry the transaction function in these cases.Transaction functions also allow specifying intent. Read or write transaction functions will automatically be routed to a correct node in the cluster when using the Neo4j scheme.A word of caution: The driver automatically reruns the transaction functions. Therefore, they should always produce the same result on reruns (idempotency). This is because if a function produces a side effect and a transient error occurs afterwards, the side effect will occur twice.Transaction function examplesUsing auto committed queriesAuto committed queries are the most intuitive and straightforward way to send queries. There really isn’t much to talk about here. You send a query, and it gets executed within its own self-managed transaction. You, the user, are responsible for handling all errors.Note: Transactions, clients, and sessions allow sending multiple statements at once. This feature is vital when using the HTTP protocol, as it drastically reduces network traffic.Auto committed queries exampleUsing unmanaged transactionsAn unmanaged transaction allows for the most significant degree of control. It is also the most error-prone because you are still responsible for handling all errors. On top of that, you decide when to commit or roll back all your queries.Unmanaged transactions exampleClient, driver, sessions, and transactionsIn the above examples, we have always used a client. A client manages drivers. A driver creates a session, which is a lightweight container for chaining transactions, the atomic units of work. This structure is publicly available in the code:Client architecture in actionNote: A driver does not support immediate access to run or transaction methods. These are only accessible through a session. The client does these method calls for you.Result structureResult formats are fully configurable. Our next blog post will be all about that. The default result structure is that of rows and columns represented by CypherList and CypherMap, respectively. On top of that, objects in the Cypher protocol map to a PHP class counterpart. For example, a node in cypher maps to a \Laudis\Neo4j\Types\Node. You can find a complete list here.Result formatReal world exampleWe’ve created a real-world example of the Neo4j movies project on GitHub. You can find it here.The example is as minimalistic as possible. It works on top of the super simple framework slim. The routing and code are straightforward and it’s all in one file: index.php.Wrapping UpThis blog post was a birds-eye view of the drivers and clients. If you find any issues, please feel free to open one at GitHub. We love to collaborate with people all over the world.These are the core takeaways:You can either construct a client or driver to connect to Neo4jResults are in rows and columns formatThere are HTTP, Bolt, and Neo4j drivers availableThere are transaction functions, auto committed queries, and unmanaged transactions at your disposalNow, please go forth and leverage the power of Neo4j in your PHP applications!🙏 Special thanks to: 🙏Michal Štefaňák for its excellent Bolt library we use under the hoodDavide from Larus for setting up GitHub actionsAbed for developing the OGM with me;Jul 7, 2021;[]
https://medium.com/neo4j/how-to-create-a-knowledge-graph-from-your-slack-archive-with-neo4j-6a1a1d59a535;Adam CowleyFollowAug 28, 2020·14 min readHow to create a Knowledge Graph from your Slack Archive with Neo4jHow to turn your Slack messages into a source of insight and information about conversations, topics, experts and questions using graph queries and analytics.TLDR: All of the code is available on Github. Follow the steps to export your messages and load them into Neo4j using the import scripts.If you want to watch me working on this, you can check out this week’s twitch stream.Earlier in the year, myself and other members of the EMEA Professional Services team put together our own Enterprise Knowledge Graph based on information from our internal Slack Workspace, supplemented with additional information from other sources including Google Drive, Github and Salesforce.Not the original slack UI but our own clone, based on the graph of slack dataThe original blog post published in April 2020 went into detail about the why’s and the what’s, but I thought it was about time to publish some information on the how and provide some information on how you can get started building you own.Original Blog Post Discovering Hidden Skills with an Enterprise Knowledge Graph”What is a Knowledge Graph?There is a lot of hype and mystery surrounding the term Knowledge Graph, and there are people who will be able to articulate what they are, much more eloquently, so I will leave it to them.To me, Knowledge Graphs are just Graphs — a set of entities linked together to form a taxonomy that represents some kind of knowledge. Commonly, these taxonomies are learned through unsupervised Machine Learning techniques.In our case, we used Natural Language Processing to create a taxonomy of Skills, Programming Languages, Frameworks ,and Technologies from the unstructured text contained in messages, and the structure of the data around that message.For example, if a user frequently mentioned a particular term within a channel that was dedicated to a particular skill, we could infer with some assurance that the User had knowledge of the term. If a user is active within a channel dedicated to a particular client or project, they would be a good candidate for anyone who has questions about the project.By supplementing that information with data from other sources, for example Wikidata entries or StackOverflow Tags, we could take a term like ‘React’ and place it in a hierarchy under Javascript and Frameworks. Someone who is an expert with React may also be able to answer questions about Javascript in general, or more specifically about ES6 or Typescript.In this article I will outline some of the basic steps that you can take to import the data, then give a short introduction to terms can be extracted with NLP techniques.Obtaining the DataAny Admin user for your Slack workspace can export. A comprehensive guide to exporting data can be found on the Slack Help Centre but the TLDR is that you can request an export by clicking on the Workspace name in the top left-hand corner of the Slack UI, then going to Settings and Workplace Settings (or heading to <your-workspace>.slack.com/admin/settingsIf you have the correct permissions, you should see an Import/Export data button to the right of the Settings & permissions header. On the Export data page you can request an export for the last 24 hours, 7 days, 30 days, a specific date range, or the entire history.The export can take some time but you should receive an email when everything is ready for you.A list of previously requested exports will be listed on that page and ready for you to download as a zip archive.Zip Archive StructureThe zip archive contains a set of JSON files organised into folders which represent their channel. The root also contains some additional JSON files which include information on the workspace, most importantly the list of Channels (channels.json) and Users (users.json).Channel information — channels.jsonThe channels.json file contains an array of JSON objects, each representing a channel.An example channel may look a little like this:{   id :  C08J11DC6 ,   name :  help-cypher ,   created : 1438676304,   creator :  U08J18KHB ,   is_archived : false,   is_general : false,   members : [     U08J18KHB ,     U08J1KK9T ,     U08J2HAE7 ,     U08J2TL6Q ,  ],   pins : [    {       id :  1456911012.000419 ,       type :  C ,       created : 1456911025,       user :  U08J18KHB ,       owner :  U08J18KHB     },  ],   topic : {     value :  Ask Cypher Questions in <https:\/\/community.neo4j.com\/c\/neo4j-graph-platform\/cypher> ,     creator :  U08J18KHB ,     last_set : 1535300193  },   purpose : {     value :  Help with all questions around Cypher for LOAD CSV.  For more questions &amp support -&gt Neo4j Community Forums <https:\/\/community.neo4j.com\/c\/neo4j-graph-platform\/cypher> ,     creator :  U7X490V7X ,     last_set : 1535300190  }}The information varies slightly by channel but the important features are:id: Slack’s Unique ID for the Channelname: The name given to the channelcreated: The time that the channel was created (in seconds since Epoch)members: A list of IDs that correspond to the members of the channel (as listed in users.json)User information — users.jsonusers.json contains an array of Objects which correspond to a member of the workspace. If we take Michael for example, you can see how the information relates to his public profile on the Neo4j Users Slack.{   id :  U08J18KHB ,   team_id :  T08J10GA2 ,   name :  michael.neo ,   deleted : false,   color :  9f69e7 ,   real_name :  Michael Hunger ,   tz :  Europe\/Amsterdam ,   tz_label :  Central European Time ,   tz_offset : 3600,   profile : {     title :  Caretaker General Community Neo4j ,     real_name :  Michael Hunger ,     real_name_normalized :  Michael Hunger ,     display_name :  michael.neo ,     display_name_normalized :  michael.neo ,  },   is_bot : false}I find it hard to believe that Michael isn’t a Robot.The majority of the interesting information is held in the root of the object, but there is also additional information contained inside the profile object.Messages — {channel}/{date}.jsonThe messages are split by date and organised into files by date. For example, to find the messages in the #announce channel on 14 August 2020, I would need to look for announce/2020-08-14.json . The message objects can vary wildly (especially the attachments), so I have made the conscious decision to only import the data that remains consistent:type (and subtype): Denotes the type of message — for example a regular message or a message to say that a user has joined or left the channel.text: Is the content of the message.ts: Timestamp (seconds since epoch) — This also seems to act as a unique ID for the message.reactions: Reactions may be useful to judge the sentiment for the message or even detect which users interact consistently with each other.attachments: The attachments array may contain links to helpful articles, knowledge base entries that answer the question, github repositories, etc.The first message in the neo4j-users workspace was:{   type :  message ,   text :  Hi <@U08J1KK9T> :simple_smile: ,   user :  U08J18KHB ,   ts :  1438677071.000003 ,   team :  T08J10GA2 ,   user_team :  T08J10GA2 ,   source_team :  T08J10GA2 ,   user_profile : {     avatar_hash :  g522bc835d74 ,     image_72 :  https:\/\/secure.gravatar.com\/avatar\/f522bc835d745f319a9567d63f5c0cc9.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0016-72.png ,     first_name :  Michael ,     real_name :  Michael Hunger ,     display_name :  michael.neo ,     team :  T08J10GA2 ,     name :  michael.neo ,     is_restricted : false,     is_ultra_restricted : false  }}Deriving a Data ModelSlack defines the majority of the terms for us already, Users are a member of a Channel. Members can post Messages to any Channel that they are a member of. Those messages can have Attachments. Members can reply to messages to form a thread and also react to messages using emojis.After a quick scan of the data I ended up with a model similar to the diagram to the left of this text.Importing the DataThe APOC library provides us with some useful procedures for importing data. In this case apoc.load.json() will provides the ability to load a JSON file into a Cypher statement. It takes a single argument, the location of the JSON (this can either be a remote URI — for example a REST API, or a local path prefixed with file:/// and relative to the import directory as set in neo4j.conf).CALL apoc.load.json($filename)YIELD value// Do something...For now, the huge import can be loaded from the export but in the future, it might make sense to load the deltas from the Slack API rather than constantly requesting exports.Note: The scripts below assume that the JSON files have already been copied into Neo4j’s import folder.If you are using a Neo4j Desktop instance, you can find the import folder by selecting ‘Manage’ from the menu at the top of the appropriate card within the Desktop UI, clicking the Open Folder button under the database name and navigating to the import folder, the top-level entry would be called slack with the exported files below.Importing UsersAs with all of the queries, this starts with a CALL to the apoc.load.json() procedure. Because the file contains an array of objects, these are streamed into their own row which can be accessed via value.CALL apoc.load.json( file:///slack/users.json )YIELD valueFirst, we can MERGE a node with a :User label —Cypher will attempt to find a node that corresponds to the pattern and if none exists, then it will be created. The SET step then uses a Map Projection to extract certain information from the root value object.MERGE (u:User {id: value.id})SET u += value { .name, .title, .color, .real_name }For certain boolean properties, it makes sense to assign these as a Label rather than a property. For example, assigning an :Admin label will allow us to quickly lookup all Admin Users in the Workspace instead of having to scan through all :User nodes and filtering on a property.For this, I have used the (admittedly old-school FOREACH hack) but you could just as easily use the apoc.do.when() procedure to execute conditional logic. This hack involves using a CASE statement to either return a collection with a single item if the condition is true, otherwise return an empty collection.The FOREACH statement then iterates over this collection — executing the containing statement once if the condition is true, otherwise not executing the statement at all.The following statement will check the is_admin property on the object and if true, set the :Admin label.FOREACH (_ IN CASE WHEN value.is_admin THEN [1] ELSE [] END | SET u:Admin)Additionally, it may be interesting to see how users interact across timezones. If the user has a timezone listed, we can extract that out into its own node and merge a relationship between the user and timezone nodes.FOREACH (_ IN CASE WHEN value.tz IS NOT NULL THEN [1] ELSE [] END |  MERGE (t:TimeZone {id: value.tz})  ON CREATE SET t.offset = value.offset, t.label = value.tz_label  MERGE (u)-[:IN_TIMEZONE]->(t))The full script is available on GithubImporting ChannelsImporting channels is similar, so I won’t go into detail. You can view the full script in the Github Repository.adam-cowley/slack-knowledge-graphContribute to adam-cowley/slack-knowledge-graph development by creating an account on GitHub.github.comImporting MessagesImporting messages is a little more complicated because of the way the files are split. Essentially, you’ll need to read the directory structure to extract the name of the channel, then pass this along with the filename as parameters to a Cypher query that contains a apoc.load.json() call.I’ve written a Node.js script to do this but you could also do this in [your language of preference].// Read directory to get a list of all directories in the pathconst files = fs.readdirSync(path)  // Convert each channel name into a full path  .map(channel => [channel, `${path}/${channel}`])  // Filter the array so only directories are included  .filter(([channel, path]) => fs.lstatSync(path).isDirectory())  .map(([channel, path]) => {    // Read directory to get the files for each day    const files = fs.readdirSync(path)      .map(file => `${path}/${file}`.replace(importDir, ))    return [channel, path, files]  })  // Run a reduction to produce an array of objects   // containing the channel and file location  .reduce((acc, [channel, path, files]) => acc.concat(    files.reduce((acc, file) => acc.concat({ channel, file }), [])  ), [])Once the directory listing has been generated, a while loop extracts the first item from the array until there are none left before awaiting the successful execution of a cypher statement before running the next:// Get the total number to calculate the percentageconst total = files.length// While there are items in the array, take the next item and execute the import cypher querywhile (files.length) {  const next = files.splice(0, 1)[0]  console.log(next, files.length,     `${(100 - (files.length / total) * 100).toFixed(4)}%`)  await session.run(cypher, next)}console.log(done)// Once all files have been processed, // close the driver to exit the processawait driver.close()The Cypher statement itself uses the $channel parameter to find the Channel node, then appends the filename ($file) to file:/// to load the correct file, and then merge the appropriate nodes and relationships.const cypher = `  MATCH (c:Channel {name: $channel})  CALL apoc.load.json(file://+ $file) YIELD value  WHERE value.user IS NOT NULL  MERGE (u:User {id: value.user})  MERGE (m:Message {id: value.ts})  SET m += value {    .type,    .subtype,    .text,    createdAt: datetime({epochSeconds: toInteger(value.ts)})  }  MERGE (u)-[:POSTED]->(m)  MERGE (m)-[:IN_CHANNEL]->(c)   //...`The full import script (available on GitHub) produces an output similar to the following, the last number representing the percentage of files processed so far:$ node index.js{ channel: announce, file: /slack/announce/2015-08-04.json } 27226 0.0037%{ channel: announce, file: /slack/announce/2015-08-05.json } 27225 0.0073%{ channel: announce, file: /slack/announce/2015-08-06.json } 27224 0.0110%{ channel: announce, file: /slack/announce/2015-08-07.json } 27223 0.0147%{ channel: announce, file: /slack/announce/2015-08-08.json } 27222 0.0184%{ channel: announce, file: /slack/announce/2015-08-09.json } 27221 0.0220%{ channel: announce, file: /slack/announce/2015-08-10.json } 27220 0.0257%{ channel: announce, file: /slack/announce/2015-08-11.json } 27219 0.0294%The full import script is available on Github.Extracting KnowledgeSo far you may be thinking, Sure, that’s a graph. But where’s the knowledge?” and that’s a fair question. We could run a simple query to find out who is the most active poster:MATCH (u:User)RETURN u.name, size((u)-[:POSTED]->()) AS sizeORDER BY size DESCIt was always going to be Michael…+----------------------------+| u.name             | size  |+----------------------------+|  michael.neo       | 23495 ||  chris.graphaware  | 8567  ||  andrew.bowman     | 8052  ||  maxdemarzi        | 5737  ||  tomasi            | 4759  |+----------------------------+Because a Node is aware of its relationships, Neo4j will return a Degree count” based on the relationship type rather than expanding any records to find this out.Or, say we have a question on Cypher, we could work out the best channel to ask the question and also which users to @mention:MATCH (u:User)-[:POSTED]->(m)-[:IN_CHANNEL]->(c)WHERE m.text contains cypherRETURN u.name, c.name, count(*) AS messagesORDER BY messages DESC LIMIT 5+--------------------------------------------+| u.name          | c.name        | messages |+--------------------------------------------+|  andrew.bowman  |  help-chat    | 168      ||  michael.neo    |  help-chat    | 149      ||  maxdemarzi     |  help-chat    | 116      ||  michael.neo    |  help-cypher  | 93       ||  michael.neo    |  neo4j-apoc   | 70       |+--------------------------------------------+If I had a question on Cypher, Andrew would 100% be my first port of call.But these queries would be trivial in most databases. How about we find out the most popular users in the Graph? The golden rule is that relational databases start to slow down when querying with more than 3 joins.MATCH (u:User)-[:POSTED]->(m:Message)<-[:TO_MESSAGE]-(r:Reaction)<-[:REACTED]-(u2)WHERE u.name <> michael.neo // Lets exclude MichaelRETURN u.name AS name, count(r) AS reactions,        count(distinct u2) as reactersORDER BY reacters DESCLIMIT 5+-------------------------------------------+| name               | reactions | reacters |+-------------------------------------------+|  maxdemarzi        | 718       | 324      ||  andrew.bowman     | 531       | 238      ||  chris.graphaware  | 577       | 234      ||  tomasi            | 226       | 112      ||  lyonwj            | 214       | 91       |+-------------------------------------------+Max has received 718 reactions from 324 different users. This goes to show how useful Max’s contributions are to the wider audience as a whole.Identifying n-grams with CypherOne of the more basic NLP techniques is to extract n-grams, sequences of text (or tokens) of n length, an n-gram with a length of 1 is a bigram, 2 are known as bigrams, 3 are known as trigrams, and so on. The higher the occurrence of an n-gram, the more interesting it becomes.We can identify n-grams using Cypher and APOC using apoc.text.split() — a function that splits text by regular expression. For example, calling apoc.text.split(abc.def ghi, jlk|mno”, \\W+”) will return an array of [abc”, def”, ghi”, jlk”, mno”].Some frequently mentioned terms may also be uninteresting, for example terms like greetings like hello there, hey there or in my case hello mate. We can exclude these by removing stopwords. For now, let’s assume that there is a Cypher parameter called $stopwords to represent these words as an array of strings.A Cypher query to extract bigrams (n-grams with a length of 2) may look like the following:MATCH (m:Message) WHERE not exists(m.subtype) AND m.text IS NOT NULL// Split words into sentencesUNWIND split(toLower(m.text), .) AS sentence// Split sentences into wordsWITH m, apoc.text.split(sentence,  \\W+ ) AS words// where the sentence is more than 3 words longWHERE size(words) > 3   // and all words are between 4 and 20 chars and not a stopword  AND all(w in words WHERE 4 < size(w) < 20 AND NOT w IN $stopwords)  // ensure there are no duplicate words (eg. really really)  AND NOT apoc.coll.containsDuplicates(words) // Produce a sliding window of results of 2 tokensUNWIND range(0, size(words) - 2) AS startWITH m, words, start, start + 2 AS end// For each n-gram, get the number of occurrences // and return the top 100 resultsRETURN words[start..end] AS ngram, count(*) AS occurrencesORDER BY occurrences DESC LIMIT 100With these stopwords::param stopwords => [ hello ,  hi ,  it ,  its ,  its ,  append ,  thanks ,  http ,  https ,  next_tick ,  users ,  documents ]After filtering for products that start with Neo4j, we can see many rows that may signify a Neo4j product. By combining this data with another data source (say a CSV file of product names), we could quite easily build a product hierarchy.+---------------------------+| ngram                     |+---------------------------+| [ neo4j ,  community ]    || [ neo4j ,  shell ]        || [ neo4j ,  admin ]        || [ neo4j ,  desktop ]      || [ neo4j ,  googlegroups ] || [ neo4j ,  supports ]     || [ neo4j ,  friends ]      || [ neo4j ,  rstats ]       || [ neo4j ,  graph ]        || [ neo4j ,  lacks ]        || [ neo4j ,  though ]       || [ neo4j ,  spatial ]      || [ neo4j ,  letmein ]      || [ neo4j ,  surveillance ] || [ neo4j ,  password ]     || [ neo4j ,  import ]       || [ neo4j ,  contrib ]      || [ neo4j ,  encrypted ]    |+---------------------------+By spotting users who are consistently asking questions around a particular topic, an astute salesman could try and up-sell a training course or a Slackbot could analyse the message and suggest potential Knowledge Base articles or Blog Posts which may help answer the user’s question.Next StepsThese queries so far only scratch the surface of what is possible. Third-party NLP libraries like Stanford NLP, OpenNLP or services such as the GPC Cloud Natural Language API could be used for more intelligent Entity Extraction. If you are more interested in the subject, head over to the Developer Guide on NLP with Neo4j or check out the NLP Functionality built into APOC.If you are interested in combining additional data sources, then I’d recommend reading the guide on Importing Wikidata into Neo4j using Neosemantics.View your own data!As I’ve mentioned, all of the import scripts are available on Github. I have also created a copy of the Slack UI as a Graph App so you can view the data as if it was the real Slack app.The Graph App is built with Vue.js with and has also been added to the repository. You can install the Graph App to Neo4j Desktop using the following deeplink:neo4j-desktop://graphapps/install?url=https://registry.npmjs.org/@graphapps/slackThe public neo4j-users slack data is available in a read-only Neo4j database on demo.neo4jlabs.com with username/password/database slack, which you also can also connect to by choosing the ‘Connect to another graph’ option at the start screen of our slack app.Keep in TouchWe’d would love to hear how you are using Neo4j to build your Knowledge Graphs. Let us know by posting in the AI, ML, NLP category on the Neo4j Community Site.Happy slacking, Adam.;Aug 28, 2020;[]
https://medium.com/neo4j/neo4j-data-access-for-your-dot-net-core-c-microservice-8dbf8a8d8a79;Chintan DesaiFollowJun 20, 2022·4 min readNeo4j Data Access for Your .NET Core C# MicroserviceAn approach to effectively use the Neo4j C# .NET driver and develop the data access layer for a .NET Core applicationWhy Graph Databases?Each node represents an entity (a person, place, thing, category or other piece of data), and each relationship…neo4j.comNeo4j is the industry leader and pioneer of graph database technology and is currently the top-rated graph database management system in the world.In my experience so far with designing solutions and applications around Neo4j, I came across many materials and online resources from Neo4j and its huge community. There are plenty of resources on Neo4j around various technologies. Neo4j provides excellent drivers for various programming languages to build seamless graph data connectivity with your application.Drivers & Language Guides - Developer Guidesneo4j.comC# is my first love as a programming language, and Visual Studio development experience will always be my preferred one. I was brought up as a programmer in the .NET and C# world. Hence, I decided to share an approach to effectively use the Neo4j C# driver to develop the data access layer for a .NET Core application.I am going to share an example of a Neo4j-based Microservice (Web API). However, with out-of-the-box dependency injection (DI) and Service Provider features from .NET Core, this approach can be used for other types of applications, too.Before you start with .NET Core implementation, I suggest you read a particularly good article by David Allen on Neo4j driver general best practices. These practices apply to all programming languages.General Neo4j Driver Best PracticesIn this article, we’ll cover some best practices for using Neo4j drivers in your application.medium.com.NET Core Specific ImplementationFor developing a Neo4j Data Access for .NET Core, I have divided the approach into the steps you’ll see below.Have a Neo4j Database Instance Up and RunningIf you already have a Neo4j database instance set up and running, you can skip this step. If not, you can easily set up a free Neo4j Aura (Database as a Service) in a hassle-free way. Use the below link to proceed.Neo4j Aura - Fully Managed Cloud SolutionAuraDB Free For small development projects, learning, experimentation and prototyping. Start Free AuraDB Professional…neo4j.comAlternatively, you can use Neo4j Desktop on your local machine or a Neo4j Sandbox.While loading the Aura instance or Sandbox, use the prebuilt Movies dataset. If you are using Neo4j Desktop or a plain database, run command :play Movies in your Cypher command bar and follow the data load step to load the test data.Add the Neo4j Official Driver NuGet Package to Your .NET Core ProjectYou can install the driver package using the NuGet package manager console or package manager interface in Microsoft Visual Studio. Install this package to the data access project. I suggest using a separate project for data access instead of the main API or application project.Neo4j.Driver 4.4.0The official .NET driver for the Neo4j Graph Database over the Bolt protocol.www.nuget.orgApplication SettingsThen go to your app settings file: Neo4j Bolt/Neo4j connection string, Neo4j username, password, and database instance name.You should implement additional security measures for protecting your Neo4j password instead of specifying it in plain text. For example, try an encryption of your password text, or use some vault to keep your credentials safe.Startup: Dependency Registration / Dependency Injection for Neo4j DriverWe are injecting the Neo4j Driver instance as a singleton so that we do not have to manage the driver instance on each request.IAsyncDisposable implementation of Neo4j Data Access ClassIAsyncDisposable interface provides a mechanism for releasing unmanaged resources asynchronously. We injected the Neo4j Data Access class as a scoped dependency. Neo4jData Access Wrapper will establish a database session. We will manage the Neo4j session using IAsyncDisposable.I have developed the Wrapper methods keeping easy implementation and reusability in mind. You are free to develop your own version of Wrapper methods.Data Access InterfaceData Access ImplementationDomain Repository Methods for Your Cypher Query and Parameter PrepYou may notice I used the Cypher statement RETURN p{ .name, born: p.born } in my method to return Person data. Cypher supports a concept called map projections,” which allows for easily constructing map projections from nodes, relationships, and other map values as shown in the below screenshot.Maps - Neo4j Cypher ManualThis section describes how to use maps in Cyphers. The following graph is used for the examples below: Cypher supports…neo4j.comWith Cypher projections, I populated List<Dictionary<string, object>> collection. If your API just needs to provide plain results without additional manipulation, you can simply return this collection. This helps you avoid extra boxing and unboxing. Clients can parse the JSON to their desired object types.For example, below is the swagger response for my API that returns plain data.Thanks for spending the time to read this. I hope you find it helpful!Please share so it reaches more people!Full Source Code for this sample project is hosted on below public repo:https://github.com/chintan196/DotnetCore.Neo4j;Jun 20, 2022;[]
https://medium.com/neo4j/covid-19-with-graphxr-and-neo4j-16ea37686ca5;Alex LawFollowMar 25, 2020·3 min readCOVID-19 with GraphXR and Neo4jThese days I find myself spending a lot more time in front of my laptop. It’s a source of anxiety that bombards me with constant updates on the COVID-19 pandemic. It’s a lifeline to my socially distanced friends, family, and colleagues. And it presents me with the opportunity to learn some new tools to try and make sense out of this situation. If an extrovert can adapt to quarantine, can a dancer become an analyst?My goal was to understand how we got here or, more specifically, how SARS-CoV-2 spread to so many countries in such a short time.I started by searching different websites for COVID-19 statistics, including this one created by 17-yr old Avi Schiffmann in Washington. I end up at the Worldometer’s Coronavirus page. It provides a daily report of new cases and deaths alongside the source news article. Sifting through the headlines, I decided to compile cases caused by cross-country travel from the available reports to create a travel log for COVID-19.Transcribing the data was time-consuming but straight forward. Taking my first step into data visualization, having recently joined the team at Kineviz, was more daunting. You can follow my process here. Of course, there was a bit of trial and error along the way the tutorial is just the parts that worked. The end result looks like this:The nice thing about this process is it didn’t require any prior experience with GraphXR or expertise writing Cypher queries. By the end, I was able to spot a few trends.First, COVID-19 may have started in China, but many countries became hubs for spreading it. Around February 21st, Italy and Iran became particularly prominent hubs.Second, it’s apparent that COVID-19 reached the same countries from different origins. For instance, cases in Sweden have been attributed to travelers coming from China, Italy, Germany, and the US. In a very real way, we are all in this together.To validate my conclusions, Weidong Yang wrote the following queries. You can run Neo4j algorithms like PageRank from the cypher query window in GraphXR:Link Data for PageRankMATCH (n:Country)-[:from_country]-(t:Transport)-[:to_country]-(m:Country)WHERE 2020–02–20 < t.date < 2020–03–01MERGE (n)-[:transport_to_s0 {date:t.date}]->(m)Graph PageRankCALL algo.pageRank(Country, transport_to_s0,{direction: ‘INCOMING’, write:true, writeProperty:”pagerank_s1 })Ultimately, seeing COVID-19 this way raises more questions than it answers.How does it spread within a nation’s borders?Are some medical systems better equipped for this kind of crisis than others?How do infection rates compare amongst regions with or without the ‘shelter in place’ initiative?What role does population density play?During GraphHack 2020, Kineviz will support anyone investigating COVID-19 by curating data via our GitHub repository:https://github.com/Kineviz/graphs4good-covid19It’s hard to find words adequate to describe what’s going on in the world today. At least now I have a picture to help me understand it.Alex Law is a Communications Coordinator at Kineviz. Her background combines the sciences (Bachelors in Human Biology from UCSC) and the arts (Pilot Artist at ODC/ Production Manager for DanceHack 2020) as well as experience in public healthcare and administration at Navigant.;Mar 25, 2020;[]
https://medium.com/neo4j/exploring-small-world-phenomena-with-neo4j-58c767dcbafd;Nathan SmithFollowJun 3, 2020·6 min readExploring small-world phenomena with Neo4jPhoto by Yulissa Tagle on UnsplashWhen my sister moved to a new city and met her neighbors, she found that her neighbor’s grandparents and our grandparents had been good friends in a different city two generations ago. It’s fun to discover unexpected connections like that. Network theory tells us that the paths connecting members of a network are often shorter than we might expect.If everyone in a network knows k other people, we might naively assume that when we start from a person and search n hops out we would find kⁿ people. With exponential growth, it wouldn’t take long to find a path from any given person to everyone else in a graph.However, in real social networks, many of the people a person knows also know each other. This overlap among friends of friends decreases the number of new people I can access with each hop away from a starting node. It might be difficult to find a paths that begin in a tight-knit community and branch out into remote parts of the network.In chapter 20 of the book Networks Crowds and Markets, authors David Easley and Jon Kleinberg provide a theoretical framework for how small-world phenomena can arise in the real world. The theory combines the ideas of homophily, where similar people cluster together, and weak ties, where relationships branch out across a network. The explanation is based on the work of Duncan Watts and Steve Strogatz. We can follow along with code examples in Neo4j.To execute the code below, sign in to sandbox and select New Project.” Choose Graph Data Science” from the starter projects offered.The sandbox comes loaded with data about Game of Thrones that we don’t need for this exercise. Let’s delete it.MATCH (n) DETACH DELETE nNow we set up Person nodes in a 30 by 30 grid. We give each person row, column, and coordinates properties.UNWIND RANGE(1,30) AS rowUNWIND RANGE(1,30) AS columnCREATE (p:Person {row:row, column:column,      coordinates:toString(column) + ,  + toString(row)})Create KNOWS relationships between each person and the person to their right in the grid.MATCH (p1:Person), (p2:Person)WHERE p1.row = p2.row     AND p1.column = p2.column - 1CREATE (p1)-[:KNOWS]->(p2)Create KNOWS relationships between each person and the person directly below them in the grid.MATCH (p1:Person), (p2:Person)WHERE p1.column = p2.column     AND p1.row = p2.row - 1CREATE (p1)-[:KNOWS]->(p2)Create KNOWS relationships between each person and the two people diagonally adjacent to them in the row below them.MATCH (p1:Person)-->(p2:Person)WHERE p1.row = p2.row-1WITH (p1), (p2)MATCH (p2)--(p3)WHERE p3.row = p2.rowCREATE (p1)-[:KNOWS]->(p3)Now we have a nice lattice pattern. Select a Person and their neighbors to see how the relationships work.MATCH (p:Person {row:5, column:5})--(n) RETURN *Neighbors of person with coordinates 5, 5I see a lot of triangles in that picture. Let’s use the graph data science library to figure out what proportion of a person’s friends are also friends with each other. First we create a named graph in memory.CALL gds.graph.create(base-grid, Person, {KNOWS:     {orientation: UNDIRECTED }})Now, run the triangle count algorithm.CALL gds.alpha.triangleCount.stream(base-grid) YIELD  coefficientRETURN avg(coefficient) AS averageClusteringCoefficientClustering coefficient resultsThe result of 0.44 tells me that on average, just under half of the people that a person knows also know each other. That level of overlapping neighbors might lead to longer path lengths than we might expect in a network where most nodes have degree eight.Let’s test that idea. Call the allShortestPaths procedure to find the shortest path between each pair of nodes in the graph. We’ll summarize the results.CALL gds.alpha.allShortestPaths.stream(small-world)YIELD sourceNodeId, targetNodeId, distanceRETURN max(distance) AS maxDistance,      min(distance) AS minDistance,      avg(distance) AS avgDistance,      count(distance) AS pathCountResults of allShortestPaths algorithmI was impressed that my free sandbox server could calculate 809,100 paths in about one second. The diameter of the graph is the longest shortest path between nodes. The diameter of 29 hops for this graph makes sense. If I have a node in column 1, and another in column 30, the most direct path across the grid would take 29 hops. No nodes can be farther apart than those at the opposite edges of the grid.Let’s look at the distribution of the path lengths.CALL gds.alpha.allShortestPaths.stream(base-grid)YIELD sourceNodeId, targetNodeId, distanceRETURN distance, count(*) AS pathCountORDER BY distanceHere’s a plot of the results.Histogram of path lengthsNow let’s modify the graph by adding random relationships between nodes that are not adjacent in the grid. You can think of these weak ties as a few people you know who fall outside your main circle of friends.We’ll use the apoc.periodic.iterate procedure to loop over all the nodes and add two nonadjacent relationships for each node.CALL apoc.periodic.iterate(     MATCH (p1) RETURN p1,     MATCH (p2)      WHERE p1 <> p2 AND NOT EXISTS((p1)--(p2))      WITH p1, p2, rand() AS rand          ORDER BY rand LIMIT 2      MERGE (p1)-[:KNOWS {adjacent:False}]->(p2),     {batchSize:1})Let’s take another look at the Person with coordinates 5, 5. She now has ten connections. Eight of them are adjacent in the grid, and two new connections are nonadjacent.MATCH (p:Person {row:5, column:5})--(n) RETURN *Neighbors of person with coordinates 5, 5Let’s create a new graph in memory to analyze with the Graph Data Science Library.CALL gds.graph.create(small-world, Person,      {KNOWS:{orientation: UNDIRECTED }})What’s the average clustering coefficient for the new graph?CALL gds.alpha.triangleCount.stream(base-grid) YIELD  coefficientRETURN avg(coefficient) AS averageClusteringCoefficientClustering coefficient resultsIn the new graph, on average 18% of the people that a person knows also know each other. With two new friends per person and less overlap among the friends of friends, we should find shorter paths between nodes.Let’s rerun the allShortestPaths and check the statistics.CALL gds.alpha.allShortestPaths.stream(small-world)YIELD sourceNodeId, targetNodeId, distanceRETURN max(distance) AS maxDistance,      min(distance) AS minDistance,      avg(distance) AS avgDistance,      count(distance) AS pathCountResults of allShortestPaths algorithmThe diameter of the graph has dropped from 29 to 5, and the average distance is just over 3. It really is a small world!Let’s look at the new distribution of path lengths.CALL gds.alpha.allShortestPaths.stream(small-world)YIELD sourceNodeId, targetNodeId, distanceRETURN distance, count(*) AS pathCountORDER BY distanceA very small percentage of nodes are five hops apart. A big majority of the shortest paths are now three or four hops long.We started from a network where everyone knew only their immediate neighbors. By adding a few non-adjacent connections for each node, we dramatically reduced the graph diameter and the average shortest path length. You can rerun the code with more or fewer nonadjacent relationship to see how it changes the results. Easley and Kleinberg show that the small world develops even if not every person has nonadjacent relationships. Creating single nonajacent relationships for a subset of the person nodes will still significantly reduce the path lengths between nodes.The rest of the chapter in Networks, Crowds and Markets extends these concepts in interesting ways. I encourage you to read it and check out my other posts illustrating ideas from the book with examples in Neo4j.;Jun 3, 2020;[]
https://medium.com/neo4j/showing-charts-for-neo4j-query-results-using-amcharts-and-structr-efae0b7a04f0;Dana CanzanoFollowMar 14, 2019·6 min readShowing Charts for Neo4j Query Results using amCharts and StructrLearn how to render a bar chart based upon the example movies graphOver my last 2 blog posts, Zendesk to Neo4j Integration and Zendesk to Neo4j Integration : Better control over your reporting needs and building a UI, I have detailed a project I have been working on to export data from Zendesk and then build a dashboard using Structr. To date most of the output is displayed with a number of traditional sortable HTML tables. This in itself provides access to the data, but has a very raw look to the the data.The next step is to enhance the display to add useful charts by using components developed by amCharts. This document will describe the steps taken to build a very simple vertical bar chart based upon the :play movies database provided with Neo4j.And although the example below is constructed within the Structr framework, if you are not using Structr, one could have just as easily done this using the Neo4j Bolt Javascript driver.The examples provided below are built using the following software and respective versionsNeo4j 3.5.3Structr 3.0.2amCharts4And to which the final result is a chart similar toThis bar chart is representing the Top 5 years when movies were released. Both the chart and the table are produced by running the Cypher queryMATCH (n:Movie)WITH toString(n.released) as yr, count(n) as MovieCountORDER BY MovieCount DESC LIMIT 5RETURN {year: yr, NumOfMovies: MovieCount}amCharts is is an open source Javascript library for data visualization and provides many charts (bar charts, pie charts, line and area charts, etc). Their website has numerous demos which show the Javascript code used to populate the charts.Once you find the chart type you want to embed into your website or application, it’s simply a matter of understanding the format of their chart.dataobject and then constructing a Cypher statement that can produce similar results.For example, for a Simple Pie Chart the chart.datais of the form[{<property name>:<property value>,<property aggregate name>:<value of property aggregate>}]for examplechart.data=[{‘country’: ‘USA’, ‘param1’: 42 }, {‘country’:’Canada’, ‘param1’: 33 }]And this would produce a Pie Chart with 2 slices where the slice for USA would represent 56% and Canada would represent 44%.If you know Cypher, generating the necessary data sets is trivial.For the chart of the Top 5 Years in which movies were released, the following steps were performed:Install and start an instance of Neo4j and create the Movie graph/dataset with the create script in the :play movies guide.Install and start Structr.Download amCharts4. For my install I choose to download the amcharts_4.2.1.zip from Download a standalone Javascript Version. I then unzipped the file to c:\amchartsConnect to Structr at http://localhost:8082/structr and a choose menu choice Files. Under the File Explore View, and under the top level directory of assets, click [Add Folder] and name the folder amcharts. Click the amcharts folder. Drag the following files (core.js, charts.js maps.js) from the amcharts install path into the Structr UI and the amchart folder. The results should then appear asRepeat the process and under the assets/amchartsfolder structure click [Add Folder] and name the folder themes. Click the themesfolder. Drag all of the files from the amCharts install path, themessubdirectory into the Structr UI. This should then result in the following UIUsing the Structr UI, click menu choice Pagesand click the Import TemplateI then choose to ..or fetch Page from URLand entered https://getbootstrap.com/docs/4.0/examples/dashboard/. This then creates a page in Structr that appeared asThis was done to establish the base look and feel for the page I wanted. From here if you click on an HTML object in the right frame the tree view structure on the left will open to the raw HTML object. For the most part I then deleted many of the objects (i.e. the line graph, and the menu choices on the left frame of the right frame.To the remaining table, I made edits such that it would only have 2 column headers like thisThe trelement is then defined aThe 2 tdHTML objects are defined with a content element and defined as ${Results.year}and ${Results.NumOfMovie} respectively. At this point showing the page will submit the Cypher statement and display its results in a table.The next step is to include the bar chart. In this case we will define, a vertical bar chart, or as amCharts refers to it a XY Chart. To do so, within Structr create a new div HTML element like thisThe name of the idis used in the next step.Under the divcreate a new HTML script element and define its content asNote most of this code is similar to what is described at https://www.amcharts.com/demos/stacked-column-chart/.This line creates the chartvar chart=am4core.create( chart-container ,am4charts.XYChart)The first arguement refers to the id of the div element created in the prior step, and the second describes the chart type.Further down, we define and submit the Cypher query. It should be noted that with Javascript if you have a multi-line Cypher statement you need to end each line with a \. Also if you need to include a where clause which qualifies on a string then you need to escape the single quote character with a leading \. Thus where n.status=deleted needs to be written as where n.status=\deleted\Finally define 4 script HTML elements and with their srcattribute respectively defined as/assets/amcharts/core.js/assets/amcharts/charts.js/assets/amcharts/maps.js/assets/amcharts/themes/animated.jsThe first should appear similar toThe treeview structure in the Structr UI should now appear asNow that this is complete your page in Structr should now display asIn summary, this is but a very simple example to add a chart to your webpage and have it constructed from the results of a Cypher statement.One of the benefits of using amCharts is that it provides a number of different chart types and provides for animation whereby hovering over a bar will show more details. Further I can define multiple stacked charts in a single view and then toggle on/off each stack.;Mar 14, 2019;[]
https://medium.com/neo4j/bloom-ing-marvellous-a2be0c3702bb;Ljubica LazarevicFollowMay 19, 2020·6 min readBloom-ing marvellous!Neo4j Bloom 1.3 is now available. With an array of new features, it’s also available for free in Neo4j Desktop!Finn-E on unsplashBloom 1.3 is out! In this post, we are going to check out some of the new features, as well as having a run-through of the cool Bloom things!Neo4j Bloom — now available for everybody!This is the headline feature: Prior to 1.3, Bloom was available only via subscription.This has now changed, and everybody now has access to Bloom via Neo4j Desktop. With this new version of Bloom, you will be able to explore data on a local Neo4j instance, as well as customising your perspective for your needs.Launching Bloom from Neo4j DesktopYou can get started right away. Check that you have the latest version of Neo4j Desktop, and you will be able to launch Bloom on a local database.Not quite ready to download Neo4j Desktop? No problem at all — come explore Bloom on our Neo4j Sandbox - no download necessary. If you require something more permanent (sandboxes only last for days), you can also use Bloom on Neo4j Aura in the cloud.New to Bloom?Welcome! Here’s a great place to start, as we highlight all of the new, recent features in Bloom below, you will start to get a feel for what you can do with this impressive visualisation tool. We will also outline some great content to get you up to speed.Here is the Bloom video from Connections:Bloom Video from Neo4j Connections event for Graph Data ScienceYou can also check out our developer guide that walks you through the whole Bloom experience.One of the many developer guides to get you started quicklyFind stuff quicklyBloom is a graph data visualisation and exploration tool. Rather than using Cypher to interrogate your data, Bloom allows you to use near-natural language to ask questions instead.When you start up your database, Bloom does a lot of initial investigation to understand the structure of your data and naming conventions used. It goes a step further to try and accommodate friendly versions of naming conventions used.For example, a CONNECTED_TO relationship type can be referred to as connected to , you can skip relationship types and node labels altogether (this is known as filling the gap), you can even specify property names and values without saying which node they belong to, provided indexes are in place.The output of a Bloom phrase looking for cities that have hosted more than one Olympic game (truncated data set), described in the tips and tricks video/blog post (Bloom phrase: City Games City)Although produced on a previous version of Bloom, you can check out some searching tips and tricks from talk of mine, or this associated write-up.Bloom Tips & Tricks talk from GraphConnect 2018A new, recent feature in Bloom now also supports case-insensitive searching. Now you can explore your data without worrying about upper and lower case. Do make sure you have indexes on any properties you would like to explore.Putting it into perspective — maximising how you visualise your dataOne of the most powerful aspects of Bloom is being able to set up the display so that your users have something that is intuitive to understand, easy to explore and has low barriers to entry.You can customise Bloom by setting up the perspective. There are a number of things you can do to customise the perspective to your specific use-case:Colour-code labels and relationships — As well as choosing colours for your node labels, you can now also select colours based on relationship types.Use icons to help visually describe nodes rapidly — Select the most appropriate image for your node categories from an extensive library of icons.Adjust the size and shading of nodes and the thickness of relationship lines — There is now a great flexibility in how nodes and relationships are displayed. You can either set default node size and relationship thickness by label and type, you can also dynamically allocate colour shading and/or size based on property values. This is especially powerful when looking to do visual inspection on data processed with graph algorithms or complex queries. For instance represent the page-rank of a node by size and the community it belongs to by colour.Customise what labels, relationships and properties should be visible — As well as deciding what captions you want shown on nodes, for example, you can also also choose what properties you do not want shown. And, you can also hide node label categories and relationship types altogether!Encapsulate parameterised Cypher queries — Search phrases give you the complete flexibility of writing custom, parameterised Cypher queries, whilst labelling and summoning them with a user-friendly name and pass in parameters into a natural language phrase. You can now specify the parameter type, further enhancing this functionality.Check out that 70’s wallpaper!I recently did a live stream session in Twitch on building a Bloom perspective from scratch — you can follow along and build one too! We live stream on a range of topics. Check out the Twitch schedule.Sharing your findingsYou now have a couple of ways you can share your insights and findings from your Bloom exploration.Export as CSV — You can now export internal node IDs and relationship types for data displayed on the scene. This can be useful if you want to take a subset of data from your database for further analysis.Exporting all that biscuity goodnessFor example, the CSV for the above will look something like this:Export as CSV outputNodes as a Cypher query — Another cool thing you can do is copy selected nodes in the scene (using the handy keyboard shortcut of cmd/ctrl + c), and a Cypher query to find those nodes will be copied to clipboard. For example, if ‘Teatime Chocolate Biscuits’ and ‘Scottish Longbreads’ are selected from the scene above and copied using keyboard shortcuts, the following will be available:MATCH (a0:Product {reorderLevel: 5, unitsInStock: 25, unitPrice: 9.2, supplierID:  8 , productID:  19 , quantityPerUnit:  10 boxes x 12 pieces , categoryID:  3 , unitsOnOrder: 0, productName:  Teatime Chocolate Biscuits })MATCH (a1:Product {reorderLevel: 15, unitsInStock: 6, unitPrice: 12.5, supplierID:  8 , productID:  68 , quantityPerUnit:  10 boxes x 8 pieces , categoryID:  3 , unitsOnOrder: 10, productName:  Scottish Longbreads })return a0, a1Other ways to get BloomAs mentioned above, the easiest way you can get Bloom, and for free, is through Neo4j Desktop. This version will allow you to explore your local Neo4j instances.If you would rather not download any software, you can launch a time-limited Bloom sandbox, no installation required!Bloom is also now directly available on Neo4j Aura. You no longer need to access Aura with Neo4j Desktop to start Bloom. You can find out more about how to access Bloom from Aura here.Some of the different ways to access Neo4j BloomBloom server edition allows you to run Bloom independently from Desktop, and on remote, as well as local, instances of Neo4j. This is available under a paid license, and you can find out more from your friendly sales rep.Last but not least, if you are a start-up, you are entitled to a free copy of Bloom server edition. Find out more about our start-up program here.Where to learn more?Get going fast by checking out the Bloom user interface guide.Read up on specific features in the documentation.Need some help? Then come join us on the Neo4j Community site.;May 19, 2020;[]
https://medium.com/neo4j/how-i-put-the-world-map-in-a-graph-422b651780e9;Mihai RauleaFollowMay 15, 2018·8 min readHow I put the World(map) in a GraphHave you ever wondered what a graph of all the roads in the world would look like? What kind of amazing insights one would be able to uncover if such a graph would be put in a Neo4j database? Amazing information can also be unveiled if one could extract the most connected cities in Europe, and it would definitely come in handy for a vacation planning platform.Atlas was built to do all of the above, and more. The library can selectively import entities from Open Street Map, write them to a Neo4j database as such, or perform summaries of the data before commiting. The resulting graph can be simplified in a number of ways — such as contracting roads, smart linking and removing entities based on a plethora of criterias.Neo4j’s capabilities in terms of speed and ease of development are quite outstanding. The elegance and flexibility of Cypher, along with the newly capabilities of APOC begged for a system to be put together, that answers high-level questions about the world.** We interrupt this program for an announcement: i am back to graph database contracting. Grab the offer while it’s hot: https://bit.ly/2WD1hjc **Atlas: Answering Questions about the WorldAtlas was born mainly out of curiosity, but a lot of business use cases exist. The goal was to answer both simple questions, and perform broader, more complex analytics on data about the world.How many green spaces does London have, compared to Berlin?How many cities close to the sea, are there in Germany? How many mountain towns in Switzerland are directly connected to the capital, with a road no longer than 200km?For questions such as these, data could be supplied by Open Street Map.Given other data sources, such as World Bank, the system would be able to answer other classes of queries.What are all the seaside cities that are hosting an outdoor rock concert next week(events data)?What are some mountain towns with precipitation levels above X that have a GDP per capita over Y(economic indexes and weather data)?Where do most Germans spend their holidays, and what type of resort do they prefer?What are the most connected cities in Western Europe, by car, train, or airplane?What regions of a country have people who are least likely to have met?The target was set for some network analytics questions as well.Given just the topology of the roads, how likely is it that a street will be crowded(betweenness centrality)?What 4,6, 10 towns enable one to move from every one town to every other town?How many cars can pass between two points(max flow)? What is the minimum number of roads that can disconnect 2 cities(min cut)?The list can go on and on, as the number of questions one can come up with are almost infinite.After a couple of experiments with existing Geo systems it became apparent that both importing and representing Geo data, to start with, could be improved. Importing selectively the data from Open Street Map to Neo4j was not implemented by any projects. Queries on existing solutions using Open Street Map would be too slow for this type of questions outlined above because of the sheer size of the data.With this in mind, I built Atlas, aiming to answer the questions above. You can read more about the specific technical details in the Readme of the repository on GitHub. Some top-level categories of what it can do:GIS data abstractions and operationsImport Open Street Map dataImport from other data sources(World Bank, NASA, etc)Data transformers(explained below)Data modelAt it’s core, Atlas is a smart GIS system, so central to the data model are data about everything that one would usually find on a map. Here is a non-exhaustive list of what one could find in this dataset. Open Street Map knows about 3 types of entities: Node, Way, Relation and they defer to each other in that order.Nodes are the basic data structure it’s a geo point, with latitude and longitude that can have tags which describe what it represents. Ways are a way of connecting Nodes together, that describe a bigger entity, such as the perimeter of a school or a highway. Relations group together very large entities, and are comprised of Nodes and Ways. You can read more here.Atlas organizes all of this in a graph, bringing to this wealth of data all the querying and analytics capabilities that Neo4j has.Putting it all in a graphAtlas can import Open Street Map data selectively. You can filter the data based on type of entity(node, way, relation) or tags associated with it. All data output is a Java 8 Stream, for efficient memory use. You decide what happens with it. Do you write it to the database on the spot? Do you collect and aggregate, then write to Neo4j?Note: if you need to do more complex filtering on your entities in memory, i recommend this project: https://github.com/osmlab/atlas . You will find another sister project from Osmlab that can even use Apache Spark for this filtering/preprocessing step.Aggregating Information on a PathThe time a query takes is proportional to the number of nodes and relationships present on the path/matching query. Sometimes, the user might not care about all points on a road, just about a summary of the data.When doing network topology analysis on roads, all we care about is how the road looks(where the intersection points are), and what the length of each road segment is. Atlas can define such a contract criteria(see below) for highways, national roads or all types of roads.By intelligently manipulating the graph, we can make it easier for Cypher to find the answers we seek. This can make specific classes of queries blazing fast.Here is a very simple example of what’s going on. Let’s assume we have a way with 10 nodes, no intersections. There is nothing interesting happening between start and end node, so we can contract (collapse) it.Atlas can traverse roads, and if no interesting entity(such as a gas station) has been smart-linked(more on this in a separate article) to a road node, or if the node brings no additional value, such as being part of an intersection, the nodes are marked for contraction and later collapsed. Here is the above graph after the contraction. Only the start of the way and the end node are kept.Atlas extracted some ways(roads) from the UK, marked as highways in OpenStreetMap. This is a very simple example, where no 2 node points are shared by a road(way). There are no intersections in this case.Here is how the graph looks after contracting it.Summarizing Points of Interest alongside RoadsLet’s talk about another example. An app might want to display the number of gas stations, museums or lakes(or all of them together) on alternate paths to destination. In this case, all the data needed for this query is road length and the number of interesting entities on each road segment.Atlas can smart link those entities to road points. When contracting the road points, the data about the connected gas stations is encoded in the relationship.Alternatively, the road point that is closest to the interesting entity can be swapped with the interesting entity point. This is a design decision, and each choice has trade offs.Business use casesHere is a couple of examples of how Atlas could be used in a live product.Smart adsYou have a large inventory left over, of high-end priced, big sized, professional mountain winter shoes.(Might not contain only boots :P)Atlas can give you a list of all location where:- the GDP per capita is 2,3 or 4 standard deviations away from the mean.- the height average is high.- the elevation is high OR the town is within reasonable driving distance of such a site.2. You are selling party gear — hats, whistles and many others. Atlas knows where concerts with 200 to 2 million people are being held, and when.3. You are selling lugagges and travel accessories.Atlas can give you a list of all locations where:- a large influx of people are going to leaving by way of plane/train/bus to another destination in the next week.- the GDP per capita is in sync with the pricing of your travel bag.4. You are selling low-end priced water sports accessories(kayaks, paddle boards). You can query Atlas to obtain all cities close to water surfaces that are fit for your accessories and where the GDP per capita correlates with the price of the accessories.Smart citiesWhat roads should not be worked on at the same time, because they would cause major disruption?What would happen to the flow of the network, if some of the streets would be made one-way?Business expansionWhat cities’/towns’ network of roads are topologically similar to the town where i run my business? To what town should i expand my coffee shop franchise, and where should the next shop be located?ConclusionsIn this article, i have introduced Atlas, a new Open Source project built on top of Neo4j and OpenStreetMap, which makes querying and analyzing data about the world easy. We looked at some example queries, technical details and talked about some business use cases.If this caught your attention, be sure to follow Neo4j or me! I plan a second article in which to present some interesting queries on real-world data, and a third one in which i will be applying some network analytics on roads.;May 15, 2018;[]
https://medium.com/neo4j/toolbelt-trifecta-connecting-to-neo4j-with-java-and-aws-lambda-3c0fda6d5c1c;Jennifer ReifFollowFeb 8, 2022·13 min readPhoto by jesse orrico on UnsplashToolbelt Trifecta: Connecting to Neo4j with Java and AWS LambdaAll three of these technologies are familiar to me, and yet using them together was entirely new. There were plenty of challenges, lots of questions, some Googling, and colleague input that led me to a working solution.I wanted to use existing code as much as possible and keep things as simple as possible. Hopefully, we accomplish those two goals.In this blog post, we will write Java code to connect to Neo4j and run a query from AWS Lambda.TechnologiesBefore we dive right in, a brief overview of the technologies involved can help us understand what we’re trying to accomplish and how to structure the code.We are using the Java programming language, so it is more strict about input/output data types than Lambdas in a language like Python (my previous experience).Next, we have AWS Lambda. As its product page states, AWS Lambda allows developers to write code without managing servers.Finally, we want to use the previous two technologies to connect to a Neo4j graph database and execute queries to retrieve data from the database.Now let’s talk a bit about code!SetupAs you might imagine, we’ll need a few things in order to connect the pieces together. If you’re a Java developer, you probably already have a JDK and an IDE set up.For the Neo4j side, we will need a Neo4j database instance. Thankfully, Neo4j provides a free cloud instance you can spin up in just a few minutes called AuraDB.Details on how to get that set up are available in my colleague’s blog post (follow at least through Explore the Sample Data” section).Announcing Neo4j AuraDB FreeWe’re excited to announce that everyone can now use Neo4j AuraDB Free, and get started without a credit card in a…medium.comNote: save the generated password in a place you can access it later. It will not be displayed again in the console.The trickiest (and longest) piece is the AWS Lambda setup. If you don’t already have an AWS account, you’ll need to create that. AWS provides some sample Github projects that include code for various languages and scenarios, so I cloned the repo and used the java-basic example app.The Github project for the java-basic example app provides a set of instructions on the README.md.Bash / TerminalJava 8Gradle 5 or Maven 3AWS Cli v1.17 or newerFollowing the instructions, I had all the requirements listed, but I had to check my AWS cli version, which turned out to be some yak shaving. I followed the link for the install/update the AWS cli item, chose the macOS dropdown, and skipped to the third step to see what version I already had on my machine.However, because I had upgraded my laptop since the last time I did AWS stuff, I was getting a zsh: killed aws –version error. That page doesn’t really give you troubleshooting help, but I re-read the red box at the top of the AWS cli instruction page and decided to try an uninstall/reinstall.The first bullet there links to the uninstall instructions, and then I came back to the install/update the AWS cli page for the install steps. It worked, so the first problem was solved!Next, I needed to add the last part of the requirements steps to my AWS configuration file.cli_binary_format=raw-in-base64-outThis step should just consist of copying/pasting the above code into the designated config file, but I ended up with a bit of confusion because I have multiple AWS account profiles. If you have the same situation, you’ll need to add that setting under a specific profile in the config file (see code below).Ok, now we can charge on to the next steps in the Github readme instructions (Setup section).I had already cloned the repository, so I moved to creating the S3 bucket by running the 1-create-bucket.sh script. This failed with errormake_bucket failed: s3://lambda-artifacts-e3b5b3e664aaf559 An error occurred (InvalidLocationConstraint) when calling the CreateBucket operation: The specified location-constraint is not valid”.I realized that the script was running against the wrong profile (to use a non-default profile, you need an extra command-line argument). So, I needed to update my default profile with the needed settings or tweak the command in the script. I chose to update my default profile settings, so back to the ~/.aws/config file.[default]cli_binary_format=raw-in-base64-outregion = us-east-2I hadn’t used this particular profile before, so I also needed to set up a key and config to deploy and run things from my local command line. Instructions for that are buried a bit in the related AWS docs page, but you canClick on your user in the right corner of the AWS console,Choose Security credentials from the options,Then click the Create access key button under the AWS IAM credentials tab.Once that’s complete, you can copy the format of the credentials file from this AWS docs page.With those fixes in place, going back to run the 1-create-bucket.sh script in the Github repo instructions yielded success! Yay!Now, can we verify it actually created the S3 bucket? If you log into your AWS account via a web browser, we can choose the Services option in the upper left of the menu bar and choose Storage -> S3. At first, I didn’t see anything in the list under Buckets, but my AWS Console was pointing to the wrong AWS region.To change the region view that you’re seeing, click on the Global option in right of the menu bar (next to your user option) and choose the region that you set in your cli settings. In my case, because I specified region=us-east-2 in my config file settings above, then that’s the region that the cli is interacting with. I chose us-east-2 (US East (Ohio)) in the browser, and I could then see the bucket was there (may require reloading the page).DeployNext, I ran the script to deploy the project to AWS — 2-deploy.sh. This one failed for me the first time because the default uses Gradle (which wasn’t installed for me). Earlier in the instructions, the requirements stated that you need either Maven OR Gradle, and the next bit of code tells you to specify mvn in your command if you want to use Maven.I missed this, so I installed Gradle. At least I have both now. :)Note: to install Gradle, I followed the official install instructions. They offer SDKMan! and Homebrew installations, but I went with SDKMan!. Install: sdk install gradle 7.3.3 and check version of gradle: sdk list gradle.After installing Gradle, I re-ran the 2-deploy.sh script, and everything was successful! The script packages the project and puts the packaged .jar file in the S3 bucket we created in the last script.I can verify this by going to the AWS console in my browser, choosing the Services option in the menu bar, and clicking on Compute -> Lambda. Under the Functions section, you should see one named something like java-basic-function-<randomID>.That looks good! Time to run the function itself!Invoke the FunctionThe next set of instructions on the Github project readme tell us how to execute the function code. When I ran the 3-invoke.sh script, it executed successfully on the first attempt (to my relief!). I let it output the results three times, then terminated the script using Cmd+c.It was outputting expected results to the console, but was it also pushing logs to the CloudWatch AWS service, as expected?In the AWS console, under our Lambda, we can choose the Monitor tab, then Logs tab. It will show a list of log streams from the times you have executed the function. The list should be sorted with latest on top, so you can click the top event and see the log output from the function!Now that the plain, vanilla version of the code is operating as expected, we can make our adjustments to put Neo4j into the mix!Combining Bits of CodeIf you followed the article linked earlier to set up a free database with Neo4j AuraDB free tier, then you should have the database and the movies data set ready to go. Reminder: you will need the password that gets generated when you create a database.From the AuraDB console, if you click on your database, four tabs appear beneath it. The Connect tab shows examples of small applications for five different programming languages (Python, Javascript, Java, .Net, Go) and two higher level approaches (GraphQL, Spring Boot).AuraDB Programming Language Examples in Connect tabSince we want to connect Lambda and Neo4j using Java, I have the Java tab selected. This code gives us a good template that we can use to copy/paste the needed pieces into our Lambda code.First, we need to add Neo4j as a dependency. Whether you put the dependency in the build.gradle or the pom.xml file will depend on whether you chose to run the Lambda using Gradle or Maven. Because I went with Gradle this round, I’ll add the following to the build.gradle file under the dependencies section:dependencies {  implementation org.neo4j.driver:neo4j-java-driver:4.4.0  //remaining dependencies}If you are working with Maven, the pom.xml file addition would be as follows in the dependencies section:<dependencies>  <dependency>    <groupId>org.neo4j.driver</groupId>    <artifactId>neo4j-java-driver</artifactId    <version>4.4.0</version>  </dependency>  <!-— remaining dependencies --></dependencies>Syntax for both of these is also available on Neo4j’s Java developer guide.Using Neo4j from Java - Developer Guidesneo4j.comAWS Lambda CodeBefore we start tweaking the code, let’s first understand what the existing Lambda code is doing. The default class that gets executed is the Handler.java class, as that’s what is specified in the template.yml file.Original Example Handler.javaThe repo mentions this under the Configure Handler Class section. The default worked great for our initial test, but the class implements RequestHandler, which expects an event of an input type of Map<String,String> and an output type of String. Since I wasn’t sure exactly what we’d be dealing with for the Neo4j database, it’s probably easiest to work with JSON objects both in and out.In that case, we can use the HandlerStream.java to work with input and output stream objects. The input/output details are spelled out a bit better in the AWS documentation.In order to run this code, we need to make a quick change to the template.yml file to execute the HandlerStream class, rather than the default Handler.function:  Type: AWS::Serverless::Function  Properties:    CodeUri: build/distributions/java-basic.zip    Handler: example.HandlerStreamThe HandlerStream class starts with a variable called gson that will serialize and deserialize JSON objects in input and output. Then, it creates a custom implementation of the handleRequest() method (using @Override annotation) with the expected input, output, and context parameters. This Lambda uses a .json file for the input and output so that we don’t have to specify the literal JSON on the command line when we invoke the function. We can see this in the aws command at the end of the 3-invoke.sh script (--payload file://event.json out.json).Because we’re reading and writing to files, we need to be able to read and write to them in the Java function. The first three lines of code in the handleRequest() method set up these variables, along with the logger that sends info to CloudWatch logs.HandlerStream.javaWe use a try ...catch block to attempt the next set of commands.In the try section, we first read the input stream into a Object and assign it to the variable event. Then, we output a couple messages to the logs to help us troubleshoot the input, if needed.Note: I personally found using logger.log() messages to be super helpful when debugging Lambdas. I could print the input and output objects with any transformation steps to figure out what the data actually looked like.We then write the event that we just read to the output stream with writer.write(gson.toJson(event)).Then closes the try section, catches and logs any errors in the catch block and closes the streams.Now that we understand what the existing Lambda code is doing, we can start adding in the Neo4j-relevant bits to connect to AuraDB and run a query.Connecting to Neo4jFirst, we need to set up the connection details to the database. The example code from the AuraDB console gives us some syntax (in main() method) that we can copy/paste for that. It’s easiest to create a global instance of the driver, so I put the connection code right below the gson variable assignment.The uri variable should fill in your database’s URI, but you can also copy/paste that from the database information listed on your database instance in the console. Since the default username is neo4j, we can assign that to the user variable. You will need to copy/paste the password from when you created the database as the value for the password variable.For a global instance of the driver, I changed the code a bit from what shows in the AuraDB console. I created a private final variable for the driver, then assigned it using the graph database driver method and passed in the database uri, auth details, and any config (defaults). Now we have an instance of the driver we can use to connect to Neo4j!*Note: ideally, you should store these values in environment variables for security and populate them from the config. More info: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-lambda-function-environment.htmlThe next lines of Lambda code for the handleRequest method stay the same in the method signature and set up of logger, reader, and writer variables. The first lines of the try block also are consistent, since we want to read the input file, this time into a Mapand log it before we do anything with the database. Now it’s time to write our Cypher query that we want to execute against Neo4j.Our Neo4j database contains the movies data set, which has movies and persons who acted in/directed/etc particular movies. I want to reuse as much existing code as possible, so I used the query in the AuraDB console Java code (lines 57–61). :)Cypher query and parametersNote the one change is the parameter that is passed (last line). In the AuraDB code, we pass a String value into the findPerson method, and that parameter is what we pass into the params variable here. However, since I wanted to avoid a separate method for now, I need to pass in a string value from our input JSON file. This means I need to change the input file (event.json) with a person’s name. Many of the Neo4j queries with the movies data set use Tom Hanks, so let’s search for him.The event.json file already has the JSON structure we need, so we just need to modify the keys and values for our parameter. The JSON file should look like shown below:{   name :  Tom Hanks }I went back and added a logger.log message to verify that the JSON passed to the event matches the input file. Using my IDE’s code auto-completion, I typed in the event. and chose get(Object key) option so that I could specify a Map key.Since I know I will be passing a name value as a string, I can type  name  for the key, then ensure the value is a string by using the toString() on the end. I checked that logger output in a test run and used that syntax to pass in as the Cypher query parameter.It’s time to run the Cypher query against Neo4j now!Back in the AuraDB console (line 63), the code wraps the call in a try...catch block, creates a session, sets up a Record variable, then uses a Java lambda expression to run the query in a read-transaction and return the results.Keeping with our copy/paste theme, I inserted lines 63–73 from the AuraDB console into our function and tweaked a couple of things. First, I wanted to write results back to our function’s output stream, so I moved that bit of code into the try block right after the query gets executed. I also needed to change the logger in the catch block because I want to send any errors to the Lambda logger in CloudWatch with our other messages. Removing the exception throw for Neo4j means it will just get logged to CloudWatch, which aligns with our other errors.Note: to get the output file format just right, I used a couple logger.log() messages to first output the whole record object, then format that using the record.asMap() method. This returns the pretty JSON output we see in the out.json file.Our updated try...catch block looks like below:HandlerStream — Lambda code to run query against Neo4jout.json file:{   name :  Tom Hanks }Note that this output also turns out the same as our input JSON! :) Now we can test our solution by running the following scripts at the command line:java-basic$ ./2-deploy.shjava-basic$ ./3-invoke.shThe first script re-packages and deploys the .jar file to our S3 bucket, then the next script calls the function. We should see some output in our console, and I waited until I got the output two or three times before terminating the script using ctrl+c.Console output at the command line is as follows (repeated as many times as we let it run):{   StatusCode : 200,   ExecutedVersion :  $LATEST }{   name :  Tom Hanks }Lastly, I checked the output in my AWS console through the CloudWatch service.CloudWatch logsWrapping UpCongratulations! We have successfully written an AWS Lambda in Java to run a query and return results from a Neo4j graph database in the cloud.We used the existing AWS Lambda example Java application with an existing Java program in the AuraDB console for connecting to Neo4j.We used JSON in input and output files so that we could read custom parameters to the query and output the return results from the database.We were able to copy/paste nearly everything with customizations on 5 lines (HandlerStream function in .yml, global driver instantiation, event value logger message, input .json file, and the value passed to the params query variable).There are many things we could do from here, such as creating a whole event pipeline, passing input or output to/from other systems, creating functions to write to Neo4j, and more!As always, if you have any questions or improvements, feel free to contact me/us through our community forum!Happy coding!ResourcesGithub project: Java AWS Lambda with Neo4jAWS Lambda example app: Java-basicAWS Lambda docs: Working with JavaFree cloud Neo4j database: AuraDBAuraDB code: Java example app;Feb 8, 2022;[]
https://medium.com/neo4j/authentication-in-a-nest-js-application-with-neo4j-31beb416538;Adam CowleyFollowJul 20, 2020·22 min readAuthentication in a Nest.js Application with Neo4jThis post is the second in a series of blog posts that accompany the Twitch stream on the Neo4j Twitch channel where I build an application on top of Neo4j with NestJS.If you haven’t already done so, check out the first article here or watch the videos back on the Neo4j Youtube Channel.Authentication is a key part of any subscription or SaaS site. For every request, we should be able to verify who the user is and whether they have the correct subscription to do what they are trying to do. As such, API calls to view or stream any content via the API will require valid user credentials. To enforce this, we will use a combination of Nest.js Guards and JWT tokens.To provide Authentication we will need REST endpoints to allow users to create an account and then to log in with those credentials. We will be using a combination of Email address and Password as the method for authentication so that the user doesn’t need to supply an email address and a username.JWTsJWT’s (pronounced JOT) — short for JSON Web Tokens — are compact tokens — backed by an open standard (RFC 7519) — that are designed to provide a secure way of transmitting information between parties — or in our case sharing user authentication information between the API and front end.On the surface, a JWT token could look a little like this:eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5cThis is a base64 encoded string which contains three pieces of information, split by a dot.The HeaderWhen decoded, the header will contain information about the type of token and the algorithm that has been used to sign the token.{   alg :  HS256 ,   typ :  JWT }The PayloadThe token’s payload is a base64 encoded JSON object that contains certain claims about the User, for example information about who they are. Although there are some reserved claims (for example the issuer or iss, subject or sub and expiry or exp), any information can be added to the payload. It is worth remembering that the payload of a JWT token can be easily decoded, so this shouldnt contain any sensitive information about the user. In our case, we will add some basic information about the User into the token so that the UI can be customised without making unnecessary requests to the API.{   iss :  our-api ,   sub :  user-1234 ,   exp : 1595245369}In the example above, the token has been issued by our-api for user-1234 with an expiry time of 1595245369 - seconds since epoch.The SignatureThe signature ensures that the information held in the payload has not been tampered with. After base64 encoding the payload, a signature is generated using a secret passphrase or PEM key. When the key is read, the signature is regenerated and checked against the value provided. If the two signatures do not match then the token is rejected. This ensures that as long as no one knows the key used to sign the original token, the contents will always be valid.Adding Authentication to NestNest.js comes with a built in module for Passport, a widely used library for authenticating users. We will be using it to validate each request, generate JWT tokens during the login process, and verify those tokens in subsequent requests. But before we get into that, we’ll need to create an Authentication Service to handle the business logic.To follow the conventions set out by Nest, we should create a new module which we can register in the main application. This module should provide access to a service which will handle the authentication and a controller to accept the HTTP requests.nest g mo authnest g s authnest g co authThe Auth Service shouldn’t be tasked with querying the user information, so we should also create a User Module and Service to handle communication with the database. This has the added benefit of allowing us to share this functionality across the application rather than duplicating the code — meaning we adhere to the principles of DRY (Don’t Repeat Yourself). If we want to create or find a User, we can just inject the User Service into a class.nest g mo usernest g s userEnsuring Emails are UniqueBecause users will be authenticating with their email address, we need to make sure that an email address is unique. We could add a check as part of the validation stage to check if the username exists but this would double the number of requests required and also add network latency to the request time.Instead, we can add a constraint to the database to ensure that the email property is unique for any node with a :User label. To do this, we can run a CREATE CONSTRAINT statement in Neo4j:CREATE CONSTRAINT ON (u:User) ASSERT u.email IS UNIQUENow if the user passes all other validation required by the application layer, the database will throw a ClientException which we can catch in the API.Registering as a User — POST /auth/registerBefore we can authenticate a User, it has to exist in the database. So the first thing to do would be to create an endpoint to allow a User to create an account.We’ll need to create a DTO (Data Transfer Object) to represent the payload that the function should receive. By using the @Body() decorator, Nest will coerce the request body into the class that has been type-hinted by the route handler.The root folder of the module is getting a little crowded already, so I personally prefer to create a dto/ folder to hold the DTO classes.mkdir src/user/dtotouch src/user/dto/create-user.dto.tsAn added benefit of DTO’s is that if we add decorators to the properties, Nest will automatically validate the request and reject any requests that don’t meet the requirements that have been set out. @nest/core comes with a ValidationPipe that well need to register as a Global Pipe in the bootstrap function in main.ts.Pipes are injectable classes that either transform or validate inputs into the application. — For example, the ParseIntPipe can be used to transform a URL parameter into a number type.The ValidationPipe uses the class-validator and class-transformer packages so well have to install those:npm i --save class-validator class-transformerThen register the ValidationPipe as a Global Pipe in main.ts to ensure that it is used for all HTTP requests.// main.tsasync function bootstrap() {  const app = await NestFactory.create(ApplicationModule)  // Use Global Pipes  app.useGlobalPipes(new ValidationPipe())  await app.listen(3000)}bootstrap()For a User to register, we’ll need an email address and password. Then to personalise their service a name, and date of birth will be useful for controlling. The user’s date of birth will be also required to ensure that a User can access the content they’ve requested.We’ll add some validation on the date to ensure that the user is at least 13 years old in accordance with UK law. To do this, we can use the moment package - npm i moment and subtract 13 years.The create-user.dto.ts should look something like this:import { Type } from class-transformerimport { IsEmail, IsNotEmpty, IsDate, MaxDate } from class-validatorexport class CreateUserDto {    @IsEmail()    @IsNotEmpty()    email: string    @IsNotEmpty()    password: string    @IsNotEmpty()    @IsDate()    @MaxDate(require(moment)().subtract(13, y).toDate())    @Type(() => Date)    dateOfBirth: Date    firstName: string    lastName: string}The @Type decorator from class-transformer will take the value and transform it into a Date.Note: At the time of writing, the moment library wasnt playing well with typescript imports. After a quick google, I found that adding  esModuleInterop : true, to compilerOptions in tsconfig.json seemed to do the trick.Next, the route in auth.controller.ts. Any routes in the auth controller are prefixed with auth/ as defined in the @Controller decorator, so to create a REST endpoint for a POST request to /auth/register, we can create the @Post decorator.// auth.controller.tsimport { Controller, Post, Body } from @nestjs/commonimport { UserService } from ../user/user.serviceimport { CreateUserDto } from ../user/dto/create-user.dto@Controller(auth)export class AuthController {    constructor(private readonly userService: UserService) {}    @Post(register)    async postRegister(@Body() createUserDto: CreateUserDto) {        // TODO: Create User        return createUserDto    }}Nest can’t resolve dependencies of ???If you’re running the code in dev mode, you’ll now see something along the lines of:[ExceptionHandler] Nest cant resolve dependencies of the AuthController (?). Please make sure that the argument UserService at index [0] is available in the AuthModule context.Potential solutions:- If UserService is a provider, is it part of the current AuthModule?- If UserService is exported from a separate @Module, is that module imported within AuthModule?  @Module({    imports: [ /* the Module containing UserService */ ]  })Ahh, dependency injection, our old friend. Nest doesn’t recognise UserService, so to fix this error well need to:Add UserModule as an import to the AuthModule so that anything exported from Auth module can be injected into classes in the User module// auth.module.tsimport { UserModule } from ../user/user.module@Module({    imports: [UserModule],    providers: [AuthService],    controllers: [AuthController]})export class AuthModule {}Add an exports array containing UserService to UserModule so that Nest’s IoC container recognises the module.import { UserService } from ./user.service@Module({    providers: [UserService],    exports: [UserService],})export class UserModule {}There’s nothing like a live coding disaster to cement a solution into your head.Testing the EndpointTo verify this is working, we could write a cURL request or open up Postman. But instead, let’s look at writing a test. So far we’ve not looked at tests, but you may have noticed that when we ran the nest g co auth command, an auth.controller.spec.ts file was also generated.If we run the following command, the jest rest runner will run in watch mode. This is really useful for development because the test runner will watch for changes to files and automatically re-run tests. So if a change in one file breaks another, youll know instantly.npm run test:watchBecause we’ve not looked at this yet there will be a load of failures but let’s not worry about them too much for now. Instead, the last few lines gives instructions on how to filter the tests that are being run.Test Suites: 4 failed, 1 passed, 5 totalTests:       2 failed, 1 passed, 3 totalSnapshots:   0 totalTime:        1.474 s, estimated 2 sRan all test suites related to changed files.Watch Usage › Press a to run all tests. › Press f to run only failed tests. › Press p to filter by a filename regex pattern. › Press t to filter by a test name regex pattern. › Press q to quit watch mode. › Press Enter to trigger a test run.I usually like to run only the tests that have failed (option f), but right now were only interested in the Auth controller.We can type p to filter the tests by filename, then type auth.controller to only run the tests in auth.controller.spec.ts.We really should add some tests to auth.controller.spec.ts but for now, end-to-end tests are more appealing because they will test whether the guard is working or not. Where the existing tests are great for testing smaller units of cod, an end-to-end test will ensure that the entire stack is working correctly.The nest new command generates an app.e2e-spec.ts file which creates a test module that automatically includes everything that has been imported into the AppModule, so it will already have our AuthController registered.If we run the end-to-end tests using the npm run test:e2e itll currently fail because the default test expects GET / to return Hello World but weve already updated it. Its kind of an irrelevant test so for now we can just comment it out.Jest’s describe function allows you to group tests together to make the results more readable. Well be testing the Auth functionality, we can create a group called Auth, then inside call the function again to group the tests for the register endpoint.describe(Auth, () => {    describe(POST /auth/register, () => {        // Tests go here:    })})Then, the it function defines the test. I always like to start my tests with should so it kind of reads like a sentence but go with whatever you see fit.The first thing it should do is validate the response based on the decorators in the DTO. Nest will return a HTTP 400 Bad Request status code, so we can verify thatdescribe(Auth, () => {    describe(POST /auth/register, () => {      it(should validate request, () => {        return request(app.getHttpServer())          .post(/auth/register)          .set(Accept, application/json)          .send({            email: a@b.com,            dateOfBirth: 2019-01-01          })          .expect(400)          .expect(res => {            // Check the body            console.log(res.body)            // Should have an error about the password            expect(res.body.message).toContain(password should not be empty)            // Should have an error about the date being later than 13 years ago            expect(              res.body.message.find((m: string) => m.startsWith(maximal allowed date for dateOfBirth))            ).toBeDefined()          })      })    })  })Right now the test will fail:> api@0.0.1 test:e2e /Users/adam/projects/twitch/api> jest --config ./test/jest-e2e.json FAIL  test/app.e2e-spec.ts  AppController (e2e)    Auth      POST /auth/register        ✕ should validate request (203 ms)  ● AppController (e2e) › Auth › POST /auth/register › should validate request    expected 400  Bad Request , got 201  Created Ran all test suites.Jest did not exit one second after the test run has completed.This usually means that there are asynchronous operations that werent stopped in your tests. Consider running Jest with `--detectOpenHandles` to troubleshoot this issue.This is because we’ve not yet registered the ValidationPipe to the test application used within the test. Well have to add this line using the useGlobalPipes function as we did in main.ts:// app.e2e-spec.tsbeforeEach(async () => {  const moduleFixture: TestingModule = await Test.createTestingModule({    imports: [AppModule],  }).compile()  app = moduleFixture.createNestApplication()  // Use Validation Pipe  app.useGlobalPipes(new ValidationPipe())  await app.init()})Now an error appear at the bottom of the test complaining that:Jest did not exit one second after the test run has completed.This is occurring because the Neo4j Driver instance is left open, and therefore the Nest application doesn’t exist within the second window that Jest expects. To fix this, we’ll have to call the close method on the app. Calling app.close() after all of the tests have run will call the onApplicationShutdown method on the Neo4jService to be called, closing any sessions that are still left open.// app.e2e-spec.tsafterAll(() => app.close())For the next test, the API should return a HTTP 201 Created status with the Users information if the user supplies the correct information.it(should return a JWT token on successful registration, () => {  const email = `${Math.random()}@adamcowley.co.uk`  return request(app.getHttpServer())    .post(/auth/register)    .set(Accept, application/json)    .send({      email,      firstName: Adam,      lastName: Cowley,      password: Math.random().toString(),      dateOfBirth: 2000-01-01    })    .expect(201)    .expect(res => {        expect(res.body.user.email).toEqual(email)    })})Persisting the DataThe tests now pass, but nothing is happening. We next need to persist the data in the database. The AuthController shouldn’t hold any logic as to how a User is created, so this responsibility should be passed on to another class — in this case we’ll use the UserService generated earlier. So, in src/users/user.service.ts, we should create a new method for creating a User.For now, we don’t have any entities in the code, so we can define the User type as a Node from neo4j-driver.// user.service.tsimport { Node } from neo4j-driverexport type User = NodeThen, the create method will take named parameters so that we can reflect the business logic defined in the CreateUserDto (email, password and dateOfBirth are required but the first and last name are optional), then pass on a cypher CREATE query to the Neo4jService.// user.service.tsimport { Injectable } from @nestjs/commonimport { Neo4jService } from ../neo4j/neo4j.serviceimport { Node, types } from neo4j-driverexport type User = Node@Injectable()export class UserService {  constructor(private readonly neo4jService: Neo4jService) {}  async create(email: string, password: string, dateOfBirth: Date, firstName?: string, lastName?: string): Promise<User> {    const res = await this.neo4jService.write(`CREATE (u:User) SET u.id = randomUUID(), u += $properties RETURN u`, {      properties: {        email,        password,        firstName,        lastName,        dateOfBirth: types.Date.fromStandardDate(dateOfBirth),      }    })    return res.records[0].get(u)  }}It’s a really bad idea to store plain text passwords in the database, so we’ll install the bcrypt library which will encrypt the plain passwords when an account is created and also check the plain text password against the encrypted value stored in the database.npm i --save bcryptNext, we’ll need to create a new encryption service which will be responsible for encrypting and comparing passwords.nest g mo encryptionnest g s encryptionThe service should offer two functions, one to hash a plain text password and another to compare the password that the user has entered against the hashed valued stored in the database.// encryption.service.tsimport { hash, compare } from bcryptimport { Injectable } from @nestjs/commonimport { ConfigService } from @nestjs/config@Injectable()export class EncryptionService {    constructor(private readonly config: ConfigService) {}    async hash(plain: string): Promise<string> {        return hash(plain, this.config.get<number>(HASH_ROUNDS, 10))    }    async compare(plain: string, encrypted: string): Promise<boolean> {        return compare(plain, encrypted)    }}Note: Here we’re using the ConfigService which was previously registered as a global module inside app.module.ts to get the HASH_ROUNDS config value from our environment variables.In order to inject the EncryptionService into other services, this will need to be added to the exports array of the EncryptionModule.// encryption.module.tsimport { EncryptionService } from ./encryption.service@Module({  providers: [EncryptionService],  exports: [EncryptionService],})export class EncryptionModule {}The EncryptionModule can then be added as an import to the UserModule:// user.module.tsimport { Module } from @nestjs/commonimport { UserService } from ./user.serviceimport { EncryptionModule } from ../encryption/encryption.module@Module({  imports: [EncryptionModule], // <-- Import Encryption into User Service  providers: [UserService],  exports: [UserService],})export class UserModule {}…and subsequently injected into the UserService to hash the password.// user.service.tsexport class UserService {    constructor(        private readonly neo4jService: Neo4jService,        // Add Encryption Service to the constructor        private readonly encryptionService: EncryptionService    ) {}    async create(email: string, password: string, dateOfBirth: Date, firstName: string, lastName: string): Promise<User> {        const res = await this.neo4jService.write(`CREATE (u:User) SET u += properties RETURN u`, {            email,            // Use the encryption service to hash the password            password: await this.encryptionService.hash(password),            firstName,            lastName,            dateOfBirth: Neo4jDate.fromStandardDate(dateOfBirth)        })        return res.records[0].get(u)    }}Next, the AuthModule needs to be updated to import the UserModule:// auth.controller.timport { EncryptionService } from ./encryption/encryption.service@Module({  imports: [UserModule],  providers: [AuthService],  controllers: [AuthController]})export class AuthModule {}Then, the UserService can be injected and used in the AuthController:// auth.controller.tsconstructor(private readonly userService: UserService) {}@Post(register)async postRegister(@Body() createUserDto: CreateUserDto) {    const user = await this.userService.create(        createUserDto.email,        createUserDto.password,        new Date(createUserDto.dateOfBirth),        createUserDto.firstName,        createUserDto.lastName    )    return {        user: user.properties    }}If we run the test again, the two tests should now pass.> api@0.0.1 test:e2e /Users/adam/projects/twitch/api> jest --forceExit --config ./test/jest-e2e.json PASS  test/app.e2e-spec.ts  AppController (e2e)    Auth      POST /auth/register        ✓ should validate request (254 ms)        ✓ should return a JWT token on successful registration (128 ms)Test Suites: 1 passed, 1 totalTests:       2 passed, 2 totalSnapshots:   0 totalTime:        3.338 s, estimated 4 sRan all test suites.A quick query in cypher-shell should show that the :User Node has been created with a hashed password:neo4j@neo4j> MATCH (u:User) WHERE exists(u.email) RETURN u.id, u.email, u.password, u.dateOfBirth+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+| u.id                                   | u.email                               | u.password                                                     | u.dateOfBirth |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+|  d786d248-af60-49ca-b389-3fe423b9a1cf  |  0.4030098705180425@adamcowley.co.uk  |  $2b$10$YWBng/jeA7nJVZ1/aCtnG.lHLJCqDctHrVL.7SW/aHpU307xEw1Ry  | 2000-01-01    |+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+But at the moment, there is still a problem. We’re currently returning all of the user’s properties including the hashed password which isn’t a good idea, and also this method will require the user to send another request in order to log in. Instead, we should generate and return a JWT token with selected information about the user.Generating JWT Tokens — POST /auth/loginTo log in, a user will have to send a POST request to /auth/login with their username and password. In exchange, they will receive a JWT token containing some basic user details and an expiry timestamp.So, the first thing to do is to create a route handler in the AuthController:// auth.controller.ts@Post(login)async postLogin(@Request() request: Request) {  // ...}Validating UsersTo validate the user, we’ll create a validateUser method in the AuthService This should accept a username and plaintext password, find the User by its email address (with the help of the UserService) and then use the EncryptionService to check the password. If the user has been found and the password check is OK, then it should return a User object, otherwise it should return null.// auth.service.tsssync validateUser(email: string, password: string) {    const user = await this.userService.findByEmail(email)    if ( user && this.encryptionService.compare(password, (user.properties as Record<string, any>).password) ) {        return user    }    return null}NB: We should probably create an interface for a User’s properties at some point.Then, in the UserService, we need to create a method that will query Neo4j for a User with that email:async findByEmail(email: string): Promise<User | undefined> {  const res = await this.neo4jService.read(`MATCH (u:User {email: $email}) RETURN u`, { email })  return res.records.length ? res.records[0].get(u) : undefined}In order to generate a token, we’ll use Passport. Passport acts as a middleware, authenticating requests using using strategies. In basic terms, a strategy is a class which implements a a validate method. The validate method will check the context of the request (ie. check the users credentials or a token) and stop the request by throwing an error if anything goes wrong.The @nestjs/passport library contains all of the helper functions required to integrate Passport into Nest.Alongside Passport, we will use Passport Local, an out-of-the-box add-on for Passport that allows you to perform basic authenticating using a Username and Password.Installing Passport and Passport Local Dependenciesnpm i --save @nestjs/passport passport passport-localnpm i --save-dev @types/passport-localBuilding a Local StrategyTo implement a local strategy, we can extend the PassportStrategy from the package and register it as a provider in the AuthModule.For the local-strategy, Passport expects a validate method with the following signature: validate(username: string, password:string): any. The strategy will be @Injectable so we can use it in any modules that import the AuthModule. Inside the auth folder, create a new file called local.strategy.ts:// local.strategy.tsimport { Strategy } from passport-localimport { PassportStrategy } from @nestjs/passportimport { Injectable, UnauthorizedException } from @nestjs/commonimport { AuthService } from ./auth.service@Injectable()export class LocalStrategy extends PassportStrategy(Strategy) {  constructor(private authService: AuthService) {    super({ usernameField: email })  }  async validate(username: string, password: string): Promise<any> {    const user = await this.authService.validateUser(username, password)    if (!user) {      throw new UnauthorizedException()    }    return user  }}By default the LocalStrategy will look for a field in the request named username, but as we want the user to log in with their email address, we can customise the behaviour by passing an object into the super() call within the constructor stating that the usernameField should be instead be the email in the request.The validate method on this class calls the validateUser method that we created earlier, and if a User object hasnt been returned it will halt the request by throwing an UnauthorizedException (imported from @nestjs/common).Then, we need to register it as a provider in the AuthModule so that it can be used within the module.// auth.module.tsimport { LocalStrategy } from ./local.strategy@Module({  imports: [UserModule, EncryptionModule],  providers: [AuthService, EncryptionService, LocalStrategy], // <-- appended to array  controllers: [AuthController],})export class AuthModule {}Adding a Route GuardTo use the strategy above, we’ll need to create a class that extends AuthGuard - a class provided by @nestjs/passport.Before a request hits the route handler function, Nest will pass the request through a pipeline of Guards which have the responsibility of validating the request and throwing an error if anything goes wrong.In this case, the AuthGuard will extract he user’s credentials from the request and then pass it to an instance of the LocalStrategy class. If the credentials are correct, it will add a user item to the Request object with whatever is returned from the validate method, otherwise it will throw the UnauthorizedException which will be dealt with further down the stack.// local-auth.guard.tsimport { AuthGuard } from  @nestjs/passport import { Injectable } from  @nestjs/common @Injectable()export class LocalAuthGuard extends AuthGuard(local) {}We can then tell Nest to use the LocalAuthGuard to guard the request using the UseGuards decorator. If all goes well, the guard will set request.user to be the users information. If there is a problem with the users credentials then the code in the route handler will never be touched.// auth.controller@UseGuards(AuthGuard(local))@Post(login)async postLogin(@Request() request) {    return request.user.properties}Generating and Returning a JWTThis route handler will now return the user’s properties, including their password which isn’t ideal. Instead, we should be returning a JWT token. To do that, we will need passport-jwt. Similar to passport-local, is a strategy for authenticating users, but instead of using Username and password, it will check a token provided in the Authorization header.The token will also expire after the expiration time (exp), but you can also tell passport to ignore the expiration time.Note: It is important to remember that these keys can be easily decoded, so they shouldn’t contain any sensitive information.To use passport-jwt with Nest, well also need to install @nestjs/jwt.npm install @nestjs/jwt passport-jwtnpm install @types/passport-jwt --save-devNext, we’ll need to register the JWTService from @nestjs/jwt with the AuthModule. The JwtModule requires either a secret or PEM, for now well add a secret key to .env that we can then retrieve using the ConfigService.The values in .env should look something like this:# .envJWT_SECRET=mySecretJWT_EXPIRES_IN=30dThen, like with the Neo4jModule, we can import the JwtModule into the AuthModule using the registerAsync function.The function takes a Dynamic Module configuration, so we can instruct Nest to import the ConfigModule, then inject the ConfigService into a factory function (useFactory) which will then return the secret and signinOptions required to instantiate the module.// auth.module.tsimport { JwtModule } from @nestjs/jwtimport { ConfigModule, ConfigService } from @nestjs/config@Module({  imports: [    JwtModule.registerAsync({      imports: [ ConfigModule ],      inject: [ ConfigService, ],      useFactory: (configService: ConfigService) => ({        secret: configService.get<string>(JWT_SECRET),        signOptions: {          expiresIn: configService.get<string>(JWT_EXPIRES_IN),        },      })    }),    UserModule,    EncryptionModule,  ],  providers: [AuthService, EncryptionService, LocalStrategy],  controllers: [AuthController],})export class AuthModule {}JWT StrategyJust like the Local Strategy, we also need a JWT Strategy. passport-jwt provides an ExtractJwt.fromAuthHeaderAsBearerToken function which we can pass through to the super call in the PassportStrategy.The constructor needs an instance of the ConfigService to get the secret, otherwise it will fail to validate the token.// jwt.strategy.tsimport { Injectable } from  @nestjs/common import { PassportStrategy } from  @nestjs/passport import { ConfigService } from  @nestjs/config import { ExtractJwt, Strategy } from  passport-jwt @Injectable()export class JwtStrategy extends PassportStrategy(Strategy) {    constructor(        private readonly configService: ConfigService,        private readonly userService: UserService    ) {        super({            jwtFromRequest: ExtractJwt.fromAuthHeaderAsBearerToken(),            ignoreExpiration: false,            secretOrKey: configService.get(JWT_SECRET),        })    }    async validate(payload: any) {        return this.userService.findByEmail(payload.email)    }}Under the hood, Passport JWT will guarantee that the token received by the validate method is a valid token, which has been correctly signed and has not expired yet. Then it is up to us to return the information that will be assigned to user on the Request object. For this, I will mimic the value returned from the LocalStrategy and return the User node from the database via the UserService.Note: We could also ignore the expiry date by setting ignoreExpiration: true, in the constructor. This would be a good idea if, for example, we issued a short-life JWT token but also issued a refresh token as part of the payload. We could then check the database for the refresh token and if found, allow the request issue another short-life JWT token.As with the LocalAuthGuard, well also need to create a JWT Auth Guard.// jwt-auth.guard.tsimport { Injectable } from  @nestjs/common import { AuthGuard } from  @nestjs/passport @Injectable()export class JwtAuthGuard extends AuthGuard(jwt) {}Then add it as a provider for the AuthModule:// auth.module.ts// ...import { JwtStrategy } from ./jwt.strategy@Module({  // ...  providers: [AuthService, EncryptionService, LocalStrategy, JwtStrategy],  // ...})export class AuthModule {}Then, to prove that the guard works, we can add it as a guard for a route handler using the @UseGuards decorator:// auth.controller.ts@UseGuards(JwtAuthGuard)@Get(user)async getUser(@Request() request) {    const { id, email, firstName, lastName } = request.user.properties    return { id, email, firstName, lastName }}Adding the token to the login requestNow we have can validate the token, we can add a method to generate a new JWT token to the AuthService. To access the JWT Service, well need to add it to the constructor of the AuthService// auth.service.tsimport { JwtService } from @nestjs/jwt// ...@Injectable()export class AuthService {    constructor(        private readonly userService: UserService,        private readonly encryptionService: EncryptionService,        private readonly jwtService: JwtService    ) {}    // ...}As I mentioned earlier, this can be easily decoded so we should be careful about the kind of information we include in the token. In reality, we only need enough information in the token to validate that it is correct (eg. their email address) but we can also add information that can be used to customise the UI. In future, we might want to add the subscription types but for now we’ll just go with name, email and date of birth.// auth.service.tsasync createToken(user: User) {    const {        id,        email,        dateOfBirth,        firstName,        lastName,    } = <Record<string, any>> user.properties    return {        access_token: this.jwtService.sign({            sub: id,            email,            dateOfBirth,            firstName,            lastName        })    }}The method takes a User object (in this case a Node), pulls a set of properties from the node and uses the JwtService to encode and sign the JWT. The result of the call to this.jwtService.sign is a base64 encoded string similar to the one mentioned at the top of this article.We can now hook this into the POST /auth/login method in the AuthController:// auth.controller.tsexport class AuthController {    constructor(        private readonly userService: UserService,        // Added authService        private readonly authService: AuthService    ) {}    // ...    @Post(register)    async postRegister(@Body() createUserDto: CreateUserDto) {        const user = await this.userService.create(            createUserDto.email,            createUserDto.password,            new Date(createUserDto.dateOfBirth),            createUserDto.firstName,            createUserDto.lastName        )        // return {        //     user: user.properties        // }        return this.authService.createToken(user)    }    @UseGuards(LocalAuthGuard)    @Post(login)    async postLogin(@Request() request) {        // return request.user        return this.authService.createToken(request.user)    }}If we head back to the end-to-end tests, we can make sure that this entire process works.By shifting the email, usernamd and password variables into the describe(Auth, ...) block, we can make them available to each test within that group. That way we can generate a random email address and password that can be used for the duration of the Auth tests.// app-e2e.spec.tsdescribe(Auth, () => {  const email = `${Math.random()}@adamcowley.co.uk`  const password = Math.random().toString()  let token  // Tests ...})To test the login flow, we will need 3 tests to ensure that the API:Rejects a request with a bad username, returning a 401 Unauthorized statusRejects a request with a valid username but incorrect passwordReturns a JWT token when correct credentials are provided.As part of the final test, I’ll also assign the returned JWT token returned by the API to the token variable in the parent describe block so it can be used in the GET /auth/user later on.// app-e2e.spec.tsdescribe(POST /auth/login, () => {  it(should return 401 status on bad username, () => {    return request(app.getHttpServer())      .post(/auth/login)      .set(Accept, application/json)      .send({        email: unknown@example.com,        password: incorrect,      })      .expect(401)  })  it(should return 401 status on bad password, () => {    return request(app.getHttpServer())      .post(/auth/login)      .set(Accept, application/json)      .send({        email,        password: incorrect,      })      .expect(401)  })  it(should return a JWT token on successful login, () => {    return request(app.getHttpServer())    .post(/auth/login)    .set(Accept, application/json)    .send({      email,      password,    })    .expect(201)    .expect(res => {      expect(res.body.access_token).toBeDefined()      token = res.body.access_token    })  })})Given the token obtained from the Login request, does it successfully identify the user and do the details returned form the API match the original details the user registered with?// app-e2e.spec.tsdescribe(GET /auth/user, () => {  it(should authenticate user with the JWT token, () => {    return request(app.getHttpServer())      .get(/auth/user)      .set(Authorization, `Bearer ${token}`)      .expect(200)      .expect(res => {        expect(res.body.email).toEqual(email)      })  })})Additionally, we’ll also want to test that the user won’t have access to any route with this guard if they either don’t have a token or supply an invalid token.// app-e2e.spec.tsit(should return error if no JWT supplied, () => {  return request(app.getHttpServer())    .get(/auth/user)    .expect(401)})it(should return error if incorrect JWT supplied, () => {  return request(app.getHttpServer())    .get(/auth/user)    .set(Authorization, `Bearer ${token.replace(/[0-9]+/g, X)}`)    .expect(401)})In the code token.replace(/[0-9]+/g, X) Im replacing all numbers with an X. In theory this mimics the behaviour of a would-be hacker who may try to change the payload of the token but in reality it doesnt matter what the change is as long as it invalidates the signature.Then, finally to clean up the database, we can add an afterAll hook to delete the user after all of the Auth tests have run.describe(Auth, () => {  const email = `${Math.random()}@adamcowley.co.uk`  const password = Math.random().toString()  let token  afterAll(() => app.get(Neo4jService).write(MATCH (n:User {email: $email}) DETACH DELETE n, { email }))  // ...}RecapThis was a lengthy session but we’ve covered a lot of ground. We’ve:Created a User module which provides a service for interacting with Users in the database. The service will allow you to find a User by its email address and create a user following business rules.Created an Auth service which will create users via the UserService, authenticate the User using their email address and password, and issue a JWT token to allow them to access protected API endpoints.Created a Guard that will read the JWT token and either permit or deny access to the protected endpoints.Created an Auth Controller with routes for registering and signing in.Added end-to-end tests to ensure that User Registration and Authentication flow works as expectedAll of the code plus a write-up of each session is available on Github at https://github.com/adam-cowley/twitch-project. If you have any questions, comments or if you would like to see a feature added to the API, feel free to open a Github Issue.Join me Tuesdays at 12:00 UTC / 13:00BST / 14:00CEST / 15:30 IST on the Neo4j Twitch Channel for the next session.;Jul 20, 2020;[]
https://medium.com/neo4j/getting-started-with-neo4j-on-pivotal-container-service-pks-49f5de5a8e52;David AllenFollowJul 16, 2019·4 min readGetting Started with Neo4j on Pivotal Container Service (PKS)Neo4j recently released a distribution of Neo4j Enterprise Causal Clustering on Pivotal Container Service (PKS). This distribution makes it easy to spin up a clustered, highly available graph database on top of Kubernetes quickly and easily.Neo4j on Pivotal Container Service (PKS). Get it?This article is going to describe what this is all about, and how to get started with it! To discuss and connect with other community members doing this, make sure to drop by the Neo4j Community Site’s cloud topic.Getting StartedYou can find Neo4j on Pivotal’s PivNet site here. You’ll want to start by downloading both artifacts, the Neo4j Docker image, and the Neo4j Helm Chart package. You’ll also want to check out the documentation that comes with it.Neo4j on PivNetInstallingThe first thing to do is to load the downloaded docker image on to your local machine, like so:$ docker load < causal-cluster_image.*.tgz(...)Loaded image: gcr.io/neo4j-pivotal/causal-cluster:3.5.7-master-eb023576dddee4f8cdc5afb6041c796f460f8a46Next, unarchive the downloaded helm chart:$ tar zxvf neo4j-3.5.7.tgzx neo4j/Chart.yamlx neo4j/values.yamlx neo4j/templates/_helpers.tplx neo4j/templates/core-dns.yamlx neo4j/templates/core-statefulset.yamlx neo4j/templates/expanded.yamlx neo4j/templates/poddisruptionbudget.yamlx neo4j/templates/readreplica-dns.yamlx neo4j/templates/readreplicas-statefulset.yamlx neo4j/templates/secret.yamlx neo4j/README.mdx neo4j/test.shFinally, we’ll use helm to install Neo4j as a package into our Kubernetes cluster. Take special note of the parameters being provided here and adjust according to your needs. For full descriptions of what these parameters do, and what your other options are, check the documentation.Take special note of the image, and how it matches the loaded image above. Yours might differ depending on release you’re using, so make sure it’s right.NAMESPACE=defaultAPP_NAME=my-neo4j-deployIMAGE=gcr.io/neo4j-pivotal/causal-cluster:3.5.7-master-eb023576dddee4f8cdc5afb6041c796f460f8a46$ helm install neo4j --namespace $NAMESPACE --name $APP_NAME \    --set namespace=$NAMESPACE \    --set image=$IMAGE \    --set name=$APP_NAME \    --set neo4jPassword=mySecretPassword \    --set authEnabled=true \    --set coreServers=3 \    --set readReplicaServers=1 \    --set cpuRequest=200m \    --set memoryRequest=1Gi \    --set volumeSize=2Gi \    --set volumeStorageClass=standard \    --set acceptLicenseAgreement=yesNAME:   my-neo4j-deployLAST DEPLOYED: Tue Jul 16 11:25:19 2019NAMESPACE: defaultSTATUS: DEPLOYEDRESOURCES:==> v1/SecretNAME                           TYPE    DATA  AGEmy-neo4j-deploy-neo4j-secrets  Opaque  1     1s==> v1/ServiceNAME                                   TYPE       CLUSTER-IP  EXTERNAL-IP  PORT(S)                     AGEmy-neo4j-deploy-neo4j                  ClusterIP  None        <none>       7474/TCP,7473/TCP,7687/TCP  1smy-neo4j-deploy-neo4j-readreplica-svc  ClusterIP  None        <none>       7474/TCP,7473/TCP,7687/TCP  1s==> v1beta2/StatefulSetNAME                           READY  AGEmy-neo4j-deploy-neo4j-core     0/3    1smy-neo4j-deploy-neo4j-replica  0/1    1sYou’ll see that helm outputs some messages related to the StatefulSets it is creating and the fact they’re not ready yet. We’ll need a few minutes for everything to come up happily. If you’re interested in a deeper breakdown of what resources are being created and how they all work together, read this article, which goes over how Neo4j works with Kubernetes.Things are looking goodWhere’s My Data?Neo4j on PKS uses persistent volume claims of the size and storage class you specified when installing via helm. These PVCs in turn map to the /data directory inside of the pod. The actual container running is essentially Neo4j’s Standard Docker Container, for the purposes of understanding layout and configuration.Neo4j stores data on Persistent Volume ClaimsOnce everything is up, you’ll see that the pods and StatefulSets are happy, and we’re ready to start using this cluster. Let’s run a cypher-shell against our new instance. The password we’re using is what we specified above on deploy.The cypher-shell tool is provided in the container that ships with this, so connecting to your cluster is a matter of running that tool, and connecting to the right URL where your service is deployed.APP_INSTANCE_NAME=my-neo4j-deployNEO4J_PASSWORD=mySecretPasswordkubectl run -it --rm cypher-shell \ --image=gcr.io/neo4j-pivotal/causal-cluster:3.5.7-master-eb023576dddee4f8cdc5afb6041c796f460f8a46 \ --restart=Never \ --namespace=default \ --command -- ./bin/cypher-shell -u neo4j \ -p  $NEO4J_PASSWORD  \ -a $APP_INSTANCE_NAME-neo4j.default.svc.cluster.localIf you dont see a command prompt, try pressing enter.Connected to Neo4j 3.5.7 at bolt://my-neo4j-deploy-neo4j.default.svc.cluster.local:7687 as user neo4j.Type :help for a list of available commands or :exit to exit the shell.Note that Cypher queries must end with a semicolon.neo4j> match (n) return count(n)+----------+| count(n) |+----------+| 0        |+----------+1 row available after 276 ms, consumed after another 0 msneo4j> return apoc.version()+----------------+| apoc.version() |+----------------+|  3.5.0.3       |+----------------+1 row available after 2 ms, consumed after another 0 msCleaning UpWhen you’re done with a cluster, you can shut it down and remove all resources with a single easy command.$ helm delete --purge my-neo4j-deployrelease  my-neo4j-deploy  deletedRelated Resources for Neo4j on KubernetesThe following is a list of other articles and tutorials you may want to check out that will help with Neo4j on Kubernetes!Neo4j on PKS DocumentationHow to Backup Neo4j on KubernetesHow to Restore Neo4j on KubernetesNeo4j Considerations in Orchestration Environments — or, all of the kubernetes-specific considerations about running Neo4j.Querying Neo4j Clusters, which describes how queries are routed in clustered setup.Neo4j Docker Container Documentation;Jul 16, 2019;[]
https://medium.com/neo4j/creating-a-knowledge-graph-from-video-transcripts-with-gpt-4-52d7c7b9f32c;Tomaz BratanicFollowApr 3·12 min readCreating a Knowledge Graph From Video Transcripts With ChatGPTUse GPT-4 as a domain expert to help you extract knowledge from a video transcript.Image by the author.A couple of days ago, I got access to GPT-4.The first thing that came to my mind was to test how well it performs as an information extraction model, where the task is to extract relevant entities and relationships from a given text.I have already played around with GPT-3.5 a bit. The most important thing I noticed is that we don’t want to use the GPT endpoint as an entity linking solution or have it come up with any other external references like citations, as it likes to hallucinate those types of information.However, a great thing about GPT-3 or GPT-4 is that it performs well in various domains. For example, we can use it to extract people, organizations, or locations from a text.However, I feel that competing against dedicated NLP models is not where the GPT models shine (although they perform well). Instead, the strength of GPT models is in their ability to generalize and be used in other domains where other open-sourced models fail due to their limited training data.My friend Michael Hunger gave me a great idea to test the GPT-4 on extracting information from a nature documentary.Photo by Jong Marshes on UnsplashI always liked the deep sea documentary as the ecosystem and animals vastly differ from terrestrial ones. Therefore, I decided to test GPT-4 information extraction capabilities on an underwater documentary. Additionally, I don’t know of any open-source NLP models trained to detect relationships between sea plants and creatures. So, a deep sea documentary makes for an excellent example of using a GPT-4 to construct a knowledge graph.All the code is available on GitHub in the form of a Jupyter Notebook.DatasetThe most accessible place to find documentaries is YouTube. Although the GPT-4 is multi-modal (supports video, audio, and text), the current version of the endpoint only supports text inputs. Therefore, we will analyze a video’s audio transcript, not the video itself.We will be analyzing the transcript of the following documentary.First of all, I like the topic of the documentary: The Spectacular Underwater World of Coral Reefs.”Secondly, extracting captions from a YouTube video is effortless as we don’t have to use any audio2text models at all. However, converting audio to text with all the available models on HuggingFace or even OpenAI’s Whisper should not be a big problem.Thirdly, this video has captions that are not auto-generated. At first, I tried to extract information from auto-generated captions on YouTube, but I learned that they might not be the best input. So if you can, avoid using auto-generated YouTube captions.The captions can be retrieved straightforwardly with the YouTube Transcript/Subtitle library. All we have to do is to provide the video id.from youtube_transcript_api import YouTubeTranscriptApivideo_id =  nrI483C5Tro transcript = YouTubeTranscriptApi.get_transcript(video_id)print(transcript[:5])The transcript has the following structure.[   {       text : water the liquid that oceans are made of ,       start :5.46,       duration :4.38   },   {       text : and it fills endless depths only few will venture\\xa0\\xa0 ,       start :12.24,       duration :4.92   },   {       text : out into the endless open ocean\\xa0\nof this vast underwater world ,       start :17.16,       duration :4.68   }]The captions are split into chunks, which can be used as video subtitles. Therefore, the start and duration information is provided along with the text. You might also notice a couple of special characters like \xa0 and \n .Even though GPT-4 endpoint support up to 8k tokens per request, more is needed to process the whole transcript in a single request. Therefore, we need to split the transcript into several parts.So, I decided to split the transcript into multiple parts, where the end of the part is determined when there are five or more seconds of no captions, announcing a brief pause in narration. Using this approach, I aim to keep all connecting text together and retain relevant information in a single section.I used the following code to group the transcript into several sections.# Split into sections and include start and end timestampssections = []current_section =   start_time = Noneprevious_end = 0pause_threshold = 5for line in transcript:    if current_section and (line[ start ] - previous_end > pause_threshold):        # If there is a pause greater than 5s, we deem the end of section        end_time = line[ start ]        sections.append(            {                 text : current_section.strip(),                 start_time : start_time,                 end_time : end_time,            }        )        current_section =           start_time = None    else:        # If this is the start of a new section, record the start time        if not start_time:            start_time = line[ start ]        # Add the line to the current paragraph        clean_text = line[ text ].replace( \n ,    ).replace( \xa0 ,    )        current_section +=    .join(clean_text.split()) +            # Tag the end of the dialogue        previous_end = line[ start ] + line[ duration ]# If theres a paragraph left at the end, add it to the list of paragraphsif current_section:    end_time = transcript[-1][ start ] + transcript[-1][ duration ]    sections.append(        {             text : current_section.strip().replace( \n ,    ).replace( \xa0 ,    ),             start_time : start_time,             end_time : end_time,        }    )# Remove empty paragraphssections = [p for p in sections if p[ text ]]To evaluate the results of the section grouping, I printed the following information.# Number of paragraphsprint(f Number of paragraphs: {len(sections)} )print(f Max characters per paragraph: {max([len(el[text]) for el in sections])} )There are 77 sections, with the longest having 1267 characters in it. We are far from the GPT-4 token limit, and I think the above approach delivers a nice text granularity, at least in this example.Information Extraction With GPT-4GPT-4 endpoint is optimized for chat but works well for traditional completion tasks. As the model is optimized for conversation, we can provide a system message, which helps set the assistant’s behavior along with any previous messages that can help keep the context of the dialogue. However, as we are using the GPT-4 endpoint for a text completion task, we will not provide any previous messages.I used the following prompt as the system message.system =  You are an archeology and biology expert helping us extract relevant information. However, I noticed that the model behaved almost identically when I provided the system message or not. Next, I developed the following prompt through some iterations, which extracts relevant entities and relationships from a given text.# Set up the prompt for GPT-3 to completeprompt =    #This a transcript from a sea documentary.#The task is to extract as many relevant entities to biology, chemistry, or archeology.#The entities should include all animals, biological entities, locations.#However, the entities should not include distances or time durations.#Also, return the type of an entity using the Wikipedia class system and the sentiment of the mentioned entity,#where the sentiment value ranges from -1 to 1, and -1 being very negative, 1 being very positive#Additionally, extract all relevant relationships between identified entities.#The relationships should follow the Wikipedia schema type.#The output of a relationship should be in a form of a triple Head, Relationship, Tail, for example#Peter, WORKS_AT, Hospital/n# An example  St. Peter is located in Paris  should have an output with the following formatentitySt. Peter, person, 0.0Paris, location, 0.0relationshipsSt.Peter, LOCATED_IN, Paris\n   The GPT-4 is prompted to extract relevant entities from a given text. Additionally, I added some constraints that distances and time durations should not be treated as entities. The extracted entities should contain their name, type, and sentiment. As for the relationships, they should be provided in the form of a triple. I added some hints that the model should follow the Wikipedia schema type, making the extracted relationship types more standardized. I learned that it is always good to provide an example of the output. Otherwise, the model might use different output formats at will.One thing to note is that we might have instructed the model to provide us with a nice JSON representation of extracted entities and relationships. Nicely structured data might certainly be plus. However, you are paying the price for nicely structured JSON objects as the cost of the API is calculated per input and output token count. Therefore, the JSON boilerplate comes with a price.Next, we need to define the function that calls the GPT-4 endpoint and processes the response.@retry(tries=3, delay=5)def process_gpt4(text):    paragraph = text    completion = openai.ChatCompletion.create(        model= gpt-4 ,        # Try to be as deterministic as possible        temperature=0,        messages=[            { role :  system ,  content : system},            { role :  user ,  content : prompt + paragraph},        ],    )    nlp_results = completion.choices[0].message.content        if not  relationships  in nlp_results:        raise Exception(             GPT-4 is not being nice and isnt returning results in correct format         )        return parse_entities_and_relationships(nlp_results)Even though we explicitly defined the output format in the prompt, the GPT-4 model sometimes does its own thing and does not follow the rules. It happened to me only twice out of a couple of hundred requests. However, it is annoying when that happens, and all the downstream dataflow doesn’t work as intended. Therefore, I added a simple check of the response and added a retry decorator in case that happens.Additionally, I only added the temperature parameter to make the model behave as deterministic as possible. However, when I rerun the transcript a couple of times, I got slightly different results. It costs around $1.6 to process the transcript of the chosen video with GPT-4.Graph Model and ImportWe will be using Neo4j to store the results of the information extraction pipeline. I have used a free Neo4j Sandbox instance for this project, but you can also use the AuraDB Free or local Desktop environment.One thing is certain. No NLP model is perfect. Therefore, we want all extracted entities and relationships to point to the text where they were extracted, which allows us to verify the validity of information if necessary.Graph schema. Image by the author.Since we want to point the extracted entities and relationships to the relevant text, we need to include the sections along with the video in our graph. The section nodes contain the text, start, and end time. Entities and relationships are then connected to the section nodes. What might be counterintuitive is that we represent extracted relationships as a node in our graph. The reason is that Neo4j doesn’t allow to have relationships to point to another relationship. However, we want to have a link between extracted relationship and its source text. Therefore, we need to model the extracted relationship as a separate node.The Cypher statement for the graph import is the following:import_query =    MERGE (v:Video {id:$videoId})CREATE (v)-[:HAS_SECTION]->(p:Section)SET p.startTime = toFloat($start),    p.endTime = toFloat($end),    p.text = $textFOREACH (e in $entities |  MERGE (entity:Entity {name: e[0]})  ON CREATE SET entity.type = e[1]   MERGE (p)-[:MENTIONS{sentiment:toFloat(e[2])}]->(entity))WITH pUNWIND $relationships AS relationMERGE (source:Entity {name: relation[0]})MERGE (target:Entity {name: relation[2]})MERGE (source)-[:RELATIONSHIP]->(r:Relationship {type: relation[1]})-[:RELATIONSHIP]->(target)MERGE (p)-[mr:MENTIONS_RELATIONSHIP]->(r)   Finally, we can go ahead and process the whole transcript and import the extracted information into Neo4j using the following code:with driver.session() as session:    for i, section in enumerate(sections):        print(f Processing {i} paragraph )        text = section[ text ]        start = section[ start_time ]        end = section[ end_time ]        entities, relationships = process_gpt4(text)        params = {             videoId : video_id,             start : start,             end : end,             text : text,             entities : entities,             relationships : relationships,        }        session.run(import_query, params)You can open Neo4j Browser and validate the import by executing the following Cypher statement.CALL apoc.meta.graph()The meta-graph procedure should return the following graph visualization.Generated graph schema. Image by the author.Entity Disambiguation With GPT-4After inspecting the GPT-4 results, I have decided that performing a simple entity disambiguation would be best. For example, there are currently five different nodes for a Moray Eels:moray eelMorayMoray EelmoraymoraysWe could lowercase all entities and use various NLP techniques to identify which nodes refer to the same entities. However, we can also use the GPT-4 endpoint to perform entity disambiguation. I wrote the following prompt to perform entity disambiguation.disambiguation_prompt =    #Act as a entity disambiugation tool and tell me which values reference the same entity. #For example if I give you##Birds#Bird#Ant##You return to me##Birds, 1#Bird, 1#Ant, 2##As the Bird and Birds values have the same integer assigned to them, it means that they reference the same entity.#Now process the following values\n   The idea is to assign the same integers to nodes that refer to the same entity. Using this prompt, we are able to tag all nodes with additional disambiguation properties.def disambiguate(entities):    completion = openai.ChatCompletion.create(        model= gpt-4 ,        # Try to be as deterministic as possible        temperature=0,        messages=[            { role :  user ,  content : disambiguation_prompt +  \n .join(all_animals)},        ],    )    disambiguation_results = completion.choices[0].message.content    return [row.split( ,  ) for row in disambiguation_results.split( \n )]all_animals = run_query(   MATCH (e:Entity {type: animal})RETURN e.name AS animal   )[animal].to_list()disambiguation_params = disambiguate(all_animals)run_query(       UNWIND $data AS rowMATCH (e:Entity {name:row[0]})SET e.disambiguation = row[1]   ,    { data : disambiguation_params},)Now that the disambiguation information is in the database, we can use it to evaluate the results.MATCH (e:Entity {type: animal })RETURN e.disambiguation AS i, collect(e.name) AS entitiesORDER BY size(entities) DESCLIMIT 5ResultsWhile this disambiguation is not that complicated, it is still worth noting that we can achieve this without NLP knowledge or having to develop any hand-crafted rules.AnalysisIn the final step of this blog post, we will evaluate the results of the information extraction pipeline using the GPT-4 model.First, we will examine the type and count of extracted entities.MATCH (e:Entity)RETURN e.type AS type, count(*) AS countORDER BY count DESCLIMIT 5ResultsMost entities are animals, locations, and biological entities. However, we can notice that sometimes the model decides to use the whitespace and other times underscore for biological entities.Throughout my experiments with GPT endpoints, I have observed that the best approach is to be as specific as possible in what information and how you want it to be categorized. Therefore, it is good practice with GPT-4 to define the types of entities we want to extract, as the resulting types will be more consistent.Additionally, the model didn’t classify 33 entity types. The thing is that GPT-4 might come up with some types for these entities if asked. However, they only appear in the relationship extraction part of the results, where entity types are not requested. One workaround could be to ask for entity types in the relationship extraction part.Next, we will examine which animals are the most mentioned in the video.MATCH (e:Entity {type: animal })RETURN e.name AS entity, e.type AS type,       count{(e)<-[:MENTIONS]-()} AS mentionsORDER BY mentions DESCLIMIT 5ResultsThe most mentioned animals are moray eels, lionfish, and brittle stars. I am familiar only with eels, so watching the documentary to learn about other fishes might be a good idea.We can also evaluate which relationships or facts have been extracted regarding moray eels.MATCH (e:Entity {name: morays })-[:RELATIONSHIP]->(r)-[:RELATIONSHIP]->(target)RETURN e.name AS source, r.type AS relationship, target.name AS target,       count{(r)<-[:MENTIONS_RELATIONSHIP]-()} AS mentionsUNION ALLMATCH (e:Entity {name: morays })<-[:RELATIONSHIP]->(r)<-[:RELATIONSHIP]-(source)RETURN source.name AS source, r.type AS relationship, e.name AS target,       count{(r)<-[:MENTIONS_RELATIONSHIP]-()} AS mentionsResultsThere is quite a lot we can learn about moray eels. They cooperate with groupers, coexist with Triggerfishes, and are being cleaned by cleaner shrimps. Additionally, a moray searching for a female moray can be relatable.Let’s say we want to check if the relationship that morays interact with lionfish is accurate. We can retrieve the source text and validate the claim manually.MATCH (e:Entity)-[:RELATIONSHIP]->(r)-[:RELATIONSHIP]->(t:Entity)WHERE e.name =  morays  AND r.type =  INTERACTS_WITH  AND t.name =  Lionfish MATCH (r)<-[:MENTIONS_RELATIONSHIP]-(s:Section)RETURN s.text AS textResultsly tough are its cousins the scorpion fishes they lie there as if dead especially when others aroundthem freak out and even when moray eels fight with lionfishes for foodThe text mentions that eels fight with lionfish for food. We can also notice that the transcript is hard to read and understand, even for a human. Therefore, we can commend GPT-4 for doing a good job on a transcript where even a human might struggle.Lastly, we can use the knowledge graph as a search engine that returns timestamps of sections where relevant entities we want to see. So, for example, we can ask the database to return all the timestamps of sections in which lionfish is mentioned.MATCH (e:Entity {name: Lionfish })<-[:MENTIONS]-(s:Section)<-[:HAS_SECTION]-(v:Video)RETURN s.startTime AS timestamp, s.endTime AS endTime,        https://youtube.com/watch?v=  + v.id +  &t=  + toString(toInteger(s.startTime)) AS URLORDER BY timestampResultsSummaryThe remarkable ability of GPT-3.5 and GPT-4 models to generalize across various domains is a powerful tool for exploring and analyzing different datasets to extract relevant information. Honestly, I’m not entirely sure which endpoint I would use to recreate this blog post without GPT-4. As far as I know, there are no open-source relation extraction models or datasets on sea creatures. Therefore, to avoid the hassle of labeling a dataset and training a custom model, we can simply utilize a GPT endpoint. Furthermore, I eagerly anticipate the opportunity to examine its promised capability for multi-modal analysis based on audio or text input.As always, the code is available on GitHub.;Apr 3, 2023;[]
https://medium.com/neo4j/exploring-power-laws-with-neo4j-c5ba6203a2d5;Nathan SmithFollowApr 27, 2020·6 min readJean-Michel Garcia on unsplashExploring Power Laws with Neo4jChapter 18 of Networks, Crowds, and Markets by David Easley and Jon Kleinberg covers Power Laws and Rich-Get-Richer Phenomena. The authors describe how a simple model of individual decision making can lead to a network where a few nodes have many more connections than the rest. The distribution of node degree follows a pattern called a power law that is found many times in the real world, from the populations of cities to the diameters of dust devils on Mars.To see how a power law distribution might arise, Easley and Kleinberg provide this algorithm based on links among web pages:Pages are created in order, and named 1, 2, 3, . . . , N.When page j is created, it produces a link to an earlier Web page according to the following probabilistic rule (which is controlled by a single number p between 0 and 1). (a) With probability p, page j chooses a page i uniformly at random from among all earlier pages, and creates a link to this page i. (b) With probability 1−p, page j instead chooses a page i uniformly at random from among all earlier pages, and creates a link to the page that i points to. (c) This describes the creation of a single link from page j one can repeat this process to create multiple, independently generated links from page j. (However, to keep things simple, we will suppose that each page creates just one outbound link.)We can execute this algorithm in Neo4j to see a power law in action.First, start a free Neo4j sandbox. When you log in, choose New Project,” and then Blank Sandbox.” Next click Launch Sandbox.” After your sandbox starts, choose Launch Browser.”We’ll seed our simulation with two pages that point to each other.CREATE (p1:Page {pageId:1})-[:LINKS_TO]->(p2:Page {pageId:2}),(p2)-[:LINKS_TO]->(p1)RETURN *The algorithm has a parameter called p that represents the probability that a page will link to either a randomly selected node or to the page linked to from a randomly selected node. You can experiment with different values for the p. Execute this code in Neo4j browser to set the value to 0.5.:param p => 0.5Now we’ll use a Cypher statement that creates a new page, selects a random previously created page, and links to either the selected random page or the target of the random page’s outbound link.//find the maximum prior page idMATCH (p:Page)WITH max(p.pageId) AS maxId//create a new node with a page id one higher than the previous maxCREATE (newPage:Page {pageId:maxId + 1})WITH newPage//choose one existing page and its outbound target at randomMATCH (source:Page)-[:LINKS_TO]->(target:Page)WITH newPage, source, target, RAND() AS randSort, RAND() AS randProb ORDER BY randSort LIMIT 1//with probability $p link to either source or targetWITH  newPage, source, target, CASE WHEN randProb < $p THEN source ELSE target END AS linkTo, randSort, randProbMERGE (newPage)-[:LINKS_TO]->(linkTo)RETURN *In the code block above, we call Cypher’s RAND function twice to generate two random variables. The first random variable randSort is used to sort the results of our (source:Page)-[:LINKS_TO]->(target:Page) pattern search in random order. The LIMIT 1 clause gives us the first result from this randomly sorted list.The second random variable randProb is compared with the value of the parameter p. If randProb is less than p, we create a link from the newly created page to the randomly selected existing page (called source in our pattern search). If randProb is greater than or equal to p, we create a link from the newly created page to the target of the randomly selected existing page’s outbound link.Run the code multiple times to create new page nodes. You will probably notice that page 1 and page 2 are coming up frequently as link targets for your new nodes.The apoc library has a charmingly-named function called apoc.periodic.rock_n_roll that will allow us to repeat query many times. The query below will run the code 990 times to create new nodes and links.CALL apoc.periodic.rock_n_roll( UNWIND RANGE(1,990) AS times RETURN times , MATCH (p:Page)WITH MAX(p.pageId) AS maxIdCREATE (newPage:Page {pageId:maxId + 1})WITH newPageMATCH (source:Page)-[:LINKS_TO]->(target:Page)WITH newPage, source, target, RAND() AS randSort, RAND() AS randProb ORDER BY randSort LIMIT 1WITH  newPage, source, target, CASE WHEN randProb < .5 THEN source ELSE target END AS linkTo, randSort, randProbMERGE (newPage)-[:LINKS_TO]->(linkTo) , 100)Let’s return the first 100 page nodes created and look at them visually.MATCH (p:Page) WHERE p.pageId < 100 RETURN pYou can see that one central page has many more incoming links than the others.Run this query to see the distribution of incoming degrees.MATCH (target:Page)OPTIONAL MATCH (source:Page)-[:LINKS_TO]->(target)WITH target, COUNT(source) AS inLinksORDER BY target.pageIdRETURN inLinks, COUNT(*) AS nodeCount, COLLECT(target.pageId) AS nodeListORDER BY inLinksIn my simulation, one node ended up with 108 inbound links. About 66% of the nodes ended up with no inbound links at all. I exported my results to CSV to plot them.Here’s a chart showing the number of number of nodes with each in-degree.It’s pretty hard to see much on those axes. I ran the model for 5,000 nodes and plotted the results with the x and y axes on a log scale. You can see that the result starts to fall along a straight line. That’s what we would expect in a power law distribution. A log-log plot like this is one way to check if our data is approximating a power law distribution.We can also visualize the data to show the number of nodes having at least a given in-degree. You might have seen a chart like this in discussions of the long tail of online sales.At this point, you can delete all the nodes or start a new sandbox and run the experiment again with a different value for p. The numbers will change each time you try the experiment, but if you include enough nodes the overall shape of the power law distribution will be recognizable.;Apr 27, 2020;[]
https://medium.com/neo4j/find-circular-money-flow-with-neo4j-c9138e1c3183;Vlad BatushkovFollowMay 27, 2020·10 min readFind Circular Money Flow with Neo4jHow to use the subgraph technique to detect the circular money flow (money laundering) in the Neo4j Database of a finance systemPhoto by Allie Smith on UnsplashGoalThis article is written for educational purposes to share cool stuff about Neo4j graph data modelling, subgraph technique, and Cypher query building. Our goal is to illustrate a type of money laundering activity called Circular money flow”, of a very simplified financial system using a graph database. This post provides a conceptual view of how this can be approached using graph techniques. It can be useful study-case material for general purposes for beginners, who are already familiar with basic concepts of the Neo4j Database.Database SchemaWe will use a financial system to represent how money transfer from one business entity to another. Business entities involved in this money-game” are split into three types: Client, Company, and ATM.The domain model initially based on the explanations from the article How to generate a huge financial graph with money laundering patterns?” written by M grin. Thank you, M grin, for a well-explained domain.Entity Nodes(:Company) — a business entity, that can send and receive money.(:Client) — a business entity, that can send and receive money.(:ATM) — a business entity, used as an exit point for the money, so ATM cannot be a money sender.In a real system, nodes would likely contain specific information about entities, such as identifiers and other attributes. However, for the purposes of this example, we are only concerned about the existence of nodes. In this tutorial, we don’t need any properties on our nodes.Clients, Companies, and ATM and the money flows between themThe arrows represent money transfers. From the diagram, you may think, that Transactions can be relationships in our graph. Well, it is a possible option, but, I do not recommend you use relationships to represent Transactions in this instance. The Transaction should be a node, and here is why.Data modelling of TransactionsTransaction — is a noun. A good old definition of a node for data modelling.From the practical experience, it is also a valid choice. We want to manage them as independent entities. For example, querying all Transaction that were made in the last month is a trivial task on a :Transaction node label:MATCH (t:Transaction)WHERE t.timestamp.month = date().month - 1RETURN tLast, but not least: Indexes. The index is a well-known feature of Node’s properties and unfortunately non-trivial story for Relationships.Transaction Node & Relationships(:Transaction) — an entity, that represents an operation of money transfer between two members. It is a self-sufficient fact, that entity A” sends money to entity B”. Relationships between (A) — (Transaction) — (B) helps us to understand who is a sender and who is a receiver.[:OUT] relationship type — connects the entity of the source node and Transaction. Relationship means that Entity is a money sender.[:IN] relationship type — connects the entity of a destination node and Transaction. Relationship means that Entity is the receiver of money.CALL db.schema.visualization()Initial Schema in Neo4j DatabaseReminder: ATM used as an exit point for the money, so ATM cannot be a money sender.The Transaction node has 2 properties: the amount of money and timestamp of operation. This is important information for our task because we need to know how much money was sent and when it happened.Visualization of a graphNow, knowing all of the domain details, we can imagine our graph. Business entity nodes are connected to each other via Transaction nodes. It is also worth mentioning that between the same entities there can be many transactions.Clients, Companies, and ATM send money to each other using transfer money Transactions.Task RequirementsCircular money flows is a type of money laundering” activity in which the money is sent to different accounts, and sent back to the starting node after a certain number of hops. Our goal is to find all of these circular flows in the graph. We don’t need to find extremely long chains, so we will limit our investigations to 10 transactions in the money flow.Each participant in the money flow may send a marginally less money in the next transaction compare to the previous one. The total payment” for the whole travel should not exceed some % loss of the source amount.The flow of transactions will expect that each next” transaction happens only after the previous” one has completed. Transactions should be ordered by the creation timestamp.Example of circular money flow with 3% loss, looped at the Red clientGraph GenerationNow, the practical part. First of all, we need an empty Neo4j Database on a local machine. I will use my Neo4j docker boilerplate to build a simple Docker image from the official one, containing the APOC & GDS plugins.Simple way is to pull ready-to-use blank image from my docker hub profile and build it.docker pull vladbatushkov/neo4j-apoc-gds:latestdocker run -p 7474:7474 -p 7473:7473 -p 7687:7687 --name=money1 vladbatushkov/neo4j-apoc-gds:latestImportant to setup enough space in the heap and page cache to be able to process a huge amount of data. My image configured for next settings.ENV NEO4J_dbms_memory_heap_initial__size=4GENV NEO4J_dbms_memory_heap_max__size=4GENV NEO4J_dbms_memory_pagecache_size=2GYou can also build totally same image and configure it manually doing this.DockerfileBuild and run a custom docker container.docker build . -t=money:dev --no-cachedocker run -p 7474:7474 -p 7473:7473 -p 7687:7687 --name=money1 money:devData GenerationNow let’s talk about how to generate the required graph. The generation of a graph data set is always an interesting task to solve. In this particular case, I will use a well-known APOC Graph Generator procedure.apoc.generate.er(noNodes, noEdges, label, type)// generates a random graph according to the Erdos-Renyi modelMy approach to building a required graph is based on the following steps:Generate N nodes (Client) with M random relationships (TRANSFER_TO) between them— our initial set of nodes and relationships of business entities.Partition them between Clients, Companies, and ATM .Break the previously created relationship TRANSFER_TO into the Transaction node structure, as follows: ()-[OUT]->(:Transaction)<-[:IN]-()The final thing is to clean up the TRANSFER_TO relationships and add Indexes on Transaction properties. Do not forget to turn on the multi-statement mode in your browser.The script is idempotent, you can run it many times with parameters to fit your needs. Parameters I used are:500 000 Entities of5 000 ATMs50 000 Companies445 000 Clients5 000 000 Transactions ofamount of money is a random value up to 1000 conventional unitstimestamp is a random value from today minus up to 1000 minutes earlierMATCH (n)-[:OUT]->(t:Transaction)<-[:IN]-(m) RETURN n,t,m LIMIT 500Company (green), Client (khaki), ATM (red), Transaction (orange)Solution ideaWe now need to write a query to find all the chains, starting and ending at the same Client node. In addition, the amount of money on each Transaction node within the chain should not lose more than x% of the source amount. The number of transactions involved in the flow can be up to 10, and they should be ordered by time.The solution we are thinking of will involve building a proper path pattern.(c1)->(t1)->(e2)->(t2)->(e3)->(t3)->...->(t9)->(e10)->(t10)->(c1)Here we have start and end in the same Client (c1), in the middle with some Client or Company nodes (e2, e3,…), and in total the longest path can have up to 10 Transactions (t1, t2, …).(tj)->(ej)->(tj+1)Each neighbouring Transactions should match the following conditions:t1.timestamp < t2.timestampt1.amount > t2.amount(t1)->...->(tN)First and last Transactions also should match these additional conditions:(t1.amount — tN.amount) / t1.amount < Xstart and ends in the same Client: (t1)<-[:OUT]-(:Client)-[:IN]->(tN)As you can see, it would be a hard problem to query with so many conditions and node types involved: Transactions and Business entities. We need to think about some smart way to minimize the complexity of the query. We need to think about how actually we can solve this problem.SubgraphThe main idea behind a subgraph technique — to reshape the graph through adding new relationships. This can help us query the data easier. Our solution query going to work with a subgraph of original graph with additional structures created to solve our specific problem. This approach is very useful and can be used in many other scenarios.What is the money flow? It is a chain of Transactions. Let’s forget about business entities for a while and actually find-out, that all we need is actually just Transactions. We can build a new network” of only Transactions, that we didn’t have before (our subgraph). We can then explore that transaction network”, writing a query to detect circular money flows.A subgraph of Transactions on the top of actual database entitiesNow let’s build a subgraph by introducing a HOP relationship.[:HOP] RelationshipThe [:HOP] relationship between Transactions exists for the following conditions:Transaction A { timestamp } earlier than Transaction B { timestamp }Transaction B { amount } less than Transaction A { amount }Transaction B { amount } compare to Transaction A { amount } not less than a defined loss percentageWithout the last condition, the subgraph becomes wide-range acceptable. You can use the same subgraph and apply different loss percentages in your future query. The disadvantage of this approach is that it gives us more relationships, and as a result query performance overhead.In this generation script, I set the exact value of loss percentage, based on my needs. I define a loss as less than 25%, assuming that in my data 25% loss is still an acceptable value for circular money flow. Why it is so big? I have a random graph, and, assume, that a 10% loss for 10 hops is statistically a jackpot for randomly generated data. With a 25% loss, I’ll have a better chance to find a long-chain result.MATCH (t1:Transaction)-[:HOP*2..10]->(t2:Transaction) RETURN t1,t2 LIMIT 500Transaction hop chainSolution QueryNow it is time to write an actual query to solve the task. Once again, a reminder of the circular money flow requirements: the same Client in the beginning and at the end, time-ordered and match less than 25% loss each progressive traversal.As you can see, the query is really short and clear.It is worth highlighting the MATCH pattern used to find a path from first to the last Transaction via the :HOP relationships — this is only one valid money flow, that match all chain requirements. Thanks to the :HOP relationship, we can be sure, that not only first and last Transactions satisfy requirements, but also all Transactions in between:path = (t1:Transaction)-[:HOP*3..10]->(t2:Transaction)The PROFILE statement is used only to look at the execution plan. Here is the query result and execution plan for 5 449 818 Nodes and 15 996 048 Relationships. Circular money flow result with longest circular money flow of 8 Transactions.Example of Circular Money Flow result. Transaction (red), Client (green), Company (yellow).761571154 total db hits in 1041340 msIt takes around 17 minutes to analyse the whole graph. But do we need to analyze everything?Use-case exampleDepends on your situation you may think about query changes and apply more ideas, how to use this query.For example, when a new Transaction created, we can run a query against only that Transaction. We will verify, that this money not came from circular money flow, organized by Client, who received money (incoming Transaction).In this story, I have nothing except id of Node to use as a filter, so, my query change will be looks like this.PROFILE MATCH path = (t1:Transaction)-[:HOP*3..10]->(t2:Transaction)WHERE ID(t2) = 2801989 AND ......I catch a potential scammer and it was really fast.171 total db hits in 94 msConclusionI hope this story gives you a good understanding that graphs are a very good tool in these types of situations, and a perfect place to apply your creativity. Pick a small problem and try to solve it with a graph. If you need any help — ping me in LinkedIn, I would gladly chat with you about graphs.Thanks for reading and enjoy Neo4j. Clap-clap-clap to catch all the scammers.ResourcesHow to generate a huge financial graph with money laundering patterns?Couple of years ago my team (compliance in one of Swiss banks) and I had an interesting task to implement — we had to…medium.comData Modeling Concepts and Techniques | Neo4jWalk through this guide as an introduction into some of the core data modeling concepts and techniques that make Neo4j…neo4j.comNeo4j Performance Tuning - Neo4j Graph Database PlatformThe page cache is used to cache the Neo4j data as stored on disk. Ensuring that most of the graph data from disk is…neo4j.com;May 27, 2020;[]
https://medium.com/neo4j/configuring-stackdriver-logging-for-neo4j-vms-on-gcp-d175f32e2e1b;David AllenFollowMar 14, 2019·5 min readConfiguring Stackdriver Logging for Neo4j VMs on GCPIf you’re using Neo4j’s images on GCP, it can be a very useful thing to implement log shipping from the VM host into Google’s Stackdriver monitoring system, which is one of the best ways to do monitoring of services in their cloud.Shipping logs to Stackdriver lets you see what’s happening on all of your hosts without individually SSH logging into them, and permits you to set up alerts so that you can be notified when certain operations events occur.This article will describe how to set this up for Neo4j. Because Google is using software called Fluentd to get this done, this approach should also work most anywhere you want to use Fluentd and isn’t limited to Google.Neo4j Log Shipping to Google StackdriverStackdriver Logging AgentGoogle provides a logging agent for Stackdriver, which is based on fluentd. Without going into all of the details, this is simply a small daemon that sits on any host where you install it, and sends logging messages to Stackdriver. You can find the documentation here.The approach we’ll take to get this done is:Install the logging agent,Authorize it to send logs to StackdriverTell the logging agent where to find Neo4j logsAnd that’s it!Install the Logging AgentFirst, install the agent as per the documentation linked above:$ curl -sSO https://dl.google.com/cloudagents/install-logging-agent.sh$ sudo bash install-logging-agent.sh(Lots of DPKG output)==============================================================================Installation of google-fluentd complete.Logs from this machine should be visible in the log viewer at:  https://console.cloud.google.com/logs/viewer?project=your-project&resource=gce_instance/instance_id/598113179421906891A test message has been sent to syslog to help verify proper operation.Please consult the documentation for troubleshooting advice:  https://cloud.google.com/logging/docs/agentYou can monitor the logging agents logfile at:  /var/log/google-fluentd/google-fluentd.log==============================================================================Give the Logging Agent PermissionsYour GCP VM has a default service account with certain privileges. They may not be sufficient to write to Stackdriver, (by default they usually aren’t) so this is something worth checking. Rather than recapping all of the docs for the agent, here is the quick version of what you need to do:That the service account associated with your VM has the Logs Writer” roleThat the google-fluentd agent on your VM is using those service account credentials.Point #1 you can verify by using the IAM console in Google Cloud. Click on the first IAM tab there, browse by members, locate your service account, and edit it to add the role Logs Writer” if it is missing.Point #2 you can verify by generating a JSON key for your service account. In the IAM view, click Service Accounts”, select yours, and then use the Generate Key” feature to create and download a JSON key. Place this JSON key on the VM host at the path: /etc/google/auth/application_default_credentials.jsonJust to be sure, I restarted the fluentd service like this:sudo systemctl restart google-fluentdIf you run into any trouble with these steps, there are quite a few troubleshooting steps for the logging agent in the docs.Test it out!Before we get to the Neo4j parts, let’s make sure it works.david_allen@bitbucket-vm:/etc/google/auth$ logger  Is this crazy stackdriver thing working? david_allen@bitbucket-vm:/etc/google/auth$If you then go to the main GCP console, and select Logging -> Logs, you should see your message. This proves that the logging agent works, and that your permissions are good.Log messages received by StackDriverYou can also confirm that the agent is happy by tailing the logging agent’s logfile:$ tail -n 1 /var/log/google-fluentd/google-fluentd.log 2019-03-14 12:23:24 +0000 [info]: #0 Successfully sent gRPC to Stackdriver Logging API.Looking good!Tell Stackdriver About Neo4j Log FilesCreate a single file as /etc/google-fluentd/config.d/neo4j.conf with the following content:<source>     @type tail     format none     path /var/log/neo4j*/*.log     pos_file /var/lib/google-fluentd/pos/neo4j.pos     read_from_head true     tag neo4j</source>This should be pretty straightforward what we’re telling google-fluentd to do. Monitor all log files in that directory, always read from the beginning, keep track of where you are with the specified position file, and tag all of the records with neo4j”.One systemctl restart google-fluentd and you should start seeing Neo4j logs flowing through!If you need the full details on how to configure the agent, to understand how and why this works, they can be found here.Don’t Forget Systemd Log files!In certain packaged installations of Neo4j, which include the Neo4j Cloud VMs, the neo4j service runs in systemd, and doesn’t output a regular neo4j.log file to /var/log/neo4j.Tip: if you have a neo4j.log file, then this step doesn’t apply to you! If you use journalctl to access your neo4j logs, this step does apply to you.Create a new config file called /etc/google-fluentd/config.d/neo4j-systemd.conf with this content:<source>  @type systemd  path /run/log/journal  filters [{  _SYSTEMD_UNIT :  neo4j.service  }]  pos_file /var/lib/google-fluentd/pos/neo4j-systemd.pos  read_from_head true  tag neo4j</source>This configuration uses a fluentd plugin for systemd to get logs straight out of journal files.Tip: Take particular note of the path! This path works with most Neo4j VMs, but different linux distros put systemd journal files in different places.Restarting the system again, we can see now that systemd logs are flowing through as expected:Showing highlighted syslog entries coming through. Notice below that you can see an entry with logName neo4j, so we can distinguish between which log stream source is coming through.Start Using Stackdriver!Now that you’ve got all of your logs shipped over, you can start to use all of Stackdriver’s features. I’ll leave you with the simplest possible example — looking through logs tagged neo4j” for the logged in” message, which from Neo4j’s security.log will give you a complete accounting of who logged in when.Happy graph hacking.;Mar 14, 2019;[]
https://medium.com/neo4j/week-10-getting-dumps-and-example-projects-into-aura-free-6980b178dc69;Michael HungerFollowOct 12, 2021·4 min readWeek 10 — Getting Dumps and Example Projects Into Aura FreeThe main topic was to explore the different ways of getting existing graph data into Aura from other Neo4j instances:New: Built In Movies Database and GuideLoad a Dump from Neo4j Sandbox BackupLoad a Dump from a Neo4j Graph Example RepositoryLoad a Dump from Neo4j DesktopDiscover Aura Free — Data Import — Live Stream VideoNew: Built In Movies Database and GuideAura Free has a new option for the newly created database. Besides the default empty database you can also select one with the small Movies dataset.On opening Neo4j Browser it shows a introductory, interactive Neo4j Browser Guide that guides you through the basics of the graph model and the Cypher query language.Just make sure to connect to your database with your credentials after Neo4j-Browser opens. Click on :server connect if the connection dialogue doesn’t open automatically.Load a Dump from Neo4j Sandbox BackupSometimes you have already loaded or computed data into a Neo4j Sandbox — e.g. via libraries or functions that are not available in Aura Free like APOC-full, neosemantics (RDF), or Graph Data Science.Then you can take a Backup in the Sandbox UI and after downloading it, load it into your newly created or existing Aura Database.In our example, we loaded the Beer Graph” by Rik van Bruggen with apoc.load.html from a Wikipedia.The basic data model is:(BeerBrand)-[:IS_A]->(BeerType)(BeerBrand)<-[:BREWS]-(Brewery)(BeerBrand)-[:HAS_ALCOHOLPERCENTAGE]->(AlcoholPercentage)Here is the code to load it.WITH  https://nl.wikipedia.org/wiki/Lijst_van_Belgische_bieren  as url    CALL apoc.load.html(url, {        brand:  table.wikitable tbody tr td:eq(0) ,        beertype:  table.wikitable tbody tr td:eq(1) ,        alcoholpercentage:  table.wikitable tbody tr td:eq(2) ,        brewery:  table.wikitable tbody tr td:eq(3) ,        timeframe:  table.wikitable tbody tr td:eq(4)         }) yield valueWITH value, size(value.brand) as rangeupUNWIND range(0,rangeup) as iWITH value.brand[i].text as BeerBrand, value.brewery[i].text as Brewery,     value.alcoholpercentage[i].text as AlcoholPercentage,     value.beertype[i].text as BeerType, value.timeframe[i].text as TimeframeMERGE (bt:BeerType {name: coalesce(BeerType, Unknown )})MERGE (bb:BeerBrand {name: coalesce(BeerBrand, Unknown )})SET bb.Timeframe = coalesce(Timeframe, Unknown )MERGE (br:Brewery {name: coalesce(Brewery, Unknown )})MERGE (ap:AlcoholPercentage {value: coalesce(AlcoholPercentage, Unknown )})MERGE (bb)-[:HAS_ALCOHOLPERCENTAGE]->(ap)MERGE (bb)-[:IS_A]->(bt)MERGE (bb)<-[:BREWS]-(br)Afterwards we took the backup in the Sandbox UI, downloaded the dump file, and then used the Aura Import UI to upload the dump.It takes a few (10) minutes for the load to finish regardless of dump size as it’s queued in the backend system.Load a Dump from a Neo4j Graph Example RepositoryAll the sandbox datasets, with:browser guidesexample queriesdata modelscode examples (for Java, JavaScript, Python, .Net, Go and GraphQL)data-dump filesimport scripts… are available on the GitHub organization neo4j-graph-examplesneo4j-graph-examplesPeople This organization has no public members. You must be a member to see whos a part of this organization. You…github.comIn our case, we used the Recommendations” that contains the MovieLens user-ratings dataset together with movie and actor/director data from https://themoviedb.org.The Movie Database (TMDB)The Movie Database (TMDB) is a popular, user editable database for movies and TV shows.themoviedb.orgThe dataset consists of:28,863 nodes166,261 relationshipswhich fit nicely into the limits of an Aura Free Instance (50k nodes, 175k relationships).So we download the Neo4j 4.3 dump file from the GitHub repository folder and then upload it via the Aura UI as before.In your Aura Database you can then also use the Browser guide with::play https://guides.neo4j.com/sandbox/recommendations/index.htmlAnd run the code examples from Sandbox (e.g. from a GitHub Codespace on a Fork of the Repository) against your Aura Free instance.Load a Dump from Neo4j DesktopYou can also create a dump in Neo4j Desktop, just by selecting the dump” Action from your database menu.Create Database Dump in Neo4j DesktopThen the upload mechanism is the same as before.;Oct 12, 2021;[]
https://medium.com/neo4j/kickstart-your-transition-from-sql-analytic-and-window-functions-to-neo4j-987d67f7fdb4;Nathan SmithFollowSep 21, 2020·9 min readKickstart your transition from SQL analytic and window functions to Neo4jBefore I started working with graph databases, I worked with relational databases using the SQL query language for years. As a data scientist, analytic functions, sometimes known as window functions, are a regular part of my SQL data wrangling tool kit.With SQL analytic functions, I can run a calculation for each row in a table and have the calculation include values from other rows within a window before and after the row in question. Analyzing a row of data in relationship to its neighbors often provides useful insights. I use analytic functions to calculate things like percent of total, change since the previous observation, rank order, and running totals.These days, much of my work involves graph databases. I’m writing Cypher as often as SQL. Analyzing data points in the context of their neighbors comes naturally in a graph database. I can answer the questions I was investigating with SQL analytic functions using tools from Neo4j’s Cypher query language and the APOC library. In the examples below, we’ll have fun comparing SQL and Cypher using data from the Women’s World Cup.Photo by Jeffrey F Lin on UnsplashKey SQL syntaxSQL aggregate functions can use a group by clause to identify the fields that should be used to group the data. You get back one row per unique value in the group by fields.Sometimes we want to calculate a value that involves multiple rows, but we need to return all the rows that were part of the calculation instead of one row per unique value in a group by clause.This is where analytic or window functions come in. Different relational database vendors implement these functions slightly differently, but the main ideas are similar. You can use the syntaxover (partition by <field name>) to calculate an aggregation based on the rows that share a common value for the field named in the partition by clause while still returning all of the rows.Here’s a simple example of what this might look like in SQL.select order_number, line_number, price,sum(price) over (partition by order_number) order_total_pricefrom order_linesThis query would returnthe line number price for each line item on an order, but it would also give usthe sum of the price column for all rows that share the same order_number value as that row.The results might look like this.╒══════════════╤════════════╤═══════╤═══════════════════╕│ order_number │ row_number │ price │ order_total_price │╞══════════════╪════════════╪═══════╪═══════════════════╡│1001          │1           │ 100.5 │             138.2 │├──────────────┼────────────┼───────┼───────────────────┤│1001          │2           │  10.5 │             138.2 │├──────────────┼────────────┼───────┼───────────────────┤│1001          │3           │  27.2 │             138.2 │├──────────────┼────────────┼───────┼───────────────────┤│1002          │1           │  80.0 │             100.0 │├──────────────┼────────────┼───────┼───────────────────┤│1002          │2           │  20.0 │             100.0 │└──────────────┴────────────┴───────┴───────────────────┘Key Cypher syntaxCypher doesn’t require a group by clause. When you use an aggregate function, Cypher understands that any non-aggregated fields returned are part of the grouping. If we want to calculate an aggregation without losing sight of the individual items within each grouping, we can use Cypher’s collect() aggregation function. That special aggregation function returns a real list of all the items in a group.For example, this Cypher query might return six records:MATCH (s:state)-[:IN_REGION]->(r:Region) RETURN r.name AS region, s.abbreviation AS state╒════════╤═══════╕│ region │ state │╞════════╪═══════╡│ North  │ ND    │├────────┼───────┤│ North  │ MN    │├────────┼───────┤│ South  │ MS    │├────────┼───────┤│ South  │ AL    │├────────┼───────┤│ South  │ LA    │├────────┼───────┤│ West   │ OR    │└────────┴───────┘You could use collect to return one record for each region along with a list of the states in that region.MATCH (s:state)-[:IN_REGION]->(r:Region) RETURN r.name AS region, collect(s.abbreviation) AS states╒════════╤═════════════════╕│ region │ states          │╞════════╪═════════════════╡│ North  │[ ND , MN ]      │├────────┼─────────────────┤│ South  │[ MS , AL ,  LA ]│├────────┼─────────────────┤│ West   │[ OR ]           │└────────┴─────────────────┘Cypher’s UNWIND clause is the opposite of collect. It accepts a list and returns one row for each item in the list.WITH [1, 2, 3, 4] AS myListUNWIND myList AS itemRETURN *╒══════╤═════════╕│ item │ myList  │╞══════╪═════════╡│1     │[1,2,3,4]│├──────┼─────────┤│2     │[1,2,3,4]│├──────┼─────────┤│3     │[1,2,3,4]│├──────┼─────────┤│4     │[1,2,3,4]│└──────┴─────────┘Notice that all the elements that are part of the result set before the UNWIND clause are repeated in each row of the output. This includes the original list itself.We can use an aggregate function together with collect and unwind to compare an individual value with an aggregate.MATCH (d:Day)WHERE date(2020-01-01) <= d.date <= date(2020-01-05) WITH avg(d.highTemperature) as avgHigh, collect(d) as daysUNWIND days AS dayRETURN day.date AS date, day.highTemperature AS highTemperature, avgHigh╒════════════╤═════════════════╤═════════╕│ date       │ highTemperature │ avgHigh │╞════════════╪═════════════════╪═════════╡│ 2020-01-01 │34               │30.6     │├────────────┼─────────────────┼─────────┤│ 2020-01-02 │29               │30.6     │├────────────┼─────────────────┼─────────┤│ 2020-01-03 │35               │30.6     │├────────────┼─────────────────┼─────────┤│ 2020-01-04 │29               │30.6     │├────────────┼─────────────────┼─────────┤│ 2020-01-05 │26               │30.6     │└────────────┴─────────────────┴─────────┘Cypher’s list syntax allows us to select elements from a list (a slice) by index using bracket notation. We can also use a list comprehension to perform calculations on members of a list. This is similar to the way that list comprehensions work in Python.The APOC library offers many helpful functions that work on lists. We will explore some of them in the examples.Sandbox setupI am providing example syntax for SQL, but I leave it to you to create test data in your relational database of choice if you would like to try out the SQL code. For Neo4j, we can use a Neo4j sandbox. Sign in at https://sandbox.neo4j.com and create a new project. We’ll use the Women’s World Cup dataset for these examples.Choose Women’s World Cup 2019, then Launch ProjectPercent of totalSometimes we want to know how much of a group total each item represents. For example, we might want to know what percent of a team’s total goals were scored by each player.In SQL, we could do something like this:WITH(SELECT team.name as team, person.name as player, count(*) AS goalsFROM team INNER JOIN person ON team.team_id = person.team_idINNER JOIN goal ON goal.person_id = person.person_idGROUP BY team.name, person.name) AS individual_goalsSELECT team,player,SUM(goals) OVER (PARTITION BY team) AS team_goals,goals AS individual_goals,goals * 1.0/SUM(goals) OVER (PARTITION BY team) AS percent_totalFROM individual_goalsORDER BY goals * 1.0/SUM(goals) OVER (PARTITION BY team) DESCWe use a subquery to count the number of goals that have been scored by each individual. Next, we sum the individual goals for each team to calculate the team_goals value. If we divide the individual goals by the team_goals, we get the players’ percent of total team goals. We multiply by 1.0 as a quick way to convert integer to floating point values for division.In Cypher, our query looks like this:MATCH (team)<-[:REPRESENTS]-(p:Person)-[:SCORED_GOAL]->()WITH team, p, count(*) AS goalsWITH team, sum(goals) AS totalGoals, collect({player:p, goals:goals}) as personGoalsUNWIND personGoals as pgRETURN team.name AS team,  totalGoals, pg.player.name AS player, pg.goals AS individualGoals, toFloat(pg.goals)/totalGoals AS percentTotalORDER BY percentTotal DESC LIMIT 10We start by counting the individual goals for each player on each team in the first two lines of the query. In the next two lines, we sum the goals to the team level. We also preserve the individual player names and goal counts in a list of map/dict using the collect function.Next, we UNWIND the personGoals list of maps so that we can compare each person’s goal count with the total for their team. We list the values that we want to return, including the percentTotal calculation, and use a LIMIT clause to return the top 10 values.Change since previous observation (lead or lag)Sometimes it can be helpful to compare a record with the prior or subsequent observation. This comes up frequently in time series data. Relational database vendors provide lead and lag functions to make this possible.If we wanted to compare team USA’s score in each match to their score in their prior match in the tournament, the SQL query might look something like this.SELECT tournament.name,match.match_date,team_match.score,lag(match.match_date, 1) over (partition by tournament_key order by match.match_date) AS prior_match_date,lag(team_match.score, 1) over (partition by tournament_key order by match.match_date) as prior_scoreFROM teamINNER JOIN team_matchon team.team_key = team_match.team_keyINNER JOIN matchON match.match_key = team_match.match_keyINNER JOIN tournamentON tournament.tournement_key = match.tournament_keyWHERE team.name = USAORDER BY match.match_dateIn a graph database, instead of calculating the next record in a result set, we might use relationships to indicate sequence. If you frequently need to compare values that are adjacent in time, adding a relationship for this purpose would make sense. The query below creates FOLLOWED relationships between each match for the USA and their prior match.The query wraps a collect function with the apoc.coll.sortNodes() function that sorts the list of matches on their date property. The ^ before the date property name tells the function to sort the list in ascending order. The apoc.coll.pairsMin() function breaks a list into a list of node pairs where each node is associated with the next node in the list.MATCH (t:Team {name: USA })-[p:PLAYED_IN]->(m:Match)-[:IN_TOURNAMENT]->(tourn)WITH tourn, apoc.coll.sortNodes(collect(m), ^date) as sortedMatchesUNWIND apoc.coll.pairsMin(sortedMatches) AS pairWITH pair[1] AS second, pair[0] AS firstMERGE (second)-[:FOLLOWED]->(first)RETURN *With this relationship in place, it is straightforward and efficient to return a comparison of each match’s score with the previous match in the tournament.MATCH (t:Team {name: USA })-[s1:PLAYED_IN]->(m1:Match)-[:IN_TOURNAMENT]->(tourn)OPTIONAL MATCH(t)-[s2:PLAYED_IN]->(m2)<-[:FOLLOWED]-(m1)RETURN tourn.name AS tournament, m1.date AS matchDate, s1.score AS score, m2.date AS priorMatchDate, s2.score AS priorScoreORDER BY m1.dateIf we don’t have a FOLLOWS relationship in our graph schema, and we don’t want to add one, we can still calculate the prior score on the fly with a query like this.MATCH (t:Team {name: USA })-[p:PLAYED_IN]->(m:Match)-[:IN_TOURNAMENT]->(tourn)WITH tourn, apoc.coll.sortMaps(collect({matchDate:m.date, score:p.score}), matchDate) AS sortedScoresUNWIND apoc.coll.pairs(sortedScores) AS pairRETURN tourn.name AS tournament, pair[0].matchDate AS matchDate, pair[0].score AS score,pair[1].matchDate AS priorMatchDate,pair[1].score AS priorScoreORDER BY pair[0].matchDateRankWe might like to have a rank order to number each row in our output. This might be helpful for formatting results, looking at the top n results for each group, or calculating percentiles. There are several ways to handle ties when we are ranking, and relational database vendors differ a bit in the syntax for those subtleties, but the query below will point you in the right direction.Let’s say we want to number the matches in the order Team USA’s matches in the order they played them in each tournament. This SQL query could look like this.SELECT tournament.name,match.match_date,rank() over (partition by tournament.name order by match.match_date) as match_numberFROM teamINNER JOIN team_matchon team.team_key = team_match.team_keyINNER JOIN matchON match.match_key = team_match.match_keyINNER JOIN tournamentON tournament.tournement_key = match.tournament_keyWHERE team.name = USAORDER BY match.match_dateIn Cypher, we could use the query that follows. First, we find the matches played in each tournament and sort the results by the match date. Next, we collect the matches, so that we have one row per tournament. Cypher’s range() function gives us a list of integers the same length as the collection of matches for each tournament. We can then UNWIND that list of integers and return the corresponding match from the collection. The collection is indexed starting at 0, but I think the matchNumbers look better starting at 1, so I added 1 to the value before returning.MATCH (t:Team {name: USA })-[:PLAYED_IN]->(m:Match)-[:IN_TOURNAMENT]->(tourn)WITH tourn, mORDER BY m.dateWITH tourn, collect(m) AS matchesUNWIND  range(0, size(matches)-1) AS rankRETURN tourn.name AS tournament, rank + 1 AS matchNumber, matches[rank].date as matchDateIf we have created the FOLLOWS relationship, we could find the first match in each tournament and then count the number of relationships that we traverse to get to each subsequent match as in the query below. Note that this query doesn’t assign a rank to the first game in each tournament because it doesn’t have any outgoing FOLLOWED relationships.MATCH (t:Team {name: USA })-[:PLAYED_IN]->(m:Match)-[:IN_TOURNAMENT]->(tourn)WHERE NOT exists((m)-[:FOLLOWED]->())MATCH p = (fm:Match)-[:FOLLOWED*]->(m)RETURN tourn.name AS tournament, fm.date AS matchDAte, length(p) AS previousMatchCountRunning TotalThe final analytic function we’ll look at is running total. We might calculate a running total to find out how much of a multi-step process is complete at each step along the way. To create a running total by match of the lifetime World Cup goals for Team USA, the SQL query might look like this.SELECT match.match_date,team_match.score, sum(team_match.score) over (order by match.match_date) as runningTotalFROM teamINNER JOIN team_matchon team.team_key = team_match.team_keyINNER JOIN matchON match.match_key = team_match.match_keyINNER JOIN tournamentON tournament.tournement_key = match.tournament_keyWHERE team.name = USAORDER BY match.match_dateTo create the running total in Cypher in the query below, we start by defining the pattern to match. Then, we sort the results by date. We collect the scores to be summed and also create a collection of the Match nodes so that we can display the individual match dates.Next, we create a range from one to the size of our matches collection. We unwind it, and then we can use it as the stopping index for a slice of the list of scores. The apoc.coll.sum() function will add up all the numbers in our slice, creating our running total.MATCH (t:Team {name: USA })-[p:PLAYED_IN]->(m:Match)WITH p, mORDER BY m.dateWITH collect(p.score) AS scores, collect(m) as matchesWITH range(1, size(matches)) as ends, scores, matchesUNWIND ends as endRETURN matches[end-1].date AS date, scores[end-1] AS score, apoc.coll.sum(scores[0..(end)]) AS runningTotalWhether you’re working in SQL or Cypher, you’ll find your own ways to express your thoughts as you tackle analytic challenges. I hope the examples above kickstart your progress.;Sep 21, 2020;[]
https://medium.com/neo4j/create-a-data-marvel-develop-a-full-stack-application-with-spring-and-neo4j-part-3-3ac3380e0edb;Jennifer ReifFollowDec 12, 2018·10 min readAvengers: Infinity WarCreate a Data Marvel — Part 3: Hydrating the Model*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)Over the last couple of weeks, we have shown the early steps of this project — the background, building the data model, and the start of the data import from an API to Neo4j. If you haven’t and want to catch up on that information, you can read the posts for Part 1 and Part 2.Today, we will continue the process of getting a maximum amount of data from the Marvel Developer Portal API into Neo4j. We saw the initial import Cypher statement last week, but the trickiest piece is the next import statement, and the issues we uncovered in running it.To recap our limitations and foundations so far, I’ve included an image of our Neo4j graph data model for the Marvel data (yours could differ), as well as some highlights.Marvel API enforces some limitations such as 3,000 calls per day and 100 results per call. So our goal was to get as much data as possible in these limits.Retrieving all comics (43,000+) was too much, so we decided to import characters first.To do that, we used the APOC library pulled characters based on names by letters of the alphabet.Next, we need to retrieve the rest of the data (comics, series, events, stories, creators) to finish populating our graph!Marvel comics data model in Neo4jMore Importing — Hydrating” the ModelOne of the best things about APOC is the possibilities it opens for different aspects of data import. Remember that apoc.periodic.iterate procedure we used in our last query to loop through each letter of the alphabet and select the characters that start with that letter? Well, we are going to use that procedure again, but in a slightly different way.This time, we will use the first statement within that procedure to select the characters we added to the database in our previous query, and the second statement will call the API to retrieve the comics for each of those characters. The next query is below with a walkthrough of the syntax. It might look complicated, but don’t worry. It simply builds upon syntax we have already discussed above.WITH apoc.date.format(timestamp(), ms”, ‘yyyyMMddHHmmss’) AS tsWITH &ts=” + ts + &apikey=” + $marvel_public + &hash=” + apoc.util.md5([ts,$marvel_private,$marvel_public]) as suffixCALL apoc.periodic.iterate(‘MATCH (c:Character) WHERE c.resourceURI IS NOT NULL AND NOT exists((c)<-[:INCLUDES]-()) RETURN c LIMIT 100’,‘CALL apoc.util.sleep(2000)CALL apoc.load.json(c.resourceURI+”/comics?format=comic&formatType=comic&limit=100 +$suffix)YIELD valueWITH c, value.data.results as results WHERE results IS NOT NULLUNWIND results as resultMERGE (comic:ComicIssue {id: result.id})ON CREATE SET comic.name = result.title,  comic.issueNumber = result.issueNumber,  comic.pageCount = result.pageCount,  comic.resourceURI = result.resourceURI,  comic.thumbnail = result.thumbnail.path +                     ”.” + result.thumbnail.extensionWITH c, comic, resultMERGE (comic)-[r:INCLUDES]->(c)WITH c, comic, result WHERE result.series IS NOT NULLUNWIND result.series as comicSeriesMERGE (series:Series {id: toInt(split(comicSeries.resourceURI,”/”)[-1])})ON CREATE SET series.name = comicSeries.name,  series.resourceURI = comicSeries.resourceURIWITH c, comic, series, resultMERGE (comic)-[r2:BELONGS_TO]->(series)WITH c, comic, result, result.creators.items as items WHERE items IS NOT NULLUNWIND items as itemMERGE (creator:Creator {id: toInt(split(item.resourceURI,”/”)[-1])})ON CREATE SET creator.name = item.name,  creator.resourceURI = item.resourceURIWITH c, comic, result, creatorMERGE (comic)-[r3:CREATED_BY]->(creator)WITH c, comic, result, result.stories.items as items WHERE items IS NOT NULLUNWIND items as itemMERGE (story:Story {id: toInt(split(item.resourceURI,”/”)[-1])})ON CREATE SET story.name = item.name,  story.resourceURI = item.resourceURI,  story.type = item.typeWITH c, comic, result, storyMERGE (comic)-[r4:MADE_OF]->(story)WITH c, comic, result, result.events.items AS items WHERE items IS NOT NULLUNWIND items as itemMERGE (event:Event {id: toInt(split(item.resourceURI,”/”)[-1])})ON CREATE SET event.name = item.name,  event.resourceURI = item.resourceURIMERGE (comic)-[r5:PART_OF]->(event)’,{batchSize: 20, iterateList:false, retries:2, params:{suffix:suffix}})To help process this lengthy query, we will break it up into sections and explain each block. Let us start with the first two sections together.//First sectionWITH apoc.date.format(timestamp(), ms”, ‘yyyyMMddHHmmss’) AS tsWITH &ts=” + ts + &apikey=” + $marvel_public + &hash=” + apoc.util.md5([ts,$marvel_private,$marvel_public]) as suffixCALL apoc.periodic.iterate(‘MATCH (c:Character) WHERE c.resourceURI IS NOT NULL AND NOT exists((c)<-[:INCLUDES]-()) RETURN c LIMIT 100’,‘CALL apoc.util.sleep(2000)CALL apoc.load.json(c.resourceURI+”/comics?format=comic&formatType=comic&limit=100 +$suffix)YIELD valueJust as with our initial load query, we start the query using the WITH clause to set up and pass the timestamp and url suffix parameters that we will use further down. The next lines of code calls the familiar apoc.periodic.iterate to pull all the characters in Neo4j (MATCH statement). Notice the criteria starting from the WHERE clause. We check the Character nodes to see if the resourceURI field contains a value. Marvel puts the url path for most entities in the resourceURI field, so this is a simple check to see if the Character has a url path for us to retrieve data. If it doesn’t, our call will fail, and it won’t find data.* Hint: this is also a good way to trim the number of API calls. If we know a call will fail, then we should not waste precious resources on it. :)The next criteria checks if a relationship type of INCLUDES already exists for the node. This sees if we have already retrieved and inserted comics for a character. If a relationship exists, then we do not pull the comic info for that character again. This avoids duplicate calls for entities where we have already added that information.Finally, we add a LIMIT 100 to that query to only pull 100 characters at a time from our Neo4j database. We ran across issues where queries would timeout because the server would stop responding. Marvel’s server instances probably have a timeout value to ensure users do not hog resources. Or, it could simply be that they need to bolster the architecture a bit to support heavier requests. )Either way, we wanted to reduce the time taken to pull in batches of data, so my colleague suggested a LIMIT clause to create smaller processing for each call. While this would increase the number of calls made to the database, it was better than larger batches failing frequently.* Note: at this point, we had 26 calls to load all of the characters for each alphabet letter. That gave us around 1,000 characters in our Neo4j instance. If we pull 100 at a time, we could have a maximum of 11 batches of up to 100 calls (one for each character in a batch).The second statement within the apoc.periodic.iterate adds 2 seconds of sleep between calls to the API for each character. This tries to avoid the timeout of our Marvel server in the middle of one of our calls. Once we wait, we use the apoc.load.json to hit the API endpoint for the comics pertaining to that character from the resourceURI field on our Character nodes. Again, we yield back the JSON object (YIELD value).//Second sectionWITH c, value.data.results as results WHERE results IS NOT NULLUNWIND results as resultMERGE (comic:ComicIssue {id: result.id})ON CREATE SET comic.name = result.title,  comic.issueNumber = result.issueNumber,  comic.pageCount = result.pageCount,  comic.resourceURI = result.resourceURI,  comic.thumbnail = result.thumbnail.path +                     ”.” + result.thumbnail.extensionWITH c, comic, resultMERGE (comic)-[r:INCLUDES]->(c)To start the next code paragraph, we check that the subsection containing our comics is not null, and unwind the object to get the subsection. Just as we did with the loading of characters, we use MERGE on the comic id to find or create the ComicIssue node and the ON CREATE SET clause to set property values if the node is created. The one thing that is different is the WITH statement and the MERGE line after it. Because we need to use the newly-created node and create a relationship between it and a Character node, we need to pass the ComicIssue node to the next MERGE statement. We do this using WITH.* Note: The creation of the node and the relationship between a comic issue and a character is not done in a single MERGE because Cypher would merge on the entire pattern. If both nodes (with all properties) AND the relationship do not exist, then Cypher creates the entire pattern new, often causing duplicates. To avoid this, you need to match any existing entities, then create a new relationship or node using the existing information.//Third section on Series, Creators, Stories, EventsWITH c, comic, result WHERE result.series IS NOT NULLUNWIND result.series as comicSeriesMERGE (series:Series {id: toInt(split(comicSeries.resourceURI,”/”)[-1])})ON CREATE SET series.name = comicSeries.name,  series.resourceURI = comicSeries.resourceURIWITH c, comic, series, resultMERGE (comic)-[r2:BELONGS_TO]->(series)WITH c, comic, result, result.creators.items as items WHERE items IS NOT NULLUNWIND items as itemMERGE (creator:Creator {id: toInt(split(item.resourceURI,”/”)[-1])})ON CREATE SET creator.name = item.name,  creator.resourceURI = item.resourceURIWITH c, comic, result, creatorMERGE (comic)-[r3:CREATED_BY]->(creator)WITH c, comic, result, result.stories.items as items WHERE items IS NOT NULLUNWIND items as itemMERGE (story:Story {id: toInt(split(item.resourceURI,”/”)[-1])})ON CREATE SET story.name = item.name,  story.resourceURI = item.resourceURI,  story.type = item.typeWITH c, comic, result, storyMERGE (comic)-[r4:MADE_OF]->(story)WITH c, comic, result, result.events.items AS items WHERE items IS NOT NULLUNWIND items as itemMERGE (event:Event {id: toInt(split(item.resourceURI,”/”)[-1])})ON CREATE SET event.name = item.name,  event.resourceURI = item.resourceURIMERGE (comic)-[r5:PART_OF]->(event)’,The next paragraphs of code follow a similar pattern for each of the Series, Creator, Story, and Event node labels. One thing that is different is setting the id field for each of those nodes. The toInt(split(item.resourceURI, / )[-1]) code trims out the id from the url path that Marvel gives for each entity with split(item.resourceURI, / )[-1]) and then converts that value to an integer for the id field in Neo4j using the toInt() function.The rest of the syntax should be familiar from our load of characters earlier. Now we have some of each entity in the database, but I want to add a few more details to a couple of node labels.Filling in More DetailsIf you take another look at our last Cypher block, you can see that the Series, Creator, Story, and Event node types only have basic information. Each has an id field, as well as the resourceURI and name fields. This doesn’t give us as much info as Marvel offers, and we could run some interesting queries on some of the other fields provided.For our project, the only additional fields I wanted were on the Series and Event nodes for some images and start and end dates. Let’s go ahead and get those imported to Neo4j!These statements are much simpler than the previous two, and we can use the same syntax and logic from our previous queries. The two queries to load these details are shown below, along with brief explanation.//load any extra Series dataWITH apoc.date.format(timestamp(), ms”, ‘yyyyMMddHHmmss’) AS tsWITH &ts=” + ts + &apikey=” + $marvel_public + &hash=” + apoc.util.md5([ts,$marvel_private,$marvel_public]) as suffixCALL apoc.periodic.iterate(‘MATCH (s:Series) WHERE s.resourceURI IS NOT NULL AND not exists(s.startYear) RETURN s LIMIT 100’,‘CALL apoc.util.sleep(2000)CALL apoc.load.json(s.resourceURI+”?limit=100  + $suffix)YIELD valueWITH value.data.results as results WHERE results IS NOT NULLUNWIND results as resultMERGE (series:Series {id: result.id})SET series.startYear = result.startYear,  series.endYear = result.endYear,  series.rating = result.rating,  series.thumbnail = result.thumbnail.path +                     ”.” + result.thumbnail.extension’,{batchSize: 20, iterateList: false, params: {suffix:suffix}})//load any extra Event dataWITH apoc.date.format(timestamp(), ms”, ‘yyyyMMddHHmmss’) AS tsWITH &ts=” + ts + &apikey=” + $marvel_public + &hash=” + apoc.util.md5([ts,$marvel_private,$marvel_public]) as suffixCALL apoc.periodic.iterate(‘MATCH (event:Event) WHERE event.resourceURI IS NOT NULL AND NOT exists(event.start) RETURN DISTINCT event LIMIT 100’,‘CALL apoc.util.sleep(2000)CALL apoc.load.json(event.resourceURI+”?limit=100 +$suffix)YIELD valueUNWIND value.data.results as resultMERGE (e:Event {id: result.id})SET e.start = result.start,  e.end = result.end’,{batchSize: 20, iterateList:false, params: {suffix:suffix}})Both of these queries use the WITH clause to set up the timestamp and suffix parameters, just as we did with our other queries. The next code block in each query unwinds the JSON object and finds the nested data object, and uses MERGE to find or create the nodes. This time, we just use the SET clause to update or add any values that may not have existed before. Finally, the last code line sets the configuration parameters.What I LearnedWe now have plenty of data in our Neo4j graph database, and we can start to run some queries to check out the information that is there!After much trial, error, and troubleshooting, the queries given and explained above and in the Part 2 post are what I used to get the data from a finicky API hosted by Marvel into my local instance of Neo4j as a graph data model. Below is a list of my key takeaways from the total data import process up to this point.It took a lot of messed up Cypher queries and clearing out the database multiple times to start the import over from a clean slate.It took several questions to expert teammates to tweak and optimize what I wanted to get and determine that some issues were related to the API on Marvel’s end and not poorly constructed Cypher on my end.I learned a LOT from an actual data set. I wasn’t playing with a cherry-picked data set that was clean and road blocks removed for easy learning. I had to come up with ways around new, inventive issues based on the data set I was dealing with.I was surprised how long the import step took. It wasn’t the actual import execution that was problematic. It was determining how I wanted the data to look and how to craft the Cypher to meet that expectation.Until you deal with a data set and technology hands-on, it is hard to break past a certain point of learning. I could read and read and read, but I needed to actually work through these problems and see what my queries did with the data to comprehend how the procedures worked and what the data looked like in Neo4j.Next StepsWhile the initial stages of this project may not have been overly glamorous, they are necessary to every project you encounter, whether personal or occupational. In all honesty, I enjoyed the learning process in these steps.The next posts will start looking at crafting the application and how to use Spring Data Neo4j to produce powerful, capable, and pretty code and webpage! Stay tuned!ResourcesPart 1 of this blog seriesPart 2 of this blog seriesFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifNeo4j Data ImportAPOC standard libraryDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j Guide;Dec 12, 2018;[]
https://medium.com/neo4j/visualize-cancer-1c80a95f5bb4;Andrew KamalFollowDec 18, 2019·3 min readVisualizing Breast Cancer Data with Neo4j and GraphXRA while ago, I have entered the Global GraphHack challenge on Devpost. The main project I was trying to demo, was a bioinformatics use-case with Neo4j.My use-case was to take existing open source bioinformatics data and visualize it in a meaningful manner. Luckily, I found a sample repository on GitHub with CSV data I can export.The inspiration for this project, also came from a previous idea I have done before, shown here. The project I was inspired by, was a RapidMiner model I was working on to mine and visualize cancer genomic case studies.This project later merged into the project Cancer@Home which allows other users to effectively visualize and mine cancer genomic case studies and garnish more and more effective data. I wanted to build a something quick and simple for the Global GraphHack so thought, why not start off with just a small data view model?Utilizing GraphXR, I was able to load the sample CSV and create property tags for different IDs.Property tags as in tags for the numerical rows in the CSVs such as 1.011 0/1 for example for data views. These tags are important because they represent measurements an oncologist or computational biologist may want to query or categorize.Next, I set different colors for organizational (categorical) purposes and helping the final researcher visualize the data more efficiently.This means instead of having to look at a CSV file with 100s and 100s of rows, they can view the visualization in a more meaningful way by looking at the numerical attributes by color in the GraphXR dashboard.For reference one can export a CSV file using the CSV load command seen here or manually load to GraphXR using add graph and exporting the data file as seen here.xThe picture seen above, is a view of different IDs and property tags (i.e. IDs related to the original data set and tags for numerical categories) and the picture seen below is an example of a menu being integrated on GraphXR for the visualization dashboard.Development Instructions:Install Node4j Desktop StarterInstall GraphXR appRun database via Node4j BrowserLoad CSV data locallySet Property TagsRun Force in LayoutAllow for Parametric View & Tree DistributionRun Enhanced TableRelationship Variables: 1001, 17.99, 2019 (Year), 842302, MCan allow f(x) connectors for data sourcingAggregate: (nodePropValue,neighborPropValues) => nodePropValueLabel (Visible)EROAD (Visible)/Set ColorLoad default properties from CSV: 0.006193, 0.006399, 0.01587, 0.03003, 0.04904, 0.05373, 0.07871, 0.1184, 0.1189, 0.1471, 0.1622, 0.2419, 0.2654, 0.2776, 0.3001, 0.4601, 0.6656, 0.7119, 0.9053, 1.095, 10.38, 1001, 122.8, 153.4, 17.33, 17.99, 184.6, 2019, 25.39, 8.589, 842302, M, Property 1, idNode Size Scale:1 (in settings)After the development of this program, I got introduced to Node4j Desktop and found out about GraphXR. I think that the ability to use simplified data visualization tools such as these, is where the future is heading.It is quite inspirational to be able to take a simple tool you discovered, and start using it to represent data in a meaningful manner besides just a bunch of numbers.I know data can be used in both ethical and non-ethical ways. I believe as data scientist, our goal should be utilizing data for ethical use-cases. I think the next step to further the development of my project, is creating inquiries for different aspects of the data and being able to dig even deeper beyond visualization.As for my insight, I wish you all happy developing!;Dec 18, 2019;[]
https://medium.com/neo4j/monitoring-neo4j-with-halin-4c11429b46ff;David AllenFollowOct 29, 2018·6 min readMonitoring Neo4j with HalinFor the last few weeks, I’ve been working a bit here and there on an application called Halin, for monitoring Neo4j databases and clusters. Now that I’ve gotten it to a useful state, I wanted to share it with others, and show a few things that you can do with it, and how it makes Neo4j administration easier.Graph App InstallationIf you want to try Halin out yourself, you can run it live here. All you need is connection details for a Neo4j instance. All of the open source code is available on GitHub. For enjoying Halin as Graph App with auto-updates in Neo4j Desktop, just paste the Halin URL into the Graph App Install sidebar. For details see the GitHub Readme.UPDATE: Recently Neo4j did a webinar about Halin on YouTube. If you’d like to see a live presentation / demo in that format, check YouTube!Monitoring BasicsA database is a complicated piece of software that takes care and feeding. Because it’s a core component, you need to know whether it’s running well, and if not, why not. Monitoring is about gathering data to determine this, and figuring out what you need to do to make your system better.In previous articles on Neo4j monitoring, I wrote about how to use built-in JMX metrics with other tools like Hawtio. This is nice because you can visualize a lot about what’s going on with just configuration, no new software. If JMX isn’t your thing, Neo4j also supports Prometheus and Graphite.The downside to these approaches is that they’re not very Neo4j specific. These external monitoring tools don’t know anything about a Neo4j database’s lifecycle. Additionally, you tend to get metrics per node in your cluster, but you don’t get metrics that show you the cluster overall.HalinWith some help from some support folks at Neo4j, I created a dashboard showing overall cluster health. Here we can see three nodes of a cluster (the one with the green star is the leader). Each graph can enable or disable the data line for each node, so you can see stats individually.To get started, first log in:Use your normal bolt connection settings, as you would for Neo4j BrowserOnce you connect, Halin fetches information about your cluster or single instance, and brings you to the cluster overview pane.Cluster overview metrics for monitoring overall health and throughputMemory is given prominent billing because memory management is so important to a performant Neo4j instance. You can also see garbage collection spikes, transaction load, and various metrics of page cache utilization, which can help diagnose performance issues.Halin is a simple React application that uses a bolt connection to your database. Since Neo4j already exposes Bolt and Cypher natively, no configuration is needed to make it work with a new Neo4j instance. In a future release, the plan is to provide a Docker container as well so that an instance of Halin can be deployed and run alongside a particular Neo4j install, for example with neo4j run in kubernetes.Diagnostic AdvisorHalin contains a diagnostic advisor. This gathers a lot of metadata about your Neo4j instance on the fly, and then runs it through a series of rules which make suggestions about what’s good, what could use improvement, and where there are errors. This rule set will grow with time and user suggestions.The intent of the advisor is to let you download a diagnostic package of information which will give you a broad overview of your entire cluster’s configuration, and to also help automate locating issues with your configuration.Halin can identify problems you might want to fixIn this screenshot above, we can see some green checkmarks indicating good things Halin checked (we have admin users, backups are enabled but not on an external port, and network port settings look good). There are some warnings there too — this cluster’s memory utilization is very high, and there are two errors. Because Neo4j manages users and roles per-node, Halin in this situation has detected that node3 is missing a user and a role that is present on other nodes in the cluster. Because this can lead to authorization errors should that user try to contact another node, this is a misconfiguration and it gets flagged.You can always hit the Download Diagnostics” button and download a JSON file containing everything Halin gathers. The rules are driven by this diagnostic bundle.You’ll also see a Configuration Diff” tool next to the Advisor. This will point out every last piece of configuration that’s different from machine to machine. This can be useful to ferret out misconfigurations as well.Per-Machine MonitoringAside from cluster overviews, the individual machines in the cluster do matter, and there are options to monitor a number of different metrics about them, including CPU/memory utilization, disk space taken up by Neo4j, real-time page cache information, and machine-specific configuration.User ManagementHalin provides a user interface for creating users and roles as well, and lets you associate any user with any role, provided you logged into Halin as an admin user. Because each machine in a cluster manages auth individually, typically if you want to create a user on all nodes, you’d have to run the same code to create that user in each place. Halin automates this for you and tries to ensure a user is defined on a cluster-wide basis, rather than per instance.Users and roles can be created and associated, cluster-wideSupport for standalone Database, and Neo4j CommunityHalin works with non-clustered installs of Neo4j. The only real difference is that you only get one tab for your database. Indeed in stand-alone mode, Halin tends to treat your Neo4j instance as a cluster with only one member. For Neo4j Community users, some monitoring components are disabled, because they require features that are only available in Enterprise, but most functionality is still available.Neo4j Desktop GraphAppNeo4j Desktop allows users to install Graph Apps to interact with their local or remote databases, and Halin works in this mode as well.To configure this, go into Neo4j Desktop, check the Graph Applications” tab on the left side of Neo4j Desktop. In the Install Graph Application” box at the bottom, paste in the published Halin URL, and click Install.https://neo.jfrog.io/neo/api/npm/npm/halinOnce Halin is installed, within each of your Neo4j Desktop Projects, you can click the Add Application” button, and Halin will now be an option to install into that project. With a running database and Halin installed in the project, you can click Halin to auto-connect to the database running in that project.Halin is now installable into your project!What’s Next?Check out the code on GitHub, or try Halin out on your Neo4j instance. Make sure to drop by the Neo4j Community site and let us know what you think and how you’re using it!In the coming months, I’m planning on adding more features, which will mostly be driven by common problems people run into with Neo4j in production. Halin’s job will be to detect whether or not this problem is present (or about to happen) and provide advice on how to avoid or fix it!;Oct 29, 2018;[]
https://medium.com/neo4j/graphing-the-floodlight-open-project-data-f566cc82a995;Frederico BragaFollowDec 10, 2018·4 min readGraphing the Floodlight Open Project dataA friend of mine mentioned the Roche Floodlight Open Project to me this weekend and the fact that Roche publicly shares the data it collects during this study.You can read about the Floodlight Project here:Floodlight Open data visualisation portalAn open data visualisation tool developed for Genentech Floodlight Open clinical trial data.www.floodlightopen.comIn Roche’s own words: Through Floodlight Open we hope to create a more holistic view of MS. By engaging with Floodlight on a regular basis, you generate data for yourself, and join thousands of others to build a unique open access dataset.”That’s cool.What’s interesting about Floodlight is that it shows us new possibilities of conducting clinical research. The subjects are enrolled online, fully electronically, the tests are conducted using a mobile phone and according to Roche, its results are pretty interesting and show how incredibly accurate the mobile apps developed by its scientists can be. It is the most extreme representation of the term ‘Virtual Trial’ I have seen to date.I had a morning flight and decided to download the data from their website and start playing with it in Neo4j during my flight while I was offline.The data is very simple and has ID’s for both the subject and the test which I leveraged to merge and build relationships. I also decided to create a separate label for the Country so we can leverage relationships for our queriesHere’s the code after downloading the csv file to my ‘import’ folder in the Neo4j database instance:load csv with headers from ‘file:///floodlight_complete_dataset.csv’ as awitha.floodlightOpenId as floodlightOpenId,a.participantIsControl as participantIsControl,a.testResultMetricCreatedOn as testResultMetricCreatedOn,a.participantSex as participantSex,a.participantHeightCms as participantHeightCms,a.participantBirthYear as participantBirthYear,a.participantWeightLbs as participantWeightLbs,a.participantCreatedOn as participantCreatedOn,a.participantCountryOfResidence as participantCountryOfResidence,a.testResultMetricTimestamp1 as testResultMetricTimestamp1,a.testResultMetricTimestamp2 as testResultMetricTimestamp2,a.testMetricName as testMetricName,a.testResultMetricId as testResultMetricId,a.testName as testName,a.testResultMetricValue as testResultMetricValuemerge (p:participant {id:floodlightOpenId}) setp.participantIsControl=participantIsControl,p.participantSex=participantSex,p.participantHeightCms=participantHeightCms,p.participantBirthYear=participantBirthYear,p.participantWeightLbs=participantWeightLbs,p.participantCreatedOn=participantCreatedOnmerge (c:country {name:participantCountryOfResidence})merge (p)-[:resides_in]->(c)merge (t:test {id:testResultMetricId }) SETt.testResultMetricTimestamp1=testResultMetricTimestamp1,t.testResultMetricTimestamp2=testResultMetricTimestamp2,t.testMetricName=testMetricName,t.testResultMetricValue=testResultMetricValue,t.testResultMetricCreatedOn=testResultMetricCreatedOnmerge (p)-[:conducted]->(t)I was curious to learn which tests did the subjects which used the app more frequently where taking:match (t:test)-[cd:conducted]-(p:participant)with p, cd, t, t.testMetricName as testname, p.id as participant_id, count(p) as test, t.testResultMetricValue as result return testname, sum(test) as num_of_tests order by num_of_tests descOK, looks like Life Space Daily”, Hand Used” and Mood Response” are the winners.Then I looked at who is taking these tests the most:match (t:test)-[cd:conducted]-(p:participant)with p, cd, t, t.testMetricName as testname, p.id as participant_id, count(p) as test, t.testResultMetricValue as resultreturn testname, p.id as participant_id, count(cd) as tests order by tests descNext, I check who was doing the top 50 tests, which tests and the connection to the Country:I also wanted to see if there is a relationship between the number of tests conducted by a subject and the results (score) achieved. I decided to look at a test called ‘Successful Pinches’ and look at the 50 subjects that had conducted this test more often:match (t:test)-[cd:conducted]-(p:participant)-[b]-(c:country)where t.testMetricName=’Successful Pinches’ withp.id as participant_id, collect(distinct t.id) as id_test, count(cd) as num_of_tests, t.testResultMetricValue as result order by num_of_tests desc limit 50return distinct participant_id, avg(num_of_tests) as avg_tests, avg(toint(result)) as avg_result order by avg_tests descIt does not look like this relationship can be established for this test.I would have loved to play a bit more with the data but the flight was short :) Roche says they are preparing to share more via API’s. In this era of clinical development, I am super excited about the possibilities created by open collaboration. Congrats to Roche for this really great initiative, I look forward to playing more with the data in the future as it gets enriched and extended.;Dec 10, 2018;[]
https://medium.com/neo4j/cypher-sleuthing-the-eager-operator-84a64d91a452;Jennifer ReifFollowOct 4, 2019·7 min readCypher Sleuthing: the eager operatorWhy is it that some query syntax seems to run faster than another when there is very little difference between the statements? Sometimes, queries will invoke what is called the eager operator in order to maintain consistent operations and avoid conflicting data changes.It still leaves the question, though, of what does this eager operation do differently and why would it be important enough to specifically avoid in certain situations? What are those situations where we would want to choose non-eager and avoid the automatic eager invocation? Let’s take a look!Cypher EagerThe Cypher documentation actually explains the concept of the eager operator quite well, but I had yet to come across this section until I was doing some research to better understand a user’s question on the topic. Another excellent resource that includes examples is my colleague’s blog post showing how to avoid it in import statements. Using these 2 resources, I constructed some examples to fully understand how this works. We will walk through my test examples in this post to demonstrate.In short, the eager operator ensures that each operation in a query does not conflict with other operations in the same query. It prevents subsequent operations from altering data from previous operations, maintaining data integrity and intended order of operations. Queries with multiple operations chained together have the potential to write some data in one piece and then read data in the next piece that is out of sync. Eager would ensure that each operation is applied to all rows before moving on to the next query piece to avoid read/write conflicts.Does it make a difference?Why does it matter whether Cypher invokes eager or not?In queries that don’t invoke eager, the executor runs each row to completion before taking in the next row and executing it. It processes the entire query for each row, adds the result to the final output, then takes the next row and executes the entire query on it, adds the result to the final output, and so on. This process is more like a stream and is usually kinder to the heap.Here it is in logical and visual representations:Non-eagerRow 1: Operation1, Operation2, Result set = Row1 finalRow 2: Operation1, Operation2, Result set = Row1 final, Row2 finalEager:Operation1: Row1, Row2Operation2: Row1, Row2Result set = Row1 final, Row2 finalAs seen above, when eager is invoked, it instead executes on an operation-to-operation basis. This can put more pressure on the heap because it executes a single operation on all rows before passing all those results to the next operation (which runs on all rows), and so on. The result set cannot begin to compile because all the results are dumped at the end after all operations are completed on all rows.Eager has been appearing less and less, as our teams find better ways for Cypher to determine conflicting statements and optimize query performance. However, you may occasionally come across this in your queries, and it’s probably avoidable by altering your query syntax or breaking up longer statements into shorter, smaller operations. At the very least, understanding when it appears and what it does can tell you a lot about how Cypher works and why the language operates the way it does.Simple Example with EagerLet’s take a silly example to see this in action. We have 5 ids that we want to create as nodes in our graph — 1, 2, 3, 4, 5. Our query will loop through the array and create each node, then add the expected next node.UNWIND [1,2,3,4,5] as idMERGE (n:Row {id: id})MERGE (x:Row {id: n.id + 1})Results from above queryBecause of the two merges with the same label (Row), Cypher avoids a potential conflict by doing the first merge for all the rows first, then doing the second merge. We can see the eager invocation if we put the PROFILE keyword before the above query and execute it.PROFILEUNWIND [1,2,3,4,5] as idMERGE (n:Row {id: id})MERGE (x:Row {id: id + 1})Running this, you should see output that looks like the below image, where the Eager operation is near the bottom in dark blue:If you notice, there are 2 main funnels that lead to the result. The first funnel is the one coming from the NodeByLabelScan at the top left. That trickles down to the Eager operation we found. The other funnel is one coming from the other NodeByLabelScan on the right. Each one of these funnels is for each of our merge operations. The first merge is creating those 5 nodes (1 for each of the rows we grab from the array) and doing that all at once with eager. The second merge is for creating the expected next node and syncing up with the first merge at the bottom of the funnel.This is very apparent if you expand each of the NodeByLabelScan operations. The top-left one for the first merge shows (n) in the details. The right one for the second merge shows (x) in the details. These are the node variables used in each of the merge statements from our query above.Avoiding Eager in our ExampleIn order to get around the eager operator, we need to ensure Cypher isn’t worried about conflicting operations. The best way to do this is to divide our query into single operations so that Cypher won’t invoke eager as a safeguard. Let’s profile this as two queries to see that.PROFILEUNWIND [1,2,3,4,5] as idMERGE (n:Row {id: id})Results from query abovePROFILEUNWIND [1,2,3,4,5] as idMERGE (x:Row {id: id + 1})Results from query aboveGreat! No eager operation visible for either of these.Another exampleLet’s try another example to differentiate and further solidify our knowledge of eager. In this one, we are using our same array ids, but this time we are using them as customer id values and creating related Employee nodes that are assigned to those customers and create the relationship between them.UNWIND [1,2,3,4,5] as idMERGE (c:Customer {id: id})MERGE (e:Employee {id: c.id*10})MERGE (e)-[r:DEDICATED_TO]->(c)Results from query aboveNow, if we run this query with the PROFILE keyword in front of it, we see that Cypher isn’t invoking eager here.Why is that? Doesn’t the relationship depend on the creation of the nodes? Actually, no, it does not. This is because these writes don’t actually conflict with one another. We are not trying to write and then read the same data again. We are writing 3 separate operations — write Customer node, write Employee node, write Customer/Employee relationship.We can better see how this works by throwing a read statement in the middle and running PROFILE on that.PROFILE UNWIND [1,2,3,4,5] as idMERGE (c:Customer {id: id})MERGE (e:Employee {id: c.id*10})WITH c, e, idMATCH (p:Customer {id: id})MERGE (e)-[r:DEDICATED_TO]->(c)Results from query aboveEager again appears close to the bottom left in dark blue. The only difference between that query and the one we had before is that we’re writing the Customer and Employee nodes, then passing those results to the next operation, which reads a Customer node (simply checking the database for the node we just created) and then using that node to create the relationship. We simply took a query with 3 write operations and turned it into a query with 2 writes, 1 read, and another write.The read is what invokes the Eager operation because we’re potentially reading the data we just created. Doing the merge, then read, we could potentially have missing results in our read that haven’t been written yet in the 1st write. This is why Cypher does all of the writes first (merge Customer, merge Employee), then it moves on to the read and final write.Removing that read statement in the middle avoids the eager operator and ensures we don’t have conflicting operations, and we’re back to optimized operations!Wrap-upYou don’t have to worry about this operator if your data set is small or if your query operations are simple. However, for heavy processing and large datasets, this might be something to check, if your queries are running slowly. When in doubt, break down operations into separate queries where possible and run PROFILE to see what Cypher is doing behind the scenes.Happy coding!ResourcesCypher manual: Eager operatorCypher manual: Eager in execution plansBlog post: Avoiding the EagerQuery tuning: Profiling your queriesAsk your questions: Neo4j Community Site;Oct 4, 2019;[]
https://medium.com/neo4j/run-the-graph-a-godot-game-with-neo4j-backend-1dd5e9ce93bb;Tom NijhofFollowFeb 21·3 min readRun the graph: A Godot game with Neo4j backendMost games are very linear, from level 1 to 2 and 3, and so on. Some games like Super Mario World are already less linear and have more of a network or graph-like structure. But what if you want to take it to the extreme?Let’s build a platformer with a Neo4j graph database to store all the levels.The level overview of Super Mario World shows the graph-like structureTech stackThe game is built with Godot, a fully open-source game engine.The backend is fast-api and deployed to deta.space.Code is hosted in GitHub, and built via GitHub actions. (game and the back-end).GitHub pages are used to host the game itself: wagenrace.github.io/PlatformerLastly, the database is Neo4j hosted in aura free instance.So I have paid nothing for this whole tech stack or the deployment tools.LevelThe game is a platformer with every level having 3 doors (red, green, and blue). This means every level is connected to 3 other levels.Each has 3 properties: name, number, and encode level.The name and number are used for presenting and identifying. The encode_level is the level itself as a base64 string.It is decoded by a look-up table with every symbol standing for a segment of the level (e.q. 8 means the red door, 9 green door, and + is the blue door). Together they form a full level.{// Example of level 2 nodeencode_level:  8012345679FFfdAe+ ,name:  Level 2 ,number: 2}GraphEvery level has 3 connections red, green, and blue doors. These are non-directional but Neo4j edges always have a direction. The first graph is extremely simple, it is 4 nodes/levels and fully closed.To go from level 1 to level 2 you can take the green door, or the red door and take the blue door in level 3, or take the blue door followed by the red door of level 4.The full graphThe GameDisclaimer: The game works best on firefox, almost perfectly on edge, and terribly on google chrome. Others I did not test.The game itself is a simple platformer. The player is dropped in a level and can choose between 3 doors. However, not every door is just as easy to get through and sometimes the fast route is via another level.The game has no objective, score, or anything else to it a real game.A print screenshot of the player in level 4;Feb 21, 2023;[]
https://medium.com/neo4j/arrows-hacks-tricks-for-your-graph-models-and-diagrams-371ca2810c56;Ljubica LazarevicFollowJan 30, 2020·5 min readArrows Hacks — tricks for your graph models and diagramsNick Fewings on unsplashIf you’ve been using Neo4j, attending any of the training courses, or seen some output diagrams, it’s highly likely you’ve come across Arrows.Arrows is a graph diagramming tool — a way to be able to generate images for documents, slide decks, and blog posts. As well as being available as a library for specific uses, there is also the hosted online version which is the most popular format.Its simplicity and ease of use has seen its use go beyond just diagramming, and it’s commonly the entry point when working on graph data models. Whilst not a full-blown modelling tool, it is very forgiving when it comes to making edits, and highly flexible in its application. This makes it understandable why many people may start with Arrows rather than moving straight to a more traditional modelling tool.As it is quite a simple tool, there can be challenges in getting Arrows to do what you want it to do beyond its intended scope. In this post we’ll cover some Arrows hacks to help with your data modelling and diagramming.Hack number one: Making it easier to use the sceneYou may notice that the screen jumps around when you look to add a new node. The best way to stop this from happening is to anchor the screen with two nodes diagonally opposite each other. Like this, the scene won’t move around as you work on your data model or diagram.Running out of space on the screen? Just move one of the anchor points further out.Using nodes as screen anchor pointsHack number two: Quickly clearing the sceneYou’ve built out your graph model or diagram, and you’re now ready to work on the next one. But how do you clear the scene?Arrows diagrams will be stored in cookies on your machine. A really quick way to clear the scene would be to:Press the ‘Export Markup’ buttonDelete everything between the <ul></ul> tagsPress ‘Save’If you want to save your anchor nodes for the next diagram, provided that you’ve added them first, you can leave in the first two <li> tagged components.Hack number three: Use Incognito/InPrivate modeWant to draw a quick diagram and have no intention of saving it? Use your browsers private browsing option. As your Arrows diagram is stored locally in cookies, by using the private browsing option, not only do you have a clear scene every time you start, you don’t have to worry about removing your diagram afterwards — just close the window!Hack number four: Figure out your data modelThis trick is a little fiddly, but can be a very useful approach. With a combination of Arrows and Neo4j, you can determine what your graph data model is likely to be through a worked example in Arrows.Let’s say you’ve got a user journey which shows how different entities are interacting together — for example, the one diagrammed below. You’re not quite ready for figuring out the generalised view, but you do have examples of how your entities interact with each other:User journey of Persons purchasing goods, or employees of a storeWe can get the Cypher code required to generate this data in Neo4j by clicking on the ‘Export Cypher’ button. Copy the code, and then paste it into the query bar in Neo4j Browser and press the run button to execute. This does need to be on an empty database.Once the graph data has been created, you can see your data model through running call db.schema.Hack number six: Using more than one node labelArrows doesn’t support multiple labels, however you can do a couple of steps that will allow you to import nodes with multiple labels into Neo4j.Firstly, add any extra labels you want on your Arrows node by using colon, e.g. Person:Customer. Arrows will wrap these kinds of statements in backticks. Backticks are used in Cypher to escape any non-compliant naming conventions. In this example, as Arrows doesn’t support multi-label nodes, it doesn’t recognise the colon as a separator for different labels. As special characters in node labels need to be escaped, Arrows will wrap what it thinks is a single node label with backticks to ensure the generated Cypher statement will run.When you click on ‘Export Cypher’ you will see the backticks wrapping around the multi-label nodes.Using your text editor of choice, you can do to search and replace operations to remove the backticks surrounding our multi-label entries: replacing . :` for . : and .` { for . {. Do not omit the leading space! With these replacements made, Cypher will now pick up the multiple labels on the entities accordingly. You can now run the modified Cypher script in Neo4j Browser (with an empty database).Wrapping upWe’ve gone through some hacks you might find useful with Arrows, if you’ve got some more to share, do post them in the comments section!;Jan 30, 2020;[]
https://medium.com/neo4j/observable-graphs-ff959f10c4a2;Andreas KolleggerFollowFeb 5, 2021·3 min readObservable GraphsCreate graphs with gram then share them on ObservableHQ, or use d3-gram to render graphs to SVG anywhere.Gram makes writing graphs as easy as (a)-->(b)<--(c). But then what? Well, graphs are eminently visual data structures. Naturally, we could visualize them with d3.js.Gram: a data graph formatGram is a textual format for data. We have CSV for tables, JSON for documents, and gram for data graphs.medium.comThere’s a convenient integration called d3-gram which we’ll exercise on ObservableHQ.Observe as I DemonstrateTo observe, watch this incredibly short video:To follow along, login to ObservableHQ. No, really, do it!Observable - Make sense of the world with data, together / ObservableExplore the largest collection of live data visualizations and tutorials anywhere. Click to open any cell to see how it…observablehq.comFirst, create a notebook:Click the New” button in the upper-right cornerGive it a nice title like # Observable Graph by ABK”Next, import the graphOf function which parses gram then renders a graph:Create a new block using the +” buttonCopy/paste the following codeHit shift-return or press the play/run button to perform the importimport {graphOf} from @akollegger/graph-input”Now creating a graph visualization is as easy as calling the graphOf function, passing in a Cypher-like gram pattern. Add a new block try this:abc = graphOf( (a)-->(b)<--(c) )Neat, right? We can tweak the visualization a little by adding a zoom level and viewbox height:abc = graphOf( (a)-->(b)<--(c) , {zoom:4, height: 60})Observe the GraphWhat is abc? The graphOf function returns an observable view, an input element with a current value. That’s right, you can use the graph visualization like a slider or a text input field.Modify the graphOf block, prepending viewof:viewof abc = graphOf( (a)-->(b)<--(c) , {zoom:4, height: 60})Add another block which only contains abc:abcWhen you run that block, notice the result is an object. That’s the current value of the input element. It contains an array of all nodes, an array of links, and an array of the current selection. Shift-clicking the graph adds multiple nodes to the selected” array.Object { nodes: Array(3) [Object, Object, Object] links: Array(2) [Object, Object] selected: Array(1) [Object]}The array elements are objects that mix d3-js properties for things like x,y coordinates along with the original graph data. Let’s add another block to focus on the graph data:selectedNode = abc.selected.length == 0 ? {} : Object.getPrototypeOf(abc.selected[0])That’s better. Now we can add more details to the graph, which will influence the graph visualization and show up in the selectedNode.Try this:viewof abc = graphOf( (a:Person {name:ABK})-->(b)<--(c) ,   {zoom:4, height: 60})With all this in place, you can create your own graphs by editing the gram string passed into graphOf(). Or you could add a text field using form-input. Or load from an attached file or URL.For fun, I re-created Wikipedia’s Gallery of Named Graphs.A gram of graphs goes a long way. Have fun! :)Footnotes:d3-gram — the js integration library driving the rendering with d3Observable Graph by ABK — the complete notebook described in this postGram of Graphs — a collection of notebooks about gram and graphs@akollegger/graph-input — the particular ObservableHQ notebook which provides the utility functions used in this postd3-gramGram is a textual format for data graphs.gram-data.github.ioObservable Graph by ABKA small example of how to use graphOf()observablehq.comGram of GraphsGram is a textual format for data graphs, easy as (a)-->(b)<--(c) / An Observable collection by Andreas Kolleggerobservablehq.comGraph Inputs(gram)-->(data)-->(input) Some handy observable views using graphs described in gram.observablehq.comObservable Graph by ABK” uses Gram in an Observable notebook.;Feb 5, 2021;[]
https://medium.com/neo4j/gr-r-andstack-the-2nd-r-stands-for-regraph-fca60f005e56;Cambridge IntelligenceFollowOct 21, 2019·7 min readGR-R-ANDstack (the 2nd ‘R’ stands for ReGraph)Originally published on the Cambridge Intelligence blog. You can catch a webinar on this topic over here.Many Cambridge Intelligence customers love Neo4j. It’s by far the most popular graph database integration for organizations using our toolkits to create efficient, easily-maintained graph visualization applications.I blogged about building an application with Neo4j and KeyLines back in 2017, using the Facebook-backed query language GraphQL to pull data efficiently from the GitHub API.GraphQL was a relatively new tool back then, but a lot has changed since. Modern developer tools for building full-stack applications have improved substantially, and Neo4j promote the best of them as GRANDstack.GRANDstack is Neo4j’s full development stack, combining their JavaScript library with Apollo Tools, React and Neo4j.We’ve evolved our own technology too with the launch of ReGraph, our graph visualization toolkit for React developers.In this blog post, I’ll show how quickly and easily you can use GRANDstack to build a ReGraph graph visualization application. It contains all the code from my talk at NODES 19, Neo4j’s online developer summit (watch it on YouTube). We’ll also use datasets featuring movies and the FIFA Women’s World Cup to showcase some of the things you can achieve with your charts.Getting started with GRANDstackOne way to get started is to use the grand-stack-starter repository from Neo4j. This simple starter project provides all the tools you need to build an application.But for this blog post, we’ll build our front and back end from scratch” to show how easy it is.Such is the nature of web-development in 2019, from scratch” actually means we’ll be leveraging some excellent open-source libraries. This takes away complexity, making it much easier to build applications.Here’s the basic architecture we’re building. When our ReGraph application issues a GraphQL query, the Neo4j GraphQL API sends a Cypher query to Neo4j via the Bolt protocol. The JSON response is sent to ReGraph where it updates the React props and the graph visualization.Architecture diagram showing how ReGraph integrates with Neo4j using GRANDstackCreate a ReGraph applicationLet’s start with the front end. We’ll create a basic React application that we can adapt to use a GraphQL endpoint. create-react-app from Facebook provides all the boilerplate we need.npx create-react-app grandstack-regraph-appcd grandstack-regraph-appNow we’ll download the latest version of ReGraph and add it to the new React application. If you don’t have ReGraph already, just request a free trial.Let’s add ReGraph as a dependency:cp ~/Downloads/regraph-1.5.0.tgz .yarn add file:regraph-1.5.0.tgzNow we’ll create our first chart, with a few small alterations to App.js:import React from reactimport { Chart } from regraphimport ./App.cssfunction App() {  return (    <div style={{ display: flex, width: 100vw, height: 100vh }}>      <Chart        items={{ node: { label: { text: Welcome to ReGraph! } } }}        options={{}}      />    </div>  )}export default AppWith minimal effort, we already have our first ReGraph application with a single node in the center of the screen.It only takes a few steps to build a very simple ReGraph applicationConfigure the server-side APINow for the back end. Let’s configure Apollo to create GraphQL workflows that’ll enable us to query our Neo4j database.mkdir graphql-server-examplecd graphql-server-exampleyarn init --yesyarn add apollo-server neo4j-graphql-js dotenvThe .env file defines the Neo4j connection details. Here we’re using a Neo4j dataset featuring the FIFA Women’s World Cup 2019:NEO4J_URI=bolt://5d37db5a.databases.neo4j.io:7687NEO4J_USER=worldcupNEO4J_PASSWORD=worldcupApollo Server also comes with GraphQL Playground — its own graphical, interactive GraphQL IDE. To configure the Apollo Server in the index.js file, use:const { ApolloServer, gql } = require(apollo-server)const neo4j = require(neo4j-driver).v1const { makeAugmentedSchema, inferSchema } = require(neo4j-graphql-js)const dotenv = require(dotenv)dotenv.config()const driver = neo4j.driver(  process.env.NEO4J_URI,  neo4j.auth.basic(process.env.NEO4J_USER, process.env.NEO4J_PASSWORD))const context = { driver }inferSchema(driver).then(({ typeDefs }) => {  const schema = makeAugmentedSchema({ typeDefs })  const server = new ApolloServer({ context, schema })  server.listen().then(({ url }) => {    console.log(`Server ready at ${url}`)  })})npx nodemon index.jsNow you can explore GraphQL in its own intuitive GUI.Interactive and intuitive: GraphQL Playground enables better development workflowsGraphQL endpoints are effectively self-documenting, we can use the hints in the browser to quickly and easily build queries against our endpoints.If we write a query against our World Cup Neo4j instance, we can see the help it offers:Autocompletion is just one of the useful features of GraphQL PlaygroundNow we have a GraphQL endpoint deployed that reads the schema directly from our Neo4j instance. Let’s go back to our application and glue it together with ReGraph.Connecting the front and back endsWe have a number of dependencies to add to our application. At the terminal, enter:yarn add graphql apollo-boost @apollo/react-hooksNow add the Apollo client to our base index.js file. First we import:import ApolloClient from apollo-boostimport { ApolloProvider } from @apollo/react-hooksThen we update our base render statement with our imported Client:const client = new ApolloClient({ uri: https://movies.grandstack.io })ReactDOM.render(  <ApolloProvider client={client}>    <App />  </ApolloProvider>  document.getElementById(root))This gives our entire app access to the client for queries. To do this, we turn to App.js and import similar functions:import { useQuery } from @apollo/react-hooksimport { gql } from apollo-boostImporting the gql function and using it as a modern JavaScript tag” lets us define our GraphQL queries in a consistent, helpful way. With modern text editors, it provides syntax highlighting and query string formatting for free.Using React HooksReact Hooks make it really simple to add querying to our application.First, let’s define our query. We’ll query the Movies” example that’s already built into Neo4j Browser. We just need to use the existing Movies endpoint and update our App function from earlier:function App() {  const { loading, error, data } = useQuery(gql`    {      Movie(        filter: { AND: [{ year_gt: 1920 }, { year_lt: 1939 }] }        first: 100      ) {        _id        title        image: poster        actors {          _id          name        }      }    }  `)  if (loading) return <p>Loading...</p>  if (error) return <p>Error!</>  const items = createItems(data)  return (    <div style={{ display: flex, width: 100vw, height: 100vh }}>      <Chart items={items} options={options} layout={{ name: organic }} />    </div>  )}We use a function called createItems here that maps the result from our GraphQL endpoint to the simple format expected by ReGraph. This is trivial due to the strong type system of GraphQL: each nested result has a __typename property and we can get unique _id properties as part of our query.With a bit of polishing, we get a nice graph of our Movie dataset.The Neo4j movie database visualized in ReGraph, our graph visualization toolkit for React developersAlmost every aspect of ReGraph visualizations can be customized to suit your preferred style. We’ve used the movie poster URLs and set them as images for nodes.Using images as nodes is just one of many ways to customize your ReGraph chartsRevealing insight through visualizationsWith our application built, we can start making sense of the connections in our data. ReGraph is full of advanced features and functions to bring that data to life.Part of the power of our new application is that it doesn’t take much to apply the same code to a different database. Let’s switch back to our Women’s World Cup dataset to look at previous tournaments and the players that took part.Here the GraphQL query is:{    Person {      _id      name      dob {        formatted      }      in_squad {        _id      }    }    Squad {      _id      name: id    }  }Visualizing the entire dataset gives a useful overview of the relative size of componentsEach component features yellow nodes representing the number of World Cup tournaments that country has taken part in. The associated smaller green nodes are the players included in those squads.This view gives us a nice overview — you can clearly see those countries who’ve been in multiple tournaments and those who’ve made only one appearance.Let’s zoom in to look at Brazil. Their women’s team have taken part in all eight World Cup tournaments so far. Using ReGraph’s social network analysis (SNA) centrality measures, we can size individual players according to the number of finals they’ve played in over the years.Degree centrality finds nodes with the highest number of links to other nodes in the networkWe can quite clearly see Formiga’s played in a record-breaking seven tournaments.Try our technologyThe GRANDstack is a rich ecosystem of tools that developers use to rapidly build and prototype graph applications. We’ve shown how easy it is to add graph visualizations using our flexible, powerful ReGraph toolkit that’s designed specifically for React developers.ReGraph trials are free. Simply sign up here or get in touch.;Oct 21, 2019;[]
https://medium.com/neo4j/new-feature-neo4j-sandbox-backups-19952adba0d3;Max AnderssonFollowOct 15, 2021·4 min readNew Feature: Neo4j Sandbox BackupsWe’ve finally added the feature to produce your backups from your running sandboxes and continue the fun locally or in Aura.To try it out, just request a backup from the backups” panel.Request Sandbox backup (https://sandbox.neo4j.com)When you request a backup, our backup server processes the request. You have to wait for the backup to finish. Depending on your database size and how many people are currently doing backups. It can take anywhere from 30 seconds to 10 minutes.Once finished, your backup will pop up in the same tab. When you request a download link, a pre-signed URL will be made available for 30 days, and you will be able to download your database dump.How to Use Your Dump FileDepending on what you want to accomplish. The dump file can be used in multiple ways. For example, Import your dump file toAura / Aura Free (push-to-cloud or Aura Console)Neo4j Desktop (neo4j-admin load)A Neo4j Docker imageor other Neo4j instances **** Neo4j Sandbox will not support dump file imports.How Does It Work?Sandbox is a demo environment where you can experiment with your Neo4j database at no cost and explore different datasets and concepts of a graph database.However, sandboxes have a limited lifespan of 3 days that can be extended by another seven days.But when you have done your initial tinkering and learning, the following steps might be for you to migrate to Aura, Neo4j’s managed cloud solution, for production loads.Neo4j Aura - Fully Managed Cloud SolutionFor small development projects, learning, experimentation and prototyping. Start Free Aura Professional For medium…neo4j.comThe good thing about Aura is that you can now get an instance for free. That’s free forever, meaning you can run light loads for as long as it takes for you to finish your POC or whatever project you are working on currently.On the backend of things, Neo4j hosts your sandboxes in the cloud on AWS, and in the cloud, there is persistent storage and ephemeral storage. Sandboxes use ephemeral storage, as part of their Fargate instances that run the Sandbox Docker images, so once you terminate that instance, your data is gone. This is not the case with Aura.So recently, we added the feature to backup your sandboxes by providing the user with a dump file of the current database. And in order to do so, we had to implement some new infrastructure.How does it do that?Neo4j-Admin provides the tools needed to make backups from a database instance, but with some limitations e.g. those are not dump files. To produce a dump from a database, we need an offline neo4j database (not server just database). The problem is that we can’t take the sandbox server instance offline without losing our data since our storage is ephemeral.To make things more secure, tty/ssh access is denied to running tasks in our infrastructure unless you circumvent good practice. This means we can‘t stop a running database without some ugly scripting baked into our images (and you wouldn’t want us to stop your live database either). So producing a direct dump from the sandbox infrastructure is a no-go scenario.Instead, we use the following procedure.Make an Online Backup neo4j-admin backup --from <sandbox> --backup-dir <backup_dir>Restore the database locally neo4j-admin restore --from <backup_dir +  /neo4j> --database <backup_id>Make a dump from the local database neo4j-admin dump --database <sandbox_id> --to <backup_id>.dumpUpload it to s3.Now you might think to yourself, that’s quite the chunk of computing. No way will the user wait for a synchronous request to do all that. And you’ll be right, so that’s why we leverage Amazon SQS and a Celery backup service hosted in our Fargate cluster to make it all asynchronous.Amazon SQS is a queueing service to schedule celery tasks when a user requests a backup from our REST service.Our backup service then picks up the task and runs the procedure until completion.How to Load Your Dump File into AuraLoading your data into Aura can be done in a couple of ways, but if you need to get it done by a dump file.That, you can get straight from sandbox or use the dump files in the data folders of the public examples hosted here. (You might need git lfs” for some of the examples).neo4j-graph-examplesPeople This organization has no public members. You must be a member to see whos a part of this organization. You…github.comLogin to https://console.neo4j.io/Create a database (Read more about this here).Select the name of the database you want to import the data.Select the IMPORT DATABASE tab.Drag and drop your .dump file into the provided window or select Select a .dump file and select your file.Select Upload.Drop your dump file to start the import.Now that you know a little more about backups in Neo4j Sandbox have fun exploring the readily available use-cases in a sandbox, save your results, and tag #neo4j with your findings on Twitter.Cheers, Max Andersson;Oct 15, 2021;[]
https://medium.com/neo4j/creating-a-neo4j-react-app-with-use-neo4j-hooks-b2848d389e0a;Timothy LinFollowApr 26, 2021·4 min readCreating a Neo4j React App with use-neo4j HooksAt Cylynx, we build tools and solutions to help financial crime investigators connect the dots between their data points. This calls for a custom graph application to provide additional time series and investigative functionalities. Building a connector to Neo4j was one of our most requested features with its popularity within the financial services space. In this post, we document how we used use-neo4j hooks to quickly create a connector with Motif, our graph investigation software.Here’s a demo of the final integration:What Is Motif?Motif is a graph visualization and investigation application built on top of React, D3 and G6. It makes analyzing financial transactions easy with multiple layout options, filtering panels and edge bundling. Currently, it supports importing data as JSON files, edge-list CSVs, node-edge CSVs and from a Neo4j database directly.As it is built on top of React, it is easy to customize and add in new data connectors. We use G6 and its React binding library Graphin as they contain many out of the box styling and layout options. The libraries require a graph data input to be specified as a node and edge JSON file, similar to D3, visjs and many other popular javascript graphing libraries.What Is use-neo4j Hooks?From its Github repository:A set of components and hooks for building React applications that communicate to Neo4j. This is a package intended to speed up the development by reducing the amount of boilerplate code required.Since we were already building a React application, the library made it very easy to speed up the development process and create an integration with Neo4j.Hooking UpTo create a good user experience, we wanted to separate the database connection step from the querying step. A user can just connect Motif to their desired database for the first time and can then execute multiple queries without needing to re-authenticate.The pseudo-code to achieve this is outlined below:use-neo4j hooks provides a set of functions and React hooks to manage the database connections and querying process. We use them in the ConnectDatabase and ExecuteQuery components. In the parent connector, we keep track of the driver settings and the step which the user is on.Connect to Database ComponentThe ConnectDatabase component is responsible for initializing the driver and we use the createDriver function to set up a bolt connection with Neo4j.While use-neo4j hooks comes with a built-in login form, we want to have greater control of the styling and use our own form component for that to allow the user to key in her database connection details (host, port, username & password).On submission of the form, we create a driver and tests its connection. If successful, we store that driver instance in the state of our parent component. Otherwise, we display an error message.Query ComponentThe ExecuteQuery component contains the Neo4jProvider which manages the driver state and a CypherQuery component which is a text form that allows a user to key in her query and submits it to the database.We also use the useDatabase hook to populate a list of database options that the user can choose from.From Neo4j to D3 FormatNext, we have to convert the returned results from Neo4j which is a list of records into a format that is D3 compatible. neovis.js provides a very good guide on how to achieve this and for the most part, we copied their code.Here’s the full code for reference (you might need to adapt the types on your application)Developer TipsUse the Neo4j sandbox to speed up the development process. The one-click setup and easy access to multiple dataset helps us speed up the testing processNeo4j records data format supports multiple labels. For our application, we just take the first label. Another alternative would be to spread them out in the returned array.use-neo4j hooks helps save time if you are already working on a React application and looking to quickly build an integration with Neo4j.;Apr 26, 2021;[]
https://medium.com/neo4j/neodash-build-neo4j-dashboards-without-writing-any-front-end-code-7a132430ac50;Niels de JongFollowJan 27, 2021·6 min readNeoDash: Build Neo4j Dashboards Without Writing Any Front-End CodeWorking at Neo4j, I frequently build front-end applications that use graph data. Fortunately, a ton of tools exist to make the life of a Neo4j front-end developer easier (great examples are the GrandStack and Neode). In many cases, however, I’m looking for something that allows me to quickly prototype a dashboard with a direct database connection. Enter NeoDash, a tool that allows you to build a dashboard in minutes.NeoDash is a lightweight web app that hooks directly into Neo4j, letting you build a front-end without touching any code. It supports a variety of reports that natively work with Neo4j data types. For easy storage and version control, dashboards can be exported as JSON files.This post will give a quick run-through on how to get started with building your own NeoDash dashboards. First, I’ll provide a summary of the main features and visualizations. Next, I’ll also explain how you can save and store dashboards with your Neo4j projects.Building a DashboardIf you’re using Neo4j Desktop, you can install NeoDash from the Graph App Gallery. Alternatively, you can try out the demo here.1. Connecting to Neo4jWhen first starting up the application, you are prompted to connect to your Neo4j database (NeoDash will automatically connect to your active database if you’re using Neo4j Desktop.) The Neo4j JavaScript driver then uses your specified connection to run queries and populate your reports. NeoDash works with all flavors of Neo4j: Desktop, Neo4j Aura, clusters, and single instances.Want to restrict the type of queries in your reports to read-only? Create a read-only Neo4j user and use those credentials to connect.2. Creating Your DashboardDashboards consist of a variable number of reports (cards) that run independently. For each report, you specify a single Cypher query — the results of which will populate the report. Reports sit together in a responsive grid layout, allowing them to be resized and moved around. When reordered, reports try to fit together to fill available space.The image below contains an example of four reports fitting together.The current version of NeoDash supports five types of data reports:Graph visualizationsTable viewsBar chartsLine chartsJSON outputAs a sixth (informative) report type, there is the option to add markdown text.Graph VisualizationsFor almost all use cases, a force-directed graph visualization is your go-to report type. In NeoDash, you’ll be able to visualize your graph similar to how the Neo4j browser would, selecting a node color and parameter for the graph layout. Any query returning nodes, relationships, or paths can be visualized as a graph, NeoDash will automatically pick up these results from your Cypher query.Doing some exploration? Clicking a node will show all properties of that node.Some style properties of the visualization can be overridden by passing in extra parameters to your Cypher query. To use custom colors for the nodes, add the following text to the Cypher Parameters in the report settings:{     nodeColors : [ navy , green , red ]}Table ViewsThe table view renders Neo4j native types in a standard table format. These types include strings, numbers, and dates, but also nodes, relationships, and paths.Table columns are ordered in the order they are returned by Neo4j, with their names set to the names of your returned variables. This means that you’ll directly control what the column headers are in Cypher.If your table contains nodes and relationships, the colors of the nodes and relationships in the table can be set using the Cypher Parameters, for example:{     nodeColor :  orange ,     relColor :  black }Bar ChartsBar charts can be used to visualize categorical data. Naturally, a bar chart will need a numeric value to plot on the y-axis. The properties to visualize can be dynamically selected based on the returned values of your Cypher query.The color of a bar chart can be customized by setting the following value in the Cypher Parameters:{     color :  red }Line ChartsLine charts can be used to visualize the relationship between two numeric variables. Just like the bar chart, you’ll select two of the resulting fields to be plotted on the x- and y-axis.The Cypher Parameters setting can be used to customize several style properties of your line chart. Add the following parameters to create a red curved line chart, with a line width of 4 and a marker radius of 4:{     curve : true,     radius : 4,      color :  red ,      width : 4}Report SettingsEach report has a settings screen that can be accessed by clicking the (⋮) icon on the top right of the report. The settings menu can be used to set the Cypher query as well as other configuration parameters:The type of reportThe size of the reportParameters for your Cypher queryA refresh interval for the report3. Saving Your DashboardDashboards are automatically cached in Neo4j Desktop or your web browser. To share/store your dashboards externally, you can export them as JSON by clicking the ‘Load/Export’ button. Copy-paste the JSON code into a text file, and you can add it to your project repository if needed.To load your dashboard back up again, paste back the JSON into the ‘Load/Export’ textbox and hit save.Performing a Hard ResetYou can reset NeoDash by clearing the ‘Load/Export’ box and saving the dashboard. Alternatively, If you’d like to start over fresh, you can consider performing a hard reset. A hard reset will also give you a chance to save your latest dashboard.To perform the reset, you can navigate to:https://neodash.graphapp.io/?reset=trueThis will clear the application cache, restore the default dashboard and print the latest cached dashboard for you.Using NeoDash from Neo4j Desktop? Reinstall the graph app to reset the cache.Wrapping UpUsing a direct database connection, NeoDash is a great tool to build a quick proof-of-concept dashboard. However, for something production-grade, I highly recommend you to put an API layer in between your DB and your front-end. If you’re in the process of building a solution and need some advice, feel free to get in touch.What’s Next?In the short term, I’m looking to add more report types, specifically geographic visualizations. More customization options for the existing reports are also on the horizon. If you are enjoying the tool, found bugs, or have any suggested improvements, get in touch!;Jan 27, 2021;[]
https://medium.com/neo4j/relationship-chain-locks-dont-block-the-rock-e8db75254b63;Stu MooreFollowJul 13, 2021·8 min readRelationship Chain Locks: Don’t Block the Rock!Welcome back to part two in the five part 4.3 Technical Blog Series. This week, it is the turn of the Relationship Chain Locks — or if you caught the 4.3 update during Emil’s NODES Keynote, How to avoid a lot of locking when you update the Rock!”Dwayne ‘The Rock’ Johnson breaking out as super node (Credit ProWrestling.Fandom.com)Relationship / Relationship Property IndexesRelationship Chain Locks (this blog)Deploying on Kubernetes with Helm ChartsServer Side RoutingRead Scaling for AnalyticsI am going to cover why you need Relationship Chain Locks in your graph database solution, the conditions under which Neo4j uses them (this isn’t something you configure or invoke), how it all works under the hood, and a speed test vs prior versions of Neo4j. Plus, if you are interested, a bonus under the hood in-depth explanation of the feature.What are Relationship Chain Locks and when do you need them?An example of a super node, AKA dense node.Imagine you have a popular social media network like Instagram, and you want to add a new follower to a celebrity like Dwayne The Rock” Johnson and his 232 million followers. Then you need to lock the Rock and the people who want to be added as followers. That is a lot of locking when you have nodes like the Rock. When a pattern of nodes connected to hundreds/thousands or millions of other nodes emerge in a graph they can be referred to as dense nodes (nodes with dense relationships) or super nodes (check out Dave Allen’s blog on Graph Modeling with Super Nodes).In order to perform an update, a lock is required to ensure you don’t get data corruption — which could occur if someone else wants to follow the Rock at the same time — say one of his posts goes viral! So locks are good, they help ensure data integrity. The trouble is that waiting (AKA lock contention) is required when your multi-threaded application needs to perform updates on the same nodes/relationships. The more nodes/relationships and load you have, the more likely you are to need to wait for a lock to be released in order to perform an update.Prior to 4.3, Neo4j obtained a lock on the Rock to add/delete a relationship flowing to/from the Rock — and with 232 million followers that can mean a lot of locking to update the Rock.Don’t Block the Rock with Neo4j 4.3Dwayne The Rock” Johnson takes out his security gate (Credit: Instagram)So how does the latest version of Neo4j avoid blocking the Rock when adding/deleting relationship types and directions? The quick answer is… the storage engine implements new locking algorithms that are used to acquire and release the locks across internal elements in the store. For an in-depth explanation, refer to the section Bonus — so how does it work?”So when can additions/deletions of a specific type and direction be handled concurrently? The new algorithm is applied when nodes reach 50+ relationships — the point at which they are considered super nodes. Note this is not reversible once a node is treated as a super or dense node it will remain so even if the relationship count drops to 48. A relaxed locking algorithm has been implemented as well. This is used when there are 10+ relationships of a given type and direction in any one group.Why not just do this all the time? Well, you don’t get something for nothing and while only a few microseconds are added by this mechanism it would have a disproportionate impact on nodes with only a few relationships.This means your code, Cypher commands, import scripts, etc. will run a lot faster when you have super nodes in your graph. Please note, if you have been putting hacks in your scripts to try and work around lock contention you should remove them before you upgrade to 4.3. It is worth noting that no store migration is required when you upgrade to 4.3.So, Let’s Rock!I am going to test this out by creating a graph with 100 super nodes (with a Person label) that represent some celebrities, and then create 100,000 nodes that follow one of the celebrities at random — not quite the Trillion relationship graph you saw at NODES but with nearly 1000 relationships the nodes are sufficiently dense to put a 4 GB Docker instance running on my laptop through its paces. If you want to follow along, here is my code to create the container and test out the relationship locking.#neo4j-the-rock.sh — script to create the docker instance#We need APOC so make sure to move the plugin aligned version from /labs to /pluginsexport PORT=`date +%S`123echo $PORTexport HERE=`pwd`mkdir -p $HERE/datarm -rf $HERE/data/*export HERE=`pwd`export CONTAINER=`docker run \— name neo4j-the-rock \— detach \— publish=7474:7474 \— publish=7687:7687 \— volume=$HERE/data:/data \— volume=$HERE/import:/var/lib/neo4j/import \— volume=$HERE/plugins:/plugins \— ulimit=nofile=40000:40000 \— env=NEO4J_dbms_memory_heap_max__size=4G \— env=NEO4J_dbms_memory_pagecache_size=2G \— env=NEO4J_ACCEPT_LICENSE_AGREEMENT=yes \— env=NEO4J_AUTH=neo4j/**e******* \neo4j:4.3.1-enterprise`docker ps -f name=neo4j-the-rockNow run the script to create the Docker container./neo4j-the-rock.shUsing Cypher, let’s generate the celebrities in Browser.// Create a list of 100 entries and iterate through the list passing the number in as an id for the person node.UNWIND range(1,100) as id create (p:Person {id:id}) return pNext, create a node that has a relationship [:FOLLOWS] to a random celebrity node” using APOC’s periodic iterate which will process the 100,000 follower nodes in batches of 5.call apoc.periodic.iterate(UNWIND range(1,100000) as id RETURN id”,MATCH (p1:Person) WHERE id(p1)=toInteger(rand()*100)CREATE (p1)-[:FOLLOWS]->(p2 {id: id})”,{batchSize:5, parallel:true})The table results page shows these batches all executed successfully, and the results Started streaming 1 record after 20 ms and completed after 2862 ms.” which is about 2.9s.Results of creating the 100,000 relationships with the celebrity nodes.From the following query we can see that people are following one of the celebrity” nodes i.e. one of the first 100.MATCH (n)-[r:FOLLOWS]->(p {id:1}) RETURN n,p LIMIT 10If we drop the limit, the results show how dense these nodes are (note that Browser’s defaults limit the results to 300, so this is about one third of the actual [:FOLLOWS] that were created.Not all return nodes are being displayed due to the Initial Node Display setting. Only 300 of 300 nodes are being displayedNeo4j 4.3 Is 6.5x Faster Than Previous VersionsTo swap out the version of Neo4j running in the Docker container, substitute neo4j:4.3.1-enterprise for neo4j:3.5-enterprise and neo4j:4.2.8-enterprise in the Docker scripts (refer to end of the blog post). The results for Neo4j 3.5.18 and 4.2.8 are pretty consistent at 19263 ms and 19224 ms respectively**, which pegs Neo4j 4.3 6.5x faster in this scenario***.**Note the 0.2% difference isn’t significant, and is down to variations in running the test on my laptop rather than any real performance difference between versions. And while the SLOTTED runtime in 4.2.8 would usually perform faster than PROCEDURE in 3.5, the lock contention is the limiting factor which is why they complete in the same amount of time.***Using a 4GB Docker instance, results will vary depending on the system and nodes/relationships being updated and use of indexes.Bonus: So How Does It Work?I provided a very lightweight explanation of how the feature works, but as you would expect, the reality is far more complex. So for those of you who are interested I thought it was worth diving into a bit more detail.The locking takes advantage of the abstraction, provided in previous releases of Neo4J, between the user’s view of the graph (as seen in Browser or Bloom) to one where nodes are connected by groups of relationship chains in the data stores on disk. This level of abstraction can be seen in the image below on the right that shows the internal structure of the graph.The nodes N1, N2 are stored in the file neostore.nodestore on disk in the directory data\database\$database_name. Th≥ ese are connected by Relationships Groups RG01, RG02, RG03 and stored in the neostore.relationshipgroupstore. Think of the RGnn as nodes” that define a group of relationships that are created for each [:RELATIONSHIP_TYPE]. For each relationship group there are up to three sets of records in the neostore.relationshiptypestore that describe the chains of linked relationships for OUT-bound, IN-bound and LOOPed relationships — these chains consist of nodes’’ that connect relationships in a chain. Neo4j can now obtain and release locks on these nodes” when it needs to update a relationship or node in the graph.Valdemar Roxling, the lead developer for this feature, explained that the most challenging part was avoiding deadlocks — where one set of locks is waiting on another set of locks that are waiting on the first set of locks to be released. The key is to take the locks in the correct order, with strict rules in the sorting algorithm — attempting to explain any more would cause my head to explode and reveal our IP ).I will leave it there. Thanks for reading and an even bigger thanks to Valdemar for his work on this feature and helping me with the blog. Stay tuned for the next blog on Helm Charts and running Neo4j on Kubernetes.The Docker Scripts (Continued)Run each of the scripts in turn, and re-run the Cypher queries above in Browser to get results for 3.5 and 4.2.#load-docker-3-5.sh#Note 3.5 uses dbms_memory_heap_maxSize and NOT #dbms_memory_heap_max_sizeexport PORT=`date +%S`123echo $PORTexport HERE=`pwd`mkdir -p $HERE/3-5-datarm -rf $HERE/3-5-data/*export CONTAINER=`docker run \— name neo4j-the-rock \— detach \— publish=7474:7474 \— publish=7687:7687 \— volume=$HERE/data:/data \— volume=$HERE/import:/var/lib/neo4j/import \— volume=$HERE/plugins:/plugins \— ulimit=nofile=40000:40000 \— env=NEO4J_dbms_memory_heap_maxSize=4G \— env=NEO4J_dbms_memory_pagecache_size=2G \— env=NEO4J_ACCEPT_LICENSE_AGREEMENT=yes \— env=NEO4J_AUTH=neo4j/**e******* \neo4j:3.5-enterprise`Cypher version: CYPHER 3.5, planner: PROCEDURE, runtime:3.5 Started streaming 1 records after 19260 ms and completed after 19263 ms.#load-docker-4-2.shexport CONTAINER=`docker run \ — name neo4j-the-rock \— detach \— publish=7474:7474 \— publish=7687:7687 \— volume=$HERE/data:/data \— volume=$HERE/import:/var/lib/neo4j/import \— volume=$HERE/plugins:/plugins \— ulimit=nofile=40000:40000 \— env=NEO4J_dbms_memory_heap_max__size=4G \— env=NEO4J_dbms_memory_pagecache_size=2G \— env=NEO4J_ACCEPT_LICENSE_AGREEMENT=yes \— env=NEO4J_AUTH=neo4j/**e******* \neo4j:4.2.8-enterprise`Cypher version: CYPHER 4.2, planner: COST, runtime: SLOTTED4.2 Started streaming 1 records after 33 ms and completed after 19224 ms.;Jul 13, 2021;[]
https://medium.com/neo4j/connecting-to-react-app-to-neo4j-148881d838b8;Adam CowleyFollowOct 2, 2020·4 min readConnecting your React app to Neo4j with React HooksFor my role as a Developer Experience Engineer at Neo4j, I’ve spent some time recently looking at how developers interact with Neo4j. It’s always been a surprise that there weren’t more community tools that make it easier to work with, especially for a front-end framework as popular as React.Granted, in a lot of cases, a Front-End Developer will be interacting with an API, and we’ve got that covered with tools along with the official drivers and frameworks like Spring Data Neo4j or GRANDstack.But if your only aim is to quickly prototype an application or to build a Graph App then there’s a lot of boiler plate. You’d need to obtain some login credentials, create a driver, open a session, and run a query/transaction all before you can get to the fun part.So with that in mind, last week I started working on a new project and posted this teaser on Twitter:The teaser contained screenshots to some experiments that served as the precursor to the use-neo4j library.React Hooks for Neo4jA while ago, I wrote the vue-neo4j plugin which was designed to cut down the boilerplate in Vue.js prototypes. Although the underlying concepts are the same, the conventions in React are very different. In React you would use Hooks or Effects to fetch data from an external source.If you’re interested in what is going on under the hood, all of the code is available on Github.Context is ImportantReact uses Context to provide a way of passing data through a component tree without having to pass a prop through the components at each level (also known as prop drilling).The use-neo4j plugin works around its own Context — this is what makes the Neo4j Driver instance available throughout the application.There are two ways of creating this context — taking inspiration from the Apollo Client implementation, the use-neo4j library exports a Neo4jProvider, a functional component, which initiates the context.You can initialize this context in one of two ways. If you know the credentials of the server up-front, you can create an instance of the driver using the createDriver function and pass it as a prop into the Neo4jProvider.import { Neo4jProvider, createDriver } from use-neo4j// Create driver instanceconst driver = createDriver(neo4j, localhost, 7687, neo4j, letmein)ReactDOM.render(  <React.StrictMode>    {/* Pass it as a prop to the Neo4jProvider */}    <Neo4jProvider driver={driver}>      <App />    </Neo4jProvider>  </React.StrictMode>,  document.getElementById(root))If you don’t know the details up-front, or neglect to pass a driver prop, you will instead be presented with a login form. On clicking Connect to Neo4j, the library will attempt to create a driver and connect to the Neo4j instance.The form fields can be pre-populated by passing additional props to the Neo4jProvider:ReactDOM.render(  <React.StrictMode>    <Neo4jProvider       scheme= neo4j+s       host= myauradb.neo4j.io       port= 7687       username= username       password= defaultpassword        database= someotherdb      >      <App />    </Neo4jProvider>  </React.StrictMode>,  document.getElementById(root))Hooks for Querying DataOnce React has an instance of the Neo4j Driver to query against, you can use the useReadCypher and useWriteCypher hooks to run a query against the database. Both procedures take three arguments the cypher query, an object to represent any parameters, and optionally the database to run the query against if running multiple databases. They both return an object which corresponds to the Neo4jResultState interface.useReadCypher(  cypher: string,   params?: Record<string, any>,   database?: string): Neo4jResultStateThe interface provides reactive properties to represent the query’s state:const {  loading,  error,  records,  first } = useReadCypher(query, params)A component can react to changes to these components and re-render based on the results. For example, a loading message could be returned if the query hasn’t finished:if ( loading ) return (<div>Loading...</div>)Or if there was a problem executing the query…if ( error ) return (<div className= error >{error.message}</div>)The result variable returns a QueryResult as it would when using the Promise API, an array of Records with a .get() function for retrieving a value returned by the query.const movies = result.records.map(row => row.get(movie))Or, if you are only expecting one row, the first variable will allow you to access that row directly. Piecing it all together, your component may look like this:function MyComponent({ title }) {    const query = `MATCH (movie:Movie {title: $title}) RETURN movie`    const params = { title } // Movie title passed as a prop    const { loading, first } = useReadCypher(query, params)    if ( loading ) return (<div>Loading...</div>)    if ( error ) return (        <div className= error >{error.message}</div>    )    // Get `m` from the first row    const movie = first.get(movie)    return (        <div>            {movie.properties.title}            was released in            {movie.properties.year}        </div>    )}More InformationFor more information, check out the README on the Github Repository. I will be adding more functionality soon, and also incorporating this into a Graph App starter kit for React developers — making it easy for you to build prototypes, or a new Graph App for the rest of the community to enjoy.These hooks may unintentionally expose Neo4j credentials and are therefore not intended for use in production (unless you’re building a Graph App) — for that, you’re better off building a REST API to sit between your application and the Neo4j database, or using GRANDstack.If you have any questions, comments or suggestions feel free to get in touch.I run a livestream on all things Neo4j and TypeScript every Tuesday — 13:00–14:00 UK time on the Neo4j Twitch Channel. You can also catch up with videos on the Neo4j Youtube Channel.;Oct 2, 2020;[]
https://medium.com/neo4j/whats-cooking-part-3-a-segue-into-graph-modelling-5666d1b05037;Mark NeedhamFollowFeb 28, 2019·5 min readWhat’s cooking? Part 3: A segue into graph modellingLast week Lju and I presented part 1 and part 2 of the BBC GoodFood What’s cooking? series as part of the Neo4j Online Meetup, and during the talk we received some great questions about the graph model we’d come up with.In case you haven’t seen the video, or read the first two blog posts, below is an example of the data that we have for each recipe.We started by modelling recipes and their ingredients, and below is a diagram of this part of the graph model:Nodes vs PropertiesOne of the questions during the meetup was why we created a relationship from Recipe to Ingredient, rather than just storing the ingredients as an array on Recipe nodes. That version of the model would look like this:A nice way of working out whether to model something as a node or as a property is to think whether we want to query through that thing. In this case do we ever want to write a query that traverses from a Recipe to another Recipe via their common Ingredients?If we use the first model it’s easy to write a query to do that. The query might look like this:MATCH (r:Recipe {name: Tomato & mozzarella couscous salad }),      (r)-[:CONTAINS_INGREDIENT]->(i)<-[:CONTAINS_INGREDIENT]-(r2)RETURN r2.id AS id, r2.name AS recipe, count(*) AS commonIngredientsORDER BY commonIngredients DESCLIMIT 10Now what about if we store ingredients as properties on the recipe instead? It’s easy to refactor our model to do this. The following query adds a property ingredients to each recipe with an array containing its ingredients:MATCH (r:Recipe)SET r.ingredients = [(r)-[:CONTAINS_INGREDIENT]->(i) | i.name]And our query to find recipes that use the same ingredients as the Tomato & mozzarella couscous salad now reads as follows:MATCH (r:Recipe {name: Tomato & mozzarella couscous salad }),      (r2:Recipe)WHERE r <> r2      WITH r2, [i in r2.ingredients           WHERE i in r.ingredients] AS commonIngredientsRETURN r2.id AS id, r2.name AS recipe,        size(commonIngredients) AS commonIngredientsORDER BY commonIngredients DESCLIMIT 10We had to write more code than in the first query, but perhaps what’s more interesting is how much extra work the database has to do. If we prefix these queries with the clause PROFILE we can get back a query plan.Below are the query plans. On the left is the one where ingredients are modelled as nodes, and on the right is the one where ingredients are modelled as properties on recipes:From a brief inspection it seems like the plan on the right is doing more work. This is confirmed by the summary that shows us the total number of db hits:Ingredients as nodesCypher version: CYPHER 3.4, planner: COST, runtime: SLOTTED. 22150 total db hits in 139 ms.Ingredients as propertiesCypher version: CYPHER 3.4, planner: COST, runtime: SLOTTED. 152677 total db hits in 251 ms.A db hit is a unit of storage engine work, and our 2nd query does roughly 7 times more work than the 1st one. This is because it has to scan ever single recipe to check for overlapping ingredients, whereas the 1st one only looks at recipes that it traverses to via the CONTAINS_INGREDIENT relationship.So in this case, given that we want to write queries to find similar recipes based on ingredients, modelling ingredients as nodes is a better choice.Better off with propertiesA part of the model where we’d be better off using properties is DietType. In the original model we had that modelled as a node, as show in the diagram below:Once we started working with the data, we realised that our queries only ever treated DietType as meta data, not something that we’d use for recipe recommendation queries.We can therefore refactor our model to store the diet types on the Recipe node instead. Our model would then look like this:We can run a similar query as we did with ingredients to refactor the model:MATCH (r:Recipe)SET r.dietTypes = [(r)-[:DIET_TYPE]->(d) | d.name]We could even delete the DietType nodes and their associated relationships in the same query:MATCH (r:Recipe)SET r.dietTypes = [(r)-[:DIET_TYPE]->(d) | d.name]WITH rMATCH (r)-[:DIET_TYPE]->(d)DETACH DELETE dAnd now if we want to return the diet types for a recipe it’s as simple as looking up that property:MATCH (r:Recipe {name: Tomato & mozzarella couscous salad })RETURN r.id, r.name, r.dietTypesAs with most things in the world of graph modelling, and indeed data modelling in general, the appropriate model depends on what we’re doing with the data.If we were building a recipe graph completely based on diets then we might take a different approach than the one described in this most. And if we want to change the model once we’ve got started, it’s not all that hard to do.If you liked this post you can also read the first two posts of the series:What’s cooking? Part 1: Importing BBC goodfood information into Neo4jWhat’s cooking? Part 2: What can I make with these ingredients?;Feb 28, 2019;[]
https://medium.com/neo4j/new-features-in-1-7-neo4j-drivers-4bde893b1374;Zhen LiFollowNov 26, 2018·5 min readNeo4j Drivers, version 1.7A few weeks ago, we released our 1.7 official Neo4j Drivers. For the first time, this covered five languages, namely Java, .Net, Python, JavaScript and Go. Here is what’s in the box.To read more about the new Go driver, have a look at this post.Photo by Austin Neill on UnsplashThe focus of this article is the 1.7 Drivers where we’ll look at the new features in detail. The highlights are as follows:Transaction ConfigCustom Server Address ResolverServer Name Indication (SNI) and hostname verificationWe added a few new classes and methods in the API to support these new features. But other than this, the surface remains the same. If you’re running Neo4j 3.3 or above, you can upgrade to a 1.7 driver without any changes to your code.Transaction ConfigTransaction Config enables fine-grained configuration control on any transaction started by a driver. It provides two settings: transaction timeout and transaction metadata.Transaction timeout defines the maximum time for which a transaction should run. It can be used to kill overrunning transactions in order to free database resources, thereby protecting the database from overload. By default (i.e. without a transaction timeout specified explicitly on transaction start) the transaction timeout defined in neo4j.conf at dbms.transaction.timeout will be used.Transaction metadata, on the other hand, is often used along with the database logs for auditing. When transaction metadata is attached to a transaction, this will appear alongside the transaction in Neo4j query.log and in the output from the dbms.listTransactions procedure. This could be used to attach information about the end users, for example. Note that as query.log and dbms.listTransactions are Neo4j Enterprise features, these will only be available to Enterprise database users.The transaction configuration can be provided when a transaction is created using the drivers.In Java, a TransactionConfig object can be created with a builder:Map<String,Object> metadata = new HashMap<>()metadata.put(  Username , myUsername )metadata.put(  Time , ZonedDateTime.now() )TransactionConfig config = TransactionConfig.builder()    .withMetadata( metadata )    .withTimeout( Duration.ofSeconds( 1 ) )    .build()This configuration can then be passed to a transaction on creation:try ( Transaction tx = session.beginTransaction( config ) ){    StatementResult result = tx.run(          CREATE (a:Greeting) SET a.message = $message   +         RETURN a.message + , from node  + id(a) ,        Values.parameters(  message , message ) )    return result.single().get( 0 ).asString()}It can also be attached directly to Cypher execution code:session.run(      CREATE (a:Greeting) SET a.message = $message   +     RETURN a.message + , from node  + id(a) ,    Values.parameters(  message , message ), config )and:session.writeTransaction( tx -> {    StatementResult result = tx.run(          CREATE (a:Greeting) SET a.message = $message   +         RETURN a.message + , from node  + id(a) ,         Values.parameters(  message , message ) )    return result.single().get( 0 ).asString()}, config )This functionality is supported in all drivers, so if you aren’t using Java, head over to our Driver Manual and Driver API Documentation to find an example for your language.Note also that you’ll have to be running at least Neo4j 3.5 to be able to use TransactionConfig, since the mechanism requires Bolt version 3.Custom Server Address ResolverThe custom server address resolver allows a user to specify an address resolver function which is used by the routing driver when resolving the initial cluster address passed in a bolt+routing URI.Inside the Driver object is a routing table which maintains a set of servers to which the client can connect. The initial population of this table can therefore be influenced by using a function that returns multiple initial server addresses, avoiding the need for complex DNS configuration or a separate load balancer above the cluster core members.The resolution process happens:during the very first rediscovery when the driver is created,when all the known routers from the current routing table have failed and the driver needs to fallback to the initial address.In other words, this address resolver only kicks in when the driver cannot perform a discovery using its rediscovery servers in its current routing table. Then it will fall back to this address resolver to retrieve more hints about where to perform rediscovery.The example below shows a possible implementation in Java:String virtualUri =  bolt+routing://x.acme.com Config config = Config.builder()    .withResolver( address -> new HashSet<>( Arrays.asList(        ServerAddress.of(  a.acme.com , 7676 ),        ServerAddress.of(  b.acme.com , 8787 ),         ServerAddress.of(  c.acme.com , 9898 ) ) ) )    .build()return GraphDatabase.driver( virtualUri, AuthTokens.basic( user, password ), config )Some drivers such as Python driver also support this feature for a direct driver (i.e. one created via the bolt URI scheme). The details for each language driver can be found in the driver’s API documentations.SNI and hostname verificationSNI (Server Name Indication) is an extension to TLS. It allows the client to pass the hostname that the client intends to talk to as part of the TLS handshake negotiation. This can be used (for example) by a proxy server to route traffic arriving at a single IP address to multiple backend servers. In 1.7 drivers, we have enabled SNI on the client side by default to make integration easier for systems that require SNI support.We have also provided support for verification of the hostname in the server certificate received by the client. So we not only verify that the server is a valid server (with a valid certificate) but we also ensure that the server is the one that the client intended to talk to (whose hostname in the CN field of the server certificate matches the host part in the client’s connecting request URL).The following code illustrates how to enable hostname verification with neo4j-java-driver via driver configuration (which is turned off by default):Config config = Config.builder()    .withEncryption()    .withTrustStrategy(         TrustStrategy.trustCustomCertificateSignedBy(certFile)                      .withHostnameVerification() )    .build()driver = GraphDatabase.driver( uri,                       AuthTokens.basic( user, password ), config )ConclusionSo you can see that the 1.7 drivers provide a good set of extra features. Don’t forget that we have four other languages available, aside from Java — Python, JavaScript, .NET and Go. All languages share a uniform API, so it shouldn’t be too hard to map these same concepts into our other supported languages. Examples can be found in our documentation.If you run into any issues while using our drivers, please feel free to find us on our Neo4j community site or open a GitHub issue.;Nov 26, 2018;[]
https://medium.com/neo4j/discover-neo4j-auradb-free-week-24-nytimes-article-knowledge-graph-b2cc3ce6393c;Michael HungerFollowApr 11, 2022·11 min readWeek 24 — NYTimes Article Knowledge GraphThe role of journalists in current times can’t be overstated. We all rely on critical and honest journalism to chronicle and alert us to the perils of humankind.But news is much more than just the articles you see on the front pages. A lot of the detail is hidden behind metadata that’s not immediately visible.If you look at the front page of the New York Times you see a lot of moving articles and images.The New York Times - Breaking News, US News, World News and VideosLive news, investigations, opinion, photos and video by the journalists of The New York Times from more than 150…www.nytimes.comToday we want to look behind the scenes, using the NYTimes API to access some of the article metadata.If you’d rather watch the livestream video, have fun! Otherwise, follow along with this article.Our colleague Will Lyon prepared a repository with a data model and import script for the API of popular articles, as well as a Neo4j Browser guide. He originally created the data exploration for an investigative journalism conference (NICAR).You can go there to follow along.GitHub - johnymontana/news-graph: New York Times articles as a knowledge graphWorking with New York Times article data in Neo4j and GraphQL. Data comes from the New York Times API.github.comLet’s first create our AuraDB Free instance to get you going. Please pick the blank database.”connect-aura.adocDeveloper APIFirst, sign up to the NYTimes developer API, and add an app to get your API Key.They provide a lot of different APIs, including book and movie reviews, semantic categories, article archive metadata, and popular articles.To keep things simple we’re looking at the straightforward popular articles” API, which is just a JSON endpoint that provides the most popular articles for the last 7 or 30 days (just change the number in the URL to adjust your time frame).https://api.nytimes.com/svc/mostpopular/v2/viewed/7.json?api-key=<key>Here you can see what part of the response looks like.{	 status :  OK ,	 copyright :  Copyright (c) 2022 The New York Times Company.  All Rights Reserved. ,	 num_results : 20,	 results : [{			 uri :  nyt://article/8428defe-c56e-5177-8798-ea2fbc3ef715 ,			 url :  https://www.nytimes.com/2022/04/06/us/politics/us-russia-malware-cyberattacks.html ,			 id : 100000008282002,			 asset_id : 100000008282002,			 source :  New York Times ,			 published_date :  2022-04-06 ,			 updated :  2022-04-07 10:45:47 ,			 section :  U.S. ,			 subsection :  Politics ,			 nytdsection :  u.s. ,			 adx_keywords :  Russian Invasion of Ukraine (2022)Cyberwarfare and DefenseUnited States International RelationsEspionage and Intelligence ServicesBiden, Joseph R JrJustice DepartmentFederal Bureau of InvestigationGRU (Russia)RussiaUkraine ,			 column : null,			 byline :  By Kate Conger and David E. Sanger ,			 type :  Article ,			 title :  U.S. Says It Secretly Removed Malware Worldwide, Pre-empting Russian Cyberattacks ,			 abstract :  The operation is the latest effort by the Biden administration to thwart actions by Russia by making them public before Moscow can strike. ,			 des_facet : [				 Russian Invasion of Ukraine (2022) ,				 Cyberwarfare and Defense ,				 United States International Relations ,				 Espionage and Intelligence Services 			],			 org_facet : [				 Justice Department ,				 Federal Bureau of Investigation ,				 GRU (Russia) 			],			 per_facet : [				 Biden, Joseph R Jr 			],			 geo_facet : [				 Russia ,				 Ukraine 			],			 media : [{				 type :  image ,				 subtype :  photo ,				 caption :  Some American officials fear that President Vladimir V. Putin of Russia may be biding his time in launching a major cyberoperation that could strike a blow at the American economy. ,				 copyright :  Mikhail Klimentyev/Sputnik ,				 approved_for_syndication : 0,				 media-metadata : [					{						 url :  https://static01.nyt.com/images/2022/04/06/us/politics/06dc-russia-hacks-1/merlin_204742779_ca6a0b7b-3630-426c-9ee7-77628e11521b-mediumThreeByTwo440.jpg ,						 format :  mediumThreeByTwo440 ,						 height : 293,						 width : 440					}				]			}],			 eta_id : 0		},This is the article data and metadata that we will import/turn into a graph and explore.Data ModelThe data model is based on the data we get back from the API.We have the main Article node with properties like:titleidurlbyline (Authors)sourcepublished_dateabstractThere are also others, but we’ll ignore those sections for now.Additionally, we get metadata for each article:Topics (des_facet)Organizations (org_facet)People (per_facet)Locations (geo_facet)Photos (media)Those metadata entries can be turned into their own nodes and connected to the article via relationships (see the model diagram). Via those nodes, we can then correlate and relate articles.Article Graph Data ModelImportWe’ve seen the direct response from the JSON API in our browser. To load the data into Neo4j we can utilize apoc.load.json, a custom procedure that provides the response of the API as Cypher data structures.It is one of the procedures that’s also available in AuraDB.https://neo4j.com/docs/aura/current/getting-started/apoc/#_apoc_loadLet’s start by just fetching the data.call apoc.load.json( https://api.nytimes.com/svc/mostpopular/v2/viewed/7.json?api-key=<api-key> )In Neo4j Browser, we can actually set the API key as a parameter with :param key⇒<api-key>. Then we can use $key in our query and don’t have to expose our key.call apoc.load.json( https://api.nytimes.com/svc/mostpopular/v2/viewed/7.json?api-key= +$key)You’ll see it looks the same as before. What we need to do now is take the value result and iterate over the results array.Let’s do that with UNWIND (that turns lists into rows) and instead of the full entry return its individual parts.call apoc.load.json( https://api.nytimes.com/svc/mostpopular/v2/viewed/7.json?api-key= +$key)yield valueunwind value.results as articlereturn article.id, article.title, article.published_date, article.byline, article.geo_facet, article.des_facetThis is now the data we can use to create our article nodes.╒════════════╤════════════╤════════════╤════════════╤════════════╕│ title      │ article.pub│ article.byl│ article.geo│ article.des││            │lished_date │ine         │_facet      │_facet      │╞════════════╪════════════╪════════════╪════════════╪════════════╡│ Satellite i│ 2022-04-04 │ By Malachy │[ Ukraine , │[ Russian In││mage        │            │Browne, Davi│Russia , Buc│vasion of Uk││            │            │d Botti and │ha (Ukraine)│raine (2022)││            │            │Haley Willis│ , Kyiv (Ukr│ , Civilian ││            │            │            │aine) ]     │Casualties ]│├────────────┼────────────┼────────────┼────────────┼────────────┤│ Grammys 202│ 2022-04-03 │ By Shivani │[]          │[ Grammy Awa││2 Wi        │            │Gonzalez    │            │rds , Gospel││            │            │            │            │ Music , Fol││            │            │            │            │k Music , Cl││            │            │            │            │assical Musi││            │            │            │            │c , Pop and ││            │            │            │            │Rock Music ,││            │            │            │            │ Blues Music││            │            │            │            │ , Jazz ]   │We will use MERGE, which is a get-or-create (or upsert), so if the data already exists in our graph it’s not added again — it’s just provided to our statement.Our key to merging the article is its URL, which should be globally unique, and then SET the remaining attributes.(For larger data volumes and consistency we’d also create a constraint, but we’re skipping that here.)call apoc.load.json( https://api.nytimes.com/svc/mostpopular/v2/viewed/7.json?api-key= +$key) yield valueunwind value.results as article // uniquely create an article  MERGE (a:Article {url: article.url})    SET a.title     = article.title,        a.abstract  = article.abstract,        a.published = datetime(article.published_date),        a.byline    = article.byline,        a.id = article.id        a.source = article.sourceSo if we now go to the left sidebar and click on the Article label, we can see a number of lonely article nodes floating in space. You can push them around and query them by attributes, but not yet follow their relationships.// find some articlesMATCH (n:Article) RETURN n LIMIT 25// find articles by dateMATCH (n:Article)WHERE n.published = datetime( 2022-04-08 )RETURN n LIMIT 25// find articles with matching (case sensitive) title contentsMATCH (n:Article)WHERE n.title contains BuchaRETURN n.title, n.bylineLIMIT 25Article NodesIn the next step, we’re starting to add relationships, first for the Topic nodes that are contained in the des_facet array of the article response.A FOREACH iterates over that array creates the nodes, and connects them to the current article.We also change the SET to ON CREATE SET so the properties are only added the first time a node is created.call apoc.load.json( https://api.nytimes.com/svc/mostpopular/v2/viewed/7.json?api-key= +$key) yield valueunwind value.results as article  MERGE (a:Article {url: article.url})  ON CREATE SET a.title     = article.title,        a.abstract  = article.abstract,        a.published = datetime(article.published_date),        a.byline    = article.byline,        a.id = article.id,        a.source = article.source  FOREACH (desc IN article.des_facet |    MERGE (d:Topic {name: desc})    MERGE (a)-[:HAS_TOPIC]->(d)  )Topic Overlap and RecommendationThis already opens up some interesting aspects — for one, we already see clusters appearing of articles that share one or more topics but are totally unrelated to other sets of articles.If you click on the HAS_TOPIC relationship in the left sidebar and remove the limit you should see something like this. You can go into full-screen mode and zoom out for a good overview.Topic OverlapThis also gives us our first way of not just correlating articles for similarity but also using that content based” similarity for recommendations.If we draw out the pattern of (a1:Article)-[:HAS_TOPIC]->(topic)<-[:HAS_TOPIC]-(a2:Article)that represents our shared topics between articles.We can use that to either find clusters visually like seen before or to compute how many topics a pair of articles shares.We just prefix the pattern with MATCH and then count the number of topics per pair of articles.MATCH (a1:Article)-[:HAS_TOPIC]->(topic)<-[:HAS_TOPIC]-(a2:Article)// exclude opposite pairWHERE id(a1)<id(a2)RETURN a1.title, a2.title, count(topic) as overlapORDER BY overlap DESC limit 5╒══════════════════════╤══════════════════════╤═════════╕│ title1               │ title2               │ overlap │╞══════════════════════╪══════════════════════╪═════════╡│ A New Wave of Covid- │ A New Covid Mystery  │4        │├──────────────────────┼──────────────────────┼─────────┤│ Russia Asked China f │ U.S. Officials Say S │3        │├──────────────────────┼──────────────────────┼─────────┤│ Satellite images sho │ A makeup artist reco │2        │├──────────────────────┼──────────────────────┼─────────┤│ Satellite images sho │ Russian soldiers ope │2        │├──────────────────────┼──────────────────────┼─────────┤│ New York Judge Dies  │ Sarah Lawrence Cult  │2        │└──────────────────────┴──────────────────────┴─────────┘If you anchor the starting article — e.g. as the one currently being read — you can recommend (similar) articles based on that topic overlap.If you want to see which topics the articles overlap on, you can collect the names (aggregation function) into a list too.MATCH (a1:Article)-[:HAS_TOPIC]->(topic)<-[:HAS_TOPIC]-(a2:Article)// exclude opposite pairWHERE id(a1)<id(a2)RETURN a1.title, a2.title, count(topic) as score, collect(topic.name) as topicsORDER BY overlap DESC limit 5╒══════════════════╤══════════════════╤═════════╤══════════════════╕│ title1           │ title2           │ overlap │ topics           │╞══════════════════╪══════════════════╪═════════╪══════════════════╡│ A New Wave of Cov│ A New Covid Myste│4        │[ Coronavirus (201││id-               │ry                │         │9-nCoV) , Disease ││                  │                  │         │Rates , Tests (Med││                  │                  │         │ical) , Coronaviru││                  │                  │         │s Omicron Variant ││                  │                  │         │]                 │├──────────────────┼──────────────────┼─────────┼──────────────────┤│ Russia Asked Chin│ U.S. Officials Sa│3        │[ Embargoes and Sa││a f               │y S               │         │nctions , Russian ││                  │                  │         │Invasion of Ukrain││                  │                  │         │e (2022) , United ││                  │                  │         │States Internation││                  │                  │         │al Relations ]    │Other MetadataSimilarly, the other metadata turns into Geo, Person, Organization nodes with their appropriate relationships.  call apoc.load.json( https://api.nytimes.com/svc/mostpopular/v2/viewed/7.json?api-key= +$key) yield valueunwind value.results as article  MERGE (a:Article {url: article.url})  ON CREATE SET a.title     = article.title,        a.abstract  = article.abstract,        a.published = datetime(article.published_date),        a.byline    = article.byline,        a.id = article.id,        a.source = article.source  FOREACH (desc IN article.des_facet |    MERGE (d:Topic {name: desc})    MERGE (a)-[:HAS_TOPIC]->(d)  )  FOREACH (per IN article.per_facet |    MERGE (p:Person {name: per})    MERGE (a)-[:ABOUT_PERSON]->(p)  )  FOREACH (org IN article.org_facet |    MERGE (o:Organization {name: org})    MERGE (a)-[:ABOUT_ORGANIZATION]->(o)  )  FOREACH (geo IN article.geo_facet |    MERGE (g:Geo {name: geo})    MERGE (a)-[:ABOUT_GEO]->(g)  )With MERGE our statement becomes idempotent — we can run it as often as we want and only new data gets added.Our graph is also now more colorful with the new metadata nodes leading to new correlations.Article MetadataAgain, we can use this to improve our recommendations.You can think of the overlap count as a score that can be weighted but also computed per metadata item, depending on reader preferences or importance.Here is an example:MATCH (a1:Article)-[:HAS_TOPIC]->(topic)<-[:HAS_TOPIC]-(a2:Article)WHERE id(a1)<id(a2)// score for topic overlapWITH a1,a2, count(topic) as topicScore// score for geo overlapOPTIONAL MATCH (a1:Article)-[:ABOUT_GEO]->(geo)<-[:ABOUT_GEO]-(a2:Article)WITH a1,a2, topicScore, count(geo) as geoScore// score for people overlapOPTIONAL MATCH (a1:Article)-[:ABOUT_PERSON]->(person)<-[:ABOUT_PERSON]-(a2:Article)WITH a1,a2, topicScore, geoScore, count(person) as personScore// compute total score with weightsRETURN a1.title, a2.title, topicScore*5+geoScore*2+personScore*1.5 as scoreORDER BY score DESC limit 5╒══════════════════════╤══════════════════════╤═══════╕│ title1               │ title2               │ score │╞══════════════════════╪══════════════════════╪═══════╡│ A New Wave of Covid- │ A New Covid Mystery  │22.0   │├──────────────────────┼──────────────────────┼───────┤│ Russia Asked China f │ U.S. Officials Say S │18.5   │├──────────────────────┼──────────────────────┼───────┤│ Satellite images sho │ A makeup artist reco │16.0   │├──────────────────────┼──────────────────────┼───────┤│ Russian Blunders in  │ They Died by a Bridg │16.0   │├──────────────────────┼──────────────────────┼───────┤│ Satellite images sho │ They Died by a Bridg │16.0   │└──────────────────────┴──────────────────────┴───────┘Imagine again that article 1 is tied to an article id or URL MATCH (a1:Article {id:$articleId})the user is currently reading. The list of articles returned would be recommendations.Photos and MediaThe Photos are in a bit of a nested structure, and different resolutions of the same picture, so we can pull them in by iterating over the media list and grabbing the caption and the third image URL.In Neo4j Browser’s property pane for the nodes you can click the URLs for the pictures to see them.call apoc.load.json( https://api.nytimes.com/svc/mostpopular/v2/viewed/30.json?api-key= +$key) yield valueunwind value.results as article  MERGE (a:Article {url: article.url})  ON CREATE SET a.title     = article.title,        a.abstract  = article.abstract,        a.published = datetime(article.published_date),        a.byline    = article.byline,        a.id = article.id,        a.source = article.source...  FOREACH (media in article.media |    MERGE (p:Photo {url: media.`media-metadata`[2].url})      ON CREATE SET p.caption = media.caption    MERGE (a)-[:HAS_PHOTO]->(p)  )Those photos could now be further analyzed with image recognition APIs to add more metadata.AuthorsThe authors are unfortunately not available as metadata, but just in the byline attribute, which is a string like,  By Malachy Browne, David Botti and Haley Willis `.Fortunately in Cypher we have a number of string operations. We skip the first three letters, replace and with comma, and split the string by comma to get a list of authors.Then we create a node for each of them and connect the articles. This doesn’t save us from duplicate Author names, but it’s better than not having them at all.We can either do that as part of the import or as a post-processing step, which is shown here.MATCH (a:Article)// string operation to turn string into array of namesWITH a, split(replace(substring(a.byline, 3),   and  ,  , ),  , ) AS authorsUNWIND authors AS author// uniquely create author nodeMERGE (auth:Author {name: trim(author)})MERGE (a)-[:BYLINE]->(auth)To import more data we can change the time range from 7 to 30 days to get all the popular articles for the last month.GeodistanceWe use another APOC procedure call for geocoding (lat, lon) of our Location metadata. This uses public geocoding APIs to resolve names to locations (openstreetmap).MATCH (g:Geo)CALL apoc.spatial.geocodeOnce(g.name) YIELD locationSET g.location = point({latitude: location.latitude, longitude: location.longitude})Then we can determine how close they are to each other, or how close the reported articles are to our current location, which could also be used for ranking them for a viewer.Here we use Berlin’s location as a starting point to compute distances in kilometers.WITH point({latitude:52.5065133, longitude:13.1445557}) as berlinMATCH (g:Geo)RETURN g.name, round(point.distance(berlin, g.location)/1000) as distORDER BY dist ASC╒═══════════════════════════════╤═══════╕│ g.name                        │ dist  │╞═══════════════════════════════╪═══════╡│ Italy                         │1099.0 │├───────────────────────────────┼───────┤│ Ukraine                       │1310.0 │├───────────────────────────────┼───────┤│ Russia                        │4689.0 │├───────────────────────────────┼───────┤│ China                         │7120.0 │Next StepsOther extension points for the data would be additional entity extraction from the abstract, or pulling the actual article text and analyzing that.Furthermore, we could add comments and data from other publications and sources like opencorporates, wikidata, gdelt, and more.If you follow Will’s repository there are three other angles:He’s building a News GraphQL API on top of the dataUsing Cloudflare Edge Workers to serve location aware dataExtending GitHub’s FlatData approach with a FlatGraph that gathers source data from websites and APIs and writes them to Neo4j using GitHub ActionsHope you enjoyed this expedition in the world of news metadata and it gives you a few ideas on how to start your own Graph Journey on AuraDB Free.This is already week 24 of our exploration. If you want to see the other videos and datasets check out the page for our livestream and our GitHub repository.Discovering AuraDB Free with Fun DatasetsWe run these  Discover AuraDB Free  live-streams alternating every Monday alternating at 9am UTC (11am CEST, 2:30 IST…neo4j.comGitHub - neo4j-examples/discoveraurafree: A repo with examples from the Discover Neo4j AuraDB Free…A repo with cypher and examples from the Discover Neo4j Aura Free Twitch stream. All streams will roughly follow the…github.com;Apr 11, 2022;[]
https://medium.com/neo4j/a-library-for-temporary-neo4j-databases-36d69e1fd96e;CristinaFollowJul 16, 2021·4 min readA Library for Temporary Neo4j DatabasesGetting started with neo4j-temp-db. Useful, session-based, self-cleaning, temporary Neo4j databases using the multi-db featureSand Castle Photo by Jaime Spaniol on UnsplashIntroductionThe library was originally created to be used with the Graph Examples Portal, which initially used in-memory Neo4j database library that had been around for seven years. But that is no longer feasible with Neo4j 4.x and also not scalable enough for our needs.So now the @neo4j-labs/temp-db library allows you to create temporary databases on the fly.With the advent of multi-database functionality, being able to quickly create and expire Neo4j databases ad hoc has become possible, and functionalities ranging from lightweight console-based applications to more efficient unit tests are feasible.This post is a walkthrough of a minimal webapp leveraging the @neo4j-labs/temp-dbs JavaScript library.neo4j-contrib/neo4j-temp-dbThis library was created initialy to be used on https://portal.graphgist.org/ where users are able to run custom cypher…github.comGetting Started ExampleWith your Neo4j database running, create a new directory — for example, neo4j-temp-db-demo-app, then cd into it with cd neo4j-temp-db-demo-app.Run npm init to get started. Picking all the defaults is OK.Add and save some necessary dependencies:npm install express dotenv neo4j-driver ejs — saveWe also want to install the babel dependencies to be able to use JavaScript ES6:npm install @babel/cli @babel/node @babel/cli @babel/core @babel/preset-env nodemon — save-devNext, install the @neo4j-labs/temp-dbs library:npm install @neo4j-labs/temp-dbs — saveOpen the directory in your favorite text editor. Create the following three files to get started:.babelrc with your babel presets:{ presets”: \[@babel/preset-env”\] }.env with your environment variables with the connection information to the Neo4j database that will host your temporary multi-databases:NEO4J_URI=”bolt://localhost:7687 NEO4J_USER=”neo4j”NEO4J_PASSWORD=”123456 index.js with the index of a simple Express app:import express from express”import dotenv from dotenv”dotenv.config()const app = express()const port = process.env.PORT || 3000app.listen({ port }, () => {    console.log(`Ready at http://localhost:${port}`)})In index.js, instantiate @neo4j-labs/temp-dbs using the environment variables sent in .env:Add a route that passes the tempDBName to index.js:Create the views/index.ejs so we have a way to interact with our databases. It’s just a form with two fields — one read-only text field, tempDbName and one in which to enter Cypher queries, cypher.Try refreshing the page — each refresh creates a new temp db.Try running a Cypher query — the tempdb stays the same.Cleaning upCreate a file called clean-expired-dbs.js at the root of the project. You can specify at what age in seconds to clean up your databases:Add it to the scripts section of the package.json file to call it from the terminal:clean-expired-dbs”: babel-node ./clean-expired-dbs.js”,It will return a number of databases found and whether or not they have expired:Check out the full code of the example app here.Next Steps: Temporary Databases for Unit TestsConsider using the @neo4j-labs/temp-dbs library to ease your unit testing. Check out a basic example in this fork of the JavaScript movies example.About the AuthorsCristina and Alisson work at The SilverLogic, a software development company based in Boca Raton.Alisson is a software engineer at The SilverLogic. Passionate about plants, he is the driving force behind Que Planta, a GraphQL-based social network for plants.Resourcesneo4j-contrib/neo4j-temp-dbThis library was created initialy to be used on https://portal.graphgist.org/ where users are able to run custom cypher…github.comsilverlogic/neo4j-temp-db-demo-appContribute to silverlogic/neo4j-temp-db-demo-app development by creating an account on GitHub.github.com;Jul 16, 2021;[]
https://medium.com/neo4j/how-i-built-the-neosemantics-n10s-graph-app-b7ada7b8d008;Adam CowleyFollowMay 15, 2020·8 min readHow I built… the Neosemantics (n10s) Graph AppExplaining how I put together a Graph App in just a few days using open-source components.In this case, I was looking to build a Graph App to help educate users about Neosemantics, or n10s. Neosemantics is a Neo4j Labs project that allows you to store RDF/Linked Data in Neo4j and then export the data back out in a lossless way.The idea behind the Graph App was to provide an educational tool that would allow you to preview how your triples would look in Neo4j without you having to know much about the underlying technology and also to teach more experienced users how the procedure API of the library actually works.If you want to install the app into Neo4j Desktop test it while we look at the code, just open the Graph App Gallery” or go to install.graphapp.io and click the install button.Graph App Gallery with the neosemantics appIf all you care about is code, skip to the TLDR at the bottom of the post…Yeah, but what are Graph Apps?Graph Apps are Single Page Applications (built with HTML/Javascript and optionally the front-end framework of your choice) that are installed to and hosted by Neo4j Desktop.The Graph App Gallery has a comprehensive list of apps built by Neo4j, our Partners and even Community Members. Two Graph Apps that I use on a regular basis are Halin, a monitoring & management tool and the Query Log Analyser which gives you a more user friendly view on your query logs.Neo4j Desktop provides you with an API that gives you access to the User’s Projects and Graphs, along with Files and Activation Keys. From here, you can run a reduce/filter on the projects and graphs to find the active graph.const context = await neo4jDesktopApi.getContext()const activeGraph = context.projects    .reduce((acc, project) => acc.concat(project.graphs), [])    .filter(graph => graph.status === ACTIVE)From there you can find the server URL and credentials and create an instance of the driver and run cypher statements against the graph.const { url, username, password } = activeGraph.connection.configuration.protocols.bolt// Create a driver instanceconst driver = new neo4j.driver(    url, // Neo4j URL - bolt://…    neo4j.auth.basic(username, password) // Auth)Making use of Open Source ComponentsAs I touched upon before, as long as your code boils down to a html file and some javascript files the choice of framework is moot. Almost everything built by the Neo4j engineering team is built using React but I’m personally more of a Vue.js kind of guy.Vue.js comes with a CLI tool that allows you create projects from pre-made templates, so with a single command you can generate the project structure:vue create neosemanticsVue & Neo4jA while back while I was building these apps for clients as my day job, I put together a Vue plugin to simplify interactions with Neo4j. The vue-neo4j plugin adds a $neo4j object to all components which contains helper functions for connecting to the database and running queries.// Something.vueexport default {    name: something,    // ...    data: () => ({        driver: false,        protocol: bolt,        host: localhost,        port: 7687,        username: neo4j,        password: trustno1    }),    methods: {        connect() {            this.$neo4j.connect(                this.protocol,                 this.host,                 this.port,                 this.username,                 this.password            )        }    },     computed: {        driver() {            return this.$neo4j.getDriver()        }    },}If the plugin detects the Neo4j Desktop API’s, it will also provide you with a driver already instantiated with the connection details of the active graph.One useful component that this plugin includes is the vue-neo4j-connect component. Most people will need a login form for their app, so this component provides you with everything you need.Opening the project in Neo4j Desktop will show a list of Projects and Graphs in Neo4j Desktop but will fall back to a generic login form if opened in a browser.When the app is opened in Neo4j Desktop, the user will be given a list of projects and graphs to connect to along with a button to connect to the current active graph. If the user clicks Connect to another graph or the app is opened in a browser a generic login form where the user fills out the Neo4j credentials manually.Look and FeelAnyone with a keen eye will also notice that a whole host of Neo4j products including Neo4j Desktop, Neo4j Browser and Neo4j Bloom use some sort of variation on top of Semantic UI so to make the n10s Graph App look somewhat similar, it would make sense to use that too.There is also a Vue plugin for Semantic UI called semantic-ui-vue. Each component is prefixed with sui- and with a quick search of the documentation you can quickly find what you’re looking for. Here is an abridged version of the config form:<sui-form>    <sui-form-field>        <label>handleVocabUris</label>        <sui-dropdown            fluid            :options= handleVocabUriOptions             placeholder= handleVocabUris             search            selection            v-model= handleVocabUris         />    </sui-form-field>    <sui-button primary         :loading= loading          @click.prevent= runQuery     >        {{ buttonText }}    </sui-button></sui-form>This also sped up the development process by providing a set of pre-made components for everything from basic layout to form elements. This also had the benefit of taking most of the design decisions out of my hands.So with @vue/cli and two open source components, I was able to build a large part of the Graph App relatively quickly. The look and feel was taken care of by Semantic UI, and the interaction with the Neo4j Database would go through vue-neo4j.Generic ComponentsMany pages in the UI also made use of generic components. For example, all of the Cypher result tables use the same component. The component simply takes the result provided from the neo4j-driver and displays the results within an <sui-table> component using a v-for loop.<template>    <div class= n10s-result >        <sui-table compact striped v-if= !noResults && isTable >            <sui-table-header>                <sui-table-header-cell v-for= key in headers                                                :key= key >                {{ key }}</sui-table-header-cell>            </sui-table-header>            <sui-table-body>                <sui-table-row v-for= (record, index) in                                        result.records  :key= index >                    <sui-table-cell v-for= key in headers                                             :key= index+key >                        {{ record.get(key) }}                    </sui-table-cell>                </sui-table-row>            </sui-table-body>        </sui-table>        <graph-result v-if= !noResults && !isTable             :result= result         />        <p class= ui tiny  v-if= summary  v-html= summary  />    </div></template>The component itself takes the result as a prop and then makes use of the keys property on the first row to create the header row as a computed property, then iterates through result.records to display the actual results of the query.export default {    name: n10s-result,    props: {        result: Object,        displayAs: {            type: String,            default: table,        },    },    components: {        GraphResult,    },    computed: {        isTable() {            return this.displayAs === table        },        noResults() {            return this.result && !this.result.records.length        },        headers() {            return this.result.records[0].keys        },        summary() {            const consumed =                 this.result.summary.resultConsumedAfter.toNumber()            const available =                            this.result.summary.resultAvailableAfter.toNumber()            return `                Started streaming ${this.result.records.length}                 record${this.result.records.length === 1 ?  : s}                after ${available}ms                 and completed after ${consumed}ms            `        },    },}That component is then capable of handling anything that is thrown at it…Generic components are a great way to speed up developmentWhere a Forced Graph layout is required, I switched out the Result component for another component which wraps the Vis.js network library.Many of the components also run a Cypher statement against the database. Rather than duplicating this code many times, you can create a Mixin. Mixins allow you to define functionality that can be inherited by importing the object and listing it to the mixins array on the component.export default {    data: () => ({        loading: false, error: false, result: false, tab: 0    }),    computed: {        cypher: () => MATCH (n) RETURN count(n) AS count,        params: () => {},    },    methods: {        runQuery() {            this.loading = true            this.error = false            this.$neo4j.run(this.cypher, this.params)                .then(res => this.result = res)                .catch(e => this.error = e)                .finally(() => {                    this.tab = 1                    this.loading = false                })        },    },}In this case, I’ve defined cypher as a computed variable but defining this within a component that uses this mixin that contains its own cypher variable will overwrite this. That way, if I forget to overwrite the variable I won’t get any errors in the console.import CypherComponent from ./mixins/CypherComponentexport default {     name: import,     mixins: [ CypherComponent ],     // ...     computed() {        cypher() {            return `CALL n10s.rdf.import.fetch(                 ${this.url} ,                 ${this.format} ,                ${JSON.stringify(this.parameters)}            )`        },    },}The TLDR ProcessUse @vue/cli to generate a new project.vue create neosemanticsInstall vue-router for navigation, and vue-neo4j and semantic-ui-vue as dependencies.npm install --save vue-router vue-neo4j semantic-ui-vueImport and register the plugins in main.js:import Vue from vueimport VueNeo4j from vue-neo4jimport SuiVue from semantic-ui-vueimport App from ./App.vueimport router from ./routerimport semantic-ui-css/semantic.min.css// Tell Vue to use these pluginsVue.use(VueNeo4j)Vue.use(SuiVue)new Vue({  router,  render: h => h(App),}).$mount(#app)Setup each of the individual routes in Vue-Router:import Vue from vueimport VueRouter from vue-routerimport Home from ./routes/Homeimport Config from ./routes/Configimport Import from ./routes/Importimport Preview from ./routes/Previewimport Delete from ./routes/Deleteimport Export from ./routes/ExportVue.use(VueRouter)export default new VueRouter({    routes: [        { name: config,  path: /config, component: Config, },        { name: import,  path: /import, component: Import, },        { name: preview,  path: /preview, component: Preview, },        { name: delete,  path: /delete, component: Delete, },        { name: export,  path: /export, component: Export, },        // Redirect to Home        { name: home, path: /, component: Home, },        { path: *, redirect: /, },    ]})Create a vue mixin to supply methods to run queries against the database:export default {    data: () => ({        loading: false, error: false, result: false, tab: 0,    }),    computed: {        cypher: () => MATCH (n) RETURN count(n) AS count,        params: () => {},    },    methods: {        runQuery() {            this.loading = true            this.error = false            this.$neo4j.run(this.cypher, this.params)                .then(res => this.result = res)                .catch(e => this.error = e)                .finally(() => {                    this.tab = 1                    this.loading = false                })        },    },}Then, piece everything together in a route component, using the runQuery method from the mixin to execute the computed cypher variable.import CypherComponent from ./CypherComponentexport default {    name: config,    mixins: [ CypherComponent, ],    // ...    computed: {        cypher() {            let params = []            const keys = [handleVocabUris, handleMultival,                           handleRDFTypes]            keys.map(key => {                if ( this[ key ] !== undefined && this[key] !==  )                    params.push(`${key}: ${this[key]}`)            })            if ( params.length ) {                params = `{\n\t${params.join(,\n\t)}\n}`            }            return `CALL n10s.graphconfig.init(${params})`        },    },}You can view the entire source code for the n10s app on Github at https://github.com/neo4j-apps/n10sA confessionAs a Developer Experience Engineer at Neo4j, it is my job to build apps like these. But I’m also here to make your life easier as a developer. If there is anything that I can do to improve your experience while developing Graph Apps whether that be improved documentation, better tutorials and guides or frameworks and reusable components for your favourite language/framework, I’m all ears.Feel free to reach out to me on Twitter or post a message on the Neo4j Community site.;May 15, 2020;[]
https://medium.com/neo4j/graphs-on-tap-with-neo4j-relate-d9833ba64529;Andreas KolleggerFollowNov 12, 2020·3 min readGraphs on Tap with Neo4j RelateNeo4j Relate delivers graphs on tap. From the CLI, from your app, or from a local server, you can provision Neo4j databases whenever you’d like to enjoy the clean, refreshing goodness of graphs.Extracted from Neo4j DesktopThe best frameworks are in my opinion extracted, not envisioned. — David Heinemeier HanssonNeo4j Desktop is a native (electron) application which packages up everything you need for working with the Neo4j DBMS.Internally, there is a GraphQL API through which tools may discover available databases, provision new ones and negotiate authentication.Neo4j Desktop is extensible. New tools can be added, and you can write your own. But it feels like a walled garden. Peering over the hedge, wouldn’t it be nice to easily manage Neo4j from the command line, or from VS Code, or even from within your app?Neo4j Relate extracts the core operational library of Neo4j Desktop then packages that intoa CLI tool,a web server,or a generic Electron app.Let’s take the CLI for a spin.Neo4j at your commandNow witness the firepower of this fully armed and operational battle station CLI — The EmperorThe @relate/cli package installs the relate command line tool. There is a PR which will make the CLI available without requiring node.js.For now, try:npm install -g @relate/cliNext, set up a provisioning environment named graphs-on-taps”:relate env:init --name=graphs-on-tap --type=LOCALAccept the default settings when prompted.You’ll need to provide this semi-secret access code r31473:Enter the access code you received from applying at https://neo4j.relate.by/invite: r31473 ✔ Do you need to enable authentication? (y/N) · false ✔ Do you need to restrict access to the GraphQL API methods? (y/N) · false ✔ Are HTTP consumers required to have an API key? (y/N) · false Creating environment... doneThe authentication mentioned above is for the GraphQL API, which we’ll explore in a later post.Now install a Neo4j DBMS named hello” into our provisioning environment. If you drop 4.1.3 from the end, you can pick which version to install:relate dbms:install -e graphs-on-tap --name hello 4.1.3Enter new passphrase: will set the password for the admin user named  neo4j :✔ Enter new passphrase · DOWNLOAD PROGRESS [████████████████████████████████████████] 100% extracting neo4j... done [b92ada41] helloFinally, start the DBMS named hello” within the graphs-on-tap” environment:relate dbms:start -e graphs-on-tap helloThe Neo4j DBMS should start up, then you’ll be able to browse to http://localhost:7474 to use Neo4j Browser. Use the admin user named neo4j and the password you set above.Convenient, right?Try relate --help to see the other available commands and options. For instance, relate env:use to set the current” environment, saving you a little typing.Next stepsNeo4j Relate is in alpha, evolving quickly with more capabilities and integrations. Stay tuned and reach out to me with any ideas or questions.In later posts, I’ll explore:e2e testing using relate to do setup & teardownintegrating relate into development project workflowshosting Browser, Bloom and other tools without needing Neo4j Desktopspinning up a server to provision DBMSes on demandwriting nest.js modules to use relate as an application serverOriginally published at https://dev.to on November 12, 2020.;Nov 12, 2020;[]
https://medium.com/neo4j/learn-geography-using-neo4j-6bc1314f57ba;Jimmy CrequerFollowNov 30, 2019·6 min readLearn geography using Neo4jIn a previous story I implemented a small app to learn Japanese characters using Neo4j. Lately, I spent time trying to remember all countries in the world (I guess I have too much free time…), and I figured out I could use a graph to help me in this journey too.In this post, I will build a small graph of European countries and write a short CLI application to interact with the graph and help learning those countries.The graphBuild the graphI created a countries.json file from Wikipedia’s data, containing all European countries. The file available here is structured as follows :[  {     name :  France ,     population : 67348000,     area : 643427,     capital :  Paris ,     neighbors : [ Andorra ,  Belgium , ...,  Switzerland ]  },  ...]We will create the following entities :Country nodes, with a name, a population and an areaCity nodes, with a nameRelationships between Country and City nodes to represent the capitalsRelationships between 2 Country nodes when they have a common borderNeo4j’s APOC library provides a very convenient way to import JSON files. We only need a few Cypher lines to build our graph :WITH  https://gist.githubusercontent.com/jimmycrequer/7aa867900d0cf0b9588d4354f09cb286/raw/countries.json  AS urlCALL apoc.load.json(url) YIELD value AS vMERGE (c:Country {name: v.name})SET c.population = v.population, c.area = v.areaCREATE (capital:City {name: v.capital})CREATE (c)<-[:IS_CAPITAL_OF]-(capital)FOREACH (n IN v.neighbors |  MERGE (neighbor:Country {name: n})  MERGE (c)-[:IS_NEIGHBOR_OF]-(neighbor))RETURN *Our graphExplore the graphLet’s start with the top 10 biggest countries in Europe.MATCH (c:Country)RETURN c.name AS country, apoc.number.format(c.area) AS areaORDER BY c.area DESCLIMIT 10Note : apoc.number.format()” returns a String, and to get the correct sorting we need to ORDER BY” the numerical value.To be honest, I would have thought that Ukraine was bigger than France. Moreover, it seems my data counted Greenland as well which explains why Denmark appears in the top 3.https://en.wikipedia.org/wiki/DenmarkWe can also calculate the density of population for each country.MATCH (c:Country)RETURN c.name AS name,        apoc.number.format(c.area) AS area,       apoc.number.format(c.population) AS population,       c.population / c.area AS densityORDER BY density ASCIt is interesting to note the presence of Scandinavia and especially the Baltic states here, despite being relatively small states.Let’s now have a look at the relationships between countries.MATCH (c:Country)-[:IS_NEIGHBOR_OF]-(c2:Country)WITH c, collect(c2.name) AS neighborsRETURN c.name, neighborsORDER BY size(neighbors) DESCNo really big surprise here. Germany and France have a lot of common borders with small countries (Belgium, Switzerland, Luxembourg) and are located are the center of Europe. Notice that this dataset doesn’t include Asian countries, so Russia and other countries like Kazakhstan do have more bordering countries.You can also render a map” of Europe just using the neighborhood relationships and the force-layout.MATCH (c1:Country)-[nb:IS_NEIGHBOR_OF]-(c2:Country)RETURN c1,nb,c2Map” of EuropeLastly, we also make use of Neo4j’s shortestPath()” function to know how many countries need to be crossed to reach 2 specified countries. Example here with France and Greece.MATCH (france:Country {name:  France }),       (greece:Country {name:  Greece }),      p = shortestPath((france)-[*]-(greece))RETURN pNow that the graph is ready, let’s create a small CLI app to play with it!Build the appI decided to create a small CLI app with Node.js since Neo4j provides a driver for JavaScript.Main functionLet’s start with the main function.First, I connect to the Neo4j instance and create a new session. Then I create the main loop of the application, which redirects the user to which game they choose to play to.Let’s dive into other functions.GuessCountryFromCapital functionThe code is pretty straightforward. I use the following Cypher query to return a pair of Country and City at random.MATCH p = (:Country)<-[:IS_CAPITAL_OF]-(:City)RETURN apoc.coll.randomItem(collect(p)) AS pThen the user is prompted the question. Finally we display a message whether his answer was correct or wrong, using simple text coloration.console.log(\x1b[32m%s\x1b[0m, Correct!)This line will print Correct!” in green.console.log(\x1b[33m%s\x1b[0m, `Wrong! The answer is ${countryName}.`)This line will print the message in yellow.GuessCountryFromNeighbors functionSimilarly to the previous function, I implemented another function which will ask, for a list of countries, the country which has borders with all of them.This time, the Cypher query will fetch all the countries and their neighbors, then I randomly pick one from the records” property in JavaScript.And that’s it. Run the app and you can start playing!Sample executionConclusionIn this post, I was able to build a small CLI application using Neo4j as a database to help learning European countries.I deeply think graph databases are useful for learning because we tend to remember more easily new knowledge by forming associations with what we already know. The very efficient method of loci, which is about remembering an ordered list of things by visualizing them to familiar locations, demonstrate this. You can associate each item of your list to :A room of your houseA shop in your preferred streetA street in your childhood cityA station of your commutation trainEvery place that is familiar to you will help you remember any thing. It’s all about connecting things together!To remember where Albania is, I could learn that its coordinates are 41.1533° N, 20.1683° E, but it would be way more efficient and easy to remember if I just learn that it is the most left country on the North of Greece. Of course, to make it work I need to know where Greece is, but once we get a solid common knowledge, it is really easy to connect additional new things to it!While this post is still trivial, by taking advantage of Neo4j’s nature it is really easy to add more nodes from additional datasources to densify this graph and extend the learning potential. In a next post, I will try to add some additional datasources and provide new questions like :Seas : Which countries have a border with the Mediterranean Sea?”, Which European countries have border with no seas?”, …Mountains : Which countries are the Alps in?”, …Rivers : Which rivers are traversing through France?”, …Happy learning!;Nov 30, 2019;[]
https://medium.com/neo4j/coloring-a-sudoku-graph-with-neo4j-f86c891f6879;Nathan SmithFollowJan 1, 2020·3 min readColoring a Sudoku graph with Neo4jI was happy to see that a recent release of the Neo4j graph algorithms contains the K-1 Coloring algorithm. This algorithm tries to assign colors to the nodes of a graph in such a way that adjacent nodes are different colors.One of my favorite puzzles, Sudoku, can be represented as a graph coloring problem. If you’re not familiar with the puzzle, you are given a 9x9 grid with some digits filled in. You complete the puzzle by filling in the blank squares so that each row, and column, as well as the 3x3 blocks indicated by the heavy lines, contain each digit from 1 to 9.You can think of a Sudoku grid as a graph with nodes representing the squares. Edges connect each node with the other nodes on the same row, column, and block. David Eppstein created this diagram of a 4x4 Sudoku graph.The version of the K-1 Coloring algorithm that was just released doesn’t support adding colors to a partially colored graph. That means it’s not ideal for solving Sudoku puzzles. However, we can still try it out on a blank Sudoku graph to get familiar with the algorithm. In my next blog post, I’ll demonstrate a way to write a Neo4j-based Sudoku solver.To follow along with my code at home, you will need a Neo4j database with a recent release of graph algorithms installed. At the time of this blog post, the latest sandbox instances didn’t include the new coloring algorithm. You will want to download the latest Neo4j desktop version to run this code, selecting/upgrading to the 3.5.14 version of the database.First, create nodes for our grid. We’ll use a parameter to describe the size of our parameter grid. I’ll demonstrate a 9x9 grid, and you can try other possibilities like 4x4, 16x16.:params {gridSize:9}Now we execute a query to create all the cell nodes with their row, column, and block properties.WITH toInteger(sqrt($gridSize)) AS blockSizeUNWIND RANGE(1,$gridSize) AS rowUNWIND RANGE(1,$gridSize) AS columnCREATE (m:Cell {row:row, column:column})SET m.block = blockSize * ((m.row-1)/blockSize) + (m.column-1)/blockSize + 1Let’s add edges between adjacent cells.MATCH (c1:Cell), (c2:Cell)WHERE (c1.row = c2.row OR c1.column = c2.column OR c1.block = c2.block)AND id(c1) < id(c2)MERGE (c1)-[:IS_ADJACENT_TO]->(c2)Now we can run the coloring algorithm and assign a number” property to each of the nodes.CALL algo.beta.k1coloring( Cell ,  IS_ADJACENT_TO , {write:true, writeProperty: number , iterations:15}) YIELD didConverge, ranIterations, colorCount, computeMillis, nodesRETURN *╒════════════╤═══════════════╤═════════════╤═══════╤═══════════════╕│ colorCount │ computeMillis │ didConverge │ nodes │ ranIterations │╞════════════╪═══════════════╪═════════════╪═══════╪═══════════════╡│12          │2              │true         │81     │13             │└────────────┴───────────────┴─────────────┴───────┴───────────────┘The documentation explains that since graph coloring is an NP-complete problem, the algorithm finds a good coloring, but maybe not the best possible.It is neither guaranteed that the result is an optimal solution, using as few colors as theoretically possible, nor does it always produce a correct result where no two neighboring nodes have different colors.The algorithm used 12 colors, when we know that 9 is possible. It’s still not a bad approximation in 2 milliseconds! How did it do at avoiding adjacent cells of the same color?MATCH (c1:Cell)-[:IS_ADJACENT_TO]->(c2:Cell)RETURN SUM(case when c1.number = c2.number then 1 else 0 end) AS conflicts,COUNT(*) AS edges,SUM(case when c1.number = c2.number then 1.0 else 0 end)/count(*) AS pctConflicts╒═══════════╤═══════╤═════════════════════╕│ conflicts │ edges │ pctConflicts        │╞═══════════╪═══════╪═════════════════════╡│4          │810    │0.0049382716049382715│└───────────┴───────┴─────────────────────┘The algorithm avoided conflicts on 99.5% of the edges.I tried the algorithm on a 4x4 board, and it avoided conflicts on 100% of the edges using 5 colors. On a 25x25 board, It avoided conflicts on 99% of the edges using 34 colors.I hope playing with Sudoku graphs gives you a fun taste of the new K1 Coloring algorithm. Check out my next post to see a Sudoku solving algorithm built in Neo4j.;Jan 1, 2020;[]
https://medium.com/neo4j/building-a-heavyweight-boxing-graph-7306f7d2008d;Yuvan HiraniFollowAug 17, 2018·2 min readBuilding a Heavyweight Boxing GraphFor the past week I have been doing a work placement at Neo4j in London. This week I have been doing Cypher, Python, and HTML programming.I’m a big fan of heavyweight boxing so I wanted to make a graph on this. I created CSV files of some boxers and the fights between them. You can see the CSV files below:Heavyweight Boxers — contains personal information about fighters and the belts they holdHeavyweight Fights — contains winner, loser, and YouTube highlights linkThis is how I loaded the CSV files into Neo4j:load csv with headers from  https://raw.githubusercontent.com/WATFORD4LIFE/boxing_rec2.0/master/boxer.csv  AS rowmerge(boxer:Boxer{name:row.boxer})set boxer += rowset boxer.dob = datetime(row.dob)LOAD CSV WITH HEADERS FROM  https://raw.githubusercontent.com/WATFORD4LIFE/boxing_rec2.0/master/fights.csv  AS rowMATCH (winner:Boxer {name: row.winner})MATCH (loser:Boxer {name: row.loser})MERGE (winner)-[beat:BEAT]->(loser)SET beat.highlights = row.highlightsNeo4j Browser visualisation of the whole graphAfter that I wrote some queries on the Neo4j browser against the boxing graph.This query finds Wladimir Klitschko:MATCH (boxer:Boxer {name:”Wladimir Klitschko”})RETURN boxerAnd this one finds the fights that Anthony Joshua participated in:MATCH (aj:Boxer {name: Anthony Joshua})-[:BEAT]-(opponent)RETURN aj.name, opponent.name, aj.dob, opponent.dobAnthony Joshua’s fightsI also created a Python application that queries this database. Below is a screenshot of all the fights:The code for the application is in this GitHub repository.SummaryThis week was very fun and I learnt a lot. One of the challenges was the Python aspect but after a few day it was a lot better .;Aug 17, 2018;[]
https://medium.com/neo4j/neo4j-treasure-map-where-to-find-all-of-the-4-0-resources-you-need-948cf71004f9;Jennifer ReifFollowFeb 11, 2020·5 min readNeo4j Treasure Map — Where to find all of the 4.0 resources you need*Last updated Feb 19, 2019With the recent release of Neo4j version 4.0, finding the best and most relevant information you need on the new and updated features might also be a learning process. We have a variety of channels to share knowledge and news, and knowing whether to look on YouTube, the developer blog, the documentation, or someplace else for specific help can be challenging. This article’s goal is to share all of the great content that is currently available (or soon-to-be coming) for the latest and greatest changes in Neo4j.While this list is constantly growing, we will mention everything that’s been gathered so far and do our best to keep things updated as new content is added in the near future. Let’s get started!DocumentationProduct documentation is always a good place to start, as it is released with the product. Make sure you’re looking at the right version. The reference docs are designed for the specification of a feature and usually give a concise definition and barebones example of how something works and acts (e.g. syntax, config options, or installation instructions). Because of this, it’s probably most useful to more experienced Neo4j users.There’s not a whole lot of fluff built into the docs, so there won’t be too much behind-the-scenes explanation of topics or particular integration/use-case details, but this can be the perfect thing for getting an idea of what the functionality looks like without any other clutter.For 4.0, we also have a migration guide that was created to help you upgrade systems from previous versions to Neo4j 4.0 and to help answer questions about differences and impacts.Migration GuideOperations Manual: multi-database, Fabric, fine-grained securityCypher Manual: multi-database, security, subqueriesDrivers Manual: reactive sessions, multi-database, encryptionAll documentation — all the above + HTTP API, Java reference, status codes, and moreDeveloper GuidesThe Neo4j Developer Guides explain concepts and contain introductory examples, often including a tutorial or walkthrough of basic usage and application for commonly-used functionality. These are a great resource for getting started with a feature to understand how it functions in a typical environment and the basics for getting it set up and running.From Cypher to multi-tenancy to Fabric, these guides help introduce major features and tools by providing in-depth concepts with hands-on examples for gaining familiarity in operating and navigating. While there may not be specifics for working with something in a complex system or making customizations, the developer guides are a good place for interacting with and learning to use new tools or features.Learn how to use Neo4j 4.0 features such as managing multiple databases, sharding with Neo4j Fabric, and using multi-db and Fabric in a multi-tenancy environment. The Neo4j Online Developer Expo and Summit (NODES) conference also gave us a way to show what would be coming in this release. Recordings of that content are also available.Video — Migration walkthrough for Neo4j 3.5.x to 4.0Managing Multiple DatabasesSharding Graph with Neo4j FabricMulti-Tenancy Example with Multi-Db and FabricCypher subqueries4.0 security, Reactive SDN, Behind-the-Tech in 4.0 — NODES online developer conferenceAll Developer GuidesStay tuned — other feature guides coming soon!Developer BlogIf you are interested in dealing with functionality in specific environments or optimizing queries and processing, the developer blog on Medium.com provides more content in this area. Our publication on Medium accepts posts not just submitted from Neo4j staff, but everyone in the community wanting and willing to share what they have learned or spread the word for tips and tricks.The blog channel has a steady stream of new content, as the content source pulls from a variety of authors and experiences. It will tell you what is working now and how to adapt it. Our authors also have a great record of updating older content or publishing the latest changes in another post, so you can find the latest versions of content you need.Covering ranges of topics for data science, visualization, how-to, import/export, Cloud/container environments, community activities/announcements, and so much more, the developer blog is a great place for keeping up on the latest of what other developers are doing with Neo4j.Neo4j Developer Blog on Medium.comReactive Spring Data (beta3) Neo4j with Multi-TenancyDrivers for Neo4j 4.0Query Log and DB Analyzer Graph App 4.0 updateNeo4j Expert BlogsSome expert content is published outside Neo4j-specific channels on personal blogs. Google searches will surface these posts, and they are also featured in the This Week in Neo4j” (TWIN4J) newsletter. You can monitor a specific author’s activity (following on Twitter, adding to RSS feed, etc) or subscribe to Twin4j to receive weekly updates on what has been published.Blog post content can give you more tailored information for your needs based on specific topics, environment setups, areas of expertise, etc. Because these articles often also show up in Google searches, it means they can fit closely what you are looking for, if you can find the right search terms. Note, however, that only one person’s perspective or experience comes through an author’s blogs.Whether you are interested in dumping data from a Docker container, running in multi-tenancy or sharded environments, using a beer or fraud graph to explore Neo4j security, or using Cypher’s post-union processing, blog posts by experts in the field are excellent resources for hashing out specific examples, overcoming uncommon errors, or optimizing solutions for certain problems.Database dump in a Docker container with 4.0 by Mark NeedhamMulti-tenancy with multi-db and security by Luanne MisquittaMulti-tenancy with multi-db and connecting an application by Adam CowleyWhen and How to implement sharding with Neo4j Fabric by Adam CowleySecuring the beer graph with 4.0 security by Rik Van BruggenSecuring a fraud graph with 4.0 security by Rik Van BruggenTesting conflicting access privileges by Rik Van BruggenCypher union and post-processing in 4.0 by Luanne MisquittaBuild a SAAS Control Panel by Adam CowleyOther ResourcesLast, but not least, if you are looking for business resources on the value of Neo4j or keep up on the latest trends and announcements, there are some places to check for the latest materials. Most of this content will be published on neo4j.com/blog and focuses on high-level architecture, features, use cases, and customer stories.Topics ranging from visualization with Neo4j, starting out with Neo4j and Cypher, implementing Neo4j for specific solutions, or more about the industry are available for guidance. There are also videos on the Neo4j YouTube channel from conferences, customers/partners, and developers.Webinar on 4.0 with Tech Panel speakers — Register (Feb 26)!4.0 Blog post by Dr. Jim WebberWhitepaper on Future of Intelligent ApplicationsNeo4j 4.0 Press ReleaseNeo4j YouTube channelHappy learning!;Feb 11, 2020;[]
https://medium.com/neo4j/new-sandbox-in-town-e126246d2605;Ljubica LazarevicFollowDec 7, 2020·3 min readCentral Park by Clay LeConey on UnsplashNew sandbox in townExploring New York’s Central Park with Neo4j Spatial, APOC, and Graph Data ScienceWhat’s a Neo4j sandbox?The Neo4j sandboxes are a great way to experiment and work with Neo4j without needing to download, install and configure the database. We have a broad range of use-cases with preloaded datasets and a step-by-step guides to get you going, as well as blank sandboxes to import and work with your own data.A sandbox lasts for three days, but can be extended to a maximum of 10 days. You’ll also find handy code snippets in various languages to connect to the sandbox from your favourite stack.So what is THIS sandbox all about?Central Park in New York is one of the most popular tourist attractions in the city. Last year, Central Park saw 42 million visitors alone. The latest sandbox uses this amazing location to do some exploration, with graphs!We have imported some routes and tagged Points of Interest for Central Park, based on OpenStreetMap, and we will be asking you to take on the role of the virtual tourist.For those of you who are interested, the data was extracted with this plugin, and there was some Cypher data wrangling applied after import. As a result, we don’t have all the sights, but we do have a pretty good sample to work with.This all sounds rather familiar…Well spotted! This sandbox is inspired by one of our Summer of Nodes challenges. We’ve implemented the easy challenge in the walk-through guide. If you’re in a problem-solving frame of mind, why not give the advanced challenge a try too!?Summer of Nodes: Final Week — Exploring the AreaFinishing off Summer of Nodes 2020 on a spatial specialmedium.comAnd if you would rather do your own thing?Please do! You’ll also find available with the sandbox, the APOC library and the Graph Data Science Library. This gives you a large variety of tools to explore and query the data. You’ll probably give the path finding algorithms a go. Perhaps you’ll find other graph-y applications too!Central Park POI rendered with NeoMapFinal wordsWe would love to know how you find this new sandbox, and if you do your own thing, what kinds of insights you’ve found.Please let us know on our twitter account, with #Neo4jOSMSandbox.;Dec 7, 2020;[]
https://medium.com/neo4j/neo4j-graph-visualization-like-a-pro-18651963ebd4;Sebastian MüllerFollowMay 29, 2018·9 min readVisualizing Neo4j Database Contents Like a Pro!There are two major use-cases for the visualization of data in graph databases like Neo4j:Visualization can help database administrators and developers understand their Neo4j database schema as well as browse the contents and verify their queries using tools like Cypher and the Neo4j Desktop.But not only developers and backend devs can benefit from visualization: Many end-user applications that let the users work with data that is stored in graph databases also benefit a lot from a more visual interface to their data:Two ways for showing the same dataCreating a user-friendly, well-performing, and informative visualization for end-users is all but trivial and the simple visualizations that developers use for their internal tools often simply don’t cut it.In this post I am going to present a tool that can help you with that task. With it, you can create high-quality graph and diagram visualizations for end-user applications, that perfectly convey the information contained in your database to the end user.With only slightly more than 60 lines of code we will create an interactive web application that renders a unique diagram of your data. This is what it will look like when used with the movie sample database:The final result showing the movie sample databaseProfessional Diagramming: yFiles for HTMLThe tool I am using for this is yFiles for HTML — a generic graph visualization programming library for the web platform that can help you realize those last ten or twenty percent of your requirements that you won’t be able to implement with other tools: yFiles is a whole family of graph visualization programming libraries that have been available commercially for various target platforms for more than 15 years now and which are used by companies and corporations around the world coming from a wide range of diverse industries.The variant for the web platform is not exclusively meant to be used as a visualization front-end for Neo4j, however in this post I will show you how well yFiles for HTML and Neo4j play together.The single-page web app that we are about to build uses yFiles for HTML and the Neo4j JavaScript Bolt driver to query your very own Neo4j database. The query results will be displayed in a graph visualization with a unique automatic layout, customized item visualizations and user interactions.To follow the instructions in this post, you can download a free evaluation version of the yFiles for HTML software package and you should have a Neo4j database running that you can use for testing.In this post we are only going to briefly touch the surface of the yFiles API and vast array of possibilities and features that yFiles offers, so if you are interested in learning more, be sure to look at the various demos and online resources, too, or see this YouTube Playlist on that topic.Also, as an existing Neo4j user, be sure to check out the free online Neo4j Explorer web application that will help you visualize your database without any coding!Let’s start coding!We begin by scaffolding a bare minimum WebPack powered ES6-sources-based yFiles for HTML application that does not depend on any third party software at runtime.We could also be using UI frameworks like Angular, React, VueJS, or anything similar, but for the sake of simplicity we are going to use plain vanilla HTML, here.After unzipping the yFiles for HTML library to some directory in our computer we can use npm or yarn and yeoman to scaffold the application. More detailed instructions are available in a separate screencast. On the console we do:> npm install -g generator-yfiles-app> yo yfiles-appAnd yeoman will lead ask you some questions. Here are the answers I chose for this task:? Application name Neo_post? Path of yFiles for HTML package C:\Path\to\your\yFilesForHTMLPackage? Path of license file (e.g. path/to/license.js) C:\Path\to\your\yFilesForHTMLPackage\demos\resources\license.js? Which kind of yFiles modules do you want to use? ES6 Modules? Which modules do you want to use? yfiles/algorithms, yfiles/layout-hierarchic, yfiles/view-component, yfiles/view-editor, yfiles/view-layout-bridge? Which language variant do you want to use? ES6? Which webpack version would you like to use? 4.x? What else do you want? WebStorm/PHP-Storm/Intellij IDEA Ultimate Project files? Which package manager would you like to use? yarnThen yeoman will do its magic and after a few seconds it will have scaffolded a shiny new web application that uses yFiles for HTML and Webpack 4 with ES6/EcmaScript2015 to load, layout, and render a trivial graph that you can edit interactively with the mouse, keyboard, and on touch devices.Initial application scaffolded by yeomanAnd this is what the app will look like when we use npm start or yarn start to compile, serve, and run the code in our browser:Boring — but it works!You will agree with me that this is not really an outstanding visualization, yet, and what we really want to do is inspect our Neo4j database contents!Introducing Cypher and BoltIn order to connect Neo4j with yFiles for HTML, we first add the JavaScript Bolt driver to our application.On the console we do:> yarn add neo4j-driverThen in the code we simply load the Neo4j driver by adding the following import statement at the top of the file:import neo4j from neo4j-driver/lib/browser/neo4j-webMake sure you have the credentials and IP address available for the Neo4j database that you would like to explore, because with the next line of code, we can already connect to the database:const neo4jDriver = neo4j.driver( bolt://1.2.3.4 ,   neo4j.auth.basic( theusername ,  theS3cr3t ))(Of course you will need to adjust the IP address, the user name and the password.)We are now going to simplify the scaffolded code first that yeoman created for us. Strip it down so that it looks like this:This will result in a boring empty graph view, because we only queried the database for the nodes, but we are not doing anything with the results, yet. At this point it is important to understand that the core yFiles for HTML library does not know how to interpret the results of a query returned by the Neo4j Bolt connector. However there are utility convenience classes that will help us with the import of any kind of structured data that we have access to in our JavaScript environment. The data could be fetched from a file, from the network, via REST, WebSockets, or created dynamically on the client. Of course this works with the Neo4j data, too: All we need to have is some understanding of the way the Neo4j response data is structured.For the simple query above the resulting JavaScript object looks like this:{   records : [{     keys : [ node ],     length : 1,     _fields : [{         identity : {           low : 0,           high : 0        },         labels : [ Movie ],         properties : {           title :  The Matrix ,           tagline :  Welcome to the Real World ,           released : {             low : 1999,             high : 0          }        }      }    ],     _fieldLookup : {       node : 0    }  }We can use the following Bolt JavaScript API to obtain an array of all the nodes queried:const nodes = nodeResult.records.map(record => record.get( node ))Now that we have the nodes, we can use the convenience class GraphBuilder from yFiles to get the graph onto the screen:graphBuilder.nodesSource = nodesgraphBuilder.buildGraph()That was easy! But the result still leaves a lot to be desired from a visual and interactive perspective:That still doesn’t look exactly overwhelming…Let’s improve that: we assign a nicer style for the node visualization and a simple label mapping: If the node has a property named  title  or  name  we use this for the label. And instead of leaving all nodes at the default location, we apply an automatic layout algorithm to arrange the nodes nicely on the screen.The graph looks nicer, now, but we still don’t see any relations.No relations, yet. So still not better than a list.Let’s change that with another query to the database! We query all the edges between the nodes that we have queried before using this code:This is what a result looks like in JSON form:{   identity : {     low : 7,     high : 0  },   start : {     low : 8,     high : 0  },   end : {     low : 0,     high : 0  },   type :  ACTED_IN ,   properties : {     roles : [ Emil ]  }}As you can see, the relationships reference their source and target nodes via the identity property that was attached to the nodes. We can use the toString function provided by Bolt, to obtain a representation of the identities that can be consumed easily by GraphBuilder. We tell the helper class to use this information to resolve the connections end points and build the in-memory graph for the display:Putting it all together this is what we have now in our loading function:And this is what the visualization looks like in the browser:Finally, we begin to see some structureWe still need to work a little on the interactivity, the layout, and the visualization. Right now, we are hardly any better than most other tools. Let’s say we want to show some details in a tool-tip, let the user select elements and react to clicks in the graph. For this we add the so-called GraphViewerInputMode to our component:const inputMode = new GraphViewerInputMode()graphComponent.inputMode = inputModeThis enables the user to select elements, pan around conveniently, and use the keyboard to navigate the graph. We can now also easily specify rich tool-tips for our elements: For this we use the information that the database provided us with and we list all the properties in a very simple HTML list:And in order to better understand the structure of the graph, we add the option for the user to center the layout around a given item that she double-clicks:That’s it. Check out the final result, again:Much better! Neo4j database visualized using trivial yFiles for HTML codeI guess you can see that with only very little customizations you are now able to adjust the Cypher queries to your specific databases and create rich, interactive, visualizations of your data!That’s it already for this very short introduction on how to use yFiles for HTML to visualize your Neo4j databases. You will find a commented version of the sources for this demo on GitHub.What next?Of course there is a lot more functionality that has not yet been covered in this post. Take a look at the many online demos for yFiles for HTML to get an idea what else you could do with the yFiles libraries and Neo4j!I leave it as an exercise for you to improve the visualization to look like this 😉:After polishing the UI, this is what a diagram can look like.And to give you more ideas about the possibilities, with some coding, yFiles for HTML, and Neo4j you can:use WebGL, Canvas, and SVG together with templating and databinding to create richer visualizations with animations and just the right information your users requireconnect to third party data-sources and services to enhance your visualization with live data and let the user trigger actions in remote systems when interacting with the visualizationembed the graph visualization as a white-label component into your very own apps, possibly using Angular, ReactJS, VueJS, or whatever framework you preferinteractively explore the database and incrementally add new elements to the visualization using incremental automatic layoutsuse different, more compelling layout styles and provide more sophisticated layout constraints to highlight certain structures, components, or elements in your dataset in the visualizationuse nesting, filtering, and drill-down techniques to let your users interactively explore your dataenable users to edit your graphs using the mouse, keyboard, pen, and touch devicesSo what are you waiting for? Go and visualize your Neo4j database like a pro! Happy diagramming!;May 29, 2018;[]
https://medium.com/neo4j/how-we-build-a-clone-of-r-place-with-graphql-c8b053b3cff9;angrykoalaFollowAug 5, 2022·13 min readHow We Build a Clone of r/place with GraphQLr/place was a collaborative project and social experiment hosted on Reddit on April Fools’ Day 2017 and repeated again on April Fools’ Day 2022.~ WikipediaThe GraphQL team atNeo4j recently released beta support for GraphQL Subscriptions in the @neo4j/graphql library. Subscriptions allow clients to listen to changes in their Neo4j database for their real-time applications.To showcase this feature at the 2022 GraphConnect conference, we decided to build a demo, based on the famous r/place, called neo/place.neo/placeThis demo contains a collaborative 30x30 canvas for users to paint where all the changes are shared in real-time across all connected users through WebSockets.Despite its simplicity, neo/place was built with scalability in mind, serving as an example of how to create a production-ready real-time service with GraphQL.How GraphQL Subscriptions WorkBefore getting into details on how neo/place is built, it is worth seeing how GraphQL subscriptions work. At a basic level, subscription operations create a long-lived connection between a client and the server. GraphQL specification does not define what mechanisms should be used for this communication — in practice, WebSockets are the most common way of building subscriptions.GraphQL subscriptionsThe following is an example of such a subscription:subscription {  movieCreated {    createdMovie {      title      genre     }  }}Just like with any other GraphQL operation, we can define the expected result. However, unlike other operations, this result is not returned as a response to the request. It will, however, be the expected format of subsequent messages sent to the client (events).How GraphQL works in @neo4j/graphqlWhen using Neo4jGraphQL, subscriptions are an opt-in feature that, if set, will automatically create subscription endpoints for all entities defined in your GraphQL schema. These endpoints allow subscribing to create,update, and delete mutations.Basic architecture of a Neo4jGraphQL serviceThese events will be triggered after a mutation to the Neo4j database.For instance, the following type definitions:type Movie {   title: String!  genre: String }… will provide the subscriptions movieCreated, movieUpdated, and movieDeleted along with the common CRUD operations. This way, the following subscription:subscription {  movieCreated {    createdMovie {      title      genre     }  }}…will automatically trigger a new event whenever createMovies is called:mutation {   createMovies(input: [{ title:  The Matrix  }]) {     movies {       title     }   } }You can find more examples and information in the official documentation.How neo/place Is BuiltNeo/place is a simple webpage, backed by an Apollo server, @neo4j/graphql library, and a Neo4j database.GraphQL schemaWhen using Neo4jGraphQL both the API and the database schema are defined by a GraphQL schema. For this demo, we used the following schema:type Pixel @exclude(operations: [CREATE, DELETE]) {    position: [Int!]! @readonly @unique    color: String}type Query {    canvas: [String]        @cypher(            statement:                MATCH(p:Pixel)            WITH p.color as color            ORDER BY p.position ASC            RETURN collect(color) as canvas                       ) @auth(rules: [{isAuthenticated: true}])}extend type Pixel @auth(rules: [{isAuthenticated: true}])The only type that we need in our database is Pixel, which holds the data of one pixel of our canvas:position: An array of two integers holding the 2D coordinates of this pixel. These coordinates are unique (no two pixels can have the same position) and read-only (once the position is set, users cannot modify it)color: The current color of that pixel, defined as a string. It will hold a hex value such as #FF00FFThis type is also defined with two directives that will help us secure our API:@exclude: We will not allow users to create nor delete pixels, only update them. Exclude will remove these operations from the generated API.@auth: Ensures only authenticated users with a valid JWT authentication can modify the pixels. Because this is a public app without login, the token will just contain a well-known password between client and server. This is a naive mechanism to make it slightly harder for third-party clients to consume the API but could be the base of a better security mechanism with login.While the library provides most of the CRUD and subscription operations that we will need, for performance reasons, we are going to need a custom query as well. When loading our canvas for the first time, we need to request all pixels. If we were to use the provided CRUD operations, the query would look like this:query canvas {    pixels {        position        color    }}The response to this query would be something like:{     pixels : [{      position : [0,0],      color :  #FFFFFF     }, {      position : [0,1],      color :  #FFFFFF     }    // ...    ]}This json format is usually okay for most queries, but in this case, we are trying to get a large number of entities (30x30 = 900 pixels), and each entity holds very little data (color and a position). So, to avoid long loading times and unnecessary networking load, we can optimize the first request by defining a custom database query with the @cypher directive:MATCH(p:Pixel)WITH p.color as colorORDER BY p.position ASCRETURN collect(color) as canvasThis will bypass our GraphQL model and perform a Cypher query directly to the database. In this example, the query will return all our pixels as an array, ordered by position and only containing the colors. The API response would look like this:{   canvas : [ #FFFFFF ,  #FFFFFF , ...]}Thanks to the ordering by position, we can safely assume that the first item of the array will always be the pixel [0,0] and the last one [29,29], which, as long as we know the canvas dimensions beforehand, allows us to remove redundant information and end up with a lightweight query for our first load.Apollo serverFor our server and GraphQL runtime, we will use Apollo along with graphql-ws.We will start by loading and connecting the Neo4j driver to our database:const driver = neo4j.driver(NEO4J_URL, neo4j.auth.basic(NEO4J_USER, NEO4J_PASSWORD))We will also import Neo4jGraphQL with two plugins and pass our type definitions and the driver to the library constructor:const { Neo4jGraphQLSubscriptionsSingleInstancePlugin, Neo4jGraphQL } = require(@neo4j/graphql)const { Neo4jGraphQLAuthJWTPlugin } = require( @neo4j/graphql-plugin-auth )const neoSchema = new Neo4jGraphQL({  typeDefs: typeDefs,  driver: driver,  plugins: {    subscriptions: new Neo4jGraphQLSubscriptionsSingleInstancePlugin(),    auth: new Neo4jGraphQLAuthJWTPlugin({      secret:  super-secret42     })  },})The two plugins that we are importing are official plugins to enhance the capabilities of Neo4jGraphQL:Neo4jGraphQLSubscriptionsSingleInstancePlugin to enable subscriptions.Neo4jGraphQLAuthJWTPlugin to enable JWT authentication with the @auth directive.Once everything is set, we can build our schema to have a complete GraphQL API, ready to be served by Apollo:const schema = await neoSchema.getSchema()await neoSchema.assertIndexesAndConstraints({   options: {     create: true   } })This code will also ensure that @unique constraints are properly set in the database for consistency and performance with assertIndexesAndConstraints.Finally, we need to pass this schema to an Apollo server:const server = new ApolloServer({ schema, context: ({ req }) => ({ req }),})For the full working example, refer to https://github.com/neo4j/graphql/tree/dev/examples/neo-placeDatabase modelWe will be using a Neo4j Graph database — this means that our pixels will be modeled as Nodes” in a graph in our database.As mentioned before, we will have a static set of pixels that users can modify, but cannot create nor delete. We need to populate the database when the server starts. To achieve this, we will simply perform a Cypher query directly from the Neo4j driver:const session = driver.session()await session.run(`  UNWIND range(0, 29) AS x  UNWIND range(0, 29) AS y  MERGE(p:Pixel { position: [x, y]})  ON CREATE SET p.color= #FFFFFF `)await session.close()This query will create new entities, with positions from [0,0] to [29, 29] and a default white (#FFFFFF) color. By using MERGE we will only create the entities if these do not exist already.Visualization of the populated databaseFor this demo, we are running a Neo4j AuraDB Free, so all the hosting and managing was done for us in the cloud.Building neo/place ClientOur client will consist of a plain HTML webpage with a <canvas> element that will display our collaborative drawing, and some buttons to select colors.<canvas id= place  height= 300px  width= 300px ></canvas><button class= color-button  style= background-color: rgb(204, 37, 75) ></button><button class= color-button  style= background-color: rgb(1, 139, 255) ></button><!-- .... -->Connecting to our serverTo fulfill neo/place needs, we will only need three GraphQL queries to our server. To perform these queries we decided to use the GraphQL client urql along with graphql-ws.We will begin by creating a simple wrapper over our server API.First, we need to call our custom canvas endpoint, which we will use to render the canvas at the beginning:async getCanvas() {  const canvasQuery = gql`    query Canvas {      canvas    }  `  const result = await this.client // urql client    .query(canvasQuery)    .toPromise()  if (result.error) throw new Error(result.error.message)  return result.data.canvas}This method will return the array of colors that we will use later to fill the canvas.We also need to listen to our server events whenever a pixel is modified. We will do this through a subscription query hooked to a callback on every event received:onPixelUpdate(cb) {  const pixelsSubscription = gql`    subscription Subscription {      pixelUpdated {        updatedPixel {          position          color        }      }    }  `pipe(  this.client.subscription(pixelsSubscription), // urql client  subscribe((result) => {   if (!result.error) {    cb(result.data.pixelUpdated)   }  }) )}Finally, we need to update a pixel in the database whenever the user clicks on it:updatePixel(position, color) {  const updatePixelQuery = gql`    mutation UpdatePixels($update: PixelUpdateInput, $where: PixelWhere) {      updatePixels(update: $update, where: $where) {        pixels {          position          color        }      }    } `  const params = {    update: {      color    },    where: {      position    },  }  return this.client.mutation(updatePixelQuery, params).toPromise()}Interacting with the canvasOur canvas is a static 300x300 pixels canvas on our screen. However, we want to display a 30x30 grid on it. Each one of our pixels” will be rendered as a 10x10 tile that users can draw with a single color.GridLike with our API, we will create a few helper methods to handle canvas interaction.First, we need to be able to draw a pixel as a 10x10 tile:function drawPixel(pixel, color) {  const ctx = canvas.getContext( 2d )  const x0 = pixel[0] * 10  const y0 = pixel[1] * 10   ctx.fillStyle = color  // We dont want our browser to smooth our pixels  ctx.imageSmoothingEnabled = false  // Paint a rectangle of 10x10 in the given coordinates  ctx.fillRect(x0, y0, 10, 10)}We also need to listen for a pixel being clicked by the user:function onPixelClicked(cb) {  canvas.addEventListener( click , (ev) => {    const screenPixel = [ev.clientX, ev.clientY]    const rect = canvas.getBoundingClientRect()        const x = screenPixel[0] - parseInt(rect.left)    const y = screenPixel[1] - parseInt(rect.top)     const pixelClicked = [      parseInt(x / 10),       parseInt(y / 10)    ]    // Calls the callback to handle interaction with the API    cb(pixelClicked)  }, false)}Wiring everything togetherWith these two helpers, we can easily hook our canvas interactions to our API and propagate any changes received by the subscription to the canvas.Draw canvas on the first request:// Receives the array with all the canvas colorsconst canvasColors = await serverApi.getCanvas() let i = 0, j = 0for (const pixelColor of canvasColors) {  drawPixel([i, j], pixelColor)  j++  if (j === 30) { // new line in the canvas    i++    j = 0  }}We need to update the canvas whenever we receive a subscription event:serverApi.onPixelUpdate((updatedEvent) => {  const updatedPixel = updatedEvent.updatedPixel      drawPixel(updatedPixel.position, updatedPixel.color)})Finally, we need to notify the server whenever the user updates the canvas:let selectedColor =  #FF0000 onPixelClicked((pixelClicked) => {  // Draw pixel immediately to provide instant feedback to the user  drawPixel(pixelClicked, selectedColor)  // Updates pixel in the database  serverApi.updatePixel(pixelClicked, selectedColor)})For the complete code example, including race conditions and errors handling, check the example at https://github.com/neo4j/graphql/tree/dev/examples/neo-place/clientHorizontal ScaleAs mentioned, the demo is relatively simple with not too much code involved, but we also promised this will showcase a production-ready setup that scales horizontally. We will see what changes are required in the different systems of this demo to scale it.DatabaseBecause we are using the cloud-hosted AuraDB, scaling the database would be as simple as switching to a higher tier. However, because subscriptions are designed in a way that will not produce extra queries or overhead to the database, the database only needs to worry about the relatively simple queries and mutations we are executing.This means that, even under a heavy load, all of our queries are simple property updates over indexed nodes (the @unique constraint over positions that we already set acts as index). Even in the free tier, we would need a high load to hit a performance limit in the database, so we will stick to the free instance for now.Server and AppEngineTo host our Node.js server with Apollo, we decided to deploy them to Google cloud AppEngine. This ensures that the server itself will scale according to the load. AppEngine will take care of load balancing (including WebSocket connections) and automatic scaling, so no extra work is needed on this front on our side.Scaling subscriptions with AMQPSubscriptions, like any WebSockets service, are traditionally hard to scale. While a client may be subscribed to a certain instance, a mutation may happen in a completely separate instance that may not know about that subscription.Problem with subscriptions with multiple instances.Sadly, this problem is not something that load balancers or AppEngine will solve for us. Having our clients subscribe to all servers or use a single instance to handle all subscriptions would defeat the purpose of scaling the server.One of the most elegant solutions to this problem is to use a separate broker, such as Redis or RabbitMQ, to efficiently handle and broadcast messages across all our instances:Multiple instances set up with a separate brokerThis means, that each instance has two jobs regarding subscriptions:Notify the broker whenever a mutation happens.Listen to the broker, and route these events to its subscribed clients.Additionally, Neo4jGraphQL will take care of filtering and authentication of the messages received by the broker before sending them to the client.While all of this may sound like a lot of work, Neo4jGraphQL is designed with this architecture in mind. That means we only need to do two changes to the system to achieve horizontal scalable subscriptions:1. Setup an AMQP server (e.g RabbitMQ). We could host our own, but for this demo we decided, as with everything else, to leave that to a cloud-hosted service: CloudAMQP. In its free tier, we are limited by queue size, connections, and total messages sent.For this use case, queue size and connections are not a problem, as we are running a real-time service, connected only by our server instances and without message persistence. Maximum messages and messages per second will be our limiting factors, but, as with all other services described, this can easily be increased by moving to higher tiers if needed.2. Configure Neo4jGraphQL to use RabbitMQ.The plugin system on which subscriptions are built is designed with brokers in mind — we have already seen how to use a plugin: Neo4jGraphQLSubscriptionsSingleInstancePlugin— which, as the name implies, is intended to be used for single instance servers. Setting an AMQP broker instead is as simple as using Neo4jGraphQLSubscriptionsAMQPPlugin instead:const { Neo4jGraphQLSubscriptionsAMQPPlugin } = require( @neo4j/graphql-plugin-subscriptions-amqp )// ...plugins: {  subscriptions: new Neo4jGraphQLSubscriptionsAMQPPlugin({   connection: AMQP_URL  }) },// ...Neo4jGraphQLSubscriptionsAMQPPlugin is one of the officially supported plugins for subscriptions with Neo4jGraphQL, but it is also possible, albeit with a little more work, to create your own plugins for any suitable broker. And just like that, we already have a scalable, production-ready demo. Of course, we didn’t want to test it live…Bonus: Testing Scale on neo/placeWe started by manually testing how neo/place behaved by sharing the demo with colleagues, but we wanted to make sure that our humble demo could handle roughly 500 concurrent users painting at the same time (we are optimists).Testing this kind of load is not trivial for a real-time service. Blindly sending lots of requests to the server will not provide a realistic load of multiple simultaneous connections. We needed proper, end-to-end, realistic interactions.Automated browsersHeadless browsers are a great way to emulate real users, helping us validate that the system works as expected under a heavy load in a realistic environment. For this simple test, we opted to use the famous Puppeteer headless browser along with the wendigo wrapper that provides some useful features to make it easier to use.The resulting code to achieve these interactions (a simple .js script) is surprisingly simple:const browser = await Wendigo.createBrowser()await browser.open( neo-place-url )// Forces display, regardless of environmentawait browser.setViewport({   width: 500})// Waits for canvas to be visibleawait browser.waitFor( #place , 2000)// Get canvas boundariesconst bounds = await browser.evaluate(() => {   const canvas = document.querySelector( #place )  const bounds = canvas.getBoundingClientRect()  return {    top: bounds.top,    bottom: bounds.bottom,    right: bounds.right,    left: bounds.left  }})// Draw a random pixel with a random color every 100mssetInterval(async() => {  const randomColor = getRandomInRange(0, 5)  // Clicks a random color button  await browser.click( .color-button , 1)   // Gets a random pixel within the canvas const coords = getRandomCoordinates(bounds) await browser.click(coords[0], coords[1])}, 100)And just for completeness, the two helper functions to get random buttons and pixels:function getRandomInRange(from, to) {  return (Math.random() * (to - from) + from).toFixed(0) * 1}function getRandomCoordinates(bounds) {  const x = getRandomInRange(bounds.left, bounds.right)  const y = getRandomInRange(bounds.top, bounds.bottom)  return [x, y]}With this bot in our hands, we can emulate a new pixel being drawn every 100ms — way faster than any normal user could. Now, by running a bunch of these headless browsers (we used pm2) across a few computers we could get some rough estimates on how many users the system could hold. With this setup, one bot is roughly equivalent to 10 concurrent users, in terms of load to the system.If you are interested in building GraphQL APIs, backed by a graph database, check @neo4j/graphql, the official documentation, and click here to learn more about subscriptions with Neo4j.This project is based on r/place. To get more information about how the actual place was built, check the fantastic post How We Built r/Place;Aug 5, 2022;[]
https://medium.com/neo4j/neo4j-desktop-1-4-0-release-c50de440c535;Liza ShkirandoFollowJan 13, 2021·3 min readNeo4j Desktop 1.4.0 ReleaseNew user interface to match the mental model of a file managerNeo4j Desktop is the database, plugin and extension management UI for your local development. It can handle as many projects and databases as you need. It is implemented as an Electron App and runs on Windows, MacOS and Linux. Download Neo4j Desktop from neo4j.com/downloadThe Neo4j Developer Tools team has been conducting ongoing user research for the past few years. One of our findings was that for many new users, especially new and beginner users, the concept of Projects and Files in Neo4j Desktop was not clear enough.Today we are bringing in support for folder structures, importing a git project as well as saving cypher files from Neo4j Browser. We decided to combine the efforts to create a new, coherent UI with a familiar mental model.Our goal was to make Neo4j Desktop look and behave like an OS file manager. This approach is hopefully reduces confusion and cognitive load.New Neo4j Desktop UIThe active database is always displayed on the top of the screen. It shows the active database regardless of which project is open. The project that the active database belongs to, can be easily reached by clicking the Project name in the top bar.Top bar: active databaseThe mental model of a File Manager is supported through a number of interactions and a tabular structure. To make the default UI cleaner, basic interactions are only visible on hover:Start/Stop/Connect (for a database),Open With (Browser or installed Graph Apps),context menu.You can create a new database (local or remote) or add a file to a Project by clicking the Add button. Files can be sorted by name, dates or size. Manual rearrangement of files and databases inside a project is planned for future releases.Files and databases can be moved between Projects by dragging and dropping them.Drag and drop databases and files between ProjectsClicking a database or a file opens up the Manage Pane on the right hand side. It replaces the old Manage Screen and is now accessible in one click. Logs and Settings are available in the context menu of a database or a file and open as an overlay window.In the Manage Pane you can:install pluginssee database detailsupgrade the database and change the passwordManage panel and Context menuDisplaying files as a list is beneficial in case you have a lot of files in a Project. For example, if you are saving Cypher Files in Neo4j Browser they will show now up in Neo4j Desktop. A list view gives a better overview of the files than the previous tile-based UI.Saving Project files from Neo4j BrowserPlease reach out to us via Community forums or Slack if you have any feedback or want to take an active part of the future feature development of Neo4j Desktop and Browser.You can also create GitHub issues for Neo4j Desktop:neo4j-devtools/neo4j-desktopNew issue Have a question about this project? Sign up for a free GitHub account to open an issue and contact its…github.com;Jan 13, 2021;[]
https://medium.com/neo4j/an-introduction-to-needle-neo4js-design-system-2560e405a794;Costa AlexoglouFollowMar 1·5 min readAn Introduction to Needle — Neo4j’s Design SystemNeedle is Neo4j’s Design System LibraryCo-authored: Sebastian Nilsson 🫶The Need for a Design SystemAs Neo4j has scaled, doubling its design and engineering teams, maintaining UI/UX consistency across its products has become a pressing issue. Back in 2021, our efforts were not scaling effectively and some of our products did not necessarily look or behave like the same company created them.Working together, design and engineering identified key areas for potential improvement. This investigation focused on how design was created and delivered for implementation and then how design was translated into code.The key issues we wanted to address were:A lack of asset libraries (both design and code) that would allow us to reuse components and patterns. Supporting those, we also recognized a need for design guidelines covering implementation.Existing processes did now allow sufficient time to perform research, exploration, and testing.Designing from scratch made it hard to achieve consistency and quality. Such repetition was also noted as a key area of waste that we wanted to eliminate.Difficulties ensuring accessibility in the absence of proper research and testing.Having identified a design system was the obvious missing asset within our processes. Nielsen Norman Group defines a design system as a complete set of standards intended to manage design at scale using reusable components and patterns.”With our needs identified, we then set about creating a strategy for operationalizing and scaling design at Neo4j.How We Started Our Design SystemThe first step we took to help designers overcome the current problems was to create a prioritization matrix to determine what components to start building first. Our matrix categorized components based on how frequently they were used in products and how complex they were to build.The results helped us create the initial roadmap for the design system and how to plan our work ahead.Prioritization Matrix from our workshopOperationalizing and Scaling DesignFast forwarding to today, our design system, Needle, has grown to include over 30 components. There is now a formal design system team dedicated to building and maintaining the design library, the code library, and our guidelines website. This operationalization of design has allowed us to scale both our development teams and our product suite while maintaining quality and efficiency. Designers now spend less time on repetitious design while producing with more consistency and quality than before. This change has allowed them to dedicate more time to other UX activities like research, testing, and collaborative efforts.Maintaining a close collaboration between engineering and UX teams has helped us to stay reactive to changing needs and requirements, which, in turn, have informed our roadmap and priorities. Needle now includes design guidelines, templates, and processes. This includes accessibility, responsiveness, content, design handoff, and contribution guidelines. These are key artifacts that transform our design assets into fully-operationalized designs.Needle provides interactive capabilities that allow developers to experiment and ship features faster than ever. But besides improved velocity, the teams can now stay consistent and focus on more complex engineering problems, rather than worry about styling and positioning of elements.Example of components from our design systemNow and ThenToday, all of our current products use the design system. The code library has over 10k weekly downloads, and the product team in Figma uses over 60000 instances from the component library. The success of the design system adoption can be seen when comparing product screenshots from two years ago with current ones.Looking at the examples, there is a clear visual improvement in a fresher and more modern UI. But many other benefits come with the introduction of the design system, such as meeting accessibility requirements, unified design and interactive behavior between products, and a stronger visual identity.Neo4j has always been proud of delivering the fastest database on the market, but with the design system we are introducing a new narrative — a narrative where the design of our products are as impressive as our performance.NeoDash before (Material UI) and after the design systemOps Manager before and after the design systemBloom, Browser, and Data Importer before the design system (as standalone products)Workspace: Explore (Bloom), Query (Browser), and model (Data importer) as a unified toolNext StepsAs of now, our component library has reached a stable state where the most common use cases have been solved for. Looking ahead, we want to start writing guidelines for designers and developers covering how to use components and other assets. We also want to add more global assets, such as iconography, branding, content writing, colors, and typography, so that non-designers across the organization can use Neo4j assets to build things like presentations, documentation, and other internal and external visual content.The home of our design system is Neo4j.design, which is our unified design space where everything design-related will eventually be included. The goal is that anyone with a design problem, regardless of department or team, should be able to go to our design system and find what they are looking for either directly or be redirected to the correct channel. By creating a unified design platform, we hope to strengthen design internally in how we set up designers and developers for successfully building products and other design content and externally in how we market ourselves and our maturing design culture.Neo4j.design start page;Mar 1, 2023;[]
https://medium.com/neo4j/introducing-scala-cypher-dsl-51d28588cd51;Manish KatochFollowJul 19, 2019·3 min readIntroducing Scala Cypher DSLA type-safe, compile-time DSL for Neo4J Cypher Query Language in ScalaPhoto by Joel Naren on UnsplashWorking with Neo4J and Scala at ThoughtWorks, I had a major qualm regarding the way we were interacting with the graph database using Cypher. Cypher is a declarative language, and hence, it is tricky to compose it programmatically. This is the reason why most of the ORMs (or micro-ORMs) for Neo4J are effective only for simple use-cases. We observed that, as the scale and complexity of our business logic started to increase, our code fragmented into two distinct flavours. There were Scala models and business implementations. And then there were string generation and manipulation methods to generate Cypher queries.String-based queries have inherent issues like no type-safety, minimal syntax checking, difficulty in composing directly proportional to complexity, etc.Scala-cypher-DSL aims to leverage the models created as part of business logic and create Cypher queries intelligently and type-safe manner.InstallationBinary release artefacts are published to the Sonatype OSS Repository Hosting service and synced to Maven Central.SBT me.manishkatoch  %%  scala-cypher-dsl  %  0.4.6 Gradleimplementation group: me.manishkatoch, name: scala-cypher-dsl, version: 0.4.6UsageConsider following domain models representing people working in a fictitious department and friendly by nature.//sample domain modelscase class Person(id: String, name: String, age: Int)case class WorksIn(sinceDays: Int)case class IsFriendOf(since: Int, lastConnectedOn: String)case class Department(id: String, name: String)To start writing query DSL, import the followingimport me.manishkatoch.scala.cypherDSL.spec.syntax.v1._import me.manishkatoch.scala.cypherDSL.spec.syntax.patterns._ //optional, import for expressing paths.using DSL for a simple match query generation for an instance of model//for a person John Doeval johnDoe = Person( AX31SD ,  John Doe , 50)//match and return Neo4J dataval johnDoeQuery = cypher.MATCH(johnDoe)    .RETURN(johnDoe)    .toQuery()johnDoeQuery.query//res0: String = MATCH (a0:Person {id: {a0_id},name: {a0_name},age: {a0_age}})//              RETURN a0johnDoeQuery.queryMap//res1: scala.collection.immutable.Map[String,Any] = Map(a0_id -> AX31SD, a0_name -> John Doe, a0_age -> 50))match Person only by a property(e.g. name)//for a person John Doeval johnDoe = Person( AX31SD ,  John Doe , 50)//match and return Neo4J dataval johnDoeQuery = cypher.MATCH(johnDoe(name))    .RETURN(johnDoe)    .toQuery()johnDoeQuery.query//res0: String = MATCH (a0:Person {id: {a0_id},name: {a0_name},age: {a0_age}})//              RETURN a0johnDoeQuery.queryMap//res1: scala.collection.immutable.Map[String,Any] = Map(a0_id -> AX31SD, a0_name -> John Doe, a0_age -> 50))Note: if the property doesn’t exist, compilation will fail. Yay!using DSL for matching any instance of model.//for any personval anyPerson = any[Person] // any instance of node labelled Personval result = cypher.MATCH(anyPerson)    .RETURN(anyPerson)    .toQuery()result.query//res0: String = MATCH (a0:Person)//               RETURN a0result.queryMap//res1: scala.collection.immutable.Map[String,Any] = Map()query for all the friends of John Doe in Science departmentval scienceDept = Department( ZSW12R ,  Science )val anyPerson = any[Person]val isFriendOf = anyRel[IsFriendOf] //any relation instance of label IsFriendOfval result = cypher.MATCH(johnDoe -| isFriendOf |-> anyPerson <-- scienceDept)    .RETURN(anyPerson)    .toQuery()result.query//res0: String = MATCH (a0:Person {id: {a0_id},name: {a0_name},age: {a0_age}})-[a1:IS_FRIEND_OF]->(a2:Person)<--(a3:Department {id: {a3_id},name: {a3_name}})//               RETURN a2result.queryMap//res1: scala.collection.immutable.Map[String,Any] = Map(a0_id -> AX31SD, a0_name -> John Doe, a3_name -> Science, a0_age -> 50, a3_id -> ZSW12R)for detailed DSL usage and more examples, there is a Wiki.Support and ContributionsScala-cypher-dsl aims to be an important library for anyone who wants to write idiomatic scala when interacting with Neo4J or any other cypher query language based platform.I aim to support 100% of cypher specification and any form of contribution (issue report, PR, etc) is more than welcome!originally published on manishkatoch.me.;Jul 19, 2019;[]
https://medium.com/neo4j/a-deep-dive-into-deep-tech-for-execs-1dc22b686915;Mihai RauleaFollowMar 1, 2019·5 min readA deep dive into deep tech — for execsGraph databases are powerful because they allow for the exploration and analysis of the connections in the data.For SQL, it’s impossible to answer in real-time to a query with 5–7 JOINS. Document-based databases(think Mongo and Couch) lock in place the connections between the data once you establish a relationship between the objects(nesting), you would have to go through 2 entire datasets to uncover a new relationship between 2 objects.For graphs, following an edge(relationship) comes with almost zero cost, which opens up a world of possibilities.Deep tech is hard to explain, and graph databases are no exception.Everything in tech has a language of its own and for good reason. Have you tried having a conversation in a foreign language, with a native, when all you knew were 200 words?Explaining deep tech is more than that it’s not only that one can’t use a word that abstracts what would be put in tens or hundreds of other words it’s also the fact that the implications and ramifications of one concept are huge. After all, every programmer uses a maximum of 20 or so basic language constructs (declaring variables, executing an instruction several times, based on a condition, etc)— but look at how diverse the tech world is.Every flavor of storing data out there does so by using rows and columns something quite similar to Excel spreadsheets. A paradigm that makes relationships between data as important as the data itself exists and the implications are huge.Without graph databases, we wouldn’t have Google its search algorithm runs on top of a graph. Facebook uses a graph internally to store the users and the relationships between them and it’s of paramount importance when suggesting new friends and events. But what is a graph?There you go! That’s a graph. Can you draw that?It labels the nodes, from 1 to 6. It tells us that 1 is connected to 2 and 5. That 4 is connected to 6. That’s it. Thank you!I don’t know the vertical you’re in but let’s assume you know and use social media(Facebook, LinkedIn, etc).Let’s have a look at how you would model a network of people based on who knows who.User 4 knows User 5. User 7 knows user 6. Ignore the arrow direction, for now.Here’s how one would model a network of friends that can author posts, and like each other’s posts. I bet you can now read it on your own. Users are not labeled, in the below. Take the User in the center how many friends does he have? How many posts has he written, and how many has he liked?Now, on to some serious business. Looking at the network, who would you contact to post something about your business? Which of the users is most connected to other users?If only friends can see each other’s posts, and some users meet in person and show posts from their network to each other, what’s the probability of two unconnected users seeing each other’s posts? Just joking.As stated earlier, the implications of this easy to understand tech are remarkable. I won’t bore you with my work on the core of Neo4j or satellite projects — but here are some high-level examples of how I brought ROI as a consultant and even made the use-cases possible.Case study 1 — Adventia Prevention(bioinformatics)Adventia is a personalized medicine tool that suggests treatments. Doctors use it to make better and more informed decisions for their patients.Each doctor has a number of patients, and each patient has blood work data, a history of previous disease and physical attributes(such as age, height, weight). Among other things I did for Adventia(built an interpreter and execution engine for a flavor of a programming language that (even) doctors can write, implemented Bayes probabilities on top of the graph), i made sure their queries were possible(SQL systems wouldn’t support them because of the huge amount of data that needed linking together) and enabled them to ship on time(writing SQL queries for our types of questions would have been too specific, and required several individual queries where graphs could do it with one, more abstract query).All in all, it took 3 months to reshape the future of Adventia. In the end, I made some of their very needed use cases, possible and trained their team to use graph databases — after all this, their team velocity increased. ROI for the win!Case study 2 — Atlas, or how to put the world map in a graphHave you ever wondered what a graph of all the roads in the world would look like? What kind of amazing insights one would be able to uncover if such a graph would be put in a Neo4j database? Amazing information can also be unveiled if one could extract the most connected cities in Europe, and it would definitely come in handy for a vacation planning platform.I have published a dedicated blog post about this with the leading graph database provider, Neo4j. Find it here.Case study 3 — Blue Air, and finding (definite)similarities in web pagesHave you ever looked at how HTML looks? It’s a hierarchical system. My client was employing AI to determine if too web pages are alike, by looking at the source code. The problem? AI works with a degree of uncertainty(heuristics). Graphs can yield deterministic answers.Not only did my client save on computing power, but the system ran smoother after the upgrade, with zero false positives.It seems this is a trend to follow in his book, Connected, Dr. James Fowler argues studying relationships is more important than the data contained in the nodes.Who else is using graphs?ConclusionI believe there is a huge opportunity in the graph space, and there is a lot of value to be created by educating yourself about the space and using graph databases. If you read so far and want to know more, I would be happy to discuss how it could be used in your company. Book a 15-minute call now!;Mar 1, 2019;[]
https://medium.com/neo4j/neo4j-community-is-migrating-from-slack-to-discord-20f7317b449c;Alexander ErdlFollowMar 29, 2021·4 min readNeo4j Community Is Migrating from Slack to DiscordWe have some exciting news to share with you! Over the past weeks and months we have been planning to move our online chat from Slack over to Discord. The Discord platform allows us to create a dedicated space for the Neo4j Community to hang out, text, (video) chat, or stream.Today we are delighted to say: We’ve launched the Neo4j Discord Server and are looking forward to seeing you there!TLDRDiscord gives us the experience we want for our graph community. It allows us to host more engaging experiences and overcome some big Slack limitations where we have our original chat server. To join the Neo4j Discord server.Neo4j on Discord in 5 minutesWatch this 5-minute video on how to get up and running on the Neo4j Discord server.To find out more about why we’re making this change, keep on reading.Why Are We Moving Off from Slack?The initial and biggest motivation for leaving Slack is the 10k message limit. Any and all messages over this threshold are lost. Had a great answer to a question from a few months ago? It may be gone today. With our ever growing community, a post on Slack could disappear in weeks in the very near future.How about going beyond posts. Want to have a video/voice call with a group of people? Perhaps do a screen share as you walk them through your project? Discord allows us to have significantly more functionality, making it even easier to engage with your fellow community members, and much more.¹What Is Discord?Discord is a lot like Slack but with a focus on external communications. While the roots are as a backchannel for playing games, the features are so compelling that it has become popular for many different communities, including vibrant programming communities. With a single sign-in users can easily discover and join any Discord community. Soon enough, they’ll also discover Neo4j 😉.Chat, video calls, live streaming, news and events — we’ll be able to do everything that was possible on Slack, and much more.We started this process by launching a quick, in-promptu survey among our core community members and it looked like many of you are already on Discord!Neo4j Users Slack SurveyNeo4j Discord Quick-Guide#RolesWe have a few roles to make it easier for you to see who is who. Our Core Community Members — the Neo4j Ninjas — deserve a special place and role in our community and therefore are placed at the top in the role @neo4-ninja.The Neo4j team are next. Everybody working for Neo4j is here. You will find @neo4j-team and @admin. You can always ask us if you have problems and we will help you.Neo4j on Discord RolesEverybody else will be @nodes#ChannelsYou will find a wide variety of categories and channels so you can stay up to date with Neo4j announcements, as well as interact in conversations on all kinds of topics from the graph world and beyond.As well as instant messaging, you can also join a voice channel to talk to people. You can even transform that into a screen share collab session with a simple click.Neo4j on Discord#Greta — The Graph GiraffeOur friendly Bot Greta — The Graph Giraffe helps us in Discord to post when we are live on Twitch or when a new blog post (just like this one) is published. You will find her in many places and can also interact with her by using our commands (try !dev )NODES 2021 Call for Papers Drop InGot a question about NODES 2021 Call for Papers? Drop your message in the Neo4j-events → #nodes channel, or find one of us hanging out in the #watercooler audio channel.AMA with the Neo4j TeamTo celebrate the launch of this new server we will be running a couple of Ask Me Anything” open office hours events over the next few weeks where you can join and chat with the development team behind Neo4j on all kinds of topics. Keep an eye out for more news!We look forward to seeing you on our Discord chat!If you don’t have an account yet, it’s quick to register one, as soon as you’re ready to join us on the Discord server.¹ https://orbit.love/blog/how-to-migrate-a-community-from-slack-to-discord/;Mar 29, 2021;[]
https://medium.com/neo4j/flights-search-application-with-neo4j-using-cypher-and-apoc-custom-procedures-part-2-401fd90bf5c4;Vlad BatushkovFollowJan 15, 2020·14 min readFlights Search Application with Neo4j —Using Cypher queries and APOC Custom Procedures (Part 2)How to write your own APOC Custom Procedures & set it up in Docker using Cypher-shellPart 1: DokerizingTargetIn this series of articles, I will share my experience of building a simple web application that you can use to search for flights. The application was built for Neo4j Workshop in Bangkok, November 5, 2019.The general idea is to have a page with a search box where you can define a place of departure and destination, choose the date and voila — a list of available flights would appear below. Our needs are limited by only a one-way flight, no class options, and no other fancy options. A brutal one-way flight for MVP. Solving this task includes doing one by one smaller sub-tasks.The Flights Search Application is a perfect use-case for building using GRANDstack framework and the how to do it” will be covered in the third part of this series (next part).In this article, I will show you how to create your own APOC custom procedure (similar to Microsoft SQL Server (MSSQL) stored procedures) to query for Flights, which takes into account real-life travel requirements as much as possible. Also, I will explain how to setup procedure and index scripts inside a Docker Image using Cypher-shell.Remark: If you are not interested in Flights Search query details at all, but would like to see how to pack Docker Image using Cypher-shell — you can skip to the last section: Setting up Cypher-shell in Docker”.Database SchemaIn the previous article, I explained how to build a Neo4j Docker Image with a Flights Database using the neo4j-admin import tool. To look at the Flights Database, simply pull my docker image and locally run the container. The image contains imported data for January 2020.docker pull vladbatushkov/neo4j-flights:latestdocker run -p 7474:7474 -p 7687:7687 -v c:/neo4j/data:/data -v c:/neo4j/logs:/logs vladbatushkov/neo4j-flights:latestThe Flights Database is based on Max De Marzi’s Flight Search schema. Max implemented his search as User Defined Procedure, written in Java. I would like to build the same functionality in Cypher.Max De Marzi original content. Flight Search Proof of Concept Database Schema.Database ImprovementsSince the date of publication of the first part of this series, I made several critical changes to my graph. The Flight node now contains local date-time of departure and arrival. For example, these properties in Flight Node from San Diego, the USA to Tokyo, Japan:...departure_local: 20200101T12:54:00,-0800,duration: 13h 57marrival_local: 20200102T19:51:00,+0900...This also means that for a flight that departs today” and arrives tomorrow” the nodes are now connected to two different AirportDay nodes representing different days (see the picture below). I also fixed the relationship [*_FLIGHT], naming it with the Airport of arrival, not with the departure Airport.Example of Sub-graph with data from Moscow SVO to Bangkok BKKI am also thinking about segregating City nodes, maybe you can help me with this feature.Query StrategyFlights Search is a graph-oriented problem with a graph-oriented solution. Finding flights requires a traverse through all possible paths of the graph to find those, those match criteria.Initial results can look like this: a single direct flight with an appropriate departure date from Airport A to Airport B.Example of Sub-graph with data from Bangkok BKK to Moscow SVOMax De Marzi’s strategy to solve this problem is to split the search query into two phases. First phase — find all paths between Airports in the upper part” of the schema.The upper part of SchemaThe second phase — using Airports go to the lower part” of the schema and find all the related flights.The lower part of SchemaSounds easy, doesn’t it? But don’t rush to plan your evening just yet. Reality is cruel.Upper PartAirport FLIES_TO Relationships exampleMATCH (a:Airport)WHERE a.code IN [ TXL ,  MUC ,  STR ,  DRS ,  BRE ]RETURN aIt’s not a flight search query, it’s just a query to get data for the picture.Let’s analyze a small example flights in Germany. In the picture above several cities are connected, thus potentially giving us many options to fly from Berlin-Tegel Airport to Munich Airport:TXL (Berlin) → MUC (Munich)TXL (Berlin) → STR (Stuttgart) → MUC (Munich)TXL (Berlin) → STR (Stuttgart) → DRS (Dresden) → MUC (Munich)TXL (Berlin) → STR (Stuttgart) → BRE (Bremen) → MUC (Munich)Lower PartPath discovery is the heart of the query. Let’s go deeper and find all possible flights between each pair of Airports for all our paths.Flights between some German Airports for Jan 1, 2020WITH [ TXL ,  MUC ,  STR ,  DRS ,  BRE ] as codes,  20200101  as dateMATCH (a:Airport)-[:HAS_DAY]->(ad:AirportDay)--(f:Flight)--(bd:AirportDay)<-[:HAS_DAY]-(b:Airport)WHERE a.code IN codes AND b.code IN codes AND ad.code ENDS WITH date AND bd.code ENDS WITH dateRETURN *It’s not a Flight Search query, just a query to get data for the picture.Now you can start to imagine the whole picture. There are six direct flights from Berlin-Tegel Airport to Munich Airport, and other flight options:TXL (Berlin) → 6 flights → MUC (Munich)TXL (Berlin) → 6 → STR (Stuttgart) → 3 → MUC (Munich)TXL (Berlin) → 6 → STR (Stuttgart) → 2 → DRS (Dresden) → 2 → MUC (Munich)TXL (Berlin) → 6 → STR (Stuttgart) → 2 → BRE (Bremen) → 2 → MUC (Munich)Knowing all the different flights available, creating all combinations of these would give us all the possible results. For example, TXL → 6 → STR → 2 → BRE → 2 → MUN can give us 24 possible combination flights to book. For all four Airports routes described above, in total we have 6 + 18 + 24 + 24 = 72 possible flights route results.But all these results are just possible”. The next filter conditions (we can call them requirements) make flights searches a bit closer to the real solution.Flights RequirementsDeparture After ArrivalWhen we have flights from place A to place B and then from B to C, we should exclude flights that leave from the next leg before we’ve arrived to catch it. For example, we should exclude flight B to flight C if flight A has not arrived yet to B. We can also consider having some between flight Airport time buffer (e.g. an hour) as a comfortable time to eat a burger. We may even need more time before the next flight. Anyway, this custom Airport transfer time-window can be an interesting feature for our service.Next-Day ArrivalUsually, international flights cross several time zones. Time-aware search is our must-have feature. In the trip from A to C via B, when we fly from place A to place B we can reach B on the next” day. That means a departure from place B to C should involve a search for matched flights on that next” date.For example, you are flight from Buenos Aires, Ezeiza International Airport to John F. Kennedy International Airport, New York.Next day Arrival EZE → JFK. All values was generated by my console app, using random & heuristic assumptions.Boundary Of DatesBigger complication come when we consider arrivals around midnight. For example,if you arrive at 22:00 , you have 2 hours on this” day and the time of the upcoming next” day. The search period should consider both today and next” day flights.Remark: This requirement is still not covered by my Flights Search Query. You can help me to develop this feature.Irrational TransferWhen we are looking for all possible paths between place A to place C via place B, we should exclude place B, if B’s distance is irrationally far. It would be weird to suggest a user buy a ticket on a flight from Paris to London with a transfer in Moscow.I decided to compare total flight distances with +5% of the direct distance to handle these irrational transfers. Of course we could use something much smarter to calculate the threshold.distance(A, B) + distance(B, C) <= distance(A, C) * 1.05Same CityOne more location-dependent filter-out condition to consider: paths should not include Airports from the same city. This can happen for trips with 2 Airport transfers. For example, Airport A (Berlin) → Airport B (Stuttgart) → Airport C (Berlin) → Airport D (Munich). For sure, we should seek to exclude such a route from the result list.Flights Search QueryTime to write a query. First of all, let’s define our input parameters. The minimal required parameters for a one-way trip are the two Cities and Departure date.UI powered by agoda.com/flightsPath-findingPath-finding is a natural and easy task for Neo4j. With the City From” and City To” parameters we can already find all necessary paths. This part of the query does not depend on the departure date, but it will be used soon.As you can see, I have set a maximum number of hops to 3 possible [:FLIES_TO] relationships between Airport nodes. This means I have limited my solution to 2 possible Airport transfers in between my departure and destination locations. Realistically speaking, I am most likely to fly with just 1 Airport transfer, but of course 2 Airport transfers is a very reasonable scenario that needs to be covered.path = ((a:Airport)-[:FLIES_TO*..3]->(b:Airport))The variable path keeps a chain of nodes and relationships based on our pattern, with only Airport nodes connected with [:FLIES_TO] relationships. The resulting paths are a combination of direct flights, flights with 1 Airport transfer and flights with 2 Airport transfers.[[A,→,D], [A,→,B,→,D], [A,→,E,→,D], [A,→,B,→,C,→,D], … ]By doing nodes(path) we can select just the nodes from these paths, called routes:[[A, D], [A, B, D], [A, E, D], [A, B, C, D], … ]A query for flights from Moscow to New York returns 7549 possible routes. Too much to be true, right? But it is a truth with only one condition: the list includes a lot of not valid” results from the point of good sense. Do not blame the query, it’s just doing what we asked of it. Let’s apply some requirements and filter out the not valid” outputs.Irrational TransferWe have city’s geographical location in the initial data, together with standalone City nodes. To calculate an approximate” City location we will use the average location of all city’s Airports.This is another Database Schema improvement to implement, but for now, I will add a distance computation to my query.Same CityApplying the Same city” business requirement is quite easy with the help of APOC. Counting the city name duplicates in the list should be equal to 0 (no duplicates). Both requirements added to the previous search query.This query gives us much more realistic result of just 78 paths that we need to process in the next step. As you can see, a lack of meaningful requirements leads to totally wrong results.IndexWe already touched on property-based filtering, so I recommend setting all required indexes to improve query performance. I expect to have the parameter of a departure date, so I need an index for AirportDay filtering by the code property.CREATE INDEX ON :Airport(city)CREATE INDEX ON :AirportDay(code)Direct FlightsDirect flights are flights that only have 2 Airport nodes in the path. Nothing complex here. From the list of all paths, we will return these direct flights with the simple (AirportDay → Flight) pattern.MATCH (ad:AirportDay)-[r1]->(f1:Flight)This is a temporary query, that contains a path with direct flights.Transfer AirportsNow using the same approach we will find a collection of Flights for routes involving 3 to 4 Airports. The WHERE condition should support the requirement Departure after Arrival”.I also want to be able to choose the transfer time-window, that I mentioned before. When we arrive to some transfer” Airport we can decide how long we’d like to remain there until our departure. So the query input parameters would look like this:{ a:  Moscow , b:  New York , date:  20200101 , tmin: 1, tmax: 6 }Plus the final step to combine all available flight routes into one result list. You can append the Flights Search query with PROFILE statement to see the execution plan.An important note, all three matches are OPTIONAL as we don’t know whether all direct flights exist or not. This is the same situation for more than one-hop transfers too.Query Performance LessonEverything seems to be straightforward with my query, but it does take around 16 seconds to find results.The root cause of this slower performance is by the unexpected NodeByLabelScan. Here is a snapshot of the Execution Plan (generated from PROFILE) for the part with the longest pattern of 2 Airport transfers (before the fix).NodeByLabelScanI already have an Index for AirportDay(code), so why isn’t NodeIndexSeek being used?The remedy and explanation for this behavior was found in the old good article Tuning Your Cypher: Tips & Tricks for More Effective Queries” written by Petra Selmer & Mark Needham, Feb 10, 2016.In graph databases, Indexes are only used to find the starting point for queries.This explains everything. I need to tell my query to use indexes for the intermediary nodes.MATCH (ad:AirportDay)-[r1]->(f1:Flight)-[r2]->(bd:AirportDay)-[r3]->(f2:Flight)-[r4]->(cd:AirportDay)-[r5]->(f3:Flight)USING INDEX bd:AirportDay(code)USING INDEX cd:AirportDay(code)Using the indexes, it can find the results just in 4 seconds using one month (Jan 2020) of data containing 1 237 759 nodes and 3 376 990 relationships.If you have more ideas on how to improve the performance of my query — feedback is welcomed. Here is a print screen of my query execution plan:Setting up Cypher-shell in DockerHaving a query is wonderful, but it is not a full solution yet. The last question is: how can one use this query monster inside my Application? Inline hard-code in application code-base? Maybe. But can we do better?If you are aware of Relational Databases and MSSQL, you will be aware of the concept of Stored Procedures. This database level solution has been widely used for decades in the relational databases world. It looks like a handy solution to me. Let’s create a custom procedure using the APOC library.APOC Custom ProcedureWrapping Cypher queries as a procedure is a really easy thing to . The documentation provides a very clear explanation of how to do it. All you need is to define a name, description, input and output parameters for your query.CALL apoc.custom.asProcedure( getFlights , // name WITH { a: $a, b: $b, date: $date, tmin: $tmin, tmax: $tmax } as params... , // totally same queryread,[ [result,MAP] // output alias with type],[ [a,STRING], // input args with types [b,STRING], [date,STRING], [tmin,INT], [tmax,INT]],get flights with 0..2 transfers) // descriptionHere is a link to the gist for those of you who would like to see the full version. The procedure is created under a custom namespace. Now you can easily CALL it.CALL custom.getFlights(Moscow, New York, 20200101, 1, 4) YIELD result RETURN resultYIELD will iterate using output alias that was declared — the result, in my case.Cypher-shell in DockerIn the first part of this blog post series I explained how to import data into your Neo4j Docker Image. The import process is executed before the Neo4j service starts. However, when we are setting up procedures and indexes, they are initiated after the Neo4j service has started.Setup your custom procedures and indexes right after Neo4j has startedI will use the solution for such a non-trivial bash-scripting task created by Shaun Elliott. Following his approach, I will set up three things for my neo4j-flights image: a procedure and 2 indexes.DockerfileFROM vladbatushkov/neo4j-apoc-algo-graphql:latestENV NEO4J_dbms_security_procedures_unrestricted=apoc.*,algo.*,graphql.*ENV NEO4J_dbms_active__database=flights.dbENV NEO4J_AUTH=neo4j/testCOPY import/*.csv import/COPY import.sh import.shENV EXTENSION_SCRIPT=import.shCOPY getFlights.cql getFlights.cqlCOPY setup.sh setup.shCOPY wrapper.sh wrapper.shENTRYPOINT [  ./wrapper.sh  ]getFlights.cqlI prefer to use .cql (Cypher query language) extension, but it also can be .cypherCALL apoc.custom.asProcedure( getFlights , WITH { a: $a, b: $b, date: $date, tmin: $tmin, tmax: $tmax } AS params... // Query bodyRETURN resultORDER BY result.stops , read,[[result,MAP]],[[a,STRING], [b,STRING], [date,STRING], [tmin,INT], [tmax,INT]], get flights with max 2 transfers)wrapper.sh#!/bin/bashset -m/docker-entrypoint.sh neo4j & ./setup.shfg %1setup.sh#!/bin/bashcreate_index(){  echo  CREATE INDEX ON :$1($2)   until cypher-shell -u neo4j -p test  CREATE INDEX ON :$1($2)   do    echo  CREATE INDEX ON ($1) FAILED, SLEEPING     sleep 10  done  echo  CREATE INDEX ON ($1) COMPLETE }create_procedure_getFlights(){  echo  CREATE PROCEDURE getFlights   until cat getFlights.cql | cypher-shell -u neo4j -p test --format plain  do    echo  CREATE PROCEDURE getFlights FAILED, SLEEPING     sleep 10  done  echo  CREATE PROCEDURE getFlights COMPLETE }while true do  sleep 5  if curl -s -I http://localhost:7474 | grep -q  200 OK  then    echo  Setup Start     create_index  Airport   city     create_index  AirportDay   code     create_procedure_getFlights    echo  Setup End     break  else    echo  Not Ready for Setup     continue  fidoneHow to use Cypher-shellThe simplest way to use Cypher-shell is to run inline execution, as I did for the index creation.cypher-shell  CREATE INDEX ON :Airport(city) If your query is not a one-liner, or you want to process a bunch of queries, you will probably want to keep the queries as separate files. Is it possible to ask Cypher-shell to run a query from a file? — Yes. But we need to read the file contents first and then send it to Cypher-shell. It’s going to be something like this:cat getFlights.cql | cypher-shell --format plainBe sure that each query ends with a semicolon. For example in indexes.cqlCREATE INDEX ON :Airport(city)CREATE INDEX ON :AirportDay(code)cat indexes.cql | cypher-shell --format plainFinally, let’s start Docker Container and open Neo4j Browser. Our getFlights procedure is in place and working as expected. By the way, you can easily test it using Neo4j HTTP API.End of Part 2Thanks for reading!In the last article of this series, I will share with you how to build a Search for Flights Web App using the GRANDstack Framework. Clap-clap-clap for every flight with transfer in your life.Photo by Juhasz Imre from PexelsResourcesvladbatushkov/flightsCypher queries Original .csv files from openflights.org Dotnet core C# console application to generate .csv files of…github.comFlight Search with Neo4jI think I am going to take the opportunity to explain why I love graphs in this blog post. I’m going to try to explain…maxdemarzi.comThe Neo4j Cypher Manual v3.5This manual covers the following areas: Chapter 1, Introduction — Introducing the Cypher query language. Chapter 2…neo4j.comAPOC User Guide 3.5This is the user guide for Neo4j APOC 3.5, authored by the Neo4j Labs Team. The guide covers the following areas…neo4j.comTuning Your Cypher: Tips & Tricks for More Effective QueriesEditor’s Note: Last October at GraphConnect San Francisco, Petra Selmer and Mark Needham — Engineers at Neo Technology…neo4j.com13.6. Custom, Cypher Based Procedures and Functions — Chapter 13. Cypher ExecutionI wanted for a long time to be able to register Cypher statements as proper procedures and functions, so that they…neo4j-contrib.github.io12.10. Cypher Shell — Chapter 12. ToolsThis section describes Neo4j Cypher Shell.neo4j.com2.1. Begin and commit a transaction in one request — Chapter 2. Using the HTTP APIIf there is no need to keep a transaction open across multiple HTTP requests, you can begin a transaction, execute…neo4j.com;Jan 15, 2020;[]
https://medium.com/neo4j/better-neo4j-plugins-for-kettle-a7453ba71814;Matt CastersFollowSep 6, 2019·3 min readBetter Neo4j plugins for KettleDear Neo4j and Kettle friends,Last week we released version 4.1.0 of the Neo4j plugins for Kettle. It’s just a point release!”, you say. Does it really warrant a whole story?”, you ask. I think that in this case we need to tell the story…The thing is, after the many changes in 4.0.0 life in Kettle land was great for Neo4j users. Loading data was working and bugs were few and far between.However we can always do better and most notably there were some fundamental changes that needed to happen to the Neo4j Output and Cypher steps.The Neo4j Output stepThis step is used a lot but in a couple of scenarios, most notably where MERGE statements were generated and executed, performance simply was not optimal. So major changes needed to happen in the code of the step to increase performance. Now, UNWIND statements are used all the time to speed up operations, even if it’s hard to do so. I hope you’ll find that your existing transformations simply run faster now.Icon of the Neo4j Output stepThe Neo4j Cypher stepThis step needed a few key changes to make sure that it played nicely with more complex clustering scenarios while loading data in transactions. It also prepares the code to handle the oncoming API changes for Neo4j server 4.0.When returning data from Neo4j, the Cypher step also needed extra information about the source property data type so that data conversions can be done a more consistent and safe way.Note the extra (optional) Source Type” column for the return values. It allows for more accurate data conversion.Hunting bugs!Every good programmer knows that if you make any serious changes to a code-base that you introduce errors or, even worse, compatibility issues.To make this easier to deal with going from release to release and to protect our users from nasty issues we created a new project called kettle-neo4j-integration with the sole purpose of validating that the Neo4j steps are working correctly.The way we do this is as follows:First load data into Neo4j with a particular step.Read the data back from Neo4j.Verify that the data is exactly as expected.A sample integration test jobThis project obviously uses Kettle Unit Tests, part of the data sets plugin. It makes it easy to write more tests in the future. While writing the various integration tests we found and fixed various issues already making 4.1.0 a better, more stable release. From now on, every time we find more issues we will write more integration tests to make sure we don’t repeat mistakes.Given the many positive benefits, I would actually encourage you to write your own integration tests for the critical parts of your own Kettle projects. It will allow you to protect your investments and requirements against inadvertent future changes. It doesn’t take much work and it pays off in the long term. Writing unit and integration tests is just a good idea. You can fairly easily run the integration tests on Neo4j and Kettle, both running in a docker container. In a next story I’ll explain how you can do that.Get the goodiesFor your convenience I’ve set up my old website kettle.be from which you can easily download pre-built releases of Kettle with all the latest plugins. In fact, the Kettle Neo4j Remix downloads have everything you need to work with Neo4j, Unit and Integration tests and much more.Links to the various plugins and projects can be found on kettle.be as well.Have fun with Neo4j and Kettle!Cheers,Matt;Sep 6, 2019;[]
https://medium.com/neo4j/introducing-the-new-graphacademy-45b0df491a23;Adam CowleyFollowSep 20, 2021·7 min readIntroducing the New GraphAcademyThe new GraphAcademy platform!Over the past couple of months, we have been working on a new version of Neo4j GraphAcademy, a free, self-paced online training platform. I’m delighted to announce that the first two of five beginners courses are now available along with a brand new platform!The Neo4j Fundamentals course is aimed at anyone — technical or non-technical — who is interested in learning at a high level about Neo4j. The Cypher Fundamentals course then guides you through how to query Neo4j using the Cypher query language.I thought I would take a few minutes to walk through what we did, why we did it, and how we did it.Start with Why?GraphAcademy was caught in the tailwind of a refresh of the Neo4j Developer Guides and Docs in 2019.A wet and rainy Saturday afternoon during the lockdown in Swindon, UK working on a couple of potential design ideas for our docs led to a full overhaul of our documentation publishing and deployment steps. We spent some time migrating the Developer Guides over to an open-source documentation generator called Antora.Antora allowed us to bin a lot of legacy publishing techniques, which involved turning Asciidoc files into HTML and merging with the neo4j.com website using the WordPress API. The process was long and cumbersome — no one really seemed to understand how it worked, and it took a long time to complete.Antora solved a lot of these problems for us, and now we could build a set of static HTML files in a matter of seconds.GraphAcademy was quickly included with this migration featuring a modified version of the new layout. The site used Auth0 for client-side authentication and a set of lambdas to store lesson progress and generate PDF certificates. Things were a little all over the place, but they worked. My main grievance was that the UI flickered on sign-in.The previous version of Neo4j GraphAcademy, featuring a very grey layout.Aside from the login flickering, there were a few issues that we needed to address.The courses on GraphAcademy very closely mirror the training that I had delivered in my previous role as a Professional Services consultant, so I had some initial thoughts on where we could improve things. But rather than let my biases lead the improvements, I thought I would reach out to our large developer community to first gather their feedback.User InterviewsI came with a list of pre-canned questions, but let the conversation flow as much as possible:What is your job role?What was your experience with Neo4j when you started GraphAcademy?Did you have a use case in mind?How did you find out about GraphAcademy?What was your motivation for choosing GraphAcademy?Which course(s) have you taken? Do you plan to take any others?Did you find the information you were looking for in the Docs or Developer Guides?What do you think about the content of the course(s)?The outcome of the User Interviews was broadly positive the main feedback that I received was that the courses were good, if not a little long. We were doing a good job of providing comprehensive content completely free of charge, if not presented in the most attractive way.Users also highlighted that GraphAcademy was a little overwhelming. There were a lot of courses, but we hadn’t really made it clear where to start.Data AnalysisAlong with the qualitative feedback from the community, I also looked at previous enrollment data to gain some quantitative insight into how learners were using the site.Our completion rates weren’t bad, but were still a few percentage points short of the industry-standard completion rates. Users were also picking and choosing their courses, which was driving down the completion figures as users dropped out along the way.Our end goal was to get users to complete a certification, and these rates were for the most part good — although possibly fueled by our offer of a free t-shirt to all Certified Neo4j Professionals!Redesign GoalsWith all of this in mind, it would be disingenuous of me to describe this as a revolution. The content that I had inherited was in good shape, but we can always do better. The key points I took from the research stage were to:Improve the look and feel of the site — get rid of the grey.Shorten the length of courses into smaller chunks, which would make courses easier to come back to, and in theory, improve completion rates.Multiple-choice questions are a little repetitive, and could probably be guessed.Provide a more interactive experience to make the courses more fun, but with the added benefit of improving the learner’s comprehension and recall of information.Provide a clear learning path towards certification.The new courses should all work with Neo4j Aura, Neo4j’s Database-as-a-Service platform.The SolutionIf I have already piqued your interest, you can view the new site at graphacademy.neo4j.com.When you first visit the site, you are immediately presented with a recommended set of courses based on your experience level and job role. For beginners, there are a set of courses that cover the fundamentals:Neo4j Fundamentals — a course aimed at beginners, whether technical or non-technical, who want to learn what the fuss is all about. (30–60 mins)Cypher Fundamentals — an interactive course designed to give you a grounding in Cypher, featuring videos, quizzes, and code challenges. (60–90 mins)Graph Data Modeling Fundamentals — everything you need to know to create performant graph models in Neo4j. (90–120 mins)Importing Data into Neo4j — a hands-on introduction to importing data into Neo4j, including exercises for importing CSV and JSON. (60–90 mins)The rest of the curriculum is curated by job role, with courses designed for data scientists, developers, and administrators.Shorter CoursesEach course has been developed from scratch, with content split into modules and lessons. Rather than having to pick up where you left off mid-paragraph, the bite-sized lessons should be achievable in 5–10 minutes.Interactive Code ChallengesEmil Eifrem is either a very talented CEO/Actor, or someone is having a laugh…The majority of the courses are designed to provide a hands-on experience of Neo4j.When you enroll in an interactive course, a new database instance is created in Neo4j Sandbox, prepopulated with a graph of data.As you learn, you are taken through a continuous loop of learning information, recognizing that information, and then actively recalling that information.For example, in the Cypher Fundamentals course, you are asked to find which movie Emil Eifrem has acted in, or subsequently to remove Emil’s node from the database.Share Your ProgressPreviously, we generated a PDF certificate, but now your certificates appear on a public profile that you can share with your friends, colleagues, or potential employers. As you continue through GraphAcademy, new badges are added to your profile.You can track my progress through my public profile here.As well as digital rewards, learners will also be able to earn physical rewards including stickers, badges, and limited edition t-shirts. But more on that soon!Coming SoonIn the early stages, you may see that many courses are listed as Coming Soon. We have decided to develop the curriculum in the open and give our community the ability to affect our road map. You can register your interest for any course listed as Coming Soon, and we will take this information into account when we prioritize our work. We are a small team so this feedback is invaluable.We Want Your Feedback!We would love to hear your feedback on the new courses — whether you have taken a course previously, or are a newcomer to Neo4j or GraphAcademy. We have added feedback widgets to every module and lesson page, where you can tell us whether the page is useful, and if not, how it could be improved.All of this information is fed into a dashboard (built with Charts) that we constantly monitor.Limited Edition T-shirt OfferComplete the Neo4j Fundamentals and Cypher Fundamentals courses within a week and provide feedback and you could be sporting a limited edition GraphAcademy t-shirt inspired by Experimental Jetset’s John & Paul & Ringo & George” design.Simply complete the courses, use the feedback widget to provide some feedback, and send an email to graphacademy@neo4j.com with a link to your profile and any likes or dislikes about the experience before 23:59:59 PT on Sunday 31 October 2021.;Sep 20, 2021;[]
https://medium.com/neo4j/neo4j-4-x-query-logging-enhancements-7943d7f4ac7f;Kees VegterFollowDec 21, 2020·6 min readNeo4j 4.x Query Loggingand how the new features in Query Log Analyzer 1.0.2 support thatSince Neo4j version 4.0 the query log possibilities have been extended. Meanwhile version 1.0.2 of the query log analyzer was released.Photo by Isaac Smith on UnsplashWe will show how this release supports these new query logging features and how it is now possible to read the log files directly from the server.Please note that Query Logging is only available in the Enterprise Edition. But that edition is available for free in Neo4j Desktop and for startups.Quick Intro Query Log AnalyzerThe Query Log Analyzer is a tool that allows you to analyze Neo4j Cypher Query logs, that contain runtime information about queries executed by the server (query, runtime, parameters, driver information and more). It allows you to visually render those queries in tables and charts to determine and drill down on problematic queries.There have been a number of past articles about features of the tool, please find them all here:Kees Vegter - MediumThe Query Log Analyzer is a Neo4j Desktop App to help you to understand the query log file of a Neo4j Enterprise…medium.comYou can install the query log analyzer in Neo4j Desktop, it works both with local databases, as well as with log files uploaded to the tool as well as remote databases that have log streaming (via APOC) enabled. You can install the tool from the Graph App MarketplaceNeo4j Developer Tools App Gallery: One-Click InstallOne click install of additional apps for Neo4j Desktop from an curated selection.install.graphapp.ioQuery Log ModeIn Neo4j versions below 4.x we could only switch the query logging on or offFrom version 4.x we have an new mode called ‘VERBOSE’. Instead of specifying ‘true’ or ‘false’ for the configuration parameter ‘dbms.logs.query.enabled’ , we must specify, in version 4+, one of the following values ‘VERBOSE’, ’INFO’ and ‘OFF’.OFFThis will switch off the query logging completely. This is the same as ‘dbms.logs.query.enabled=false’ in previous Neo4j database versions.INFOWhen the value is INFO then the query logging has the same behaviour as we know from the previous Neo4j database versions. Queries which take more time than the given treshold will be logged after they are finished. Note: a finished query can also be finished with an ‘error’.VERBOSEIn this mode every query will be logged when started AND will be logged when finished. This setting will ignore the threshold, and is the default setting.Logging the queries when started makes it possible to track the queries which started but never finished. For instances queries that were running on the server while the server was restarted. This is useful in scenarios when there are bad queries taking all the resources on the server, which could make the server non-responsive. Restart is then sometimes the only option when you don’t have time to wait.Note that since Neo4j version 4.1 there are more possibilities to control ‘bad’ queries. Before that version there was already the option to set a transaction timeout, which aborted statements that were running longer than a given time.With 4.1 it is now possible to set the maximum size of memory used in a transaction. When then a query hits the limit, it fails with an error without causing the server to become unresponsive. The default for a single-instance Neo4j server is ‘no maximum’. The default for a Neo4j Cluster is 2G. More information about memory limiting can be found here.Query Log Analyzer 1.0.2With this new release we support now the VERBOSE mode completely with the possibility to show the ‘unfinished queries’ and also enhance the usage of the tool by making it possible to change the query-log related configuration parameters dynamically and to stream the query log files directly from the server(s).Unfinished QueriesWhen the query log is in VERBOSE mode, the start and the finish of the query is logged. In this tab the queries are shown which are not finished when the server is stopped or restarted or at the moment the query log is streamed or copied. This is useful to find problematic statements that don’t finish properly.Dynamic Query Log ParametersFor a number of versions now, it is possible to change some database configuration parameters while the database is running. The Query Log Analyzer now has the ability to change these dynamic query log parameters in the ‘Query Log Settings’ tab.If a property is not ‘dynamic’ or the ‘neo4j’ user does not have the proper permissions to change the settings, then the settings are displayed in a read only form.The Query Log Analyzer now supports cluster configurations with the possibility to set the query log parameters for every cluster member.Note @1, @2, @3 are referring to the list of connected servers in the left bar.Log StreamingBesides the existing functionality of uploading a query log file we added now the Analyze Current Log” button.When clicked, a procedure is called on the database to stream the contents of the query log file directly to the analyzer. The possibility to change the query log settings dynamically and the possibility to stream the current query log file makes it easy to temporarily switch on the query logging and analyze the query log directly without restarting the server and copying the query.log file from the server and upload it in the analyzer tool.The streaming of the log file also supports a cluster setup. Then all the log files of the cluster members are retrieved and analyzed in one step giving the following result in the ‘Query Analysis’ tab.Note neo4j@1, test@2, neo4j@3 are referring to the database and the list of connected servers in the left bar.The Analyze Current Log button is only available when there is a connection to the database with the proper permissions and the APOC library installed. The procedure apoc.log.stream() must be available in the server setup.Note that only the current query.log file is streamed, by default the query.log file follows a rotation plan where 7 log files are kept and the log file rotates when it reaches 20MB.If inspection of those ‘rotated’ log files is needed, you have to upload them manually in the analyzer tool.Showing ErrorsFor a number of Neo4j versions errors are also logged in the query.log file They will now be shown in red in the Query Log AnalyzerFilteringIn the Query Analysis, Query Log and Unfinished Queries tabs it is now possible to filter the content, to quickly find a specific query. Just enter characters in the Filter fields and hit enter to start the search.Incremental search filters in the query log analyzerResourcesWhen you want to have more insight in how to optimise the performance of your Cypher queries take a look at this post.You can also take the Cypher Query Tuning - Online Training that teaches a lot of important skills in this area.Cypher Query Tuning in Neo4j 4.0 - Cypher Query Tuning in Neo4j 4.0Learn how to get the best performance out of your Cypher queries.neo4j.comThe source code for the Query Log Analyzer is on Github at kvegter/query-analyzer-app. There you can read the documentation and report issues.kvegter/query-analyzer-appThe query log analyzer is a Neo4j Desktop App to help you to understand the query log file of a Neo4j Enterprise…github.comIf you have questions regarding the query performance, you can always head to the #help-cypher channel on the Neo4j Users Slack or on the neo4j community.;Dec 21, 2020;[]
https://medium.com/neo4j/securing-your-graph-with-neo4j-graphql-91a2d7b08631;Daniel StarnsFollowJul 7, 2021·8 min readSecuring Your Graph With Neo4j GraphQLGood day, all. Allow me to navigate you on a journey through Authentication and Authorization in the land of Neo4j GraphQL. Bind your acquired knowledge of our GraphQL directive to Type Definitions, where applied, enables sophisticated auth patterns.This blog is the written format of my NODES 2021 talk, which you can also watch if you rather do that:What is Neo4j GraphQL?The Neo4j GraphQL Library is a GraphQL to Cypher query execution layer for Neo4j and JavaScript implementations. To use the library, you provide some Type Definitions and it will generate a GraphQL schema. The schema contains CRUD resolvers that produce Cypher and are backed by a Neo4j instance.Once you have the schema, you can pass it into any compatible JavaScript GraphQL tool, such as Apollo Server, and have a functional API ready for your clients. This means you dont have to spend time writing repetitive resolvers or logic, thus enabling you to focus on building great applications.Find out more about the Neo4j GraphQL library hereAuth in GraphQLIn GraphQL there isnt one way of going about auth, and its highly dependent on your business requirements and needs. Due to GraphQL being a newer technology, and highly different from REST, lots of patterns need to either be replicated or repeated. There are, however, some really cool GraphQL tools and patterns for auth — though, many can be cumbersome and slow. Most either lead to surplus database calls or extra resolver invocations.Due to the fact that Neo4j GraphQL aims to produce a single Cypher query from a given GraphQL operation, we wanted to roll our own auth solution so that we could embed rules into the Cypher query. Rolling our own auth means its easier for you, as a developer, to get started, plus it increases performance due to the reduction in database calls.Auth in Neo4j GraphQLThe term auth” is encapsulating both authentication and authorization. For the most part, you will use our GraphQL directive for authorization, and for authentication, we suggest using the OGM, which you will learn about shortly.Throughout this blog we will use the following data model:Made with https://arrows.app/That can be expressed with the following GraphQL type definitions:The model is arbitrary and I will add and remove fields to this as and when needed.Authentication in Neo4j GraphQLJSON Web TokensThe Neo4j GraphQL library uses JSON web tokens. Below is a JSON representation of an arbitrary token:Typically you will see the JWT attached to the authorizationheader of an HTTP request:In the example above, the JWT has been encoded. https://jwt.io/ is a great tool for playing around with JWT’s.When setting up your Neo4j GraphQL server you should dependency inject the incoming request into the context so we can pick up on the headers:Injecting the reqhere allows the Neo4j GraphQL directive to pick up on the authorization header.Due to the fact the library uses a JSON web token, you can authenticate users however you would like. One may use Auth0, keycloak, okta, or other authorization services.Here we will use the Neo4j GraphQL OGM to authenticate users using a custom resolver:OGMThe OGM is a wrapper around the Neo4j GraphQL API. You use the OGM by supplying some GraphQL Type Definitions, the same ones as your API, and out comes a class instance with lots of methods. Each method you invoke will call the relevant GraphQL query or mutation, and thus when invoked, Cypher will be produced and executed.Below is an example of setting up the Neo4j GraphQL OGM:Above, you can see we invoke the .model() method on the OGM. For each type in your definitions, you can receive a model. In our schema we would have access to all three: User, Comment, and Post models. Using a model, you can call the following methods:Each method will call the GraphQL API and fundamentally interact with your Neo4j database.With this knowledge of the OGM, below we will use it to sign in users. Here I define a custom mutation signIn in our GraphQL schema:Then I create a function called signIn and pass it into the Neo4j GraphQL library as a resolver:Finally, now users of your API can invoke that mutation by performing:The returned value would be the JWT. With this token, you should append it to the header of the request. Here is a simplified fetch example of appending this header:When in development, you can append this JWT in the headers section of GraphQL playground:https://github.com/graphql/graphql-playgroundThis was an example of using the .find() method on a Model. You could, however, use the OGM for a signup method where you use the .create() method.Authorization in Neo4j GraphQLNow that we know how to use the OGM to sign in users and return a JWT, in this section we can dive deeper into the auth directive. However, just before we dive deep into authorization lets use a simple example to ease our way into this. In our application, we want to make sure that only signed-in users can create a post:Here is your first introduction to the auth directive. The directive can be used on Type definitions, as seen above, and or on fields. Each auth directive must have an array of rules where each rule is evaluated inside a logical OR. The isAuthenticated key is enforcing that there is a valid JWT in the request headers.OperationsIn the first example, above, when using the auth directive you may have noticed the usage of the operations key. It does what it says on the tin and only applies the rule on that operation. The full list of operations are shown in the example below:Often you will want to specify all the operations, and so if you dont specify any operations all of them will be applied:Its worth noting that by default your APIs are insecure from all angles. When deploying to production you should ensure that each and every operation is covered.WhereAutomatically append predicatesBriefly, we have seen how to use the auth directive to ensure that authenticated users can create a post. However, what about finer-grained auth rules? In the example below, we have Bob and Jane both creating many posts. The catch is that each post is private and hypothetically there are no public posts:How can we ensure that all posts are private?Given this example, we would like to ensure that Bob can’t interact with any of Jane’s posts and that Jane has the same restrictions on Bobs posts.First, notice how we have used extend here. Using extend is helpful when dealing with larger schemas and auth rules. For the rest of this blog, I will use extend.You see that we have used the where key. This key is used to specify an equality predicate between the creators id and the JWT sub property. Normally without the where rule, applied users could query the API for users:This would produce the Cypher statement:However, with the where rule applied, and if applicable, the generated Cypher would be:Using where will get you off to a great start, although its rather restrictive and would be hard to enable other interactions by anyone other than the creator themself. Read on to learn more about allow, bind, and roles where applied will enable you to express even finer-grained rules.AllowEnable specific users to perform specific actions.For most use cases you will find yourself using allow. It enforces things! Here we have Jane, who is a troll, commenting on Bobs post. How can we allow Bob to delete Jane’s comment?How can Bob remove Jane’s comment?First, lets look at how we can enable Bob to delete his own comments:Allow, in Cypher, will add a validate check just after matching a node. For this example, inside the check is a predicate that ensures the comment creator id is equal to the JWT.sub property, and if not throws out of the transaction.Now we have enabled Bob to delete his own comments, let’s use OR logic to enable the post creator too:Notice the nested traversal of the post creator.BindEnforce equality between a JWT property and the target node property.bindcan be used in tandem with allow, and some may ask whats the difference? allow enables the ability to perform an action, whereas bind enforces that patterns, on upsert, exist. Below is an example where we would want to enforce a pattern. Bob has posted for Jane, and this is something we dont want:How do we stop Bob from posting for Jane?We can use bind on create operations so that when Bob tries to create a bog for Jane the transaction fails:As well as allow, bind uses a validate check, however bind validation is done after the operation.RolesUse to add RBAC (role based access control) to your schema.This is the last key we‘re’ going to dive into. You may have roles in your JWT claims, and you want to enable someone with a particular role to do something. Here we have a hypothetical recoveryCode on our user node, and not only are we putting auth on the field, but using OR logic to enable either the user themself or an admin:RecapNot only have we dived into all of the keys in the auth directive, but we also learned how to authenticate using our understanding of the OGM. The techniques used in this blog will enable you to build production-ready Neo4j GraphQL APIs. What are you waiting for?Learn More About Neo4j GraphQLNeo4j GraphQL Library is available today via npm.@neo4j/graphqlA GraphQL to Cypher query execution layer for Neo4j and JavaScript GraphQL implementations.www.npmjs.comWe are Hiringhttps://neo4j.com/careers/Linksneo4j/graphql💡 Welcome to the Monorepo for Neo4j + GraphQL. Want to contribute to @neo4j/graphql? See our contributing guide and…github.comJoin the Neo4j Discord Server!Neo4j is the open source graph database that allows you to manage, query and visualize your connected information |…discord.comNeo4j GraphQL Library - Neo4j GraphQL LibraryIt is a GraphQL to Cypher query execution layer for Neo4j and JavaScript GraphQL implementations. Such an…neo4j.com;Jul 7, 2021;[]
https://medium.com/neo4j/the-spark-of-neo4j-abd68b93c6ab;Davide FantuzziFollowMar 18, 2021·6 min readThe Spark of Neo4jHear the story about how we at Larus, together with Neo4j, built the official Neo4j Connector for Apache Spark, and how we overcame some of the obstacles we found on our way.What you’ll read in this article are experiences we had between the start of the project in July 2020 and the time of this writing, March 2021.TLDRIf you just want to see how to use the connector API, jump down to the Final Result” section, or check out our examples in the docs.We wanted to give users an official library to avoid them to use custom hacky” solutions, and to offer a continuous service, both from development and support points of view.We decided to deprecate the old connector, in favour of a complete rewriting of the code using the new DataSource API V2 that enable us to leverage the multi-language support of Apache Spark. We also focused a lot on the documentation website, keeping the docs up to date is actually part of our backlog.I also presented the story at the GraphRM meetup in Rome, if you rather want to watch the video:Meetup Talk at GraphRM, (Slides)ChallengesGetting to where we are now wasn’t really straightforward. We faced some problems and here’s a quick overview on how we dealt with them.Lack of DocumentationBeing DataSource API V2 relatively new, we couldn’t find so much official Databricks documentation (especially for Spark 3). Examples, videos, and tutorials were not going enough in-depth for what we have to do, so we had to find another way build things the good old look at the source code” mantra came handy in this case and we were able to figure out what we were doing wrong when the documentation was not enough.Breaking ChangesWe found breaking changes even between minor versions, and of course also between Spark 2.4 and Spark 3.0.Unfortunately on Spark 3.0 the API were still marked as Evolving, so we were expecting breaking changes in the future.Note: we didn’t find any breaking change on Spark 3.1, but that was the situation at the time we started the development.Deal with VersionsDealing with these breaking changes among the versions wasn’t easy. We couldn’t address all the Spark versions on the first release so we had to pick one.Spark 2.3 was relatively old and would have been unsupported in the future. On the other hand Spark 3.0 was new, and not widely adopted so we decided to start with Spark 2.4.Finally after some month of coding we did out first pre-release on September 30th! 🎉And just like that, a few hours later we got this.I have the feeling this would have happened anyway, regardless the initial version we chose.Speaking of versions, we also needed to take care of different Scala versions supported by each Spark version:Spark 2.4 supports Scala 2.11 and Scala 2.12.Spark 3.0 supports Scala 2.12 and Scala 2.13, but dropped the support for Scala 2.11.And we obviously need a JAR file for each combination!To manage this version hell we used the Maven modules feature that allowed us to have common code among different Spark versions and to keep the development process agile and smooth.neo4j-contrib/neo4j-spark-connectorThis repository contains the Neo4j Connector for Apache Spark. This neo4j-connector-apache-spark is Apache 2 Licensed…github.comAs you can see on our repository we have 4 modules: 2 for each Spark version we support, one for the code shared between the versions, that’s basically the core of the connector, and one module for the test utilities. Should a breaking change be introduced in a future Spark release, we’ll just need to add another module. If you want to dive more into this process, take a look at this two pull requests: #282 and #283.Neo4j vs TablesThe pain of integrating Spark and Neo4j is that Spark works with tables, and Neo4j doesn’t have anything similar to that, but instead nodes and relationships. We had to find solutions to a couple of problems here as well.Tables or Labels?To map a graph made of nodes and relationships into a table we created a column for each property of the nodes, and two columns for the relationship, that contain the source node ID and the target node ID. In this way we are able to represent nodes and relationship in a tabular way to work with the DataSource APIs.Schema or Not Schema?Tables have schema, Neo4j doesn’t. We had to find a way to extract a schema from a schema-less graph. For doing this, once we got the result from Neo4j, we flatten the results, and go through each property, extracting the type and eventually creating the schema of the result set.If a property doesn’t exist on all the nodes, it’s added with NULL value on the entries where it’s missing.Can also happen that the same property has mixed types across nodes in this case each value of that property is casted to a String, no matter the initial type.Final Result — And How To Use ItLet’s quickly look at the API of the Neo4j Connector for Apache SparkHere you can see how to read all the nodes that have the labels :Person:Admin using Scala, Python and R.Loading Nodes as Tables from a GraphOther ways to read from (and write to) Neo4j are by using relationship-based or Cypher-query based APIs.Loading Relationships from a GraphLoading tabular results of a Cypher statementAs said, here’s also an example on how to write a DataFrame to Neo4j that contains the people that made this connector happen:Write Spark DataFrame to Neo4jYou can find extensive examples in this repository that contains Apache Zeppelin notebooks you can play with!utnaf/neo4j-connector-apache-spark-notebooksgithub.comBehind the scenes the connector uses the official Neo4j Java Driver, so you can connect with all the strategies allowed by the driver. The connector uses the options specified via the API to generate Cypher queries (kudos to Cypher DSL for making our life easier)These examples only scratch the surface, these is much more to do with the Spark connector, so make sure to check the docs for more details.We are not done yet! We will release the support for Spark 3.0 and Spark 3.1 soon! You can follow our roadmap on our Github Board.We also managed to test the multi-language support for Scala, Java and Python (and soon R), you can read more about it in the article I wrote.Testing polyglot libraries with GitHub ActionsThis is the story of how we managed build a CI pipeline to test the official Neo4j Connector for Apache Spark with…medium.comThe slides for my talk are also available hereSpark of Neo4j talk slidesI wish you a sparkling graph experience, please let us know via GitHub issues or Documentation feedback/edits if there are things that we can improve or that you need.;Mar 18, 2021;[]
https://medium.com/neo4j/twin4j-world-cup-graph-and-graphql-api-tuning-cypher-queries-querying-spatial-datapoints-6e46b8a2b463;Mark NeedhamFollowJun 23, 2018·4 min readTWIN4j: World Cup Graph and GraphQL API, Tuning Cypher Queries, Querying Spatial datapointsWorld Cup Graph and GraphQL API, tuning Cypher queries, and the Intro to Graph Databases YouTube series is back!Welcome to this week in Neo4j where we round up what’s been happening in the world of graph databases in the last 7 days.This week we have the World Cup Graph and GraphQL API, an article explaining how to tune Cypher queries by understanding cardinality, querying Spatial data points, and the Intro to Graph Databases YouTube series is back!Featured Community Member: Bea HernándezThis week’s featured community member is Bea Hernández, Data Scientist at DATMEAN.Bea is part of the Neo4j community is Madrid as well as being the organiser of R-Ladies Madrid and a member of the NASADatanauts.Bea presented Neo4j gRaphs at a combined R-Ladies and Neo4j Madrid meetup in February in which she showed how to analyse and visualise Neo4j data in R.This week DATMEAN were accepted into the Neo4j startup program so Bea will get to work on Neo4j even more.All of us in the DevRel team are excited to hear about your experiences, perhaps at a future GraphConnect or Neo4j event!The World Cup GraphWe’re well into the 2nd round of matches at World Cup 2018 and Michael and I decided to revive the World Cup Graph that we first created 4 years ago.The dataset contains the matches, players, and tournaments for every World Cup from 1930 to the present day.Read the blog postGraphQL APITo make the World Cup Graph accessible to people not yet fluent with the Cypher query language we also created a GraphQL API on top of the database.This only took us a few hours thanks to the excellent GRANDstack Starter Kit. All the scaffolding had been done for us — all we had to do was fill in details about our database and create a GraphQL schema.Read the blog postIntro to Graph Databases Episode #5 — Cypher, the Graph Query LanguageBased on popular demand, Ryan this week resumed the Intro to Graph Databases YouTube series with a video explaining the Cypher query language.Ryan starts by explaining how the developer surface of Neo4j has evolved over the years, from the embedded Java API, via the REST API, up to the present day of Bolt drivers and stored procedures and functions executed via Cypher.Tuning Cypher queries by understanding cardinalityThis week from the Neo4j Knowledge Base we have an entry by Andrew Bowman that explains how to tune Cypher queries by understanding cardinality.Andrew starts with a high level overview of how Cypher execution works, and then takes us through a worked example from the in built movies dataset, showing various tricks to improve the performance of the query.If you’ve ever wondered why your queries aren’t doing what you expected them to this is a great post to read.Learn how to tune CypherQuerying spatial data points in Neo4jLast week we featured a blog post where Rik showed how to import the Open Beer Database along with Spatial data points, and in this week’s blog post he shows how to write queries against this new data type.Read the blog postProjects to play with: Knowledge Graph, Mortality Explorer, RDF → GraphOn my GitHub travels I came across a few interesting projects that you can take a look at if you get some free time this week.neo4j-knowledge-graph — an example of a simple, queryable knowledge graph implemented using Neo4j that can be used with Microsoft’s LUIS NLU or google’s Dialogflow.com NLU.mortality — a bit of a morbid dataset containing causes of deaths in the USA.rdf2neo — a tool to convert and load RDF into Neo4j.My favourite tweet this week was by Tunisian Gunner:;Jun 23, 2018;[]
https://medium.com/neo4j/apoc-hidden-gems-what-you-could-have-missed-89d64ff6a5e6;Andrea SanturbanoFollowMay 24, 2021·5 min readAPOC Hidden Gems: What You Could Have MissedAPOC contains about 500 procedures and functions, here we’ll discuss some of them that are very interesting and use cases about their applications. Enjoy!Started as a very simple collection of useful Procedures and Functions in 2016 the APOC project raised its popularity very quickly, having more or less 500 useful methods to cover 90% of uses cases in the Neo4j ecosystem.In the past, I wrote articles about how to leverage useful hidden gems like:Transform MongoDB collections automagically into GraphsEfficient Neo4j Data Import Using Cypher-ScriptsNow I’ll talk about others that we recently added to the project and can be useful in several scenarios.apoc.load.directorySometimes you may need to ingest files into sub-directories with the same business rules (= cypher query), so when in the past you needed to execute the query every time for each specific file now you can leverage the apoc.load.directory procedure order to search recursively into a directory, so instead of doing this:WITH path/to/directory/ AS baseUrlCALL apoc.load.csv(baseUrl + myfile.csv, {results:[map]}) YIELD mapMERGE (p:Person{id: map.id}) SET p.name = map.nameRETURN count(p)WITH path/to/directory/sub/ AS baseUrlCALL apoc.load.csv(baseUrl + myfile1.csv, {results:[map]}) YIELD mapMERGE (p:Person{id: map.id}) SET p.name = map.nameRETURN count(p)WITH path/to/directory/sub/sub1/ AS baseUrlCALL apoc.load.csv(baseUrl + myfile2.csv, {results:[map]}) YIELD mapMERGE (p:Person{id: map.id}) SET p.name = map.nameRETURN count(p)You can simply do this:CALL apoc.load.directory(*.csv, path/to/directory, {recursive: true}) YIELD value AS urlCALL apoc.load.csv(url, {results:[map]}) YIELD mapMERGE (p:Person{id: map.id}) SET p.name = map.nameRETURN count(p)Very nice right?!As you can see the procedure takes 3 parameters:The file name patternThe root directoryA configuration map that at this very moment (May 2021) manages just one property recursive=true/false that allows searching recursively from the root directoryAnd returns a list of URLs that match the input parameters so you can easily chain with other import/load procedures.apoc.load.directory.async.*Please raise the hand if you hate Cron and other similar mechanisms!Unfortunately, they are quite used when you need to ingest data from files, but in order to overcome this, we build a proactive” feature that allows you to define a listener into the File System that gets executed every time a file is created/updated and deleted.These procedures are extremely helpful when you’re ingesting data via files like csv, json, graphML, cypher and so on because they don’t rely on a Cron job, but they are constantly active this has several upsides because if a job fail for some reason (network issues that block the upload of the file into the import dir, and so on…) you don’t need to wait for next cron loop, you can upload the file whenever you want, and the ingestion starts right after!It works in a very similar way to APOC Triggers so the first thing to do is define the listener:CALL apoc.load.directory.async.add(insert_person,  CALL apoc.load.csv($filePath) yield list  CALL apoc.load.csv($filePath, {results:[map]}) YIELD map  MERGE (p:Person{id: map.id}) SET p.name = map.name,*.csv,path/to/directory,{ listenEventType: [CREATE, MODIFY] })As you can see the procedure takes 5 parameters:The listener nameThe cypher query that has to be executed every time the listener catch a new eventThe file name patternThe root directoryA configuration map that at this very moment (May 2021) manages just one property listenEventType=CREATE/MODIFY/DELETE that defines the type of the file system event that you want to listenAn important thing to consider is that into the cypher query (the second argument) we inject these parameters that you can leverage:fileName: the name of the file which triggered the eventfilePath: the absolute path of the file which triggered the event if apoc.import.file.use_neo4j_config=false, otherwise the relative path starting from Neo4j’s Import DirectoryfileDirectory: the absolute path directory of the file which triggered the event if apoc.import.file.use_neo4j_config=false, otherwise the relative path starting from Neo4j’s Import DirectorylistenEventType: the triggered event (CREATE”, DELETE” or MODIFY”). The event CREATE” happens when a file is inserted in the folder, DELETE” when a file is removed from the folder and MODIFY” when a file in the folder is changed. (n.b note that if a file is renamed, will be triggered 2 events, that is first DELETE” and then ”CREATE”)There are these other procedures that help you to manage the listener lifecycle:apoc.load.directory.async.list(): that returns a list of all running listenersapoc.load.directory.async.remove(‘<listner_name>’): that removes one listenerapoc.load.directory.async.removeAll(): that removes all the listenersapoc.periodic.truncateThe procedure is useful when you’re in the prototyping phase and you’re defining your graph model or your ingestion strategies because it allows you very easily to wipe the entire database:CALL apoc.periodic.truncate({dropSchema: true})As you can see we manage a configuration map that at this very moment (May 2021) manages just one property dropSchema=true/false that eventually drops indexes and constraints.apoc.util.(de)compressSometimes you may need to store large string values into Node or Relationships, in this case we added two procedures in order to (de)compress between a string to a compressed byte array data in a very easy way.You can compress the data in the following way:CREATE (p:Person{name: event.name,  bigStringCompressed: apoc.util.compress(event.bigString, { compression: FRAMED_SNAPPY})})You can decompress the data in the following way:MATCH (p:Person{name: John)RETURN apoc.util.decompress(p.bigStringCompressed, { compression: FRAMED_SNAPPY}) AS bigStringSupported compression types are:GZIPBZIP2DEFLATEBLOCK_LZ4FRAMED_SNAPPYOne other option is to compress source data (e.g. Gigabytes of JSON) on a client and send the compressed byte arrays of data (a few MB) via bolt as parameters to the server, and then decompress, parse and process the JSON on the server.Other Minor ChangesWe did tons of minor changes, following some of them:We updated the Couchbase procedures to the last driver version in order to have state-of-the support for the database.We added stats for the computation of apod.periodic.iterate, now you’ll get the number affected entities and properties by the procedure.Now UUID and TTL features work per database .The Road AheadAPOC growth is not stopping! We’re working hard in order to add new cool features that you can leverage in the near future:Support for read/write the Apache Arrow file format!Support for read/write data from/to Redis!And many more!By the way, if you need a feature please fill an issue into our GitHub repository we’re very happy to listen to what you need!;May 24, 2021;[]
https://medium.com/neo4j/building-graph-apps-with-react-part-1-d6b1a7aa33;Neo4jFollowNov 9, 2018·4 min readBuilding Graph Apps with React (Part 1)Neo4j Desktop added the capability to build and run your own custom apps, connecting to your Neo4j backend. The Graph Gallery and the Halin monitoring app were already featured here. This post is the first in a series to get you going with Graph App development.This post has been written by Cristina Escalante from our partners at tsl.io.Getting started with any new project can be both exciting and frustrating. How do I see a working example of what I want to do? Where do I report bugs? Keeping frequently asked questions in mind, this article will assume you have Neo4j Desktop installed and walk you step-by-step through getting started with Graph App development.What is a Graph App?In contrast to the more common web applications that need to be hosted on a server, Neo4j Graph Apps are more similar to plugins for Neo4j Desktop. A Graph App is a single-page web application (written in JavaScript using e.g. Vue, React, Angular) designed to work inside of Neo4j Desktop, which is an Electron-based application that provides an API for discovering and connecting to Neo4j graphs. Although a Graph App may interact with a server, it does not need to be connected to the internet and interact solely with the data available from Neo4j Desktop.Furthermore, you could take an existing webapp and wrap it into a Graph App, or take a Graph App and promote it to a standalone application.How do I use an existing Graph App?Published Graph AppIf a Graph App has already been published, installing and running it is relatively painless.Open Neo4j Desktop.Under Graph Applications” you can see a Install Graph Application” fieldFind the Graph Application you wish to install — here’s a list https://github.com/neo4j-apps/awesome-graph-appsCopy the URL of the graph app that interests youPaste into the Install Graph Application” field and tap the Install buttonPick a project and add the application in the top row.Done!How do I build my own Graph App?You can create your own Graph App from scratch or by wrapping an existing single page Neo4j app (or any other online URL”). The examples apps in the graph-app-starter repository will help get you off the ground.But really, how would you get started?Let’s get the Single File Graph Application started. Although merely an index.html file, it is a good introduction of the Neo4j Desktop API.First, clone the graph-app-starter repository and navigate to graph-app-starter/examples/basic-single-file/, taking note of the path to the root folder.Open the index.html file in a browser and take note of the file URL. You could also statically serve it locally or from somewhere else.In Neo4j Desktop:Open Settings on the sidebarToggle Development Mode: OnCopy paste the App’s root path and entry point into the appropriate fieldsStart the Development App under My ProjectsVoila!If you’re familiar with React you should find your way through that Graph App. It uses theExisting Graph App RepositoryIf you want to test or modify a Graph App someone else has published, you use the same steps. Below is an example using the open- source Graph App: Graph Examples.Clone the repository and take note of the App’s root path on your diskRun the app using the instructions in the readme (e.g. with yarn start) and note the app entry pointEnable Developer Mode and add the root path and entry pointStart the Development App under your selected projectThe Development Mode also shows Developer Tools and Reload buttons with each Graph App which are really handy for problem solving.What if I find a Problem?If you find a problem with how Neo4j Desktop and the Graph App you’re working on are trying to interact, create an issue report here or contact the developers of the Graph App you’re trying to use.Graph Apps published by the Neo4j team would have repositories to report issues here too: https://github.com/neo4j-appsIn the next instalment of the series we will have a closer look at the Neo4j Desktop API and the existing React components in Graph App Kit.;Nov 9, 2018;[]
https://medium.com/neo4j/announcing-the-stable-release-of-the-official-neo4j-graphql-library-1-0-0-6cdd30cd40b;Darrell WardeFollowApr 27, 2021·9 min readAnnouncing the Stable Release of the Official Neo4j GraphQL Library 1.0.0Migrate your application from neo4j-graphql-js to @neo4j-graphqlFollowing on from our announcement of the beta release, we are excited to announce that the stable release of the official Neo4j GraphQL Library 1.0.0 is now available to download and use!For those of you hearing about this library for the first time today, welcome! You can find a high-level overview of this library on our product web page.The Neo4j GraphQL Library consists mostly of an npm package, @neo4j/graphql, which is the successor to the Neo4j Labs project, neo4j-graphql-js. It’s also a complete redesign and rewrite in TypeScript, for that sweet type safety that suits GraphQL and Neo4j so well.Responsibility for the project has been shifted from Neo4j Labs to Product Engineering. With a team now dedicated to the new library on a full-time basis, expect high-quality support and continuous improvement.Additionally, with today’s release also comes the stable version 1.0.0 of @neo4j/graphql-ogm, an Object Graph Mapper. This is an amazing programmatic API for manipulating data in your Neo4j database, all powered by your GraphQL type definitions! While it uses @neo4j/graphql under the hood, you don’t have to use them in conjunction.My name is Darrell. I’m one of the software engineers working on the Neo4j GraphQL Library. I’m fortunate, not only to be working on this exciting product but also to be working with an amazing team who are passionate about bringing native graph data storage to GraphQL.Today, I’m going to take this opportunity to walk you through a few of the common steps that will be required to migrate an application over to use the new library. Even if you’re not migrating an existing project, do read on to hear about some of the powerful features in today’s release, as well as some of the exciting ideas that we have for the future… 🚀This blog contains a mixture of code examples from both the new and old libraries, separated under subheadings. If you see a 🚀in a subheading, go ahead and use any of the code there in your @neo4j/graphql application. See a ⚠️? Code in there is for neo4j-graphql-js, which will soon be deprecated!Migrating to the Neo4j GraphQL LibraryThe Neo4j GraphQL Library was never intended to be a drop-in replacement for its predecessor. However, the migration process isn’t too complicated. I’m going to take you through the steps of migrating a small but non-trivial GraphQL API through this process.If you follow Neo4j at all, you’ll know that we love movies! So we’re going to take an example graph that looks like the following:Out with the Old… ⚠️The code snippets in this section are for the legacy neo4j-graphql-js library! If you just want to read about the new library, skip ahead to the next section.Based on the graph above, a simple implementation using the old library might have looked something like the following:Type definitions for use with neo4j-graphql-jsPoints to note in those type definitions are:@relation directives connecting all of our data togetherAn averageRating field to using a @cypher directive to aggregate an average rating for each MovieThe unique ID field for the Review type, decorated with the @id directiveFinally, you would have made a schema and hosted it using Apollo Server, as per the following code block:Creating a schema using neo4j-graphql-js and hosting with Apollo ServerTo add a user, movie, and review to the database we have to run a few mutations! Make sure you keep track of the id field of the review so that we can use it to connect our data together next.Mutations to create nodes using neo4j-graphql-jsRight now, our graph” isn’t going to look an awful lot like a graph!With that ID value in hand, we can use it in our next mutations to connect everything together:Mutations to connect nodes together using neo4j-graphql-jsFinally we have all of our data connected together!In with the New! 🚀We need to start by switching over our npm packages from neo4j-graphql-js to @neo4j/graphql using npm or the package manager of your choice:You’ll note that we’re having to install graphql and neo4j-driver — these are now peer dependencies in the new library, so these need to be installed explicitly.You’ll be pleased to know that migrating the basic application above is going to be remarkably simple. The most common change is that you will need to rename all of your @relation directives to @relationship, and change the name argument of that directive to be type instead.The @cypher and @id directives which we have used can remain unchanged.Your type definitions should now look like the following:Type definitions for use with @neo4j/graphqlThen you just need to make a few code changes to create a schema with the new library and host it:Change over your require statement (line 2 below)Swap out the old function for making the schema for the new constructor (line 7 below — you’ll notice I’ve passed in the driver here instead of in the context: the driver will be available in the context whichever method you use, so take your pick!)When constructing your Apollo Server instance, make sure you pull the schema out of the constructed Neo4jGraphQL instance!With these changes made, standing up your server will look like the following:Creating a schema using @neo4j/graphql and hosting with Apollo ServerThen for the best part. Let’s say I wanted to write another review for a different movie using the nested mutations feature of the new library:Nested Mutation using @neo4j/graphqlIn one mutation, I’m able to create a new movie with a new review connected to it, which in itself is connected to an existing user, before returning all of that nice connected data (but returning no nodes twice by using that filter in the selection set) in one object. Powerful stuff!Our graph is looking a lot more like a graph at this point!Nested mutations are a brand new feature, so you might want to migrate the syntax of your mutations first, before considering how you could combine certain mutations later down the line.You’ll be feeling like a Kung Fu master as you create entire subgraphs in one Mutation!We envision that client changes are going to form the bulk of migration work, as the structure of queries and mutations has changed a lot:All query and mutation fields are now in camelCase instead of PascalCase, and are also pluralized — CreateMovie has become createMovies.Query and mutation arguments, which were previously at the root, are now nested under commonly named arguments — CreateMovie(title: String!) has become createMovies(input: [MovieCreateInput!]!).Mutation responses now have an additional level until you reach the fields of the mutated node.Specifically relating to the DateTime scalar (as is used in this example), these are now purely dealt with as ISO 8601 strings, so you no longer need to use arguments such as formatted.You can check out this commit in our repository to see the inline difference, and run the above Nested Mutation if you like!But Wait, There’s More!In my opinion, the example above is a little too simple:Any user can author a review under the guise of another user, or edit other users’ reviewsThe createdAt field needs to be manually set when authoring a reviewWe should really build on our schema and add the functionality needed to address these shortfalls.Rolling Your Own Solution Using neo4j-graphql-js ⚠️The code snippets in this section are for the legacy neo4j-graphql-js library! If you just want to read about the new library, skip ahead to the next section.I’m not going to go into the detail of implementing an auth solution in this blog. However, using the old library, we would have likely overridden the generated CreateReview mutation with either a custom resolver or a pretty complex @cypher directive using the cypherParams feature (you can do pretty similar thing in the new library, too!).Whichever method we chose, it would have needed to create a review with createdAt set to the current timestamp, and then connect it to the currently logged-in user, and also to the movie that it addresses.We would have had added the following mutation type to our type definitions:Overriding a generated Mutation using neo4j-graphql-jsUsing the @isAuthenticated directive, we would have also needed to pass a config object to makeAugmentedSchema specifying which auth directives we want to use:Auth can be quite complex with neo4j-graphql-jsUsing GraphQL Directives for Fun and Profit 🚀Using the new Neo4j GraphQL Library, I’m happy to say that we offer a number of directives that can essentially remove the need for a custom CreateReview mutation entirely:@timestamp and @auth directives available in @neo4j/graphqlThe directives to note from the code block above are:The @timestamp directive which can be used to automatically set timestamps on create and/or update operationsThe use of the @auth directive, which in this example is being used to restrict users from creating reviews under the guise of another author, or updating other users’ reviewsThis won’t automatically connect your nodes together for you, but it will protect from nefarious users taking credit for reviews that aren’t theirs!It’s worth noting that you’ll need a method for creating users and dishing out their JWTs, such as a signUp mutation. Using the OGM is a really nice way to achieve this, which I won’t detail in this blog, but you can expect plenty of news in the coming weeks and months.There’s also less boilerplate when it comes to creating your schema and server — you just need to pass your request into your context object:Just pass your request into the context and auth is good to go using @neo4j/graphqlNeed a Hand?We realize that hypothetical example applications are very different from the real thing, so we (and the growing community) are on hand to help you figure things out as you migrate.This blog is by no means comprehensive, so we have written a migration guide which has a bit more fine-grained detail of schema, query, and mutation differences. This will be an evolving document that will grow and change as we continue development on the new library.If you believe we’ve missed some functionality that should be there or something isn’t working quite as you’d expect it to, please raise an issue in our GitHub repository.If you want to ask us or the community for a hand (or generally want to chat Neo4j and GraphQL!), please join the Neo4j Discord server where you can find a #graphql channel and a variety of other great topics to discuss.You can also check out the GraphQL and GRANDstack category in the Neo4j Online Community.Where Do We Go from Here?Don’t worry, this certainly isn’t the end of development for the Neo4j GraphQL Library. Below you can find just a small insight into some of the ideas that the team has for future functionality.Relay ImplementationIf you have used them in your current neo4j-graphql-js applications, you have likely noticed the absence of relationship properties in this release. While we realize this prevents immediate migration for a number of users, we’re aiming for this to be one of the first major features we look at after today’s release.After plenty of discussions, we believe that implementing the Relay specification is going to provide the perfect framework for introducing relationship properties. Additionally, it will allow us to add in the much-requested feature of cursor-based pagination to help you build applications with infinite scrolling.AggregationsWe’re pleased with the comprehensiveness of the current filters offered by the Neo4j GraphQL Library, but we want to expand these further with aggregations, allowing you to query averages, sums, and more.Have Amazing Ideas for Neo4j and GraphQL?On Global Graph Celebration Day, Neo4j announced the Leonhard Euler Idea Contest. This is a hackathon running from April 27 until June 4, and we want to see you turn your innovative ideas into reality using our new GraphQL library!Not only is this an amazing opportunity to get hands on with the new library, there are also $10,000 worth of prizes available and an opportunity to gain access to Aura Free Tier for early registrants! Registration is now open.;Apr 27, 2021;[]
https://medium.com/neo4j/finding-influencers-and-communities-in-the-graph-community-e3d691296325;Mark NeedhamFollowMay 15, 2019·6 min readFinding influencers and communities in the Graph CommunityIn this post we’re going to analyse the Twitter social graph of the Neo4j community using graph algorithms.Social Network AnalysisMany graph algorithms originated from the field of social network analysis, and while I’ve wanted to build a twitter followers graph for a long time, the rate limits on its API have always put me off.This changed a couple of weeks ago when Michael showed me the twint library.Twint is an advanced Twitter scraping tool written in Python that allows for scraping Tweets from Twitter profiles without using Twitter’s API.This library allows us to work around these rate limits and easily download twitter users and their followers, as well as the users that they follow.The Twitter Social GraphWe’ll download all tweets for the search term neo4j OR  graph database  OR  graph databases  OR graphdb OR graphconnect OR @neoquestions OR @Neo4jDE OR @Neo4jFr OR neotechnology using the twint library, extract the users who wrote those tweets, and download the follower and following network for each of those users.TwitterOnce we’ve done that we’ll import all the users, their followers, and people that they follow into Neo4j.If you want to follow along at home, you can execute the import script available on this gist. If we execute that script, we’ll have a graph like this:Neo4j Twitter GraphSo that’s the data that we’ll be working with. Now let’s learn about the types of analysis that we’re going to do on this data.Analysing the Neo4j social networkWe’re going to do two types of analysis on the data.InfluencersLet’s start with a definition:A Social Media Influencer is a user on social media who has established credibility in a specific industry. A social media influencer has access to a large audience and can persuade others by virtue of their authenticity and reach.We’ll use centrality algorithms like degree centrality, betweenness centrality, and PageRank to find the most important people in our graphSub CommunitiesWe’ll use the Louvain community detection algorithm to find sub communities within the larger graph community.These communities are likely to be language or interest based, and will help us learn about the different types of people who are interested in graphs.NEuler: The Graph Algorithms PlaygroundWe’re going to do this analysis using the Graph Algorithms Playground Graph App and the Neo4j Graph Algorithms Library.You can read more about the app, including installation instructions, in the release blog post.Now it’s time to explore the data and find the most important people and sub communities in the graph community.Degree Centrality: Direct ImportanceThe simplest measure of influence is Degree Centrality, which measures the number of relationships connected to a node.We can use this algorithm to find out which users have the most followers, which is the metric that people most frequently reference when identifying social media influencers:Most important users based on Degree CentralityThe top three accounts are all organisations — unsurprisingly the Neo4j account has the most popular, and a lot of the graph community also follow TechCrunch and AWS Cloud.After that we come across some people, including Emil (CEO of Neo4j), Kirk Borne (a big name in the Data Science community), and Michael Hunger (Director of Neo4j Labs).These accounts are considered important because lots of other accounts opt in to seeing the content they publish on their timelines. If we want to broadcast some general information about graphs, these would be good accounts from which to do that.PageRank: Transitive ImportanceThe PageRank algorithm measures the transitive influence or connectivity of nodes. This is the most famous graph algorithm, and was named after Google co-founder Larry Page.We can use this algorithm to find important accounts based not only on whether they’re followed by lots of other accounts, but whether those accounts are themselves important.Most important users based on PageRankWe see similar accounts as we did with the Degree Centrality algorithm. Jessica Kerr and Kelly Sommers are the only two accounts in the top 10 that didn’t appear in the Degree Centrality top 10.If we wanted to post some content and have it spread across a large number of accounts, the accounts that rank highest for PageRank would be the best placed to post that content.Betweenness Centrality: Local BridgesThe Betweenness Centrality algorithm detects the amount of influence a node has over the flow of information in a graph. It is often used to find nodes that serve as a bridge from one part of a graph to another.We can use this algorithm to find people who are well connected to a sub community within the larger graph community:Most important users based on Betweenness CentralitySeveral of the accounts that had high Degree Centrality and PageRank feature here as well, but there are also some accounts we haven’t seen yet. Will Lyon and GraphQL Weekly are interesting additions — these two accounts are local bridges to the GRANDstack/GraphQL sub community.Local BridgesLucasoft.co.uk is another interesting one — after some exploration I believe this account is a local bridge into the PHP sub community.If we wanted to find out what’s happening in these sub communities or if there’s some information that would be useful to those sub communities, one of our local bridges would be best placed.We can explore those sub communities using community detection algorithms, so let’s do that next.Communities: Louvain ModularityThe Louvain Modularity algorithm detects communities in networks, based on maximising a modularity score.We can use this algorithm to find sub communities in the larger graph community:Sub communities based on Louvain ModularityWe’ll run the algorithm in NEuler, but the results view isn’t very easy to consume, so we’ll write the following Cypher query to explore the resulting communities:// Find all users and order them by community and PageRank scoresMATCH (u:User)WITH uORDER BY u.louvain, u.pagerank DESC// Return top 10 ranked nodes grouped by communityWITH  u.louvain AS community, collect(u.name) AS peopleRETURN community, people[..10]ORDER BY size(people) DESCSub communities in the graph communityFrom visualising inspecting the results, we can identify the following communities:1 and 11 - Data Science5 - Neo4j2 - Spring6 - Semantic Web13 - Microsoft3 - FileMakerI find it fascinating that we can find this sub communities just by looking at the follower graph - the content of tweets posted by these accounts isn’t required.SummaryIn this post we’ve learnt how to use the centrality and community detection algorithms in the Neo4j Graph Algorithms Library to explore a Twitter Graph.If you enjoyed learning how to apply graph algorithms to make sense of data, you might like the O’Reilly Graph Algorithms Book that Amy Hodler and I wrote.You can download a free copy from neo4j.com/graph-algorithms-book;May 15, 2019;[]
https://medium.com/neo4j/neo4j-community-on-aws-quick-start-65736644ebc;David AllenFollowJul 31, 2018·4 min readGet up and running quickly with the Neo4j VM Image on AWSUPDATE JUNE 2020: This article is now deprecated. Neo4j’s cloud image documentation is now hosted in the Neo4j Operations Manual, Cloud Deployments Section. Please consult that documentation going forward!— -Neo4j Community 3.4.1 has recently been released on Amazon Web Services. There are a lot of options for running Neo4j I’ll provide a list below, but this article is going to focus on launching a Community Edition VM, licensed under the GNU GPL, so it’s free (except for the EC2 charges), quick, and easy.Community edition is really cool because people all over the internet pick it up and do fun things with it that we at Neo4j never would have anticipated. So availability of community on the biggest cloud was a no brainer. We want to see all of those cool applications of Neo4j, from doing cell biology to tracking murder investigations, to building a dating site.Before we get started, just a note that Neo4j now has several options on AWS depending on what you need:Neo4j Community AWS Marketplace, which permits launching instances into VPCs easilyNeo4j Enterprise Causal Cluster, which deploys multi-node clusters for license holders, those in our startup program, and evaluators.Getting StartedGo to your AWS EC2 Dashboard, and select Images > AMIs” on the left-hand side. Select Public Images” in the drop down, and simply search for neo4j-community”. You’ll see that several results come up, but you’ll know you have the right one (highlighted) when you see that the source field includes aws-marketplace”.Right click on that image, and select Launch”.AWS is going to step us through a configuration process next. We’re going to choose a very small machine (t2.micro) which has very little hardware, but it’s good enough to get a proof of concept going, and it’s cheap.We can then click Review and Launch” which gets us to a summary screen and allows us to skip a lot of other steps which we don’t need right now.Configuring Network Access RulesMake sure to do this next step carefully probably the most common way users get frustrated launching cloud instances is that they forget to specify network access rules, the public clouds let almost nothing in by default.In the screenshot above, click Edit Security Groups” and fill out the form as follows. This is going to permit us to send traffic to our instance on three different ports as indicated. 0.0.0.0/0 just means allow from all IPs on the internet”, which you can tailor to be more specific if you know your IP range.Finally, after clicking Review and Launch”, we get back to the screen that asks us to confirm everything. Simply click Launch”. One last step — choosing an SSH keypair. This is an important step, without which you can’t SSH into your new machine. If you need help creating a new one, consult the AWS documentation here. For now, I’ll choose a key I use frequently, and hit Launch Instance”.That’s Really It!Your instance will take a few minutes to come up. In the EC2 console, once you see that your instance is running like so, then you’re good to go.Accessing Your New GraphAccessing your instance just requires that you look up the public IP of the instance, and then go to https://MY_PUBLIC_IP:7473Your username will be neo4j” and your password will be the Instance ID from the EC2 console above.Your browser may warn you about the untrusted SSL, because we haven’t set up a certificate, but you can accept this new certificate and move on.If connecting to this IP address times out, by far the usual case is an error with the security groups listed above. Remember that AWS firewalls everything out (and I mean everything) unless you explicitly unblock it. So if you run into trouble, make sure TCP traffic on port 7473 is permitted.What’s Next?If this is your first time with Neo4j, drop by the Neo4j user slack and say hello to us. Even better, build a cool GRAND Stack app, or your own experiment and let us know about it, we really love hearing about new applications.Happy Graph Hacking!;Jul 31, 2018;[]
https://medium.com/neo4j/neo4j-data-modelling-2982bd90aa0c;Siddhartha SehgalFollowAug 14, 2020·8 min readNeo4j Data modelling 101What is Neo4j?The world’s most flexible, reliable and developer friendly graph database as a service.” It is an online database management system with Create, Read, Update and Delete (CRUD) operations that stores data as a graph.What is a graph database?If you are coming from my previous blog about setting up Neo4j, skip this and the next section. Go to Data modelling directly.A graph database, also called a graph-oriented database, is a type of NoSQL database that uses graph theory to store, map and query relationships.A graph database is essentially a collection of nodes and edges.A graph is composed of two elements: a node (or vertex) and a relationship (or edge). Each node represents an entity (a person, place, thing, category or other piece of data), and each relationship represents how two nodes are associated. This general-purpose structure allows you to model all kinds of scenarios — from a system of roads, to a network of devices, to a population’s medical history or anything else defined by relationships.Why would you ever require a graph database anyway?A graph database, unlike a relational database management system (RDBMS), treats relationships as first class citizens. It is not required to use approaches such as complex join queries or accessing foreign keys to get data related to each other. We join entities as soon as we know they’re connected, so these mapping methods are unnecessary. Since graph databases employ object oriented thinking at their core, the data model you draw on your whiteboard is the model of data you can store in your database.Modern data has, implicitly, lots of relationships. In order to leverage these data connections, organizations need a database technology that stores relationship information as a first-class entity. That technology is a graph database. Unfortunately, legacy RDBMS are poor at handling data relationships. Also, their rigid schemas make it difficult to add different connections or adapt to new business requirements.Not only do graph databases effectively store data relationships they’re also flexible when expanding a data model or conforming to changing business needs. You can read more about advantages of using Graph databases.Data ModellingNeo4j databases store data in the form of Nodes and Properties. The node label signifies the table and the properties signify the columns (as in RDBMS). Further the nodes (labels) are connected to other nodes via relationships that signify a connection in the real world (for example- Company ‘is located in’ City, or Contact ‘is lead for’ a Company). In Neo4j, relationships can have properties as well.Data modelling is a significant part of any project and how you design your database will drive your data ingestion process as well as the consumption layer. Often making big changes down the line maybe not be possible, certainly not easy, making this process more important.The data modelling process in graph databases (and especially Neo4j here) involves decisions between representing a data point as a node or relationship, and often clever designs that will help the querying later in the consumption layer.RDBMS to Graph — how it translatesFrom Neo4j:Model: Relational to Graph - Neo4j Graph Database PlatformFor those with a background in relational data modeling, this guide will help transfer your existing knowledge of the…neo4j.comTable to Node Label — each entity table in the relational model becomes a label on nodes in the graph model.Row to Node — each row in a relational entity table becomes a node in the graph.Column to Node Property — columns (fields) on the relational tables become node properties in the graph.Business primary keys only — remove technical primary keys, keep business primary keys.Add Constraints/Indexes — add unique constraints for business primary keys, add indexes for frequent lookup attributes.Foreign keys to Relationships — replace foreign keys to the other table with relationships, remove them afterwards.No defaults — remove data with default values, no need to store those.Clean up data — duplicate data in denormalized tables might have to be pulled out into separate nodes to get a cleaner model.Index Columns to Array — indexed column names (like email1, email2, email3) might indicate an array property.Join tables to Relationships — join tables are transformed into relationships, columns on those tables become relationship properties.Designing data model as a graphNeo4j describes the data modelling process as creating a whiteboard friendly design, which means that the nodes and relationships which we can build on the whiteboard, or as English sentences, often drive how we model our data.Often the data that we have can be represented as a property or a node label in itself. For example in case of email transactions data, we could depict the data using:Email sent” as a relationship between the sender and receiverHowever it becomes clear that this model, while being a whiteboard model, lacks ability to extend its capabilities to store other information such as CC, BCC, replied_to, forwarded_to, etc. Hence we modify our model to depict Email as a separate node label:Thus, we can see that our data drives the design process.Another example involving City and Company.We could include the City value for the Company label in the Company node itself.Company nodes storing the City propertyTo query such data to filter by a location would require traversing all the Company nodes in the graph, and then filtering by their City value. As it is evident here, since the number of Company nodes can potentially be huge, querying for all Companies in a particular City will be take a lot of resources, in terms of time and CPU.We could identify such downstream applications and use that knowledge to drive our data modelling process. In our case, the graph would look like:Company nodes connected to the City nodesNow the query can be modified to filter the City first, and only look for Companies that are connected to the City node. This improves our query time significantly.Our queries (consumption) drive the data design process.The EXPLAIN keywordCypher keyword EXPLAIN is used for query profiling, using it will display the plan of how the query will be run, as well as approximate records at each level.Explanation for queries to get Companies located in a particular State, for example, will differ vastly based on how the data is modelled and how it is queried. Below, on the left, shows what happens when we filter for a State first and then look for Companies that are attached to that State node. As you can see, the number of records in each step are nominal as compared to the explanation on the right, which queries all the Companies and attempts to filter them based on the State value which is stored as its property.Comparison between queries when filtering using a intermediate State node versus filtering by node propertyExplain is a powerful tool that, in the initial stages of data modelling and ingestion will give a fair idea of how the queries are going to look in the consumption step, or at least how they should look like. That should drive the data modelling.PROFILE is a similar keyword which explains the query plan as well as executes it.Index and ConstraintsAs with RDBMS, Neo4j supports creating Index for properties of labels. Simply put, a database index is a redundant copy of some of the data in the database which helps making searches of related data more efficient. This however uses more storage and affects the write speeds, therefore what columns or properties to index is a decision that is made while designing the data and treating it like a magic bullet to speed queries is not recommended.Judicious use of indices can improve the queries many folds and it should be a significant part of data modelling discussion.Constraints on the other hand are business rules that restrict data duplication for key properties for specific labels. For example, if we get data from sources that has multiple entries for City named London, we don’t want to create them as separate records. Constraints help us maintain data integrity.Handling datetime valuesMore often than not, data that you will try to ingest will have date values. As of Neo4j 3.4, the system handles duration and dates. Neo4j allows making indexes on numeric properties and run range queries that use the index. We can take advantage of this for dates by storing them as millisecond timestamps, allowing us to perform date range queries.Another creative solution is time trees. The idea is to create hierarchical date month and year nodes that are connected via a child relationship from top to bottom and via next relationship to neighbouring nodes in the same level. As visible below, this helps structure the queries for date filtering by filtering the year, then month, then day and so on. This helps reduce the number of records significantly at each step and improves filter queries.Time tree graph sampleTo create the time tree graph, there is Graphaware Timetree product, that provides jar files which can be accessed as plugin in the Neo4j environment.To create the general purpose time tree graph to start with quickly, find the code below. Change the years’ range in the code on the top and it will create the time tree graph as depicted above.This is the forked version that I have on my own github, original writer is Laurie Boyes, described in his blog here:Adventures with Neo4j and Timetreesmedium.comThis concludes few of the considerations while designing your database as a graph database in Neo4j.;Aug 14, 2020;[]
https://medium.com/neo4j/spring-data-neo4j-rx-beta-iii-79573a773584;Gerrit MeierFollowFeb 3, 2020·5 min readSpring Data Neo4j RX — Beta III.On the Road to GAPlease note that Spring Data Neo4j RX replaces Spring Data Neo4j in the future. More information can be found here.We are happy to announce that the third beta release of Spring Data Neo4j RX (SDN/RX) has just arrived on Maven central. While it has been pretty silent around the features we put into SDN/RX lately, we want to give you an overview of what’s in there, and what should follow to make it GA-ready for us.What’s new in the latest versionsIn our first introduction to SDN/RX, we have already presented the first(limited) mapping capabilities, like modelling nodes and their direct relationships. But today (and some 86 commits later), SDN/RX has evolved into a grown-up framework to help you work with your graph.Projection supportSometimes it is more convenient to load a view-like presentation of your data based on your already defined domain. With projections, you can get the data in the representation you need for your use-case.Assuming that we have a Person entity defined as:@Nodepublic class Person {   @Id @GeneratedValue private Long id   private String firstName   private String lastName   @Relationship( LIVES_AT )   private Address address   @Node   static class Address {      @Id @GeneratedValue private Long id      private String zipCode      private String city      private String street   }   public String getFirstName() {      return firstName   }   public String getLastName() {      return lastName   }}and you want to return only specific fields of the entity, you could either define an interface like so:public interface PersonSummary {   String getFirstName()   AddressSummary getAddress()   interface AddressSummary {      String getCity()   }}which also shows how to use nested projections. It is also possible to use the Spring Expression Language in your projections:public interface NamesOnly {   String getFirstName()   String getLastName()   @Value( #{target.firstName +   + target.lastName} )   String getFullName()}In situations where it is better to work with a class instead of an interface, you could define something similar to:public class NamesOnlyDto {   private final String firstName   private final String lastName   public NamesOnlyDto(String firstName, String lastName) {      this.firstName = firstName      this.lastName = lastName   }   public String getFirstName() {      return firstName   }   public String getLastName() {      return lastName   }}and add your presentation model logic there.Dynamic relationshipsWith dynamic relationships, you get more freedom in an ever-changing domain. It is as easy as:Map<String, TargetEntity> dynamicRelationshipsto define them. The key is the relationship type, and the value points to the entity you want to link to. This feature can be used for reading and writing.Relationship properties mappingAnother feature that targets the relationships is the mapping of properties on relationships. You define such a relationship in an entity by providing the target entity, and a class that holds the properties within a Map:@Relationship( LIKES )private Map<Hobby, LikesHobbyRelationship> hobbiesThe properties class needs to get annotated with @RelationshipProperties to show its purpose. The class itself looks like a simple POJO:@RelationshipPropertiespublic class LikesHobbyRelationship {   private final LocalDate since   private Boolean active   public LikesHobbyRelationship(LocalDate since) {       this.since = since   }}As you can see, it is possible to harness all the supported types that you can also use in entities. Of course, it is also possible to keep those classes immutable, if desired.New abstraction layer: Neo4jTemplateWe have introduced this layer between the repositories and the Neo4jClient, and based all operations in the repository layer on the templates. This gives us not only a clean separation of concerns, but also enables you to work with an abstraction that is not bounded to a repository. You are also still aware of how to load and persist your domain.Kotlin supportBesides the general support for Kotlin (data) classes and repositories, SDN/RX has got an extra treatment for reified types.Using the Neo4jClient or its reactive counterpart allows you now to write more fluent code. For example:neo4jClient    .query( MATCH (n:User) RETURN n.name )    .fetchAs<String>().one()or:neo4jClient    .query( MATCH (u:User{name:Michael) RETURN u )    .mappedBy {_, u ->        val name = u[ name ].asString()        User(name)    }    .oneAdditionally, we introduced coroutines support when working with the ReactiveNeo4jClient.Database selectionOf course, you have heard about Neo4j 4.0’s multi-database feature, and you want to use it in SDN/RX. You can either define a static database name in your configuration (if you are using Spring Boot in the application.properties), or define a DatabaseSelectionProvider bean that can dynamically switch between the databases.You can find our example here in the SDN/RX repository, and a more detailed blog post written by Michael Simons to give you more information on this topic.Testing with DataNeo4jTestThe test annotation @DataNeo4jTest brings in additional autoconfiguration for testing purposes. It will start an embedded server to run your tests if the Neo4j test harness is on the class path, and there is no other driver bean already in place.Also, it recognizes existing ServerControls or Neo4j (part of the Neo4j test harness) beans, in case you wanted to provide a test fixture, or you have configured the Enterprise Edition. The autoconfiguration then steps back from creating that bean and configures only a driver instance connected to your harness.What elseYou can’t get enough of the features?Conversion support for all Neo4j Java Driver types and moreImproved immutable support: Related entities will get created firstDerived relationship names from property names for missing defined relationship typesThe (still not stable(!), but available as a public API) Cypher DSL has got more featuresBug fixing and improvement (thanks to YOU for reporting issues and feature requests ♥️).The ecosystem around Spring Data Neo4j RXWe are currently working closely together with the JHipster team to not only integrate SDN/RX there and make Neo4j accessible through this project, but also to give you support for creating entities and their relationships, based on what we think the best practices are.Thanks to the growing communityMissing features implemented or typos fixed, we appreciate all of your contributions to the project. In no specific order: Vivek Singh, Philipp Tölle, Ján Šúr, Dmitrii Abramov, Harpreet Singh and Frederik Hahne.But we are also thankful for your non-code contributions like reporting issues, asking for features, and bringing your ideas to us.The future is brightNow is the time for us to think about our GA release and what needed features are missing. Looking at the current state of open issues, there are some minor additions left, but also some missing features like optimistic locking and support for multiple labels that we really want to provide you in a 1.0 version.If you have any questions about Spring Data Neo4j RX, visit our community site. The best channel for your ideas or bugs (yes, please report them) is the project’s GitHub repository.Thanks to Michael Simons, Michael Hunger and Ljubica Lazarevic for providing feedback.;Feb 3, 2020;[]
https://medium.com/neo4j/neo4j-operational-dashboards-with-hawtio-69e5ac3a093f;David AllenFollowMar 4, 2018·7 min readNeo4j Operational Dashboards with HawtioWhen running Neo4j, it’s very useful to have an operational dashboard of metrics that tell you what’s happening on the server. From time to time you’ll want to get insight into things like how memory management is working, garbage collection, and also monitor aspects of Neo4j such as growth of your store, active transactions, and many other operational facets. This can give you insight into how a client is behaving, whether the server is appropriately configured for the load it is dealing with, troubleshoot issues, and so on.While Neo4j does not currently come with an operational dashboard, in this article I’ll show you how to set up one simply, owing to a lot of support Neo4j already has in place for JMX.To show how this works, we’ll go through several pieces and discuss the neo4j configuration as we go. We’ll cover:What JMX is, and how neo4j uses it to expose metrics outside the serverJolokia as a method of getting those JMX metrics on the webHawtio, the web-based operational dashboardNeo4j metrics through HawtioJava Management Extensions (JMX)JMX is a java technology that allows applications to expose Managed Beans” (or MBeans) that have methods that can be called to manage the application. JMX has been a part of java for some time, and all JDKs come with an app for connecting to and working with JMX endpoints called JConsole. It’s a bit primitive, and limited as it’s a fat-client desktop application, but proves the concept. In this article, we’ll be able to do much better than JConsole though.JConsole, fat-client desktop JMX monitoringNeo4j Support for JMXNeo4j already exposes a wide variety of JMX beans. As just one example of Neo4j’s JMX support, under the Transactions MBean, Neo4j publishes the number of open transactions at any given moment. So if you have a good JMX client (such as JConsole or the dashboard we’ll build here) you can monitor these values. The official documentation already covers how to enable raw JMX and use JConsole like the screenshot above, today we’ll just take that a bit further.The main point about Neo4j’s JMX support is that there are enough great metrics already in place, the data exposure part in the product is already done.Exposing JMX to the Web: JolokiaThe trouble with regular JMX is that it’s too java”. Plenty of java-based applications work with JMX, but what if you want to write a shell script to check monitoring data, or work with it in javascript? This is what makes the Jolokia project so useful. It helps automate JMX exposure, and makes it available as a REST-ful JSON-based API, meaning that any tech can easily use JMX services, since it’s just another REST API.Jolokia comes with a number of agents” (ways of plugging Jolokia into a java app to produce JMX metrics) and a number of clients” (things that talk to the Jolokia server and use the metrics).We’re going to use the Jolokia JVM Agent. The setup is straightforward:Download Jolokia (as of this writing I used 1.5.0) and copy agents/jolokia-jvm.jar to a convenient spotTell neo4j to use Jolokia by adding this line to your neo4j.confdbms.jvm.additional=-javaagent:/path/to/jolokia-jvm.jar=config=/path/to/jolokia.confFor our quick setup, make sure to create a jolokia.conf file that contains just a single line:cors.allow-origin=*This ensures that Jolokia sends the right headers to allow web clients to use the API within a browser. Jolokia has many other security settings that allow finer tuning, which you can have a look at.3. Restart your neo4j server, and Jolokia is exposed on port 8778 by default. You can check that jolokia is doing the right thing with curl. Here, we’ll just ask for how much heap memory is in use.$ curl -s http://localhost:8778/jolokia/read/java.lang:type=Memory/HeapMemoryUsage{   request : {     attribute :  HeapMemoryUsage ,     mbean :  java.lang:type=Memory ,     type :  read   },   status : 200,   timestamp : 1520173291,   value : {     committed : 536870912,     init : 536870912,     max : 1073741824,     used : 111612944  }}At this point, even without the operational dashboard you’ve got a cool new method of working with Neo4j via this RESTful API. The Jolokia protocol docs contain all of the details on this API and how it works.Launching our Operational Dashboard: HawtioHawtio is a modular dashboard for managing your java stuff”. Remember — because of how JMX works, it can work with any java application. I happen to need this for Neo4j, but the approach is generic.Setting it up is pretty simple. You can of course deploy it as a webapp inside of a container such as Tomcat or Jetty, but let’s go the even easier route for this.Download the stand-alone JAR of HawtioTake all of the defaults and launch it simply by running java -jar hawtio-app-1.5.7.jarOpen a web browser and go to http://localhost:8080/hawtio/jvm/connectYou’ll see this connection screen. Here we’ve already provided some reasonable defaults. Remember Jolokia is running on port 8778, and mak sure that path” is set to jolokia”.Dashboards and JMX MetricsHawtio presents you with a default dashboard, mostly showing operating system and load details, presented at the top of the article. This is very useful, but all generic. Let’s jump into the Neo4j specific parts. While using Hawtio, I started with an empty database that I then put under a heavy write workload, just to make the numbers fluctuate for illustration purposes.Go to the JMX” tab and navigate the tree to the left to find the entry for store file sizes, as seen in the image below. This tree you are seeing is a complete catalog of all JMX metrics exposed by the Jolokia server, and it contains a lot of useful things you might want to look into. All of the Neo4j stuff will (naturally) be under org.neo4j but there is a host of other useful things under java.lang and java.nio that you get for free here.Snapshot of store file sizesThis screen shows a snapshot of the values. By clicking on the Chart” tab, you can see a scrolling chart of just these values. Because of my write workload, you can see how the numbers fluctuate over time.Store file sizes fluctuating under write workload, and some indexingAnother example to illustrate this is the primitive count” JMX metric. As the server is under a write workload, the number of nodes and properties increases, as you’d expect. The flat areas are where my load script isn’t running, and the steep rises are when I start it again.Neo4j Configuration can also be seen via JMX, under the appropriate MBean:As seen in the tree on the left, there are a number of other MBeans you can explore. If you’re wondering what any of those individual metrics mean, consult the Neo4j JMX documentation here.Advanced: Running JMX OperationsSo far, these charts are nice for passive monitoring. What if you’re looking for something specific?JMX MBeans can expose a series of operations, which under the covers are really just methods on Java objects. Neo4j doesn’t expose a whole lot of interactive JMX methods at this point, but there are some you may want to investigate. Choose a JMX MBean on the left hand side, and then click the Operations” tab at the top of Hawtio. Here, we’re looking at diagnostics.Neo4j Available Diagnostic OperationsThis is really just an interface for using Hawtio as a web client to call methods on java objects inside of Neo4j.Let’s click on the dumpAll()operation. You’ll notice that Hawtio gives you the full URL for this. The URL shows that Hawtio is just proxying a particular call back to Jolokia, which is handling all of the JMX stuff. In turn, you can see how you’d execute the same operation from the command line using curl.About to execute a JMX operationClicking the green Execute” button, we get all of our diagnostics back as we’d expect.Output from Neo4j’s Diagnostics MBeanConclusionHawtio is a great basis for getting a quick visual representation on many aspects of Neo4j server monitoring. It isn’t perfect though, because many of these views weren’t built for Neo4j specifically. I did run into some layout issues and cross-browser issues when trying to design my own dashboards.Hawtio allows plugins to be developed so that quite a bit more can be added, if you’re so motivated, and it’s a surprisingly fast way to get started with Neo4j monitoring, considering it was not built for Neo4j.;Mar 4, 2018;[]
https://medium.com/neo4j/neo4j-youtube-channel-onwards-and-upwards-106198bdd0d9;ryan boydFollowJul 6, 2018·4 min readNeo4j YouTube Channel — Onwards and Upwards!When I worked on the Google Cloud Platform, we produced many developer-focused videos. While my videos have over 1M views in total, the Android Developer Relations team produced some of the best developer video content on the web.The Michael and Ryan Show” on the Google Developers YouTube channelUpon starting at Neo4j, my VP wanted to see Android-quality videos on a startup budget. I was slightly hesitant because I wanted to make sure we were able to have a high ROI on our content production and video content is notoriously hard to develop and maintainInstead of a bunch of professional studios with full-time staff, I needed to retrofit a windowless and airflow-less conference room to start filming.First camera setup Handmade Trolley!With the video studio in place, I started doing single-handed filming of a lot of content about Graph Databases and Neo4j.The most important videos I produced starring myself were the Intro to Graph Databases Series. This series now has close to 170k total views and 828 likes.Large Library of VideosThrough the help of many other producers, interviewees and engineers, we now have 700 videos on the channel, 1.1M views and 10k subscribers.My colleague, Mark Needham, started the Neo4j Online Meetup in the middle of this journey. This YouTube Live stream now has 39 episodes where he has hosted himself and other developers talking about GraphQL, data science in practice, enterprise data silos, software analytics, database querying standards and even learning Chinese.Just last week, Michael Hunger started a series of HowTo videos on the APOC Utility Library.Additionally, we’ve had the opportunity to interview dozens of developers, including Ashley Sun of Lending Club and David Meza of NASA.Learning the TradeThis has been an incredible learning experience. When I first started, I knew nothing about video editing or production. Nowadays, I find myself fairly competent but forever learning.A few (big) things I realized along the way:Equipment matters. An old colleague still at Google convinced me of that and another shared the equipment he was using. Of course, he was using $1,500+ of sound gear for simple podcasts. Thanks to my friend David McLaughlin, I received some great guidance from Jamie Baughman. Dan Fitzpatrick also helped me tune some sound and made me realize that my hearing isn’t as good as I thought, despite perfect hearing tests!Sound matters. I had static in recordings for the longest time. Didn’t notice it due to bad headphones, but eventually realized it and struggled for along time to eliminate it. Turns out it was because the camera was producing (and syncing sound to) video at 29.97fps and the Blackmagic capture device was capturing at 30fps.Lapel mics are awesome. I kept trying to get away from lapels because they would ruffle with movement or I’d (gulp) forget to connect them before shooting a long video. I got awesome shotgun mics mounted on (statically-positioned booms). Unless you have a crew to assist, these shotguns can be difficult to position in a way that avoids background noise and room echo.Too many effects distract the viewers. The original videos in my Introduction to Graph Databases Series had a few animations and effects which the audience harped on. I learned to learn from YouTube comments, but not take them to heart. I suggest you do the same!Do Not include both green and blue in your company logo if you ever want to do a chromakey background while you’re modeling your company’s t-shirts. Otherwise, you’ll have to learn how to do track mattes in post production and track mattes are not fun.Buying can be cheaper than building. This was a depressing lesson to learn. I spent a while hand crafting an awesome wooden camera dolly largely because I thought it would be cheaper. It wasn’t. Wood is expensive. Amazon is cheap.B&H photo doesn’t accept orders on Saturday. Saturday often was my experimenting day due to it being super quiet around the office. When I found new equipment to solve problems, I’d head over to B&H’s website, but due to Shabbat, I was unable to order the gear! This led me to be more thoughtful about my solutions.There are many other learnings on the software and equipment side that I hope to be able to share in future posts.What’s next?You tell us! We certainly have more videos coming as part of the Online Meetup, Intro series and APOC series. And we’ll continue to do interviews, webinars and more. Let us know if there’s anything else you’d like to see!p.s. check out one of the latest popular videos on our channel;Jul 6, 2018;[]
https://medium.com/neo4j/creating-a-data-marvel-part-10-lessons-and-resources-8ffb5bf0ad1;Jennifer ReifFollowMar 28, 2019·10 min readCreating a Data Marvel: Part 10 — Lessons and Resources*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)If you have followed this series through all 10 blog posts, I commend you! Thank you for reading along as I documented our project and code. From the analysis of the Marvel API and evaluation of the project direction to building the final UI components in the webpage, this process has exposed the time and energy required not only to populate a database and build a pretty simple full-stack application, but also how to focus on the goal of the project. In this post, we want to discuss 3 remaining topics.Future of the project — additions, improved features, more functionalityLessons learned — which ones stuck out the most to meResources — from code to foundational, where to find infoFuture of the ProjectFrom the beginning, we designed this project to be a demo for a presentation. However, as we started building the application, I think we began to realize the data could support a much more complex and intriguing application than our original intent. We obviously could not (and did not want to) build such an application for demo purposes, but it sparked plenty of ideas for continuing this project after the demo was complete.Data CleanupFirst, we could spend an entire effort and many hours cleaning up the data from the API. There were several fields and values in Marvel’s API that we had no way of knowing they weren’t valid until they were in our database. It might take some extra exploring in the API to find these inconsistencies and broken images, but since the data owners haven’t been perfect stewards of the data, then we can improve the data integrity on our end.If there are certain values we cannot clean up, then we may need to explore better ways to handle those on the frontend. After all, most websites already focus on failing gracefully so that the user does not see confusing error messages or ugly displays in rendering errors. We could do the same on our webpage so that missing images are thrown to a prettier default or just don’t appear.clean up missing image on our webpageWe could also likely retrieve additional data about our Character, Creator, Event, Series, and Story entities to populate in our database for retrieval. There might be additional insight we could gain by tying some of the other entities to one another, as well, though this would quickly add complexity in our application.Additional EndpointsThe application could also be built out to include additional endpoints for reaching more detailed information about the outlying Character, Creator, Event, Series, and Story nodes. Then, users could drill down into deeper information about each of these nodes from the webpage and find additional information about each one.include a link to more Character info?For instance, if users clicked on the Character in a ComicIssue from our main html page, we could send them to another webpage with more information about that character and possibly add another custom query to see which other superheroes this person appeared with the most (who they often teamed up with).Adding functionality like this would mean more complexity for adding all the relationships and properties across them, but it would definitely help with exploration of the network and analysis of the connectedness of the data set.Making the Webpage PrettierAs mentioned in our previous blog post, the frontend page could use some refinement. We gave it a great place to start, but I think we could add some relatively minor features that would greatly improve the usability and tidiness of the interface.only render chosen ComicIssue and its connectionsTo start, I want to add an adjustment to the graph rendering where it pulls the related ComicIssue and the relationships around it, rather than a segment of the graph each time.I began this process when I updated the render() method to execute each time the user clicked on a ComicIssue from the list in the left pane. However, I would need to adjust the query in the application to retrieve the nodes and relationships around the particular comic that was clicked and pass those nodes/relationships to the endpoint that builds the visualization. This would be a combination of frontend and backend work, but I think it would make the application and visualization more interesting and relevant.This was mentioned above, but we also want to handle some of the missing or bad data in a better way on the webpage. If empty results or unhelpful values return, then there isn’t much point in displaying them. While we might be able to clean some of this up in the actual database, we may not be able to handle it all. Blank or missing data can still be helpful, and we can make it valuable through better display.As an example, a ComicIssue might not be a part of a Series or an Event. Instead of showing blanks or null values, we could possibly use a message that states the comic is not in a series or event. The user may find that odd and drill into more information about the creators or characters involved to see if there is a pattern. Or, if the values are not empty, but unknown, then perhaps the API doesn’t have that information documented, and the user could do external research to fill in those values manually.One final thought on improving the webpage would be to make the list of ComicIssues in the left pane pageable. This would shorten the scroll bar and result set and allow the user to sift through search results in more manageable segments. Changing this would require some research on how to chunk the results from the database and then display them with arrows to move forward and backward through the results by certain amounts, but I think it would be a nice feature.Project Lessons LearnedIf you have followed this blog series, you probably know that we put some lessons learned for each step towards the end of each post. This may have been helpful to you, but has definitely been helpful for us in seeing the progress and tracking unusual developments along the way. I wanted to highlight a few of the learnings from previous posts, as well as add any others on the process as a whole.our data model for this data setPart 1: There is much to be learned from modeling a real data set. We can be introduced to concepts with provided examples, but those are neat and clean, with headaches removed for easy on-boarding and learning. We can practice with other people’s examples, but they have often done the work for us in cleaning up the data, creating a data model, and sharing an import script. We truly learn how to overcome when we face all these obstacles on our own with no predefined process or shortcuts to defeat the challenges. We don’t have to start from scratch, as we were given the tools and ideas for accomplishing these tasks in the other examples, but we must work out our unique situation as we go.Part 2: Along the lines of the one above, being given a practical example helps develop your skills to that unique situation. When we were faced with a buggy, inconsistent API, we had to figure out what to add to our Cypher queries to reduce timeout and still stay within the API restrictions of call limit and number of results. Sometimes, you just need to experiment with a data set hands-on to gain a deeper understanding. Of course, the implication of this is added time for experimentation. Having a deadline isn’t a bad thing, but it helps to have some built-in time for exploration to find the best possible solution.Part 3: Data import seems to be the long pole in the tent for most of the projects I have tackled so far. I think it is easy to assume that the main result of the project (application, process, demo, etc) will take the longest, but I have found that getting data from one place to another takes the most time and effort. After all, the structure and integrity of the data must be good or nothing else is. Factoring data format, missing/null values and handling, and transformations took some time and careful planning in order to get the results we were looking for.Part 4 and Part 5: Try to research tools and choose those that best fit the goals of the project. For us, Spring Data/Spring Boot was already familiar, but it also met the criteria of being simple and concise. Both those capabilities allowed us to meet our criteria for building a live-coding demo of the application. Simple and concise code meant we could cover everything we needed with good explanation in a presentation time slot. While using Spring Data Neo4j was a bit new to us, our project team (Mark and I :D) brought the foundational skills needed to apply to SDN. He had Spring expertise, and I had Neo4j knowledge. Together, we could depend on one another and stretch both our skills for the new project.Part 6 and Part 7: You shouldn’t reinvent the wheel, but sometimes you might have to customize the wheel a bit. :) Once we got one set of domain classes outlined to match one entity, we were able to copy/paste the same outline to our other entities and adjust for differing properties. There were plenty of code examples for creating relationships between entities, but none that were quite as tightly interconnected as our Marvel data set. It was here where we needed to go off script” and write/test code for our particular use case and data set.putting in the final piece in the St. Louis Gateway ArchPart 8: When you get to the point where you are knitting together all the pieces you have built into the completed end result, try to understand why each piece fits a certain way and make informed decisions. It’s easy to get caught in the scramble to cobble it together because you know it works that way. In our project, we divided our code into controller and service for the ComicIssue entity because we felt it kept responsibilities cleanly separated and made it easier for other developers to follow what method handled specific functionality. It was also tempting to simply copy/paste the D3 example code from other projects, but then I couldn’t have explained it to all of you in a blog post. Taking the time to take each puzzle piece and know why it belongs helps you reassemble and modify the pieces for your next project(s).Part 9: You also don’t need to over-engineer things right out of the gate. The saying Rome wasn’t built in a day” applies to software development, too. You don’t have to have the whole project in perfect order on the first iteration. For us, we didn’t need to build the perfect webpage with all the endpoints coded and functional. We could allow the project to evolve as needs and interest arose. Enjoy the learning process and let ideas surface for future improvements.*Disclaimer: I do realize that not every developer and every project has the leisure to expand timelines or experiment with new technologies. However, I do think it is important to tackle one small thing in each new project. After all, if you never do anything new and never allow time for better ideas, your output will never reach full potential. Developers may not have the voice for some of the decisions, but we can still do our best to champion those thought processes. Best of luck! :)Project ResourcesWe wanted to provide a place for all of you to have a single place to retrieve all the documentation we have used and referenced on this project. There is a lot of material here, but if you have followed along with all the previous posts, you have conquered all of these topics! Congratulate yourself on your learning and feel free to pick up additional information from the resources below as you embark on your next project!Github source codeAll previous posts in this blog series:Part 1 — Marvel data set and data modelPart 2 — data import to Neo4j from API (initial pass)Part 3 — data import (adding details)Part 4 — Spring Data Neo4j choice and structurePart 5 — first domain classes (Character, repository, controller)Part 6 — rest of domain classes (Creator, Event, Series, Story)Part 7 — tying entities together with relationshipsPart 8 — endpoints, queries, data formatting, and morePart 9 — web page and graph visualizationFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j GuideMarvel API (data set)Project Lombok docsSpring/Neo4j movie application exampleOf course, feel free to pull down the repository and add your own customizations or functionality to it! You can also reach out to us with questions, if needed. We would love to see more projects with Spring Data Neo4j and see how you are taking your applications to the next level. Keep building amazing things!;Mar 28, 2019;[]
https://medium.com/neo4j/graph-layouts-with-neo4j-graph-algorithms-and-machine-learning-9f92a108f35e;İrfan Nuri KaracaFollowJan 11, 2019·5 min readGraph Layouts with Neo4j Graph Algorithms and Machine LearningA couple of months ago we were having a chat with my colleague Mark Needham about their amazing work in developing Graph Algorithms in Neo4j when we somehow ended up in discussing use of them in graph layouts which has been a consistent matter of discussion in graph visualisation. We sat down together to see what we can make out of them and managed to generate a useful layout for communities within a dataset.Weeks later I had time to wrap up our work and present what we have done here. In this post we will see how graph algorithms and machine learning practices can help data visualisation, specifically layout of connected data.In 3 simple steps we willDetect the communities within a datasetCalculate a layout that helps identification of clusters at onceVisualise communities with d3.jsFirst some motivationForce directed layout algorithms are known to be the most useful ones for general purpose visualisation of a network of data aka graphs. Briefly these algorithms consider the graph as a physical system where the energy in the system is calculated using repulsive forces between nodes and attractive forces of relationships. The goal is to minimise the energy by simulating physic in the system which is expected to result in a better positioning of nodes.But for more specific cases, different approaches can facilitate user to interpret the nature of data under examination. The specific case we will focus on today is the detection and visualisation of communities within a graph.To give a better understanding, below is a Force directed layout of communities (people acted in same movies) in Movies dataset.Force directed layout of communities in Movies dataset (Neo4j Browser)As you can see one can not easily spot what are the communities and their members. We need a different approach to calculate a better layout to serve our purpose. In the following steps, we will use graph algorithms and ML practices to produce a better visualisation of these communities.Below is the process flow we are going to follow.Community DetectionFirst of all lets get some data and find communities within it. We are going to use the most popular graph database, Neo4j to store the graph data and detect communities (You can make a quick start to use Neo4j by downloading the Neo4j Desktop). Our sample dataset is well known movies database that includes some popular movies and actors. For the ones who are not familiar, you can create this data by typing :play movies to the cypher / command bar in Neo4j browser located at the top and following the guideline that will be presented. The objective is to detect the communities of actors considering the movies they co-acted in.We do not have to implement a community detection algorithm, since our brilliant colleagues in Neo4j labs have developed some of them as a plugin to the Neo4j in the Graph Algorithms library. In order to be able to use them you will need to install APOC and Graph Algorithms plugins in Neo4j Desktop. If you haven’t installed plugins before, our colleague Jennifer Reif has an excellent post explaining how to do this.We will use the Louvain algorithm to detect and store the community of each person in the database. This is as easy as running the cypher below in the Neo4j browser.CALL algo.louvain(MATCH (a:Person) where (a)-[:ACTED_IN]->() RETURN id(a) as id”, MATCH (a)-[:ACTED_IN]->()<-[:ACTED_IN]-(b) return id(a) as source, id(b) AS target, count(*) as weight”, { graph: cypher”, iterations: 1, includeIntermediateCommunities: true})This will store community value of each node in the node itself as a property. To check if it worked properly, we can look at community values of some people in the same movie, for instance the great movie UnforgivenMATCH (p:Person) WHERE (p)-[:ACTED_IN]->(:Movie {title: Unforgiven”}) return p.name, p.communityTo double check lets see another movie, The Devil’s Advocate this timeMATCH (p:Person) WHERE (p)-[:ACTED_IN]->(:Movie {title: The Devils Advocate”}) return p.name, p.communityAs you can see Keanu Reeves did not end up in the same community with two others since he is more strongly connected to the people of The Matrix trilogy.Layout CalculationI believe for the ones familiar with ML, the community of the nodes appeared as a charming feature candidate at first sight. But how can one feature help us to come up with a layout, simply x and y coordinates of the nodes? If you have dealt with neural networks before, you will at once recall that you would prefer those community information encoded as binary values, like one hot encoding. So the node in a community ‘2’ will be represented as [0, 0 ,1, 0, 0, 0…] where the length of the array is equal to the number of communities detected. We will again use the One Hot Encoding function from the Neo4j graph algorithms plugin for this purpose:MATCH (n) where exists(n.community) with collect(distinct n.community) as communitiesMATCH (n) where exists(n.community)set n.oneHotEmbedding = algo.ml.oneHotEncoding(communities, [n.community])Lets check the outputNow the problem has been reduced to assigning x & y values to nodes using this binary vectors. Here comes in the t-SNE (t-Distributed Stochastic Neighbor Embedding), which is a machine learning algorithm for dimensionality reduction. Oh wait. What does it mean and what does it have to do with our problem? Well, it means if you have a n-dimensional point, like the binary vectors above, than t-SNE will reduce that to a lower dimension at your will, which will be 2D for us now. The magic is the proximity of positions will be preserved during the reduction process, resulting nodes in the same community to be posinitied closely.Visualise data with d3.jsWe will use a javascript implementation of t-SNE to calculate the layout and draw it with d3.js as seen below.The community members are positioned in the vicinity of each other which enables users to spot communities at first sight, without any effort.You can find the code for layout and drawing in Codesandbox, here: https://codesandbox.io/s/q9z09nnpv9An important advantage of using ML practices in data visualisation is we can reuse the outputs of a layout in other parts of the dataset, other datasets or in different sessions. We will explore that in our following posts about this subject.;Jan 11, 2019;[]
https://medium.com/neo4j/having-another-go-e50823b6fc79;Nigel SmallFollowJun 11, 2020·3 min readHaving Another GoThe summer of 2018 marked the launch of the first new language driver since the introduction of our official drivers back in 2016. Here at Neo4j, we’d seen significant demand from the Go community for an official driver, and so we stepped up to fulfil that demand.A little bit of history…Neo4j was founded and built by Java engineers. We’ve since diversified into providing solutions for JavaScript, Python and .NET developers, but until 2018 we had never focused on the Go community. So, this was a brand new language and ecosystem for us to dive into. How hard could that be?At the time, we were also already prototyping a low-level C connector library. So, in order to give ourselves a head start, we chose to build on top of that. This seemed technically feasible, given the existence of the Cgo project. And with that in place, we would only need to build the upper API layer in Go.This worked in theory. But quite soon we started to see issues raised by users who were having difficulty installing, configuring and working with the C part of our solution. We made some attempts to patch issues and add more detail to the documentation, but it eventually became clear that the fundamental architecture on which we’d built wasn’t viable for the Go community at large.Back to the drawing boardKaleidico on UnsplashAt this point, our best option was to take a step back, re-evaluate what we wanted to achieve, and decide how best to move forward. It was clear that we hadn’t fully understood the needs and idioms of the Go community. We would need to replace the C component with pure Go, and we’d need to bring in developers with knowledge of the Go ecosystem to help us get this right.And so that’s exactly what we’ve done. Following a new intake into our Drivers Team, we’ve spent the past few months building our way back to where we need to be. The C library has been removed from the Go Driver, and a pure Go replacement layer has been installed. So we’re now delighted to be able to release the official Go Driver for Neo4j, version 1.8. Now without a C library in sight!Version… 1.8?Wait a moment, though… version 1.8? What does that mean? I’ve never heard of a version 1.8 anything before?Version 1.8 is a stepping stone release between the old 1.7 driver (built for Neo4j 3.x) and the new Neo4j 4.x software. It is 100% drop-in compatible with the 1.7 driver, but with a severely improved installation process.We’ve built the driver with the assumption that application developers are either using Go modules or vendor-based dependency management.How to get the Go driverThe GitHub repository for the driver contains tags for each major.minor.patch release. So to get the 1.8.0 driver installed, simply use the following incantation:go mod edit -require github.com/neo4j/neo4j-go-driver@v1.8.0But that’s not all. On top of full drop-in compatibility, we’ve also included preliminary support for the Neo4j 4.x series. In particular, the Go Driver 1.8 introduces support for Neo4j’s highly popular multi-database feature. This provides a solid bridge between the 3.x and 4.x series, helping to bring our support for the Go community back up to date. Note though that you’ll need to be using Neo4j 3.5 or above to pick up and use the 1.8 driver.Next stepsGoing forward, we are now working on full support for the Neo4j 4.x series in a new major Go Driver release. This will bring improvements to the API and optimisations under the surface. And we currently expect to be able to deliver this before the end of 2020.;Jun 11, 2020;[]
https://medium.com/neo4j/running-django-neomodel-within-a-docker-container-f0d68c1d38c7;CristinaFollowMay 26, 2021·3 min readRunning Django-Neomodel Within a Docker ContainerHere’s your quickstart guide for posterity, using the example in Django-Neomodel.A Neomodel community member asked about how to run Django Neomodel (a Neomodel plugin for Django) within a Docker container. So let’s show how to do it.Photo by frank mckenna on UnsplashHaven’t tried out Django-Neomodel yet? Start here.Neo4j for Django DevelopersFrom zero to deployment with Neo4j’s Paradise Papers dataset.medium.comLocal Setup — ExamplePrerequisites:An updated version of DockerDjango NeomodelThe Django-Neomodel repository comes with a Docker example. If you’re just getting started with Docker, this may be a good place to start.From the root of the Django-Neomodel repo, go to the tests directory:cd tests/Run the Docker Command (make sure your Docker service is running and up to date).docker-compose upVerify the Django admin is running: http://localhost:8000/admin/Verify Neo4j Browser is running: http://localhost:7474/browser/You’ll be then able to log in with the test credentials admin / 1234.Under the HoodDockerfileThe Dockerfile collects the commands Docker will be using to create your environment.FROM python:3WORKDIR /usr/src/appCOPY ./tests/requirements.txt ./RUN pip install — no-cache-dir -r requirements.txtCOPY .. .RUN chmod +x ./tests/docker-entrypoint.shCMD [./tests/docker-entrypoint.sh” ]docker-compose.ymldocker-compose.yml is where you’ll configure the ports you’ll be using to communicate with your Neo4j database.docker-compose.yml :version: ‘3’services:    backend:        build:            context: ../            dockerfile: ./tests/Dockerfile        command: ‘/bin/bash -c chmod +x /usr/src/app/tests/docker-entrypoint.sh && /usr/src/app/tests/docker-entrypoint.sh”’        volumes:          - ..:/usr/src/app        ports:          - 8000:8000”        expose:          - 8000        depends_on:          - neo4j_db        links:          - neo4j_db        environment:          - NEO4J_BOLT_URL=bolt://neo4j:foobar@neo4j_db:7687          - DJANGO_SETTINGS_MODULE=settingsneo4j_db:        image: neo4j:4.2-enterprise        ports:          - 7474:7474”          - 7687:7687”        expose:          - 7474          - 7687        volumes:          - db:/data/dbms        environment:          - NEO4J_AUTH=neo4j/foobar          - NEO4J_ACCEPT_LICENSE_AGREEMENT=yes          - dbms.connector.bolt.listen_address=:7687          - dbms.connector.bolt.advertised_address=:7687volumes:    db:docker-entrypoint.shdocker-entrypoint.sh is where you’ll configure the python/Django portion of your app. You’ll add your DJANGO_SUPERUSER_USERAME and other Django-related environment variables here.docker-entrypoint.sh :#!/bin/bash -xecd testspython -c <<EOF |from django.db import IntegrityErrortry:    python manage.py install_labelsexcept IntegrityError:    print(Already installed”)EOFpython manage.py migrate # Apply database migrationsif [ $DJANGO_SUPERUSER_USERNAME” ]then    python manage.py createsuperuser \        — noinput \        — username $DJANGO_SUPERUSER_USERNAME \        — email $DJANGO_SUPERUSER_EMAILfi$@python manage.py runserver 0.0.0.0:8000ResourcesEmpowering App Development for Developers | DockerWhat’s New Register now for DockerCon LIVE | May 27 DockerCon LIVE is a free, one day virtual event on May 27 for…www.docker.comDjangoDjango is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Built by…www.djangoproject.comneo4j-contrib/neomodelAn Object Graph Mapper (OGM) for the neo4j graph database, built on the awesome neo4j_driver Familiar class based model…github.comneo4j-contrib/django-neomodelThis module allows you to use the neo4j graph database with Django using neomodel Install the module: $ pip install…github.comneo4j-examples/paradise-papers-djangoA simple Django web app for searching the Paradise Papers dataset backed by Neo4j Welcome to Paradise Paper Search…github.comDocker-compose: db connection from web container to neo4j container using boltThanks for contributing an answer to Stack Overflow! Please be sure to answer the question. Provide details and share…stackoverflow.comNeo4j for Django DevelopersFrom zero to deployment with Neo4j’s Paradise Papers dataset.medium.comAbout the AuthorsCristina and Alisson work at The SilverLogic, a software development company based in Boca Raton.Alisson is a software engineer at The SilverLogic. Passionate about plants, he is the driving force behind Que Planta, a GraphQL-based social network for plants.;May 26, 2021;[]
https://medium.com/neo4j/get-ready-for-neo4j-summer-of-nodes-3aa110652662;Ljubica LazarevicFollowJul 15, 2020·3 min readGet ready for Neo4j Summer of Nodes! 🌞Thinking about how to pass your time this summer? Do you have the urge to learn or want to tackle a new challenge? Never fear — Summer of Nodes is here!Starting on 3 August, 2020, we’ll be running a four-week themed event for you all to tantalise those thought processes. We’ll be hosting regular live streams, as well as technical blog posts on various topics around graphs. With something for beginners and experienced alike, you will not want to miss out!Tell me more about the challengesThe theme for this year’s Summer of Nodes is Staycation”, and the challenges will be based on:The online day out — deciding which virtual museums and galleries to visitThe whodunit — solving a murder mysteryThe barbecue — planning for a socially distancing eventExploring the local area — discovering your neighbourhoodEach weekly challenge will have an easy version for those of you new to Neo4j, and a hard version for the seasoned graphistas. The challenges should take around 1–2 hours to complete.How’s it all going to work?There’ll be a stream on Twitch and YouTube: Our schedule is Mondays (08:00 PDT / 11:00 EDT / 16:00 BST / 17:00 CEST) and Thursdays (07:00 PDT / 10:00 EDT / 15:00 BST / 16:00 CEST)We’ll also post out a blog post each week detailing the current challenge, as well as the solution to the previous oneHow do I get ready?A great question! You will probably find the following tips helpful to get you hitting the ground running. Of course, we recommend you register to receive updates!I am new to Neo4jWelcome! You’re going to have lots of fun! The challenges are a gentle, enjoyable way to introduce you to Neo4j and graphs without prior knowledge. However, if you want to get a head start (no harm in that 😉), you can:Watch a video on why might you use a graph databaseOr another video on the property graph modelHave a go at the Movie Graph on Sandbox. Follow the introductory guide on how to navigate the environment, and then run :PLAY movie-graphDo the free introductory training courseI’ve got some Neo4j experience under my beltWelcome too! These challenges will be testing your Cypher prowess, as well as using some of the available plugins and other resources. It might be a good idea to be comfortable looking up various documentation. Whilst we won’t suggest anything specifically to prepare on, if you want to do something before we get started, perhaps consider doing the Neo4j Certification, if you’ve not already done so!For all participantsIn general:You’ll probably find it useful to register on the community forum in case you have any questionsYou can follow our Twitch stream to be notified when we go liveAnything else?Oh yes! We have more to announce, stay tuned to find out more!I have a question…If you’ve got any queries about Summer of Nodes, please let us know on this thread on the community forum.I’m sold! How do I sign up?You can fill out the following form, and we’ll keep you updated with all the details as and when they happen.We look forward to seeing you all!;Jul 15, 2020;[]
https://medium.com/neo4j/k-means-clustering-with-neo4j-b0ec54bf0103;Nathan SmithFollowAug 27, 2020·12 min readK-means clustering with Neo4jWhen faced with a set of many items, we often want to organize them into groups. Simplification from many individual examples to higher-level groups of similar items can help us wrap our heads around what might otherwise be an overwhelming amount of data. If we already know the different categories that we want to group our data into, and we have examples of each category, we can use supervised machine learning to classify our data. If we don’t know the categories in advance, we can use unsupervised machine learning to discover new groupings from our data.Photo by Jarek Ceborski on UnsplashThe community detection algorithms that come in Neo4j’s Graph Data Science library are one way to apply unsupervised machine learning. We can use them to find groupings based upon relationships among items. If we need to group items based on similar properties instead of relationships, we can use clustering algorithms such as k-means. In this article, we’ll look at how we could implement k-means and the k-means++ seeding algorithm in Neo4j’s Cypher query language. We’ll also compare the results of k-means to clusters we could generate through the Louvain community detection algorithm.I like k-means because it is a powerful algorithm that I also find easy to understand. It has a long history in data analytics. Hugo Steinhaus was one of the first to propose the idea in the 1950s.You can use k-means when you believe that your data could be grouped into a known number of natural clusters. The algorithm assigns each example in the data to a cluster of items with similar values for the properties that you are considering. A very simple example could be your data points are the weights and heights of some volunteers, and you want to determine which categories they would map to, based on these data point values.We decide in advance how many clusters we want to divide the data into. The number of clusters that we pick is called k”, and that’s the k” in k-means. Next, we select one example at random from our data set for each of the k clusters. These randomly-selected points become the centroids of our clusters. Each example in the data set is assigned to the cluster that corresponds to the nearest centroid. Then we move the centroid so that it sits at the average value along each dimension for the items in the cluster. This averaging step is where we get the means” in k-means. We repeat this process of assigning examples to their closest centroid, and then shifting the centroids to respond to their new cluster members. In time the algorithm should converge to a stable state where no examples (or at most a very few examples) are switching clusters at the assignment step.Implementing k-means in Neo4j sounds like a fun challenge. However, there are efficient, easy-to-use implementations of k-means like this one in Python and this one in R. Do we really need to build one in Neo4j? Let’s not be like the scientists in Jurassic Park — so preoccupied with whether or not they could, they didn’t stop to think if they should.Photo by Amy-Leigh Barnard on UnsplashI probably wouldn’t use a Neo4j-based implementation of k-means in production, but I think it could be worth your time to keep reading. If you are already familiar with k-means, thinking about implementing it in Neo4j could be a fun exercise to sharpen your Cypher skills. If you are already familiar with Cypher, using it to write k-means could deepen your understanding of unsupervised machine learning.Implementing k-meansStart with a Neo4j sandbox. We can use the blank project for this example.Choose the Blank Sandbox project to get startedThe computer science department at the University of Eastern Finland has made some clustering datasets available online that we can use for this example available at http://cs.joensuu.fi/sipu/datasets/ In particular, we’ll use synthetic data from the paper by P. Fränti and O. Virmajoki, Iterative shrinking method for clustering problems”, Pattern Recognition, 39 (5), 761–765, May 2006.The data consists of 5,000 examples that fall into 15 clusters. The file contains an x and y coordinate for each example. We’ll store the coordinates for each example using Neo4j’s point data type. This will make it easy for us to calculate the distance between examples. Load the data with this Cypher command.LOAD CSV FROM  http://cs.joensuu.fi/sipu/datasets/s3.txt  AS rowWITH row, toInteger(trim(substring(row[0], 0, 11))) AS x, toInteger(trim(substring(row[0], 12))) AS yCREATE (i:Item {location:point({x:x, y:y})})RETURN COUNT(*)We know from the documentation of our data set that there are 15 clusters. Let’s create 15 random centroids. Each one has a location taken from an item and an iterations property we can use to keep track of how many times we have moved the centroid.MATCH (i:Item)WITH i, rand() AS sortOrderORDER BY sortOrderLIMIT 15CREATE (c:Centroid)SET c.location = i.location,c.iterations = 0RETURN *Let’s assign each cluster a number so that we can tell them apart.MATCH (c:Centroid)WITH collect(c) AS centroidsUNWIND range(0, 15) AS numSET (centroids[num]).clusterNumber = num + 1RETURN centroids[num]I exported the items and the centroids to CSV and plotted them to show our starting point.MATCH (n) RETURN labels(n)[0] AS nodeType, n.location.x AS x,n.location.y AS yK-means random starting pointCypher’s distance function will calculate the distance between each point in the x-y plane. We can use this Cypher query to assign each item the cluster number for the nearest centroid.//Assign each item to the cluster with the nearest centroidMATCH (i:Item), (c:Centroid)WITH i, c ORDER BY distance(i.location, c.location)WITH i, collect(c) AS centroidsSET i.clusterNumber = centroids[0].clusterNumberNow we recalculate the location of each cluster’s centroid to be the mean x and y values for all points in the cluster. I also want to keep track of how many times I have iterated through the algorithm, so I will increment the iterations” property on each centroid.//Move each centroid to the mean of the currently assigned itemsMATCH (i:Item), (c:Centroid)WHERE i.clusterNumber = c.clusterNumberWITH c, avg(i.location.x) AS newX, avg(i.location.y) AS newYSET c.location = point({x:newX, y:newY}), c.iterations = c.iterations + 1The last two statements make one iteration of the k-means algorithm. We can combine them using Cypher’s with clause. We can also tweak the first part of the query to return the number of items that were reassigned to a new cluster with this iteration. The combined query looks like this.//Assign each item to the cluster with the nearest centroidMATCH (i:Item), (c:Centroid)WITH i, c ORDER BY distance(i.location, c.location)WITH i, i.clusterNumber AS oldClusterNumber, collect(c) AS centroidsSET i.clusterNumber = centroids[0].clusterNumberWITH i, oldClusterNumberWHERE i.clusterNumber <> oldClusterNumberWITH count(*) AS changedCount//Move each centroid to the mean of the currently assigned itemsMATCH (i:Item), (c:Centroid)WHERE i.clusterNumber = c.clusterNumberWITH changedCount, c, avg(i.location.x) AS newX, avg(i.location.y) AS newYSET c.location = point({x:newX, y:newY}),c.iterations = c.iterations + 1RETURN changedCount, c.iterations AS iterationsLIMIT 1Finally, we can wrap this statement in an apoc.periodic.commit() function to run it repeatedly until no points switch clusters after an iteration. Keep in mind that while the algorithm usually converges to a state where no points are switching clusters after each iteration, that is not guaranteed to happen. We set a hard stop after 20 iterations if the algorithm has not converged by then.call apoc.periodic.commit( MATCH (i:Item), (c:Centroid)   WITH i, c ORDER BY distance(i.location, c.location)   WITH i, i.clusterNumber AS oldClusterNumber, collect(c) AS       centroids   SET i.clusterNumber = centroids[0].clusterNumber   WITH i, oldClusterNumber   WHERE i.clusterNumber <> oldClusterNumber   WITH count(*) AS changedCount   MATCH (i:Item), (c:Centroid)   WHERE i.clusterNumber = c.clusterNumber   WITH changedCount, c, avg(i.location.x) AS newX,       avg(i.location.y) AS newY   SET c.location = point({x:newX, y:newY}),   c.iterations = c.iterations + 1   RETURN CASE WHEN c.iterations < 20 THEN changedCount ELSE 0 END   LIMIT 1    ,{})I exported and plotted the clusters and centroids. We’d like to find clusters that are cohesive and distinct from each other. Overall, it looks like the algorithm did a pretty good job, but there are a few clusters that we would draw differently. If we started over from a different set of random locations for our centroids, we might get a slightly better or worse result.K-means clustering resultsImplementing k-means++Since the starting point for our centroids influences the quality of our resulting clusters, we’d like to find a way to avoid a beginning configuration that will lead to a dead end. David Arthur and Sergei Vassilvitskii developed a straightforward seeding algorithm called k-means++ that helps pick good starting centroids for k-means. We start by picking any item at random to be the first centroid. Items are selected to serve as subsequent centroids with a probability proportional to the square of the distance to their closest existing centroid.We can code this out in Cypher and get a better sense of how it works in practice. First let’s delete our old centroids.MATCH (c:Centroid) DELETE cNow, select a single item at random and use its location for our first new centroid.MATCH (i:Item)WITH i, rand() AS sortOrderORDER BY sortOrderLIMIT 1CREATE (c:Centroid)SET c.location = i.location,c.iterations= 0RETURN *We’ll build up the code to execute the rest of the algorithm one step at a time. First, we need to know the distance from each point to the closest existing centroid. Here’s a fragment of Cypher to do that. Don’t try to run it by itself. We’ll assemble multiple fragments to create the whole algorithm.//Step 1. find the closest centroid to each itemMATCH (i:Item), (c:Centroid)WITH i, min(distance(i.location, c.location)) AS minDistanceWe’ll extend the previous statement to collect all the items into one list, and all the squared distances into a second list. We’ll end up with paired arrays, with the order of the items corresponding to the order of the squared distances.//Step 2. Collect the items and squared distances into listsWITH collect(i) AS items, collect(minDistance^2) AS squareDistancesWe’d like to get a running total of the values in the squared distances list. Cypher list comprehensions give us a concise syntax for accomplishing this.Imagine our list of square distances starts out like this:[4, 9, 1, 25…]Our first list comprehension will give us a list of progressively longer sublists. For each number idx in range (1, size (squareDistances) in the range from 1 to the size of the squareDistances list return a subset of the list beginning from the start of the list and going for idx values squareDistances[..idx] The code snippet looks like this:[idx in range(1, size(squareDistances)) | squareDistances[..idx]]Our output would start out like this:[[4], [4, 9], [4, 9, 1], [4, 9, 1, 25]...]The second list comprehension will add up the values in each of the sublists, giving us a running total. For each subList in the listOfLists, start a variable called sum at 0. Add each value i in the sublist to sum. The code snippet looks like this:[subList in listOfLists | reduce(sum=0.0, i in subList| sum + i)]Our output would start out like this:[ 4, 13, 14, 39 …]Here’s our query so far.//Step 1. Find the closest centroid to each itemMATCH (i:Item), (c:Centroid)WITH i, min(distance(i.location, c.location)) AS minDistance//Step 2. Collect the items and squared distances into listsWITH collect(i) AS items, collect(minDistance^2) AS squareDistances//Step 3. Turn the squared distances into running totalsWITH items, [idx in range(1, size(squareDistances)) | squareDistances[..idx]] AS listOfListsWITH items, [subList in listOfLists | reduce(sum=0.0, i in subList| sum + i)] AS runningTotalsWe can think of our running totals object as a number line divided up into segments corresponding to the square distances for our items. We select a number at random along this number line, and then select the first segment with a starting point to the left of our random number. We choose the corresponding item from our item list that goes with the segment. The probability of selecting any item being selected is proportional to the squared distance to an existing centroid.//Step 4. Select an item with probability proportional //to square distanceWITH items, runningTotals, rand()*runningTotals[-1] AS cutoffWITH items, runningTotals, [v in runningTotals where v > cutoff][0] as selected, cutoffWITH items, selected, apoc.coll.indexOf(thresholds, selected) AS selectedIndexWITH items[selectedIndex] AS selectedItemWe create a new centroid at the selected item’s location, then repeat the algorithm until we have the desired number of centroids. The complete code for the algorithm is below.//K-means++               CALL apoc.periodic.commit( // Step 1. Find the closest centroid to each itemMATCH (i:Item), (c:Centroid)WITH i, min(distance(i.location, c.location)) AS minDistance// Step 2. Collect the items and squared distances into listsWITH collect(i) AS items, collect(minDistance^2) AS squareDistances// Step 3. Turn the squared distances into running totalsWITH items, [idx in range(1, size(squareDistances)) | squareDistances[..idx]] AS listOfListsWITH items, [subList in listOfLists | reduce(sum=0.0, i in subList| sum + i)] AS runningTotals// Step 4. Select an item with probability proportional // to square distanceWITH items, runningTotals, rand()*runningTotals[-1] AS cutoffWITH items, runningTotals, [v in runningTotals where v > cutoff][0] AS selected, cutoffWITH items, selected, apoc.coll.indexOf(thresholds, selected) AS selectedIndexWITH items[selectedIndex] AS selectedItem// Step 5. Create the centroid CREATE (cnt:Centroid)set cnt.location = selectedItem.location,c.iterations = 0WITH selectedItemLIMIT 1 // Step 6. Return the number of centroids left to createMATCH (c:Centroid)RETURN 15 - count(c) )Here are the centroids selected by a run of k-means++.Centroids chosen by k-means++Here are how the clusters turned out after running k-means from that starting point.It may not seem like a huge improvement. There’s still randomness involved in this process, and starting from k-means++ doesn’t guarantee the best possible k-means clusters every time. It helps avoid the worst ones though. It also helps our model converge with fewer iterations.Compare k-means with LouvainWe said at the outset that clustering algorithms work based on property similarity, and community detection algorithms work based on relationships. However, we can play with that distinction by creating relationships among nodes that have similar properties. Then, we can use the Louvain community detection module to identify communities based on those relationships.We’ll be using Neo4j’s Graph Data Science library. Our first step will be to load the graph into memory. In our data, the x and y axes are on a scale of 0 to 1,000,000. I expect that most items in the same cluster should be no more than 200,000 units apart. We will project relationships between any pairs of nodes closer than 200,000 units. The Louvain algorithm can use a weight on each relationship. After some experimentation, I found that weighting each relationship as the inverse square of the distance between the points worked well. This inverse square weighting shows up in physics, including the way sound gets quieter as it travels across distances. I imagine the tiny voices of nodes echoing across the void.Here’s the code to load the graph.CALL gds.graph.create.cypher(clustering-demo,MATCH (i:Item) RETURN id(i) AS id,MATCH (i1:Item), (i2:Item) WHERE distance(i1.location, i2.location) < 200000AND id(i1) <> id(i2)RETURN id(i1) AS source, id(i2) AS target, (distance(i1.location, i2.location)) ^(-2) AS similarity)Now we can run Louvain community detection and count the number of examples in each community.CALL gds.louvain.stream(clustering-demo, {relationshipWeightProperty:similarity})YIELD nodeId, communityIdRETURN communityId, count(nodeId)Community detection resultsUnlike k-means, I can’t specify the number of communities that I will get back from Louvain. This run of the algorithm gave me 22 communities. From the first five rows of the results, I can see that that a few of the communities have very few nodes.Let’s write these communities back to the graph, then export and plot them.CALL gds.louvain.write(clustering-demo, {relationshipWeightProperty:similarity, writeProperty:louvainCommunity})YIELD communityCount, modularitiesRETURN communityCount, modularitiesLouvain communitiesThe fifteen large clusters look really good. The small communities could be merged into their bigger neighbors.I hope you have had fun exploring unsupervised machine learning from a few different angles in Neo4j. The tools of Cypher and the Graph Data Science library can help you see your data and the algorithms we use in a new light.;Aug 27, 2020;[]
https://medium.com/neo4j/neo4j-desktop-release-command-bar-security-deep-linking-be899d72b947;Oskar HaneFollowJan 31, 2019·4 min readNew Neo4j Desktop Release With Cool and Important FeaturesThe focus for this release (1.1.14) of Neo4j Desktop has been to tighten up the security for our users and to continue to evolve the integration support for third party app vendors. But we also added a command bar as a neat feature that should make you more productive and happy. Plus deep-linking which improves integration inside and outside of Neo4j Desktop.Installations and UpdateIf you haven’t installed Neo4j Desktop yet, it’s about time. You find it at the Neo4j Download Center and it comes even with a free license of Neo4j Enterprise for Developers.Install Neo4j DesktopOtherwise you see a green notification in the side-bar with the bell icon, which takes you to the update button to download and install this update.Notification Center on the UpdateGo, do it now, we wait.We’ll highlight a few important new features in this blog post.Command barOur users like to be productive and we have added a new tool to help with that.The Command bar, invoked by pressing cmd+k on macOS and ctrl+k on Windows / Linux, is a productivity tool that will help our users perform various operations.It learns over time and will fit better and better into the user’s usage pattern.Command Bar With SuggestionsHere are a few operations it currently can perform:Start graphsStop active graphShow log file of Neo4j DesktopShow log file of active graphTake the user to the manage view of any graphOpen graph appsShow ip addressMore commands will be added, we’re open to ideas and feedback on what to add here. So please let us know which operations you want to have quick access to.Animated GIF for Command Bar in ActionSigned Graph ApplicationsPrior to this release when our users were about to install a graph application they would be prompted with a modal saying along the lines of: Graph app is given access to your graph data, only add apps that you trust”.In Neo4j Desktop 1.1.14, graph applications can be signed and verified as trusted by Neo4j. Any signed and verified application will directly install.With a signed graph application the user can trust that the vendor of the application is who it claims to be and that the contents of the graph applications hasn’t been tampered with.The user can still install unsigned graph applications but the prompt have changed it contents to this.Privileged Graph Application operationsThis change is about the user giving graph applications permission to perform certain operations.Think of a newly installed application on your mobile device. If the app wants access to the camera, it needs to ask for it and the user grants/denies it.This is the first iteration of this feature and can be found in the graph application sidebar.Security Settings for Graph ApplicationsDeep linkingSince Neo4j Desktop 1.1.13 we have supported deep linking into graph applications.Enable Deep Link IntegrationWith this release we have iterated on it to support installing graph applications through deep links.To enable that we needed to restructure the format a bit, so deep linking into for example Neo4j Browser now looks like this (old format still supported).neo4j://graphapps/neo4j-browser?cmd=play&arg=cypherInstalling Graph ApplicationsThere are two new ways of installing graph applications.The first is that the users now can install from the file system, using the `file://` protocol. This is useful if you are in an disconnected/offline environment and you cannot load apps from a repository URL.The other way is automatic installation via a deep link.Given a user has Neo4j Desktop installed and clicks on a link like:neo4j://graphapps/install?url=<url-to-graph-app>the user will automatically start the application installation process without having to enter it in the applications sidebar.Here you can for example install the Neo4j ETL Toolneo4j://graphapps/install?url=https://r.neo4j.com/neo4j-etl-appNeo4j Desktop 1.1.14 ChangeLogThanks,The Neo4j Desktop Team;Jan 31, 2019;[]
https://medium.com/neo4j/handling-neo4j-constraint-errors-with-nest-interceptors-6d0c5909af9c;Adam CowleyFollowAug 10, 2020·6 min readHandling Neo4j Constraint Errors with Nest InterceptorsThis post is the third in a series of blog posts that accompany the Twitch stream on the Neo4j Twitch channel where I build an application on top of Neo4j with NestJS.This article assumes some prior knowledge of Neo4j and Nest.js. If you haven’t already done so, you can read the first three articles at:Building a Web Application with Neo4j and Nest.jsAuthentication in a Nest.js Application with Neo4jAuthorising Requests in Nest.js with Neo4jUnique ConstraintsThe definition of insanity is doing the same thing over and over again, but expecting different results.- Albert EinsteinOne thing that we haven’t tested yet is that our constraints are correctly handled in our API. You may recall that while implementing the Authentication functionality, we created a unique constraint to ensure that the email property was unique for any node with a User label:CREATE CONSTRAINT ON (u:User) ASSERT u.email IS UNIQUEWe can simulate the behaviour of two users attempting to sign up with the same email address by using Cypher’s range function to generate a collection with two numbers and using UNWIND to unpack them on to their own rows.UNWIND range(1, 2) AS rowCREATE (u:User {email:  duplicate@email.com })Running a CREATE command with the same email address will cause Neo4j to throw a Client ErrorNeo.ClientError.Schema.ConstraintValidationFailedNode(54776) already exists with label `User` and property `email` = duplicate@email.comSimilarly, if we create an exists constraint,CREATE CONSTRAINT ON (t:Test) ASSERT exists(t.mustExist)CREATE (:Test)This will also return a ConstraintValidationFailed error, but instead with a different error message:Neo.ClientError.Schema.ConstraintValidationFailedNode(54778) with label `Test` must have the property `mustExist`The JavaScript driver will instantiate these errors as a Neo4jError. If this, or any other Error is thrown during the request lifecycle, it will be caught by an Exception Filter.There are a number of built in Errors that the Exception Filter will recognise, but by default the application will return a HTTP 500 error code, representing an Internal Server Error. This is a generic response to signify that something has gone wrong on the server, rather than it being the cause of the request.Instead, because this is an error caused by the input, we should provide a response similar to the one provided by the ValidationPipe that checks the response against the directorators of the CreateUserDto Data Transfer Object.To do this, we can create an Exception filter.@Catching the ErrorAn Exception Filter is a class decorated with the @Catch decorator (imported from @nestjs/common) - the decorator accepts many arguments representing the type of Error that should be caught by the filter. The class implements the ExceptionFilter class exported from @nestjs/core and contains a catch method. This method takes the error object that has been thrown in the application, and an instance of ArgumentsHost which allows us to get the Request and Response objects.The Nest CLI has a method for generating filters: nest generate filter {name}. The command generates the file inside the current directory, so well have to navigate into the correct folder before running the command:cd src/neo4jnest g f neo4j-errorIf we replace the default error in the generated class with Neo4jError and update the type of the first argument in the catch method we should have a class similar to this.// neo4j-error.interceptor.tsimport { ExceptionFilter, Catch, ArgumentsHost } from @nestjs/commonimport { Request, Response } from expressimport { Neo4jError } from neo4j-driver@Catch(Neo4jError)export class Neo4jErrorFilter implements ExceptionFilter {    catch(exception: Neo4jError, host: ArgumentsHost) {        const ctx = host.switchToHttp()        const response = ctx.getResponse<Response>()        const request = ctx.getRequest<Request>()        // ...        response            .status(statusCode)            .json({                statusCode,                message,                error,            })    }}Because all of the errors thrown from Neo4j instantiate the same class, we’ll have to use the message property to work out what the error is. Based on the error messages earlier, we can tell that a string containing already exists with will indicate that the error is due to the unique constraint, and a string containing must have the property means that the query has failed on an exists constraint.if ( exception.message.includes(already exists with) ) {    // The value supplied isnt unique}else if ( exception.message.includes(must have the property) ) {    // Fails the exists constraint}Neo4j will throw a variety of errors from anything from failed connection to the server to things like Cypher syntax errors. For this reason, we should treat all other cases as a 500 Internal Error.We can see from both error messages that the label and property name are contained in backticks:Node(54776) already exists with label `User` and property `email` = duplicate@email.comNode(54778) with label `Test` must have the property `mustExist`So we can use the match function to extract a regex pattern with anything.const [ label, property ] = exception.message.match(/`([a-z0-9]+)`/gi)The /gi flags at the end of the statement tell the function that it should run a global match (g) and return an array of matched values and the i flag denotes that the pattern is case insensitive.As the function will return an array of values, and we know that the order will be correct, we can use destructuring to pass the label and property straight into a variable.All that is left is to update the status code, error, and message :let statusCode = 500let error = Internal Server Errorlet message: string[] = undefined// Neo.ClientError.Schema.ConstraintValidationFailed// Node(54776) already exists with label `User` and property `email` = duplicate@email.comif ( exception.message.includes(already exists with) ) {    statusCode = 400    error = Bad Request    const [ label, property ] = exception.message.match(/`([a-z0-9]+)`/gi)    message = [`${property.replace(/`/g, )} already taken`]}// Neo.ClientError.Schema.ConstraintValidationFailed// Node(54778) with label `Test` must have the property `mustExist`else if ( exception.message.includes(must have the property) ) {    statusCode = 400    error = Bad Request    const [ label, property ] = exception.message.match(/`([a-z0-9]+)`/gi)    message = [`${property.replace(/`/g, )} should not be empty`]}response    .status(statusCode)    .json({        statusCode,        error,        message,    })The final thing that is needed is to add this as a global filter in main.ts:// main.tsimport { NestFactory } from @nestjs/coreimport { AppModule } from ./app.moduleimport { ValidationPipe } from @nestjs/commonimport { Neo4jTypeInterceptor } from ./neo4j/neo4j-type.interceptor// Import the new Neo4jErrorFilter classimport { Neo4jErrorFilter } from ./neo4j/neo4j-error.filterasync function bootstrap() {  const app = await NestFactory.create(AppModule)  app.useGlobalPipes(new ValidationPipe())  app.useGlobalInterceptors(new Neo4jTypeInterceptor())  // Use the Neo4j Error Filter on all rooutes  app.useGlobalFilters(new Neo4jErrorFilter())  await app.listen(3000)}bootstrap()Testing the FilterTo ensure that this works correctly, we can add a new test case to the end-to-end tests in app.e2e-spec.ts. Firstly, in the beforeEach hook, we need to register the global filter on the test application:// app.e2e-spec.tsbeforeEach(async () => {    const moduleFixture: TestingModule = await Test.createTestingModule({        imports: [AppModule],    }).compile()    app = moduleFixture.createNestApplication()    app.useGlobalPipes(new ValidationPipe())    app.useGlobalInterceptors(new Neo4jTypeInterceptor())    app.useGlobalFilters(new Neo4jErrorFilter())    await app.init()})Then we can copy down the same test from should return HTTP 200 successful on successful registration. Because these tests are run in sequence, we can guarantee that the user has been created in the first test before the second is run.We’ll need to update the status code to be 400 instead of 201, and the body of the response should include an array of error messages including one saying that the email address has already been taken:it(should return HTTP 200 successful on successful registration, () => {    return request(app.getHttpServer())        .post(/auth/register)        .set(Accept, application/json)        .send({            email,            password,            dateOfBirth: 2000-01-01,            firstName: Adam,            lastName: Cowley        })        .expect(201)        .expect(res => {            expect(res.body.access_token).toBeDefined()        })})it(should return HTTP 400 when email is already taken, () => {    return request(app.getHttpServer())        .post(/auth/register)        .set(Accept, application/json)        .send({            email,            password,            dateOfBirth: 2000-01-01,            firstName: Adam,            lastName: Cowley        })        // Should return 400 instead of 201        .expect(400)        .expect(res => {            // The body should return the `already taken` error            expect(res.body.message).toContain(email already taken)        })})If everything has been registered successfully, the npm run test:e2e should report that all of our tests are passing:npm run test:e2e> api@0.0.1 test:e2e /Users/adam/projects/twitch/api> jest --detectOpenHandles --config ./test/jest-e2e.json PASS  test/app.e2e-spec.ts (5.041 s)  AppController (e2e)    Auth      POST /auth/register        ✓ should validate the request (347 ms)        ✓ should return HTTP 200 successful on successful registration (108 ms)        ✓ should return HTTP 400 when email is already taken (106 ms)      POST /auth/login        ✓ should return 401 if username does not exist (49 ms)        ✓ should return 401 if password is incorrect (104 ms)        ✓ should return 201 if username and password are correct  (98 ms)ConclusionNow that we are returning the correct response to errors caused by database constraints, the UI be able to treat these errors the same as it would with the validation errors returned by the ValidationPipe. As this error is also fully expected, we can reduce the amount internal server errors sent to our monitoring tools and cut down on the amount of debugging time.Futher ReadingNeo4j Cypher Manual: ConstraintsNest.js Exception FiltersIf you have any questions, comments or if you would like to see a feature added to the API, feel free to open a Github Issue.Join me Tuesdays at 12:00 UTC / 13:00BST / 14:00CEST / 15:30 IST on the Neo4j Twitch Channel for the next session.;Aug 10, 2020;[]
https://medium.com/neo4j/data-loading-into-neo4j-whats-new-in-data-importer-a9c19e3cab70;Jonathan TheinFollowNov 3, 2022·3 min readData Loading Into Neo4j — What’s New in Data Importer?We’ve made some Data Importer improvements that make importing your flat file types into Neo4j easier.Neo4j Data Importer is the no-code approach to loading your flat file data into Neo4j, and it’s come a long way since its beta debut in March 2022.In our last update on Neo4j Data Importer, we introduced a new feature called Preview, which lets you, as the name suggests, preview your data model and ensure you have the correct mapping details before doing a full import.Data Importer PreviewCheck out the feature blog here. Super handy, and it helps reduce potential errors while loading the data to Neo4j.In this edition of What’s New in Data Importer, we have some improvements that make importing your flat file types into Neo4j easier. Let’s jump in.First is the ability to detect Datatypes from your flat file data auto✨magically.That’s right — we’re going full-blown Harry Potter over here.Data Importer will scan the first few rows of each file to assign what it thinks the correct datatype should be. But don’t worry — if you think something’s out of whack, simply select the correct datatype from the available drop-down menu.Neo4j Data Importer Type DetectionSecond, and speaking of datatypes…Data Importer now supports the DateTime datatype, which is designed to automatically detect the appropriate date format and assign this to your data property.Behind the scenes, we achieve this by using JavaScript’s lenient date parsing, which takes a best guess at how to parse the string.But there’s one caveat — there are certain limitations, where specific date formats could potentially lead to undesired results.To really ensure that DateTime datatypes are always handled correctly in Data Importer, the date time time string should be formatted in accordance withISO 8601 : YYYY-MM-DDTHH:mm:ss.sssZAnd third, Houston, we have a problem… with the file.Ever run into an issue while importing your data, and you weren’t sure why? Well, one probable cause could be that the file you’re working with has a missing column name associated with the data, or there could be duplicated column name(s), confusing the load progress.Now, you’ll be alerted when you pull in a data file with suggestions on how to remedy the potential issue.That wraps up all of the new features in Data Importer so far.One quick shoutout: our awesome colleague and Developer Education lead, Adam Cowley, demonstrates in under a minute how he imported 18MB of football (or soccer, depending on where you’re from) data into Neo4j using Data Importer.To start importing your data into Neo4j, sign up for a Neo4j Aura account, our fully-managed cloud service offering, and you’ll be up and running with graph databases in no time.Also, you’ll have a completely FREE Neo4j instance upon sign-up for the rest of your life.For feedback and feature requests, please let us know here.Data-Importer | Neo4jfeedback.neo4j.com;Nov 3, 2022;[]
https://medium.com/neo4j/creative-technical-web-content-how-we-redesigned-neo4js-developer-guides-afee8b0bc982;Jennifer ReifFollowJun 19, 2019·13 min readCreative & Technical Web Content: How We Redesigned Neo4j’s Developer GuidesAs with all web pages, style and content need refreshed with new features and user-friendly functionality. Our team decided to tackle this and present users with an all-new design of Neo4j’s developer guide pages. The team has worked hard to bring a polished look to content on concepts, explanations, and walkthroughs for topics surrounding Neo4j products, tools and libraries, and development with Neo4j.Web development is never as simple as hoped. We learned a lot in the process and were able to tackle some very specific goals to minimize support, increase maintainability, and (hopefully!) improve user experience. Our 2 main goals for the project were the following:Provide a functional, useful interface for learning Neo4j. We wanted the changes to be valuable for users interacting with the webpages to learn about our products and tools. We needed to ensure navigation was simple and easy to use and users could leave and come back to content easily. Also, users should be able to see the full list of all the content to find information on specific topics.Increase attractiveness and positive user interaction through styles and coloring. Content should be helpful for learning objectives, but it should also appeal to the user. Menus and other supporting components needed to be unobtrusive and colors should highlight important content and reduce distractions. We wanted a clean, simple design that didn’t obstruct the user from learning Neo4j.To accomplish these goals, there were a few components where we made changes. In the end, we were able to go live with an improved version that serves as the foundation for additional changes in the near future. Let’s take a look at what we did and how we got there!Structure and layout of pageAs a tech company, we needed to structure components for a variety of learning approaches. Some users walk through page-by-page and studiously follow an outline, while others search for adhoc topics or answers. We needed to provide for these two extremes, as well as for a mix of scenarios and users.This is where our multi-section layout is based. Along the top of the page, there is a header with our company/product logo, along with key resources for developers to learn about our technology. Beneath the header is the core of our material in 3 columns — left navigation menu for topic sections, middle column for core content, and right menu for outline of core contents.To build this frame, we used the CSS flexbox, which helps design a flexible and responsive structure (W3Schools CSS Flexbox). Our HTML outline looks something like below.<header>....</header><article id= content  role= main >  <div class= content-wrapper >    <div class= dev-menu-nav large-2 hide-for-medium-only hide-for-small-only columns >....</div>    <div class= guide-section-content large-8 columns >....</div    <div class= guide-toc large-2 hide-for-medium-only hide-for-small-only columns >....</div>  </div></article><footer>....</footer>And the CSS properties, then, use the display: flex property on the parent <div> with a few other properties set on the child elements..content-wrapper {  display: flex  align-items: flex-start}.dev-menu-nav {  flex: 1.5  align-self: stretch}.guide-section-content {  float: left}.guide-toc {  position: sticky  flex: 1}As you can see, the content-wrapper div sets the child components as flexible and also aligns the items at the start of the container. Each of the child items can then be customized and adjusted to the desired spot on the page.The dev-menu-nav element is the left navigation menu. This should take up the entire left side of the page until it reaches the footer, which extends across the bottom. We defined the flex property with a value of 1.5, which means the column can grow up to 1.5x times its size before it stops expanding. We also set the align-self property to stretch, so that the menu will extend to the bottom of the parent container and not stop at content height.The middle column is for our main content. It kept a pretty simple layout without much configuration. We only defined it to float to the left in our CSS, and this section extends the length of the contents in the guide.The final column is for the hovering right menu. This was a bit tricky to figure out, especially in tandem with the left menu. While the left menu extended the full parent height and menu contents remain static, the right menu should only be the size of its contents and scroll with the page. To accomplish this, we set the position to sticky, allowing the container to stick to the viewport and move separately from the page contents (letting it hover when scrolling) and set the flex value at 1, so that the menu size didn’t fluctuate with window size.While it seems rather simple, it took us some time to do the research and understand how to make the components somewhat independent, as well as somewhat aligned with each other.Page and content width with adaptable sizingUsers read and learn in all kinds of formats, including from phones, tablets, and desktop monitors. To support and improve experience from different devices, we hid non-essential content from the page in a mobile view and maximized content on the page for wider or larger screens. The design needed to be flexible to adjust to screen size and content.Since our layout used the flex display property, the columns scale according to the content width and the flex values we set. We set the left menu to scale up to 1.5 (flex: 1.5) and the right menu to grow up to 1 (no scaling). Our middle section, however, scaled too much on a large screen. This made the text sparse and difficult to read, so we needed to tell this section when to stop extending. It should also adjust menu width to the content for menu items with short or long strings of text.Our team’s general rule is about 120 characters per line. We can use the max-width property to set a maximum size for certain components to grow, but there wasn’t a way to use characters for the value of that property. Using a converter, 120 characters in our line calculated to 960px, which we adjusted slightly for font and size..dev-menu-nav {  max-width: 290px}.guide-section-content {  max-width: 920px}.guide-toc {  max-width: 300px}These values ensured that all three sections scaled, but only to a certain point. This allowed text, videos, diagrams, and images in the middle section to efficiently use screen real estate without stretching and making it unreadable.To handle the screen sizing for varying devices, we used standard column classes to show and hide certain elements.<div class= dev-menu-nav large-2 hide-for-medium-only hide-for-small-only columns >....</div><div class= guide-section-content large-8 columns >....</div><div class= guide-toc large-2 hide-for-medium-only hide-for-small-only columns >....</div>The small/medium/large-# tells the browser how many columns at each size scale that the div should take up out of the 12 in a web page. Our left menu div (dev-menu-nav) has large-2 to take 2 columns in a large size, then hide-for-medium-only and hide-for-small-only, so it will be hidden on medium and small screen formats. Note: medium screen size is smaller than anticipated. )The middle column (guide-section-content) has large-8 to use 8 columns at large scale. Since we don’t specify medium or small size classes, it will maintain that scale in all formats. The last div for our right menu (guide-toc) has the same classes that our left menu does — 2 columns at large scale, then hidden at both smaller sizes.The classes and max-width CSS that we added here ensured both menus are hidden on a mobile phone or narrow window, but that all elements scaled well on a large screen, with most screen real estate and scaling flexibility given to the main content in the middle.Fully-expandable left navigation menuWe needed to enable the left menu to expand and collapse all sections at once to allow users to view all of the guides and topics. A simple Javascript function was able to handle this for us, with a function call added to the html element.HTML:<dl class= menu-list >  <dd onclick= expandMenu(this)  class= accordion-dev-nav >Getting Started</dd>  <div class= panel-dev-nav >    <ul>      <li><a href= /developer/get-started/ >Getting Started</a></li>      ...    </ul>  </div></dl>Javascript:function expandMenu(domElement) {  $( domElement ).toggleClass( active )  var elem = $( domElement ).next(.panel-dev-nav)  elem.toggleClass( active )  if (elem.css(display) == block) {    elem.css(display, none)  } else {    elem.css(display, block)  }  if (elem.css(max-height) != 0px) {    elem.css(max-height, 0px)  } else {    elem.css(max-height, elem.prop(scrollHeight)+px)  }}The expandMenu function is called in the <dd> tag of each menu list section and passes the current element (this) to the function. The function defined in our Javascript file toggles an active class on both the current element (<dd>), as well as the next element (panel-dev-nav). The function also shows and hides the inner panel with all the links in the section (panel-dev-nav) and lets the section expand to the scroll height for varying numbers of links in each section. CSS handles the active class styling for change in color, altering the + icon to a - when it’s open, and dropdown speed.Page content outline in right menuTable of contents sections are helpful to see what a page covers or to find a specific topic. We created a hovering menu on the right side of the page with an unobtrusive, bookmark-like style. As a plus, it follows the window scrolling and highlights the current section in the view.Coding this involved setting up section headers with anchor tags. Most of our pages are written in Asciidoc with some Ruby templates and converters to render the Asciidoc to HTML. It’s simple enough to use an anchor in Asciidoc. You just need two items. 1. Some configurations at the top of the file for the renderer to include section link anchors, a table of contents (toc), the header of the toc, and the number nested levels anchored.:sectanchors::toc::toc-title: Contents:toclevels: 12. The [#tag] syntax above each header at the specified level. Note: what you put in the brackets will be the id value of that element in the HTML.[#starting-neo4j]Taking the first steps with Neo4j?In our Ruby template, we pull the tag from current page and set the id of the HTML element to that retrieved tag value, like below.toc_id = @id<div<%= toc_id && %(id= #{toc_id} ) %>>Finally, all that is left to do is track the scroll in the webpage and highlight each section as it hits the viewport. This seems fancy, but it was easily handled with some Javascript and CSS. There are a few libraries that implement this functionality, as well, but many seemed to bundle unnecessary dependencies that bloated size and complexity. First, let’s look at the Javascript.var visDict = {}var handleIntersect = function(entries, observer) {  entries.forEach(function(entry) {    visDict[ entry.target.id ] = entry.intersectionRatio    var sortable = []        for (var vis in visDict) {      sortable.push([vis, visDict[vis]])    }  sortable.sort(function(a, b) {    return b[1] - a[1]  })  var mostVisibleDivId = sortable[0][0]  $(.sectlevel1).find(li).removeClass( active )    $(.sectlevel1).find([href^= # +        mostVisibleDivId +  ]).parent().addClass( active )  })}var setObserver = function () {  var observer  var sections = document.querySelectorAll( .sect1 )  var options = {    root: null,    rootMargin:  0px ,    threshold: [0.25, 0.5, 0.6, 0.8, 0.9, 1.0]  }  observer = new IntersectionObserver(handleIntersect, options)  sections.forEach(section => {    observer.observe(section)  })}jQuery(document).ready(function() {  setObserver()})The first section of the code finds which div is most visible in the viewport and adds an active class to that element. The next code paragraph sets up the observer and its options on sensitivity for how much each section needs to be in the viewport before changing whether it’s active or not. Then, the next block loops through sections in the page, and the final block is the jQuery(document).ready() that calls the observer function when the document changes (including on scroll).For the highlighting, we simply add a block to our CSS file that changes the text color and font weight of the elements when the active class is present. This is what the CSS looks like..toc li.active a {  color: #428bca  font-weight: 600}Focusing the header menuWhen looking at technical content, we wanted to keep options succinct to reduce distractions and decisions on where to go or what to look at. To meet this, we created a developer header with only the key developer-related resources. We felt that these links represented resources that users would frequent, as well as places we wanted users to go for learning or knowledge-sharing.<header role= banner  class= global-header control-menu developer-header >  <div class= top-bar-container row column >    <nav class= top-bar primary-nav  role= navigation  id= responsive-menu >      <div class= top-bar-left >...</div>      <div class= top-bar-right >...</div>    </nav>  </div></header>On this container, we borrowed most of the styles from the rest of our neo4j.com header, but we altered a couple of things. We wanted a mental and visual separation/shift between main site content and technical guide content. Using a change in color helped us cleanly differentiate between the two, as well as create stark contrast between menus and guide text (dark menu background vs light guide background).Just as with our page content, this menu needed to be adaptable for various sizes, as well. Though a lot of the styling was defined from the main site, we altered one small CSS property on the <ul> elements within the top-bar-right div. This ensured that the page would not wrap the menu on a narrow window. On small screens, the wrapping left an ugly white margin on the right and cut off the bottom of wrapped content.ul.menu.large-horizontal {  flex-wrap: unset}Color and style contrastIn the paragraphs above, we touched on the fact that we wanted separation between technical and other sections of the Neo4j website, with color shifts as a way to accomplish this. There were a few other factors at work in this decision, as well as some reasoning about color choices.One of the general things we noted is that dark mode themes for screens, editors, and other developer interfaces are very popular, so this styling mimics what many developers are already familiar with and may prefer. However, dark text on light backgrounds is still the standard way to read material (books, webpages, etc), so we blended the two. Since darker elements tend to draw less attention, this seemed like a good fit for the menus and side elements. This also allowed us to contrast the darker trim with a lighter guide in the middle and draw focus to the learning material.We were very specific in the colors that we chose. The grey colors are the same ones that appear in our products like Neo4j Desktop and Browser. This created a consistent visual sense, whether interacting with our products or learning how to use them. These colors have also already been tested for user experience and compatibility.The dark styling extended to our code blocks, as well. We used a dark background and text highlighting similar to IntelliJ IDEA’s darcula theme. This has the additional benefit of familiar code styling that pops against the main content text throughout many pages. We used CodeMirror library’s out-of-the-box darcula mode to apply this style to the code blocks. To implement this, we included the darcula.css file in our project, then used uglify to bundle the styles and programming language modes into a Javascript file.uglify -s ./lib/codemirror.js,./mode/clike/clike.js, ./mode/cypher/cypher.js, ./mode/javascript/javascript.js, ./mode/shell/shell.js, ./mode/xml/xml.js, addon/runmode/runmode.js, ./mode/sql/sql.js, ./mode/go/go.js, ./mode/python/python.js, ./mode/php/php.js, ./mode/ruby/ruby.js -o ./neo4j-cm.jsWith these dependencies and the CSS classes on the appropriate HTML elements, we have code blocks that now look like this!Section anchor linksIn a previous section, we used anchor links for the right menu to track and highlight where the viewport was in the content. It’s also helpful for another thing, though — leaving and returning to a specific section within a page. The anchor link marks the location of tagged sections and adds the tag to the URL so the browser window displays that particular section.This avoids having to send a page link to a colleague or friend and telling them which paragraph or section to look at for the desired information. Just click the anchor link icon next to the related headline, copy the browser url (which has the anchor tag at the end), and send it along. The receiving person can open the page in the subsection that the sharer wanted.Our quick (not necessarily permanent) solution was to do this in Javascript. We definitely plan to explore other ways for a simpler and more elegant solution. Let’s take a look at what we have.jQuery(document).ready(function() {  $(h2,h3).css(display, inline)    .after($(<i class= fa fa-link  aria-hidden= true  style= margin-left: 20px color: lightgrey >)    .wrap( $(<a>) ).parent() ).next()    .attr(href, function () {       return $(this).prev().children().first().attr(href)     })})What’s coming next?Over time, our team will adjust a few styles and make updates. We will work (as always) on better and cleaner solutions and hope to continue improving the user experience Neo4j’s developer content.If you have any comments, feedback, or content you’d like to see added on the developer guides, feel free to post on the Community Site, and we will work on incorporating and personalizing it. Best wishes in your learning journey!;Jun 19, 2019;[]
https://medium.com/neo4j/week-17-discover-auradb-free-analysing-nft-trades-2186fc4a82ba;Michael HungerFollowJan 31, 2022·7 min readDiscover AuraDB Free: Week 17 — Analyzing NFT TradesThis week we used a subset of a published NFT Trades dataset to model, import, and analyze in Neo4j AuraDB Free.If you missed our recording here it is (sorry for the audio issues):It all started with a Tweet that I came across one late night about a research paper on NFT trades.Fortunately the authors also published the accompanying data, a whopping 6.1M trades in a CSV format.I spent a bit of time running full data import with the data importer tool and looking at the data.After sharing the dataset with @Tomaz.Bratanic, he took it even further, cleaning it up and doing a proper analysis that he published on TowardsDataScience. It’s a really insightful article, but even that still just scratches the surface of the data.Exploring the NFT transaction with Neo4jExploratory graph analysis of 6 million NFT transactionstowardsdatascience.comDatasetThe full CSV data set is too large for the limits of AuraDB Free — 3.8GB uncompressed CSV with 6M rows (it has 7.9M lines due to many descriptions with newlines).That’s why we look at a single day of trades, Jan 1, 2021, which has 5119 rows and is 3.6MB large.If you want to analyze a larger portion of the data, feel free to use an AuraDB Pro instance (paid) or Download Neo4j Desktop.We used xsv to filter the data down, and published the small CSV file to GitHub for you to download.xsv search -s Datetime_updated 2021-01-01  Data_API.csv | xsv count    5119xsv search -s Datetime_updated 2021-01-01  Data_API.csv  > nft_2021-01-01.csvxsv frequency -s Market nft_2021-01-01.csvfield,value,countMarket,Atomic,3214Market,OpenSea,1857Market,Cryptokitties,25Market,Godsunchained,19Market,Decentraland,4ModelThe model is pretty straightforward, with a few small tweaks.Graph Model for NFT TradesWe have an NFT which is in a Category and Collection. It is traded in a Transaction for this NFT, sold by a Seller, and bought by a Buyer, both of which are also Trader.The unique id’s for NFT are ID_token, for Traders their equivalent addresses and for the Transaction it’s the transaction_hash. (Actually during the livestream we learned that Unique_id_collection is a better fit as ID_token has duplicates across markets.)Data ImportWe use the data importer tool for a quick and easy modeling and data import run.TLDR If you don’t want to draw the model yourself we also have prepared a model + CSV archive that you can load directly with this link to the data importer.Load the CSVCreate the graph modelMap the same file time and again to the different nodes and relationshipsI labeled the Buyer and Seller both Trader and removed the prefix from buyer_address and buyer_username field-names, so that they are uniquely created once, no matter the role they have in a transaction.We mostly mapped the other properties directly to the different nodes (don’t forget to select the id-fields!) and for the relationships select the appropriate id-field for each node.We need to convert the currency fields Price_USD and Price_Crypto to floating point numbers.After all elements have gotten their blue checkmarks and all the columns in the CSV file are green, you can proceed to the import.Neo4j AuraDB FreeCreate a new Neo4j AuraDB Free instance, e.g. called NFT, make sure to save the password.The database takes 2–3 minutes to be provisioned, and after it is running you also need the connection URL.Aura Create DatabaseRun ImportWith the connection information, go back to the data importer and click Run Import.”Put in the details and click run.Afterwards you’ll see the the result overview with the runtime and can look at each import statement.Neo4j Browser and BloomIn the AuraDB UI you can Open” your database with Neo4j Browser a Graph Query UI that allows you to run statements, visualize the results as graphs and tables. This is where we will do our post-processing.With Neo4j Bloom, you can explore and visualize the data without needing to know Cypher.For both, you will need your saved password to log in.Post ProcessingAnd we need to post convert the Datetime_updated and Datetime_updated_seconds to datetime format, which the data importer doesn’t yet support.MATCH (t:Transaction)SET t.Datetime_updated =    datetime(replace(t.Datetime_updated, ,T))SET t.Datetime_updated_seconds =    datetime(replace(t.Datetime_updated_seconds, ,T))For Trader`nodes that have a `SOLD relationship, we set the Seller label, similar for Buyer. Some Traders will have both.MATCH (t:Trader)WHERE exists { (t)-[:SOLD]->() }SET t:SellerMATCH (t:Trader)WHERE exists { (t)-[:BOUGHT]->() }SET t:BuyerData ExplorationLet’s look at the data at a high level. Remember we only imported one day of trades, so the whole dataset is much more insightful.Number and volume of trades:MATCH (t:Transaction)RETURN count(*) as count, round(sum(t.Price_USD)) as volumeUSDWhich gives us an impressive half-a-million dollars in trades on New Year’s day of 2021 in only 1871 trades.╒═══════╤═══════════╕│ count │ volumeUSD │╞═══════╪═══════════╡│1871   │521768.0   │└───────┴───────────┘We will follow Tomaz’ blog post and only share a few of the queries here, so you can also copy the queries from there and read his analysis.Exploring the NFT transaction with Neo4jExploratory graph analysis of 6 million NFT transactionstowardsdatascience.comNFTs sold at the highest price:MATCH (n:NFT)<-[:FOR_NFT]-(t:Transaction)WHERE exists(t.Price_USD)WITH n, t.Price_USD as priceORDER BY price DESC LIMIT 5RETURN price, n.ID_token as token_id, n.Image_url_1 as image_urlPretty impressive prices — 65k, 33k, and around 15k for pretty ugly images (if you follow the links).Traders with the highest transaction count and volume:MATCH (t:Trader)OPTIONAL MATCH (t)-[:BOUGHT]->(bt)WITH t, round(sum(bt.Price_USD)) AS boughtVolume, count(*) as buysOPTIONAL MATCH (t)-[:SOLD]->(st)WITH t, boughtVolume, buys,     round(sum(st.Price_USD)) AS soldVolume, count(*) as salesRETURN       t.address AS address,       boughtVolume, buys,       soldVolume, salesORDER BY buys + salesDESC LIMIT 6Here we see clearly pure sellers (artists?), buyers, and some traders that buy and sell. Remember that this is only a single day.╒════════════~════════╤════════╤══════╤═══════╤═══════╕│ address             │ bought │ buys │ sold  │ sales │╞════════════~════════╪════════╪══════╪═══════╪═══════╡│ 0x327305a79~2b1a8fa │0.0     │1     │25314.0│253    │├────────────~────────┼────────┼──────┼───────┼───────┤│ 0xab5853ddb~703f5be │18443.0 │52    │0.0    │1      │├────────────~────────┼────────┼──────┼───────┼───────┤│ 0x95a437e4c~5e25d81 │110.0   │1     │1436.0 │33     │├────────────~────────┼────────┼──────┼───────┼───────┤│ 0x709a911d6~f24aef9 │0.0     │1     │210.0  │25     │├────────────~────────┼────────┼──────┼───────┼───────┤│ 0x75dffacbc~5322c4e │481.0   │17    │265.0  │5      │├────────────~────────┼────────┼──────┼───────┼───────┤│ 0x68aef8296~e111d0b │0.0     │1     │264.0  │18     │└────────────~────────┴────────┴──────┴───────┴───────┘We can also compute the highest profit someone made on this day:MATCH (t:Trader)-[:SOLD]->(st:Transaction)-[:FOR_NFT]->(nft)WHERE st.Price_USD > 1000MATCH (t)-[:BOUGHT]->(bt:Transaction)-[:FOR_NFT]->(nft)WHERE st.Datetime_updated_seconds > bt.Datetime_updated_secondsRETURN coalesce(t.username, t.address) as trader,       nft.Image_url_1 as nft,       nft.ID_token AS tokenID,       st.Datetime_updated_seconds AS soldTime,       round(st.Price_USD) AS soldAmount,       bt.Datetime_updated_seconds as boughtTime,       round(bt.Price_USD) AS boughtAmount,       round(st.Price_USD - bt.Price_USD) AS differenceORDER BY difference DESC LIMIT 5We find only one trader who sold something for more than 1000 USD and they made a nice 4867 USD profit.trader: KorniejtokenID: 8000067boughtAmount: 367.0soldAmount: 5134.0difference: 4767.0boughtTime: 2021–01–01T18:58:35ZsoldTime: 2021–01–01T19:47:48ZFor this NFT, not bad heh :)Example Picture of NFT sold for 5134.0 Jan 01 2021Another interesting aspect are traders with self loops, as exemplified by this statement:MATCH p=(t:Trader)-[:BOUGHT]->()<-[:SOLD]-(t)RETURN p LIMIT 10Or traders that repeatedly occurred in the same transaction:MATCH p=(t1:Trader)-[:BOUGHT]->()<-[:SOLD]-(t2)WITH t1, t2, count(*) as cORDER BY c DESCLIMIT 5RETURN substring(t1.address,0,10) as t1,        substring(t2.address,0,10) as t2, cSo on the same day some folks traded several times with each other:╒════════════╤════════════╤═══╕│ t1         │ t2         │ c │╞════════════╪════════════╪═══╡│ 0x7e9e93d7 │ 0x68aef829 │16 │├────────────┼────────────┼───┤│ 0xa0f80f5c │ 0x327305a7 │9  │├────────────┼────────────┼───┤│ 0x75dffacb │ 0x327305a7 │8  │├────────────┼────────────┼───┤│ 0x8d09aeac │ 0xfd62e6db │6  │├────────────┼────────────┼───┤│ 0x010bc884 │ 0x6958f5e9 │6  │└────────────┴────────────┴───┘Data VisualizationWith Neo4j Bloom we can look at the data visually, even the whole dataset of 9886 nodes and 19944 relationships.You can just open Neo4j Bloom from the AuraDB’s Open” button drop-down.You can look at co-buying behavior by entering the phrase.Seller Transaction NFT”Right click and choose Clear Scene” to remove the current visualization — otherwise it’s additive.You can also look at collections of NFTs with the search phrase Collection NFT.”We can also style the transactions based on volume and cryptocurrency.Pick the Transaction” entry in the right side legend and choose Rule-Based-Styling”:Price_USDSizeRangeRefresh Range0.25x to 4xApplyThen you can do the same for the color by adding another rule, going for:MarketColorUnique ValuesApplySo you can see the transactions stand out by volume and immediately spot the market they were on.ConclusionThis only scratches the surface of what you can do with the data.You can query and analyze moreVisualize more intricate relationshipsAdd new data and computed relationshipsRun graph algorithmsWrite apps that allow people to search for and visualize NFT tradesLet us know what you come up with.Happy graphing!ResourcesResearch paper on NFT tradesPublished the accompanying dataTowardsDataScience ArticleGitHub repositoryOther Discover AuraDB Free VideosOther AuraDB Free Medium articles;Jan 31, 2022;[]
https://medium.com/neo4j/building-a-location-aware-endpoint-using-neo4j-and-cloudflare-workers-b0886150f39;William LyonFollowNov 20, 2020·11 min readBuilding A Location-Aware Endpoint Using Neo4j And Cloudflare WorkersA look at the new HTTP Jolt format introduced in Neo4j 4.2In this post, we take a look at some updates made to the Neo4j HTTP API in the latest Neo4j 4.2 release, specifically a new result serialization format called Jolt (short for JSON Bolt). We then see how to use the Neo4j transactional Cypher HTTP endpoint in a Cloudflare Worker to build a location personalized election result endpoint.I covered this topic in a recent Neo4j Livestream so if you’d prefer to watch the video recording you can find that here:Overview of the Neo4j HTTP APIIn most cases when we’re building applications with Neo4j, we use one of the Neo4j language drivers to connect to Neo4j, send Cypher queries, and work with the results. These drivers allow us to use the language of our choice depending on what we’re using for our application. Under the hood, these drivers are using the Bolt protocol to connect to Neo4j and results are serialized using an efficient binary serialization format called PackStream. The drivers abstract away this protocol and serialization, and allow us to focus on building our application.However, in addition to the Bolt endpoint, there is also a transactional Cypher endpoint that can be used with Neo4j in cases where Bolt isn’t a viable option. To find the HTTP transaction endpoint you can use the discovery HTTP endpoint to list all available endpoints for your Neo4j instance:GET http://localhost:7474/Accept: application/json----------------------------200 OKContent-Type: application/json{   bolt_direct :  bolt://localhost:7687 ,   bolt_routing :  neo4j://localhost:7687 ,   cluster :  http://localhost:7687/db/{databaseName}/cluster ,   transaction :  http://localhost:7474/db/{databaseName}/tx ,   neo4j_version :  4.0.0 ,   neo4j_edition :  enterprise }The Transactional Cypher HTTP EndpointThe transactional Cypher HTTP endpoint enables us to open transactions, execute Cypher statements, commit or rollback transactions, and work with the results of our Cypher statements via HTTP requests. Results are serialized using either JSON or (as of Neo4j 4.2) Jolt.Jolt = JSON BoltJolt is short for JSON Bolt and is a JSON based format that encodes the response value’s type along with the value in a singleton object. This allows us to represent all types from the PackStream type system used by Bolt. One of the problems with using JSON for data serialization is that we can only natively represent a subset of PackStream types (think of datetime types, geospatial type, 64-bit integers, etc).In formats like MessagePack and PackStream a marker is used to encode the type of the value.With Jolt, the key of the singleton object indicates the type, and the value stores the value. Refer to the Jolt documentation for the full encoding documentation but let’s look at a few examples.Note that the value for nodes and relationships is a tuple (a 3-tuple for nodes and a 5-tuple for relationships) since indexing, in most cases, into a tuple is more efficient than working with an object.Jolt NodeHere’s an example of how a Node is serialized using Jolt.{   () : [    9285,    [       Airport     ],    {       code :  REH ,       name :  Rehoboth Airport ,       location : {         @ :  SRID=4326POINT(-75.122 38.72)       }    }  ]}This represents a node with internal id 9285, a single label Airport and three properties, code, name, and location where location is a Point type.Jolt RelationshipAnd here’s an example of a relationship represented in Jolt. Specifically, a relationship connecting the node with id 5129 to node with id 9285, of type IN_STATE, internal relationship id of 8, and no properties.{   -> : [    5129,    9285,     IN_STATE ,    8,    {}  ]}Jolt nodes and relationships can be composed in an array to represent a path in the graph. We’ll see an example of that later on in this post.Example Using the Election Results DatasetOn election night a few weeks ago I thought it would be fun to import live elections results data into Neo4j while we were all waiting for the results to come in. I won’t cover how to import that dataset in this post, but you can access the database at https://elections.graph.zone using the username elections and password elections.Let’s query this database and see what Jolt looks like for a real Cypher response. Here’s a simple Cypher query that will return nodes and relationships:MATCH (state:State {name: $state})<-[rel:IN_STATE]-(cas) RETURN * LIMIT 1To construct the HTTP request for sending this Cypher query, I like to use Postman which makes it easy to add the appropriate authorization and content headers.But of course, we can also use tools like curl:curl --location --request POST https://elections.graph.zone/db/neo4j/tx/commit/ \--header Accept: application/vnd.neo4j.jolt+json-seq \--header Content-Type: application/json \--header Authorization: Basic ZWxlY3Rpb25zOmVsZWN0aW9ucw== \--data-raw {   statements  : [ {     statement  :  MATCH (state:State {name: $state})<-[rel:IN_STATE]-(cas) RETURN * LIMIT 1 ,     parameters  : {       state  :  Delaware     }  } ]}Note the Accept header application/vnd.neo4j.jolt+json-seq which indicates we want Jolt to be returned from this request. Heres what the response body looks like:{     header : {         fields : [             cas ,             rel ,             state         ]    }}{     data : [        {             () : [                9285,                [                     Airport                 ],                {                     code :  REH ,                     name :  Rehoboth Airport ,                     location : {                         @ :  SRID=4326POINT(-75.122 38.72)                     }                }            ]        },        {             -> : [                5129,                9285,                 IN_STATE ,                8,                {}            ]        },        {             () : [                8,                [                     State                 ],                {                     result :  winner ,                     biden_winner : true,                     absentee_votes : 0,                     name :  Delaware ,                     votes : 504010,                     id :  DE ,                     electoral_votes : 3,                     trump_winner : false                }            ]        }    ]}{     summary : {}}{     info : {}}This Jolt response represents a path in the graph, an airport node connected to a state node:If we replace the Accept header with application/json, we can see how the result would be serialized using JSON. Notice how the different types are encoded via JSON.{   results : [    {       columns : [ cas ,  rel ,  state ],       data : [        {           row : [            {               code :  REH ,               name :  Rehoboth Airport ,               location : {                 type :  Point ,                 coordinates : [-75.122, 38.72],                 crs : {                   srid : 4326,                   name :  wgs-84 ,                   type :  link ,                   properties : {                     href :  http://spatialreference.org/ref/epsg/4326/ogcwkt/ ,                     type :  ogcwkt                   }                }              }            },            {},            {               result :  winner ,               biden_winner : true,               absentee_votes : 0,               name :  Delaware ,               votes : 504010,               id :  DE ,               electoral_votes : 3,               trump_winner : false            }          ],           meta : [            {               id : 9285,               type :  node ,               deleted : false            },            {               id : 5129,               type :  relationship ,               deleted : false            },            {               id : 8,               type :  node ,               deleted : false            }          ]        }      ]    }  ],   errors : []}In most case when were building applications using Neo4j, we make use of one of the Neo4j language drivers, specific to whichever language were using to build our application. These drivers use the Bolt protocol for working with Cypher and give us a language-specific and idiomatic way of using Cypher in our applications without having to think about the underlying transport or serialization layer. The Neo4j drivers are available for many languages and officially supported by Neo4j. So why would we ever want to use the HTTP transactional Cypher endpoint for working with Neo4j?There are a few cases where it makes sense to send Cypher over HTTP to Neo4j instead of using the Bolt language drivers. Perhaps were using one of the few languages that doesnt currently have a Neo4j driver. Or perhaps were using a system that will trigger a webhook and we want to use a POST request to insert some data into our database. Or perhaps we are using edge handlers like Cloudflare Workers. In a Worker, were not (currently) able to make arbitrary TCP requests, which means we cant use Bolt - but we can use the transactional Cypher HTTP API.Cloudflare WorkersEdge handlers like Cloudflare Workers are the next iteration of serverless and a technology that Im really excited about. Workers are comparable to function-as-a-service (FaaS) offerings like AWS Lambda, but address many of the shortcomings of FaaS, such as handling global deployment and eliminating the cold-start problem that can impact the performance of FaaS. A Cloudflare Worker is deployed to the global CDN edge network and runs on the same machines tasked with delivering static content as part of the CDN.Since Workers are deployed to the global CDN, that means each worker can be location-aware, taking into account the location of the user who initiated the request and serving a personalized response based on the location of the user. Lets see how we can use Cloudflare Workers with our election dataset in Neo4j to create an endpoint that will serve election results relevant for the user, based on their location.Because Cloudflare Workers run in a custom runtime on Cloudflares global CDN, there are currently some limitations. One of those limitations is that a Worker currently is not able to open an arbitrary TCP connection. That means we arent able to use Bolt to connect to our Neo4j database - however, we can use HTTP requests to connect to Neo4j. Good thing we just learned about the transactional Cypher endpoint!A Location Personalized Election Results EndpointIf we take a look at the Cloudflare Workers example page we see an example worker for  accessing the Cloudflare object . The Cloudflare object is attached to the request object passed to our worker and will contain some location-specific information.addEventListener( fetch , event => {  const data =    event.request.cf !== undefined ?      event.request.cf :      { error:  The `cf` object is not available inside the preview.  }  return event.respondWith(    new Response(JSON.stringify(data, null, 2), {      headers: {         content-type :  application/jsoncharset=UTF-8       }    })  )})You can hit this endpoint and see relevant information based on your request here: https://accessing-the-cloudflare-object.workers-sites-examples.workers.dev/{tlsExportedAuthenticator: { ...},tlsVersion:  TLSv1.3 ,httpProtocol:  HTTP/2 ,edgeRequestKeepAliveStatus: 1,requestPriority:  weight=256exclusive=1 ,country:  US ,clientAcceptEncoding:  gzip, deflate, br ,clientTcpRtt: 27,colo:  SEA ,tlsClientAuth: {certIssuerDNLegacy:   ,certIssuerDN:   ,certIssuerDNRFC2253:   ,certSubjectDNLegacy:   ,certNotAfter:   ,certVerified:  NONE ,certFingerprintSHA1:   ,certSubjectDN:   ,certFingerprintSHA256:   ,certNotBefore:   ,certSerial:   ,certPresented:  0 ,certSubjectDNRFC2253:   },asn: 33588}What were most interested in is the colo value. According to the Cloudflare Workers docs, the colo is the three-letter airport code of the data center that the request hit. Now you see why our Neo4j database includes airports!What we want to do now is find the airport that represents the data center where the request was resolved, then traverse our graph to find the state node, traverse to all counties in the state, and return election result data for those counties so that were showing the user the election results for only their state. Cloudflare has a geoip service as well, so we could enable that to get the actual latitude and longitude of where a request originated, however, Im using the free tier of Cloudflare and dont have access to that feature.Lets modify our Cypher query from above to find the relevant airport given the data center, and return the election results for all counties in that state. Using a Cypher feature called a pattern comprehension well project out the object we want to return to the user, returning only the relevant properties and computing others (such as the vote percentage for each candidate).MATCH (a:Airport {code: $colo})-[:IN_STATE]->(s:State)RETURN {    state: s.name, votes: s.votes, absenteeVotes: s.absentee_votes, bidenWin: s.biden_winner,    trumpWin: s.trump_winner,    counties:    [(s)<-[:IN_STATE]-(c:County) | { name: c.name, trumpVotes: c.trump, bidenVotes: c.biden,    pct_trump: (toFloat(c.trump) / (c.trump+c.biden)),    pct_biden: (toFloat(c.biden) / (c.trump+c.biden))}]} AS dataRunning this query using the ILG airport code (for New Castle Airport in Delaware) we see the election results for each Delaware county.{   votes : 504010,   state :  Delaware ,   trumpWin : false,   bidenWin : true,   absenteeVotes : 0,   counties : [    {       pct_trump : 0.556867221214585,       name :  Sussex ,       pct_biden : 0.44313277878541496,       bidenVotes : 56682,       trumpVotes : 71230    },    {       pct_trump : 0.31180177700618916,       name :  New Castle ,       pct_biden : 0.6881982229938108,       bidenVotes : 195034,       trumpVotes : 88364    },    {       pct_trump : 0.47929547340493917,       name :  Kent ,       pct_biden : 0.5207045265950608,       bidenVotes : 44552,       trumpVotes : 41009    }  ]}Now lets create a new worker using the Cloudflare console and execute this Cypher query against our Neo4j instance using the HTTP API (remember that in a Cloudflare Worker we currently arent able to make arbitrary TCP connections so using one of the Bolt language drivers for Neo4j isnt an option here)We’ll use fetch to make our HTTP request and return a JSON object with the relevant election result data, depending on the location of the user:addEventListener(fetch, (event) => {  const colo =    (event.request && event.request.cf && event.request.cf.colo) || SFO  event.respondWith(handleRequest(event.request, colo))})/** * Respond to the request * @param {Request} request */async function handleRequest(request, colo) {  var myHeaders = new Headers()  // add this header to enable Jolt format  //myHeaders.append( Accept ,  application/vnd.neo4j.jolt+json-seq )  myHeaders.append(Accept, application/json)  myHeaders.append(Content-Type, application/json)  myHeaders.append(Authorization, Basic bmVvNGo6bGV0bWVpbg==)  const statement = `MATCH (a:Airport {code: $colo})-[:IN_STATE]->(s:State)    RETURN {      state: s.name, votes: s.votes, absenteeVotes: s.absentee_votes, bidenWin: s.biden_winner,         trumpWin: s.trump_winner,      counties:         [(s)<-[:IN_STATE]-(c:County) | { name: c.name, trumpVotes: c.trump, bidenVotes: c.biden,         pct_trump: (toFloat(c.trump) / (c.trump+c.biden)),         pct_biden: (toFloat(c.biden) / (c.trump+c.biden))}]    } AS data`  var raw = JSON.stringify({    statements: [{ statement, parameters: { colo } }]  })  var requestOptions = {    method: POST,    headers: myHeaders,    body: raw,    redirect: follow  }  const response = await fetch(    https://elections.graph.zone/db/neo4j/tx/commit/,    requestOptions  )  const result = await response.json()  return new Response(    JSON.stringify(      (result &&        result.results[0] &&        result.results[0].data[0] &&        result.results[0].data[0].row &&        result.results[0].data[0].row[0]) || { colo },      null,      2    ),    {      status: 200,      headers: {        content-type: application/jsoncharset=UTF-8      }    }  )}Since this dataset is US-specific if the request hits an edge outside the US, instead of election results, we just return the data center of that edge. Also, note that in the Cloudflare testing console the cf object isnt available on the request, so we fall back to a default airport code of SFO.Our worker is immediately globally deployed. If we hit the endpoint, we’ll now see election result data based on our location. Here’s what it looks like if we hit the San Francisco data center:{   state :  California ,   votes : 17175022,   absenteeVotes : 8420433,   trumpWin : false,   bidenWin : true,   counties : [    {       pct_trump : 0.12156237491482494,       pct_biden : 0.8784376250851751,       name :  San Francisco ,       bidenVotes : 656185,       trumpVotes : 90806    }, ...  ]}You can try it at this endpoint: https://shrill-bread-4677.graphstuff.workers.dev/ and you should see election results relevant for your location.ResourcesLearn more about the Neo4j HTTP API in the Neo4j HTTP API docs.Learn more about Bolt, PackStream, and underlying details of the Neo4j drivers at 7687.orgLearn more about Cloudflare Workers at the Cloudflare Workers landing page.More details about using pattern comprehensions with Cypher in this blog post.;Nov 20, 2020;[]
https://medium.com/neo4j/first-proof-of-six-degrees-of-separation-55e39d37cba3;İsa YETERFollowAug 24, 2021·6 min readFirst Proof of Six Degrees Of Separation”Three months ago, I was talking with my friend about the COVID-19 virus.COVID-19 has taken the whole world under its influence in a very short time. So how is it that the planet we live on can succumb to a virus that spread from only one person in such a short amount of time?Well, if I told you that you can reach everyone in the world through a maximum of six people by following the path of your friend’s friend, would you better understand how the virus spreads so quickly?Everything is connected to each other, including the universe itself.Welcome to the graph world.Going back to the issue I mentioned above about reaching everyone through a maximum of six people — have you ever heard of the Six Degrees of Separation” theory?According to the theory of six degrees of separation, everybody on the planet is on average six or fewer social connections away from each other.This means that if we follow a chain of a friend of a friend,” we can reach anyone in a maximum of six steps.This theory was put to the test by Stanley Milgram in his small-world experiment in 1967, where the goal was to send a letter from Kansas to Boston with the chain of a friend of a friend.” As expected, the letter reached its destination after five people. Later, this theory was popularized by the 1993 film Six Degrees of Separation, starring Will Smith and Stockard Channing.Today, with the advent of social media, this number has been reduced from an average of six people to 2.9 to 4.2 people in the chain. This data is backed by research done by Facebook and other big social media platforms.Now, if you like, let’s put this story into mathematical terms.As I’m a big fan of Will Smith, let’s continue with him. Let’s say you want to meet him and you want to find out who you can reach him through.To do this, of course, you need to look at the accounts Will Smith follows.Will Smith follows 200 people. If there is someone in your followers among those 200 people, bingo! You can reach Will Smith through that person.Otherwise, what you need to do this time is look at the accounts of those 200 people and look at all the accounts they follow one by one.Let’s say that each profile you look at follows an average of 200 accounts again, so the number of accounts you need to look at in the second round will be 200 * 200 = 40,000 people.If there is someone in your followers among those 40,000 people, bingo again! You can reach Will Smith through two people!Otherwise, you need to look at the accounts of those 40,000 people, as well as all of the accounts they follow one by one.Again, assuming that each profile follows an average of 200 people, the number of accounts you need to look at in the third round will be 40,000 * 200 = 8 million people.If we continue like this, the number of accounts you need to look at in the forth round will be 1.6 billion. Oops! You’ve already surpassed the Instagram population!Let’s say you use Instagram so fast that it takes only one second to look at each profile. It will take you approximately 50 years to look at the 1.6 billion profiles in the forth round.That night, I searched for a website or app where I could test the six degrees of separation” theory. I found a few websites:The Oracle of BaconIt basically links any actor to any other according to the common movies they have made together.Erdos NumberThe Erdős number describes the collaborative distance” between mathematician Paul Erdős and another mathematician, as measured by authorship of mathematical papers.With the exception of those two, unfortunately I could not find any live examples related to this subject. That was the night I started coding Pathica.Pathica is an app built upon this revolutionary theory that — for the first time ever — allows you to put this amazing theory to the test.PathicaThe logic of the application is very simple. You search for the account you want to meet (it does not have to be of a famous person you can search for anyone), or you choose one of the ready-made lists on the homepage. When you press the connect button, Pathica finds the people who are between you and that person and shows them to you. For example, in the example above, you can see that there are only three people between Will Smith and myself. I can easily reach Will Smith by following this path of three people.The world is really small, isn’t it?Now, if we go back to mathematics again, in the story I told above, it takes 50 years for one person to analyze 1.6 billion accounts, while this process takes about 0.03 seconds in Pathica (yes, 1/30 of a second). It is almost impossible to do these operations in relational or NoSQL databases in such a short time, so I decided to use Neo4j as a graph database.There are only two data types in graph databases — one is a node” and the other is the relation.” Nodes are connected to each other through relations. You can create as many node and relation types as you like.Thinking from an aspect with regard to Pathica, you can think of nodes as people” and relations as following.” Currently, the node and relations numbers in the Pathica database are as follows:MATCH (n:User) RETURN count(n) as total_userstotal_users => 373809936MATCH ()-[r:follows]->() RETURN count(r) as total_relationstotal_relations => 2199487555373 million users and around 2.2 billion relations between users. It’s kinda huge, right? When I query the shortest connection path between two people in my Neo4j database, it only took around 0.03 seconds to analyze connections in this huge data. That’s why I love Neo4j so much ❤️They have great community support in their support forum and also in their discord channel. They have a startup support program also. This is what I receive after I applied to it :)Hello isa, We’d like to welcome you to the Neo4j Startup Program. Your application is approved and you now have access to the Enterprise Edition of the world’s leading graph database.As I finish my article, I would like to thank the Neo4j family for making my dream come true in proving the six degrees of separation” theory, which has been talked about, experimented with, filmed, and contested on talk show programs for more than 50 years.In my next article, I plan to address the problems and some technical issues I encountered while dealing with such big data.Pathica: https://www.pathica.com/AppStore: https://apps.apple.com/us/app/pathica/id1564780182About me: While studying at the Department of Mathematics in Boğaziçi University, I left the university in my 3rd year and found myself in the world of mobile applications. Since then, I have been actively writing code for 15 years, I have been the CTO of one of the biggest companies producing mobile applications in Turkey, Teknasyon, and for the last two years, I have been working as a single-man company in my home-office trying to produce something by myself.;Aug 24, 2021;[]
https://medium.com/neo4j/training-week-is-just-around-the-corner-ec20378e4d68;Ljubica LazarevicFollowSep 7, 2021·3 min readTraining Week Is Just Around the Corner!Starting on September 13, we are back with a week of live, hands-on training. From writing your very first graph database query to using machine learning to build a knowledge graph, we’ve got you covered.What’s It All About?Hot on the trail of a very successful Pre-NODES week of training, we are back once again to deliver a series of bite-sized, hands-on *LIVE* sessions for you to join. All sessions assume no/little experience with Neo4j and Cypher — this is a great opportunity to have your first experience with all things graphs!Each session starts at 1 p.m. UTC/GMT (9 a.m. EDT | 2 p.m. BST | 3 p.m. CEST | 6:30 p.m. IST | 9 p.m. SGT). All sessions are two hours long each.If you know a friend, colleague, or acquaintance who’d also be interested, please let them know!Let’s dive in and see what’s on offer.Monday, September 13th — Hands-on Introduction to Neo4jHeard about graph databases? Curious about what they are and how they work? Want to know where they’re best used? Then this is the session for you!In this workshop we will:Introduce you to graph databasesCover approaches for identifying graph-shaped problemsGet our hands on our very first graph database experience where we will load and query data, using Neo4j Aura FreeWe’ll also cover what resources are available and how to continue your graph journey.We recommend that you use either Neo4j Sandbox or Neo4j Aura Free tier for this session. Both of these services are completely free, and require no download or installation.[Register now for this session!]Tuesday, September 14th — Hands-on with Neo4j Aura Free TierInterested in testing a project, building a proof-of-concept, or playing with a graph database — all without handling database administration yourself? Neo4j Aura Free tier is the option for this and so much more, without any fees or costs of any kind to you!In this hands-on session, we will cover the following:What Aura is, what’s provided with free tier, and how to access itHow to manage and monitor the database from the control boardHow to import data, run queries, and interact with the databaseHow to connect to the database from other sources (like Desktop, drivers, and more)How to access and deploy an application for the database (time permitting)We will also show you where and how to find help and other information specific to using Neo4j in the cloud![Register now for this session!]Wednesday, September 15th — Getting Started with Neo4j BloomWant to get started with Neo4j Bloom? Keen to visualize your data? Then this is the session for you!In this workshop we will:Introduce you to Neo4j Bloom and how it worksShow you how to create and set up a perspectiveHow to customize what you show and hide away from usersGet the most out of your Data Science visualizationsWe recommend that you use Neo4j Sandbox for this session. This service is completely free and requires no download or installation.[Register now for this session!]Thursday, September 16th — Build APIs with Neo4j GraphQL LibraryWant to know how GraphQL and Neo4j work hand in glove?During this hands-on session we will explore:What is GraphQL and how to build GraphQL APIsBuilding Node.js GraphQL APIs backed by a native graph database using the Neo4j GraphQL LibraryAdding custom logic to our GraphQL API using the @cypher schema directive and custom resolversAdding authentication and authorization rules to our GraphQL APIUsing the Neo4j GraphQL OGM (Object Graph Mapper) for type-safe database interactions without using CypherWhile not strictly required, some familiarity with Node.js, GraphQL, and Neo4j will be helpful.[Register now for this session!]Friday, September 17th — Create a Knowledge Graph: A Simple ML ApproachInterested in knowledge graphs? Want to have a go at building one based on NLP processes?During this session, we’ll start with unstructured text and end with a knowledge graph in Neo4j using standard Python packages for Natural Language Processing. From there, we will explore what can be done with that knowledge graph using the tools available with the Graph Data Science Library.[Register now for this session!]We hope to see you at any or all of these sessions!;Sep 7, 2021;[]
https://medium.com/neo4j/a-new-neo4j-integration-with-apache-kafka-6099c14851d2;Michael HungerFollowNov 27, 2018·5 min readA New Neo4j Integration with Apache KafkaIn the past year we have received a lot of requests for an integration of Neo4j with Apache Kafka and other streaming data solutions. So a few weeks ago, with the help of our Italian partner LARUS (esp. Andrea Santurbano) and our colleague Stefan Armbruster, we started to work on a first integration.Today we want to make this available in a first release under an Apache License for you to try out and test. It works with Neo4j from 3.4.x and Kafka from 0.10.x.Our integration is meant to be deployed as a Neo4j plugin, not as a Kafka Connect Connector yet.The plugin has three modes of operation:via a user defined Procedure to send individual payloads to a topic,as a Producer, publishing Change Events from Neo4j, andas a Consumer consuming Events from Kafka using templated Cypher statements.Streams ProcedureTo use the procedure you have to add the Kafka server config. Additionally, we require an explicit flag to start up the integration: streams.procedures.enabled=trueThen you can use the CALL streams.publish(topic, payload) statement to send arbitrary data. Scalar values, but also nodes, relationships, paths, maps, or lists. You can find more details in the documentation.For example:CALL streams.publish(my-topic, Hello World from Neo4j!)The message retrieved from the Consumer is the following:{ payload : Hello world from Neo4j! }Kafka ProducerTaking inspiration from earlier ideas and our discussions with the Debezium team, the first focus area was making change events from Neo4j available to Kafka, so that other systems can consume them and, for example, update downstream systems.You can configure what information is published to which TOPIC by providing patterns.streams.source.topic.nodes.<TOPIC_NAME>=<PATTERN>For examplestreams.source.topic.nodes.master-data=\Person{*}Address{street, zip, city}Profile{-description}streams.source.topic.relationships.master-data=\LIVES_AT{since}HAS_PROFILEAfter installation and configuration of your Kafka endpoints, the plugin is automatically up and running after Neo4j is started. Each transaction communicates its changes to the Neo4j event listener.We expose creation, updates, and deletes of Nodes and Relationships and provide before-and-after information. There is already some auditing metadata available but we will add more to that. Those events are sent asynchronously to Kafka, so the commit path should not be influenced by that.The Event structure was inspired by the Debezium format and looks, for example, like this:{ meta”: { timestamp”: 1532597182604, username”: neo4j”, tx_id”: 3, tx_event_id”: 0, tx_events_count”: 2, operation”: created”, source”: { hostname”: neo4j.mycompany.com” } }, payload”: { id”: 1004”, type”: node”, after”: { labels”: [Person”], properties”: { last_name”: Kretchmar”, email”: annek@noanswer.org”, first_name”: Anne Marie” } }}}More examples can be found in the producer documentation. Currently, we expose Events as binary JSON but Avro is on the roadmap.Kafka ConsumerAnother important feature, was the ability to consume events from Kafka.Initially, we thought about a generic consumer with a fixed projection of events into Nodes and Relationships. But we felt this fell short of real-world needs. So we decided that instead, we want to give the user the ability to use custom Cypher statements per topic to turn Events into arbitrary graph structures.So you can choose yourself what to do with a complex Kafka event. Which parts of it you want to use for which purpose.Besides your Kafka connection information, you just add entries like this to your Neo4j config:streams.sink.topic.cypher.<TOPIC>=<CYPHER_QUERY>For example for an event like this:{ id”: 42, properties”: { title”: Answer to anyting”, description”: It depends.”}}you can use a configuration like this:streams.sink.topic.cypher.my-topic=\MERGE (n:Label {id: event.id}) ON CREATE SET n += event.propertiesUnder the hood, the consumer takes a batch of events and passes them as $batch parameter to the Cypher statement, which we prefix with an UNWIND, so each individual entry is available as event identifier to your statement.So the final statement executed by Neo4j would look like this:UNWIND $batch AS eventMERGE (n:Label {id: event.id})ON CREATE SET n += event.propertiesYou can affect the batch sizes with your Kafka configuration for the topic.For more information, see the Streams Consumer documentation.Give us Your FeedbackGrab the release JAR from GitHub and put it into your $NEO4J_HOME/plugins.Then, depending on which feature you want to use, you need to add the configuration to $NEO4J_HOME/conf/neo4j.conf. You can find more details in the project’s README.In Neo4j Desktop, open the plugins folder via the Button-Drop-Down in the Manage” view and add the config in the Settings” tab.kafka.zookeeper.connect=localhost:2181kafka.bootstrap.servers=localhost:9092# andstreams.procedures.enabled=true# orstreams.source.enabled=truestreams.source.topic.nodes.<topic-name>=PATTERN# orstreams.sink.enabled=truestreams.sink.topic.cypher.<topic-name>=CYPHER-QUERYYou can also run the setup with Docker Compose.It would be very helpful for us if you could help test the producer and consumer part of our configuration in more real-world Kafka and Neo4j settings than we can provide and fill out our feedback survey.If you run into any issues or have thoughts about improving our work, please raise a GitHub issue.The existing features are also covered in the documentation. If you have suggestions on how to improve it or the getting started experience, please let us know.A Kafka Connect Neo4j ConnectorAfter a really good discussion with the folks from Confluent we agreed to extend the scope of the current extension towards a fully supported and verified connector for the Kafka Connect framework. So stay tuned for that.Happy Streaming,Michael;Nov 27, 2018;[]
https://medium.com/neo4j/behind-the-scenes-of-creating-the-worlds-biggest-graph-database-cd22f477c843;Chris GioranFollowJun 17, 2021·7 min readBehind the Scenes of Creating the World’s Biggest Graph DatabaseTrillion Entity Demo in Numbers1128 forum shards, 1 person shard, 3 Fabric processors.Each forum shard contains 900 million relationships and 182 million nodes. The person shard contains 3 billion people and 16 billion relationships between them.Overall, the full dataset is 280 TB, and 1 trillion relationships.Took 3 weeks from inception to final results.Costs about $400/h to run at full scale.100 machines isn’t cool. You know what’s cool? One trillion relationships.”I’m not saying that someone actually uttered these exact words, but I’m pretty sure we all thought them. That was a month ago, when we’d decided to try and build the biggest graph database that has ever existed.We managed to do it in three weeks.MotivationWhen we introduced Neo4j Fabric, we also created a proof of concept benchmark that was presented at FOSDEM 2020.It showed that, for a 1TB database, throughput and latency improve linearly with the number of shards that it’s distributed across. More shards, more performance.FOSDEM LDBC BenchmarkThe results looked good and confirmed that we had a very good understanding of the approach to scaling a graph database. Development of Fabric continued toward making it an integral part of Neo4j.New technologies were created and improved upon (server-side routing is a good example) and made useful for non-Fabric setups as well.But, that 1TB dataset from FOSDEM was always nagging us. 1TB is not that big, at least for Neo4j. We routinely have production setups with 10TB or more and, although they run on considerably large machines, Neo4j scales up pretty well.We didn’t really need a solution for that we needed a solution for really big databases. That’s why we had created Fabric, but we hadn’t found its limit yet.And what would that scale be? A year ago we built clusters of 40 machines, and they worked out pretty well. Going to 100 machines didn’t seem that much of a challenge. Billions of nodes, perhaps? Well, billions of nodes is the same as a few TB of data.Plus, the richness of a graph schema comes from relationships between nodes, not the nodes themselves. Remember: We’re trying to see how far we can push Neo4j, not just make ourselves feel or look good.Every now and then the same discussion would come around, and it was becoming clear that we were looking for an opportunity, a lightning rod, that would ground us in a realistic goal.Turns out, that opportunity was NODES 2021 on June 17, our biggest online developer conference.Scale That Demands AttentionWe decided that we needed to show the world what we mean when we talk about scale. Not just that Neo4j can retain or even improve performance when scaled horizontally, but to show how far we can go while still retaining performance.We’re going to need bigger numbers. Something that demands attention. Not hundreds of machines or billions of nodes.Trillions.A trillion relationships would do it, right?Where do you even find a database with a trillion relationships? Using a production setup would present logistics problems — the data transfer itself would be complicated, not to mention obfuscating the data so it’s appropriate for public view. We’ll need to generate it for ourselves, from a known model. And we knew we needed to start with the data model, since that would give us the number and size of machines, the queries we’d run, and the tests we’d create. In short, start with the data model to get a sense of the effort it would take.LDBC is a good candidate. It is a social network that contains people, forums, and posts. It’s easy to understand and explain and we are very familiar with it. We decided on 3 billion people, surpassing the largest social network on the planet. The degree of social connectivity was chosen so that the person shard would come out 850GB, and every forum shard would come at 250GB with about 900 million relationships each. To get to the target of 1 trillion relationships we’d need about 1110 forum shards in total, each its own machine.That’s 1110 forum shards, plus a few more for redundancy, each of which will need to have a store generated for it.We also wanted 3 Fabric proxies connecting to 10, 100 and all the shards to see how the system scales as data sizes grow. And, with the clock ticking, we needed a plan to orchestrate all these machines.It was all about managing risk. We expected that, if trouble found us, it would be either during store generation or something glitching badly in the network. As it turned out, we were half right.Making It RealityThe main sources of complexity came from two places.One was the large number of machines that would host the shards. The other was generating the shard data, which we knew needed to happen in parallel and would, therefore, also need orchestrating a lot of machines.We needed a two-step approach. The first step would be full-sized stores but a small number of Neo4j processes. That would let us test the parallel store creation, the generators, the installation of the stores from the buckets to the shards, and also test the MVP of the latency-measuring client.It wouldn’t put any stress on our orchestration tools (that would come later), and allows us to focus on getting everything wired properly for our proof of concept. Things worked out pretty well, and we had all the pieces in place. Everything seemed to work together, and now we could hold our breath and go for the second phase — full size.With two weeks left, we moved to step two: We pulled out all the stops and braced for impact.The first issue that came up was that AWS instance provisioning started failing unpredictably at around 800 machines. Some detective work led to discovering that we have a vCore limit on our AWS account that didn’t let us create any more machines. AWS Support lifted it promptly and we continued creating instances only to hit a more serious limitation:We currently do not have sufficient x1e.4xlarge capacity in zones with support for ‘gp2’ volumes. Our system will be working on provisioning additional capacity.”Hmm. It would seem Amazon had run out of capacity. This left us with two options. Either go for multiple Availability Zones or for smaller instance types. We decided that there’s more complexity and unknowns going for multi AZ, so we’d do smaller instances for now and, if performance was a problem, we’d deal with it later. Getting to the full number of instances was the most important goal.Using smaller instances did the trick, and the next day we had the full contingent of 1129 shards up and running. The latency measuring demo app was almost ready so we decided to take some measurements to see where we stand.But it wasn’t going to be that easy.The complete fabric proxy, the one pointing to all 1129 shards, was timing out. Neo4j log files didn’t have any relevant error messages, just the timeouts. No GC pauses, no firewall misconfigurations none of the usual suspects were to blame. Each individual shard was responding normally, and the Fabric proxy didn’t show any problems either. It took an evening of investigative work to find that the issue was DNS query limiting. As AWS documentation points out:Amazon provided DNS servers enforce a limit of 1024 packets per second per elastic network interface (ENI). Amazon provided DNS servers reject any traffic exceeding this limit.”Yup. That should do it. Our Fabric configuration was using DNS names for the shards, so that limit was reached immediately on every query we submitted. The solution was quite simple — just make the DNS entries static on the Fabric instance and no longer depend on DNS (/etc/hosts FTW).And that was the last limit we had to overcome. Our initial latency numbers were looking very nice and we decided there was no reason to move to larger instance types, which would also help keep the costs reasonable.Overall, we had build tools that allowed us, at the press of a button, to set up a 1129 shard cluster hosting 280 TB of data, with 3 Fabric proxies, in under 3 hours. Yay! And it took us only 16 days to get there.We spent the rest of the time fine-tuning the configurations and the queries, and trying out the latency measuring app. We also played around with the graph itself, created new ad-hoc queries to get a feel of working with such a large setup.Seeing It Live and in ActionThe results of this work can be seen at the NODES 2021 Keynote.DIY — Run It YourselfWe didn’t want to create a pure demo-ware, so we decided to make everything public under the ASL, you can try to run the setup yourself (at different scales) by using the trillion-graph repository. Just remember to watch your AWS bill 💸!neo4j/trillion-graphThis repository contains the code necessary to reproduce the results for the Trillion Entity demonstration that was…github.comParting ThoughtsThis is just the first step in a long journey. We have a lot more work to do to understand how large graph databases behave, how network variations aggregate as noise and how we should improve the system to scale beyond the numbers we achieved for this demo.;Jun 17, 2021;[]
https://medium.com/neo4j/neo4j-certification-how-to-pass-like-a-pro-eed6daa7c6f7;Jennifer ReifFollowApr 25, 2018·4 min readNeo4j Certification — Pass Like a Pro*updated June 8, 2020Have you taken the Neo4j certification exam and failed? Have you studied but been too nervous to take the plunge and start the exam? Or have you simply thought you’ll get around to it at some point and haven’t found time?In this post, I want to highlight key study points and resources to focus on in order to help you study and pass the Neo4j Certification exam. This stems from my own experiences studying for the exam and things that I wish I’d known before I took the test.Exam GuidelinesFirst, the exam is completely free of charge and allows anyone to take (and retake) the exam as many times as needed in order to pass. Didn’t pass the first time? No worries. Simply study up on a few more things during the 24-hour waiting period and try again. No one except for those internal Neo4j employees monitoring exam results will ever know how many times you failed before passing. )If you’re a nervous test-taker and feel panic each time before a test, I would even recommend studying well and going through the exam once, simply to get a feel for the test and the time. If you pass, great. If not, you got the nerves out of the way and have a good starting point for further study and test content.Oh, yes, there is a time limit. But, have no fear. The test gives you an hour to complete 80 questions (approximately 45 seconds per question). If you have extra time at the end, you can also go back and review your answers to any of the 80 questions you are unsure of.The scoring is simply Pass or Fail, and you simply need to get 80% of the questions correct to pass. That means each question is roughly worth 1.25 points. However, some questions ask you to check all that apply”, so those questions would be further divided by the number of correct answers (which varies).Again, you don’t need to stress about this too much. The way I like to think of it is that the more pieces to an answer means that your probability of choosing a correct piece increases. Any portion of a point still counts, after all! :)Study MaterialNow, the section you’ve probably been waiting for — what to study. Probably the best resource for this exam is the free online training course for Introduction to Neo4j. This covers all the topics that are on the exam and gives you the information and experience needed to take the exam.If you prefer other types of materials for learning, the Neo4j developer pages provide good starter material on the Neo4j graph database, Cypher query language, data modeling, data import/visualization, drivers and language guides, and other integrations and resources.Once you have a grasp of the basic concepts, I recommend digging a bit more into Cypher with the Cypher manual and covering a bit of application-building in the driver manual (drivers for each language have a standard design).The bulk of the test questions are pulled from that content. Out of everything, I would spend extra time and focus on the Cypher manual. Just as with any language, there are many clauses and logic constructs, so it may take some time and testing to really lock in the foundation of Cypher. This is especially true if you have experience in another query language. Cypher is different (hopefully easier!), but it may take some time to adjust what you are used to.Cypher functions and examples are common questions on the exam. Knowing how to read different syntax and phrases will help you answer questions, even if you are not a wizard at the language. There are several questions that give a Cypher statement and ask you to pick which answer has the correct return results.General questions about the graph database model and components are also important to understand for some questions and as a foundation for much of the other material. Overall, there is a good mix across topics. While you don’t need to know the materials cover-to-cover, I would understand key points and concepts, as well as make notes of a few smaller points.Other Ways to StudyIf you’re not much of a reader, I mentioned the key resource above with the Introduction to Neo4j online course. There is also an in-class version (though not free) that is based on the same material, but given in either a live, in-person or virtual classroom.There are also some introduction videos on the Neo4j YouTube channel that review the basics of graphs and the property graph model, as well as some Cypher.The Neo4j Certification page also has some great links and FAQ. This is also where you go when you’re ready to take the exam. Simply click on the Sign In” button on the right side and fill out a couple of info fields. This will take you to an entry screen. From there, you can start the exam.What Next?Once you study and pass the exam (no matter the retakes), be sure to add the title/certification to your resume and post your accomplishment somewhere on social media so that Neo4j can celebrate with you! You will also receive access to more advanced Neo4j training, available only to Neo4j Certified Professionals.Plus, you will get added to our community as a certified developer and become part of our graph! Certified developers are added to a private channel on our Community Site to discuss a variety of topics. Any questions, don’t hesitate to reach out to the Neo4j team and best of luck studying and becoming a Neo4j Certified Developer!ResourcesFree online training: Introduction to Neo4jDeveloper Getting Started guidesNeo4j Cypher manualNeo4j driver manualNeo4j YouTube channelNeo4j Certification page;Apr 25, 2018;[]
https://medium.com/neo4j/graph-data-modeling-categorical-variables-dd8a2845d5e0;David AllenFollowOct 7, 2019·7 min readGraph Data Modeling: Categorical VariablesProperty graphs provide a lot of flexibility in data modeling the most frequent question I see that comes up about graph data modeling is whether to make some piece of data a property, a label, or a node.The image above shows all three examples an employee name is a property, City is a node label, and there are three nodes. But behind the hardest questions about how to model specific things are usually categorical variables, so I thought I’d put together a short description of what this challenge is about and how to think it through.As we go through this, try to keep in mind that data modeling is part art, part science, and requires experience in a domain. So we’ll talk about general principles and trade-offs, but there’s no substitute for thinking your problem through for yourself.Categorical VariablesFirst, what’s a categorical variable? (Which can also often be called Reference Data, or Reference Master Data)A categorical variable is a variable that can take on one of a limited, and usually fixed number of possible values.Examples might include the state a person lives in, the Type” of an object (is a flight international or domestic) — or a person’s gender.Examples of things that are not categorical variables:A temperature measurement, which could be any floating point value (i.e. continuous variables”)Strings, like names and addressesThese variables typically don’t have a naturally limited number of possibilities, or fixed set of selections. So 9 times out of 10, they’re easier to work with and people end up just making them properties of a node in a graph.Scenario: Census DataTo illustrate our modeling thinking, and give an example, we’ll take a simple scenario. Say we need to design a database for the census that stores person information, along with the person’s citizenship, self-identified gender, and job (a code that stores their occupation, if they’re employed).Options!Categorical variables get the most questions in graph data modeling because there are three different ways you can model them. Let’s take just gender as an example:As a label that indicates boolean existence of a category (for example (:Person:Female))As a property value (:Person { gender: Female })As a separate node (:Person)-[:GENDER]->(:Gender { name: female })CardinalityBefore we get into a deep dive on the options, there’s one other thing to consider about our variables, which is their cardinality, or number of options in the category. In the gender variable we have a small handful of options depending on how you model it. For citizenship we have a medium number, but a hard upper limit: there aren’t going to be 5,000 countries next year. Finally, there are probably a few thousand job codes and more will be added for sure with time. So we might say that gender is low cardinality, citizenship is medium, and job is high cardinality.Change VelocityIn addition to how many values there are, there’s also a consideration about how often they change genders do not change or very infrequently citizenships do change, and a Job, or something like a postal code, may change many times over a person’s lifetime.So what do we do?All three approaches express basically the same semantics. So which should you use? Let’s look at the options in depth, but keeping in mind that:Data modeling is part art and part science there are no hard and fast right answers, there’s only what works well for your use case.With this article I’m hoping to teach by example, but you can only improve with practice.LabelsWhen to consider: Labels provide for fast lookups in Neo4j, but they only really express booleans, i.e. the presence or absence of a category such as male or female. When we label a node such as (:Person:Male) it is almost equivalent to having a property called male with a value of true, because labels can be either present or absent. Labels tend to work great for low cardinality categorical variables, and when the categories can overlap. For example if a person type can be friend and an enemy, an overlapping category of someone with multiple citizenships can easily be expressed as (:Person:American:German). Labels also are typically used as a way of partitioning graphs.When to Avoid: Labels are a bad choice for medium or high-cardinality categorical variables like postal codes. Sure, we can make a label like 23226, but it’s going to be unwieldy really fast if you have a data model with thousands of labels. They’re also a bad option when overlapping labels could create confusing semantics. For example if the gender options include Female” and Did Not Report”, combining those two labels would make the semantics of your data unclear.Property ApproachWhen to consider: properties will never steer you wrong, they’re probably the easiest go-to option. They’re flexible, indexable, and easy to use. They support high-cardinality categories with ease, and they can be targeted by constraints. They work well when the data changes frequently. So if you need a category to be unique, or you need it to be present and never null, then a property is a great choice.When to avoid: when categories overlap or multiple apply, a property will often need to be an array, which brings with it a number of other problems like array sorting, uniqueness, and other issues. Properties are a poor choice when you need to look up other nodes that share that property as part of a regular query pattern. For example, you don’t want to write queries like this:MATCH (p:Person { id: 5 })WITH pMATCH (othersInSamePostalCode:Person { postalCode: p.postalCode })RETURN othersInSamePostalCodeSeparate NodeWhen to consider: Separate nodes are ideal when you need to look up other nodes that share a property value, or when the cardinality of the categorical variable is very high. For example, if a predicate on one of your queries is to filter people by shared occupation, that might argue for making job a separate node that you can navigate through” to ease your queries. For example we might write:MATCH (p:Person { id: 5 })WITH pMATCH (p)-[:HAS]->(j:Job)<-[:HAS]-(other:Person)RETURN count(other)This would make it very easy and performant to count how many other people share the same occupation as person #5.Another form of finding nodes with something in common would start with the job.// Find the number of accountants in a given postal code.MATCH (j:Job { name:  Accountant  })-[:HAS]-(p:Person { postalCode: 23226 })RETURN count(p)Finally — a separate node is ideal when you might want to capture other metadata later about the category. For example, our Job Code” is a category, but later on we might want to include a definition of that job code, or provide details about a licensing board for that job. If you’re thinking that something might be a complex object, and not just a category then a separate node is probably a good option.When to avoid: Separate nodes can risk becoming Supernodes” when they are too densely connected. For example, imagine a census graph with 200 million people in it if we modeled gender as a separate node, then the Male” node would be expected to have close to 100 million relationships! That’s a super-node for sure, and will slow down queries that access it. There is often a relationship between variable cardinality and selectivity the fewer options a variable can take on (gender only has a small handful) the larger number of relationships it would have in a large data set. On the other hand, the more options a variable has, the less likely you are to end up with a supernode. Job” probably has thousands of possible codes.You’d be in trouble if you tried to do this on a big dataset, because you’d be navigating through a supernode” (the :Gender node).MATCH (p:Person { id: 5 })-[:GENDER]->(:Gender)<-[:GENDER]-(p2:Person)-[:JOB]->(:Job { name:  Accountant  })RETURN p2Decision Time & ConclusionsIn this example we’re not going to consider a sample query workload. It’s just an example. Always keep in mind that the decisions we’re suggesting here are for teaching purposes — data models ultimately exist to answer questions you want to ask of them.Always stay flexible to design your data model in a way that makes sense for the queries you need to ask.With that though, I’d generally opt to make:Person Citizenship a label because people can have overlapping citizenships, they act as good graph partitions, they are booleans (you’re either a German citizen or you’re not) and because Citizenship has low enough cardinality.Person gender a property because it’s dense and non-overlapping (bad choice for a label) and because it would make supernodes all over the place if we made it a separate node.Job a separate node because it’s very high cardinality, because we might want to assert more data about jobs separately on that node.This article is part of a series if you found it useful, consider reading the others on labels, relationships, super nodes, and categorical variables.;Oct 7, 2019;[]
https://medium.com/neo4j/cosine-similarity-in-neo4j-d617b0442439;Mike PaleiFollowMar 26, 2019·4 min readCosine similarity in Neo4JThis post will showcase the use of cosine similarity algorithm in Neo4J and also provide examples in addition to the available documentation.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comWithout further ado, here is the problem set:I had a collection of people’s images and I wanted to find images showing the same person. Since the collection was not labeled, I could not tackle this task as a pure classificaton problem.I manually created a list of persons of interest and downloaded their prototypical” pictures from the web.Next step was to extract embeddings from both the prototypical” images and from each image in my collection. For this I used a pretrained Resnet50 model from keras.applications.import globimport numpy as npfrom scipy.misc import imresizefrom keras.applications import resnet50from keras.models import ModelIMAGE_SIZE = 224IMAGE_DIR=<directory_with_images>resnet_model = resnet50.ResNet50(weights= imagenet ,                                  include_top=True)preprocessor = resnet50.preprocess_inputmodel = Model(inputs=resnet_model.input,        outputs=resnet_model.layers[-1].output)image_names = glob.glob(IMAGE_DIR+/*.jpg)num_vecs = 0image_names = sorted(image_names)batched_images = []for i in range(len(image_names)):    image = plt.imread(image_names[i])    image = imresize(image, (IMAGE_SIZE, IMAGE_SIZE))    batched_images.append(image)X = preprocessor(np.array(batched_images, dtype= float32 ))vectors = model.predict(X)Once I obtained the embeddings, I saved each image as a node in Neo4j using the following schema (the person_name property is optional, it was only filled for the manually downloaded prototypical” images):--nodes.csvperson_id:ID,person_name,url,embeddingAnd here comes the similarity part. My graph did not have any relationships, therefore, I could not use examples where embeddings are a relationship property, i.e. the following example was of no good:MATCH (p1:Person {name: Michael})-[likes1:LIKES]->(cuisine)MATCH (p2:Person {name:  Arya })-[likes2:LIKES]->(cuisine)RETURN p1.name AS from,       p2.name AS to,       algo.similarity.cosine(collect(likes1.score),        collect(likes2.score)) AS similarityLuckily, Neo4j supports embeddings as a node property, here is the docs example:MERGE (french:Cuisine {name:French})            SET french.embedding = [0.71, 0.33, 0.81, 0.52, 0.41]MERGE (italian:Cuisine {name:Italian})          SET italian.embedding = [0.31, 0.72, 0.58, 0.67, 0.31]MERGE (indian:Cuisine {name:Indian})            SET indian.embedding = [0.43, 0.26, 0.98, 0.51, 0.76]MERGE (lebanese:Cuisine {name:Lebanese})        SET lebanese.embedding = [0.12, 0.23, 0.35, 0.31, 0.39]MERGE (portuguese:Cuisine {name:Portuguese})    SET portuguese.embedding = [0.47, 0.98, 0.81, 0.72, 0.89]MERGE (british:Cuisine {name:British})          SET british.embedding = [0.94, 0.12, 0.23, 0.4, 0.71]MERGE (mauritian:Cuisine {name:Mauritian})      SET mauritian.embedding = [0.31, 0.56, 0.98, 0.21, 0.62]MATCH (c:Cuisine)  WITH {item:id(c), weights: c.embedding} as userData  WITH collect(userData) as dataCALL algo.similarity.cosine.stream(data, {skipValue: null})YIELD item1, item2, count1, count2, similarityRETURN algo.getNodeById(item1).name AS from,        algo.getNodeById(item2).name AS to, similarityORDER BY similarity DESCThere is, however, a catch. The above query will yield cosine similarity for ALL pairs in the graph, whereas, I am only interested in pairs between a given node and all other nodes in the graph. If I add a filter to match a specific node:MATCH (c: Cuisine{name:”French”})The data aggregation:WITH {item:id(c), weights: c.embedding} as userDataWITH collect(userData) as datawill contain a single node and cosine similarity will not be calculated.If I do something more complex like:MATCH (c:Cuisine{name: French })MATCH (c1:Cuisine)WHERE NOT c1 <> cWITH {item:id(c), name: c.name,  weights: c.embedding} as userData,      {item:id(c1), name:c1.name, weights: c1.embedding} as userData1WITH collect(distinct(userData)) as my_node,      collect(distinct(userData1)) as other_nodesWITH my_node + other_nodes as dataCALL algo.similarity.cosine.stream(data, {skipValue: null})YIELD item1, item2, count1, count2, similarityRETURN algo.getNodeById(item1).name AS from,        algo.getNodeById(item2).name AS to, similarityORDER BY similarity DESCthe data variable will still contain a list of ALL nodes in the graph, thus cosine similarity will be again calculated between ALL possible pairs. One could, of course, add a WHERE clause after YIELD filtering the result set, but such solution provides no computational gain.I was thus looking for a query that would give me a list of node pairs which I could then pass to the algo.similarity.cosineprocedure.Thanks to the generous help from Neo4J’s slack community I finally came up with the desired query that calculates similarity (using the user-defined function for consine similarity) and creates is_similar relationships between p1 node and all other Person nodes that are not p1:WITH 1 AS startIdMATCH (p1:Person{person_id:startId}),(p2:Person)WHERE p2 <> p1WITH p1, p2, algo.similarity.cosine(p1.embedding,p2.embedding) as similarityMERGE (p1)-[r1:is_similar{score: similarity}]-(p2)RETURN p1,p2,r1WITH 2 AS startIdMATCH (p1:Person{person_id:startId}),(p2:Person)WHERE p2 <> p1WITH p1, p2, algo.similarity.cosine(p1.embedding,p2.embedding) as similarityMERGE (p1)-[r1:is_similar{score: similarity}]-(p2)RETURN p1,p2,r1MATCH (p1: Person{person_name: Dave Brubeck })-[r:is_similar]-(p2:Person)WHERE r.score > 0.8RETURN p1.person_id, r.score, p2.person_idFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Mar 26, 2019;[]
https://medium.com/neo4j/a-comprehensive-guide-to-cypher-map-projection-2622b879e886;Estelle ScifoFollowMar 7, 2022·6 min readA Comprehensive Guide to Cypher Map ProjectionLet Neo4j Format Your Cypher Result as a Ready-to-Use JSON Object for GraphQL or Object-Graph Mapping ApplicationsCredits: https://unsplash.com/photos/qDG7XKJLKbsCypher is a visual graph query language used by the Neo4j graph database. It lets you write very precise pattern matching queries like:MATCH (movie:Movie {title: The Matrix”})       <-[:ACTED_IN]-(person:Person)WHERE person.born > 1960RETURN personThis returns the people who acted in the movie The Matrix” and were born after 1960.But a lesser known feature also allows you to determine with equal precision what gets returned by the query in a JSON format similar to a GraphQL output. This post is all about this fantastic feature.The Classical” Return Statement and Its Drawbacks”Let’s start from a very simple example:MATCH (movie:Movie {title: The Matrix”})RETURN movieIf you run this query, you’ll get a node object, whose representation looks like the following (this is the result as displayed in the Neo4j Browser — you may have different results if running the query through a specific driver):{   identity : 0,   labels : [     Movie   ],   properties : { tagline :  Welcome to the Real World , title :  The Matrix , released : 1999  }}That’s fine, but maybe your application does not need the tagline? It’d be better to exclude this field in order to reduce the size of data transferred. Then you can replace the preceding return statement with:RETURN movie.title, movie.releasedThe result here is quite different: instead of a single column containing a JSON element, you end up with a two-column result:╒═════════════╤════════════════╕│ movie.title │ movie.released │╞═════════════╪════════════════╡│ The Matrix  │1999            │└─────────────┴────────────────┘If you also want to retrieve the actors involved in this movie in the same query, you have to write:MATCH (movie:Movie {title: The Matrix”})       <-[:ACTED_IN]-(person:Person)RETURN movie.title, movie.released, person.nameThis returns a table with a lot of duplicate information:╒═════════════╤════════════════╤════════════════════╕│ movie.title │ movie.released │ person.name        │╞═════════════╪════════════════╪════════════════════╡│ The Matrix  │1999            │ Emil Eifrem        │├─────────────┼────────────────┼────────────────────┤│ The Matrix  │1999            │ Hugo Weaving       │├─────────────┼────────────────┼────────────────────┤│ The Matrix  │1999            │ Laurence Fishburne │├─────────────┼────────────────┼────────────────────┤│ The Matrix  │1999            │ Carrie-Anne Moss   │├─────────────┼────────────────┼────────────────────┤│ The Matrix  │1999            │ Keanu Reeves       │└─────────────┴────────────────┴────────────────────┘The movie’s title and release date are repeated as many times as the number of actors in the movie! So, what can we do to avoid these repetitions and always return a consistent data type?Using Map ProjectionMap projection is a Cypher feature inspired by GraphQL. If you don’t know GraphQL, the only relevant characteristic here is the ability to customize the return fields for each query.Image from https://medium.com/neo4j/introducing-graphql-architect-19b0f2035e21With Cypher, you can hence write:MATCH (movie:Movie {title: The Matrix”}) RETURN movie {.title, .released }This query returns a single row with a single column named movie, which contains a nicely formatted JSON:{   title :  The Matrix ,   released : 1999}You can also use the wildcard * if you want to retrieve all properties of a given node, like this:MATCH (movie:Movie {title: The Matrix”}) RETURN movie {.* }Collecting” Results in a Single ArrayWhat if we have several movies matching the query? If we write:MATCH (movie:Movie)RETURN movie { .title, .released }We end up with a result containing several rows, each of them similar to the above object. If you want a single JSON array, you can use collect:MATCH (movie:Movie)RETURN collect(movie { .title, .released })This time, we get an array of JSON elements similar to:[  {     title :  The Matrix ,     released : 1999  },  {     title :  The Matrix Reloaded ,     released : 2003  },....]That’s already a nice result! But let’s go ahead and include the actors.Traversing RelationshipsWe now want to retrieve the person who acted in a given movie. This is achieved thanks to the following MATCH statement:MATCH (movie:Movie {title:  The Matrix })       <-[:ACTED_IN]-(person:Person)Collecting Traversed RelationshipsInside the map projection, you have access to previously MATCHed elements, like movie and person:MATCH (movie:Movie {title:  The Matrix })       <-[:ACTED_IN]-(person:Person)RETURN movie {           .title,           actors: collect( person { .name } )       }In this new syntax, we add a new key called actors in the final result. The value is built by collecting all matched person for a given movie and extracting their property called name (as we did in the previous example retrieving several movies):{   title :  The Matrix ,   actors : [    {       name :  Emil Eifrem     },    {       name :  Hugo Weaving     },    {       name :  Laurence Fishburne     },    {       name :  Carrie-Anne Moss     },    {       name :  Keanu Reeves     }  ]}Ok, but what if the objects we want to retrieve are not in the MATCH statement?Traversing Relationships in the Projection (Local Scope)That’s not a problem, since map projection also has the ability to understand patterns. The preceding query is hence equivalent to the following one:MATCH (movie:Movie {title: The Matrix”})RETURN movie {     .title,      actors: [(movie)<-[:ACTED_IN]-(person:Person)                  | person { .name }            ]}In the preceding example, we’ve used Cypher list comprehension to parse the result. The bold part of the query actually means: find all persons who acted in movie and for each of them, extract her name”. The result of this query is strictly identical to the result of the preceding query (the order in which the persons are returned is not guaranteed).Extracting Relationship DataSimilarly to the way we retrieved related node data for Person, we can extract relationship properties. In the following query, we’re extracting the roles property of the ACTED_IN relationship between a person and a movie:MATCH (movie:Movie {title: The Matrix”})RETURN movie {     .title,      actors: [(movie)<-[rel:ACTED_IN]-(person:Person)                  | person { .name, roles: rel.roles }            ]}Result of the previous query, with relationship data extraction. Click to zoom in.Limiting Results With WHERE ClauseWith normal Cypher, we can add constraints on the related nodes using WHERE conditions:MATCH (movie:Movie {title:  The Matrix })       <-[:ACTED_IN]-(person:Person)WHERE person.born > 1965RETURN movie, personThis is also doable in a map projection, like this:MATCH (movie:Movie {title: The Matrix”})RETURN movie {     .title,      actors: [(movie)<-[:ACTED_IN]-(person:Person)                WHERE person.born > 1965                 | person { .name }            ]}In both cases, the result will only contain the actors whose date of birth is greater than 1965.Pagination: Order by, Limit, Skip (or Offset)We would like to be able to write:MATCH (movie:Movie {title: The Matrix”})RETURN movie {     .title,      actors: [(movie)<-[:ACTED_IN]-(person:Person)                WHERE person.born > 1965                ORDER BY person.born LIMIT 1 SKIP 2                 | person { .name }            ]}However, as of March 2022 and Neo4j 4.4, this is not (yet?) possible.The good news is that we can achieve similar result using the APOC plugin, and more specifically the apoc.coll.sortMaps function. That will sort a list of maps based on a specific key — born” in this example:MATCH (movie:Movie {title:  The Matrix })RETURN movie {     .title,      actors: apoc.coll.sortMaps(              [                 (movie)<-[:ACTED_IN]-(person:Person)                  | person {.name, .born }               ],                 born              )[0..3]}The final [0..3] selector lets us return only the first three actors, sorted by date of birth.When to Use Map Projection?I introduced map projection in a GraphQL context, making it easier to take full advantage of GraphQL queries by fetching only the required fields from the database (they do not have to transition through another layer of filtering in your backend). But another application of this concept I like is the ability to write Object-Graph mappers” (OGM) in an easy way. You can even imagine having a graph schema different from your domain” schema, and still build proper objects thanks to map projection. Here is an example graph schema featuring products and categories:Product/category graph schema — built with https://arrows.app/With the following Product model, containing information about the category it belongs to, you can use map projection in the get_products function in order to build Product objects with all their fields:NB: In the previous example, a product belongs to only one category, for the sake of simplicity. Feel free to adapt the example for a multiple category case — for instance by changing the type of Product.cateogry to list[str].ConclusionThat’s all folks! I hope this helped you better understand what map projection is and when it can be useful!ReferencesNew Cypher Features Inspired by GraphQL [Neo4j 3.1 Preview]Cypher map projection documentationAPOC documentationGraphQL, a query language for your APIThe Neo4j Python driver;Mar 7, 2022;[]
https://medium.com/neo4j/meet-the-query-log-analyzer-30b3eb4b1d6;Kees VegterFollowNov 26, 2018·6 min readMeet the Query Log AnalyzerAnalyzing Neo4j Query Log files on your Neo4j DesktopThe Query Log Analyzer is a Neo4j Desktop App to help you to understand the query log file of a Neo4j Enterprise server.When you experience unexpected slowness of Neo4j, your queries may be inefficient or the query load on the server is too high. A good step is then to enable the query log via the neo4j.conf file.dbms.logs.query.enabled=true# If the execution of query takes more time than this threshold, # the query is logged. If set to zero then all queriesdbms.logs.query.threshold=100msdbms.logs.query.parameter_logging_enabled=truedbms.logs.query.time_logging_enabled=truedbms.logs.query.allocation_logging_enabled=truedbms.logs.query.page_logging_enabled=truedbms.track_query_cpu_time=truedbms.track_query_allocation=trueNormally you will set a threshold to log only those queries which take more than an x amount of time (start with 100ms for example, or zero for all queries). This means that the queries shown in the query log tool are not the complete query load on the server! This tool however can give you a direction to find the possible causes for your query bottlenecks quickly.I’ll explain that in more detail in my follow-up article.It is good practice to switch the query logging on for development and test servers and analyse your queries frequently when you develop your solution.Install the Query Log Analyzer as follows in Neo4j Desktop 1.1.10+:Open the Graph Applications”-sidebar, and paste the url: https://neo.jfrog.io/neo/api/npm/npm/query-log-analyzer into the Install Graph Application” field and press Install”Select a Project and press ‘+ Add Application’ in the applications listChoose here the Query Log Analyzer” to add it to your Project .In the following I explain the Query Log Analyzer App.Query Log AnalyzerThe Query Log Analyzer needs a query.log file. You can upload this file to the tool and then the tool will analyse this file. After analysing the file the following message will be shown:In this example the query log file has 17341 rows (each query one row) and 249 distinct queries are found. These 249 distinct queries are shown in the Query Analysis” tab where you can find per query the statistics.Query AnalysisIn the Query Analysis tab you will see the distinct queries ordered by Query Count * Avg Time descending. Which means that the most expensive query from the log file is placed on top.The following fields and functions are available in the Query Analysis tab:The Query (the cell below AvgTime — Avg Mem values)This is the actual ‘distinct’ query string.Query CountThe count of this distinct query in the log file and access to the following functions for this query:FilterShow only the query log records for this query in the Query Log tab.HighlightHighlight this query in the query log records in the Query Log tab. It can be useful to see which queries are send to the server around the same time.TimelineExperimental! Show the occurrences of this query in the Query Timeline tab.Avg Time, Min Tim, Max TimeThe Time is here the total time the query uses to execute (query cpu + planning + waiting).Avg CPU This is actual query execution time on the CPU. When detailed time logging is disabled a 0 will be shown here. requires: dbms.logs.query.time_logging_enabled=true and dbms.track_query_cpu_time=trueMax PlanningThis is the maximum time spend in the query plannings phase. When you hover over the value you will see also the Min and Avg planning times. Normally the first time a query is fired the query is planned and the query execution plan is placed in the query cache. So the next time a query is executed the Planning time will be almost 0. Only when the plan is evicted from the cache, the next query has to be compiled again.When Time logging is disabled a 0 will be shown here.requires: dbms.logs.query.time_logging_enabled=trueAvg WaitingThe average waiting time before executing the query. The wait can be caused by heavy loads, so the query has to wait to get some execution time or the wait can be caused by waiting for database locks to be released. When Time logging is disabled a 0 will be shown here.requires: dbms.logs.query.time_logging_enabled=trueCache Hits %This gives the percentage of the data for this query which could be read from cache. 100% means that all the data is read from cache.requires: dbms.logs.query.page_logging_enabled=trueAvg MemThis is the average allocated bytes in memory of this query. Note that this is a cumulative value and tells something about how memory intensive the query was.requires: dbms.logs.query.allocation_logging_enabled=true and dbms.track_query_allocation=trueProtocol + ClientsWith Protocol you can see in which context the query is fired from. Values can be:bolt”This is any bolt client connecting to the database.http”This is any http client using the Neo4j rest interface (applicable to older Neo4j versions)”embedded” This is a cypher call from database logic like procedures and functions.Also a client list is shown, this may be useful to identify how many different ip numbers are sending requests to the Neo4j server. Note that the bolt driver keeps a pool of connections open to the database, so you can have many clients from one ip number.Query LogThe Query Log tab shows every query log row with proper headings. There is a lot of information in there so you need to scroll horizontally to see all the columns. From the first Query Analysis tab you can click on Highlight, then the selected query is highlighted. When you press Filter from the first tab only those query log records are shown in this tab. When you want to profile a query than you can copy the query and the used query parameters from this tab.Query TimelineThe Query Timeline is an experimental feature and it plots the amount of queries per time unit (default is 5 minutes) and the average query time in seconds. This is based on the log time which is not the same as the query start time. It will give you a quick overview when it was very busy on the server.LinksThe source code for the Query Log Analyzer is on Github at kvegter/query-analyzer-app. You can read documentation there and report issues.If you have questions regarding the query performance, you can always head to the #help-cypher channel on the Neo4j Users Slack or on the neo4j community.In the next post I am looking at optimizing your Cypher query performance using this tool.;Nov 26, 2018;[]
https://medium.com/neo4j/neo4j-graph-algorithms-release-pearson-similarity-articlerank-louvain-performance-improvements-946c0c82a9da;Mark NeedhamFollowFeb 1, 2019·5 min readNeo4j Graph Algorithms Release — Pearson Similarity, ArticleRank, Louvain Performance ImprovementsAt the end of last week we released our first version of the Graph Algorithms library for 2019, which introduces a couple of new algorithms, some performance optimisations, and of course bug fixes.Update: The O’Reilly book Graph Algorithms on Apache Spark and Neo4j Book is now available as free ebook download, from neo4j.comtldrThis release introduces the ArticleRank and Pearson Similarity algorithms, Cypher projection support for similarity algorithms, and performance optimizations for the Louvain algorithm.You can download these updates for Neo4j 3.4 or 3.5, and if you’re using the Neo4j Desktop it will automatically pick up the latest version.Don’t forget to read Jennifer Reif’s detailed post explaining how to install plugins if you haven’t used any yet.Pearson SimilarityWe’ve added support for Pearson Similarity at the request of my colleague Will Lyon. Pearson Similarity is a measure of the linear correlation between two sets of values.We can use the Pearson Similarity algorithm to work out the similarity between two things. We might then use the computed similarity as part of a recommendation query.Will has previously showed how to compute Pearson Similarity directly in Cypher as part of the Recommendations Sandbox, so I thought I’d try and translate his work. The following query find the most similar users to a specific user in an extended movies dataset:MATCH (u1:User {name: Cynthia Freeman })-[r:RATED]->(m:Movie)WITH u1, avg(r.rating) AS u1_meanMATCH (u1)-[r1:RATED]->(m:Movie)<-[r2:RATED]-(u2:User)WITH u1, u1_mean, u2, COLLECT({r1: r1, r2: r2}) AS ratingsMATCH (u2)-[r:RATED]->(m:Movie)WITH u1, u1_mean, u2, avg(r.rating) AS u2_mean, ratingsUNWIND ratings AS rWITH sum( (r.r1.rating-u1_mean) * (r.r2.rating-u2_mean) ) AS nom,     sqrt( sum( (r.r1.rating - u1_mean)^2) * sum( (r.r2.rating - u2_mean) ^2)) AS denom,     u1, u2 WHERE denom <> 0RETURN u1.name, u2.name, nom/denom AS pearson, nom, denomORDER BY pearson DESC LIMIT 100The equivalent query using the function in the Graph Algorithms library reads as follows:MATCH (p1:User {name:  Cynthia Freeman })-[x:RATED]->(m:Movie)WITH p1, algo.similarity.asVector(m, x.rating) AS p1sMATCH (m)<-[y:RATED]-(p2:User)WITH p1, p2, p1s, algo.similarity.asVector(m, y.rating) AS p2sRETURN p1.name,        p2.name,        algo.similarity.pearson(p1s, p2s,{vectorType:  maps }) AS simORDER BY sim DESCBecause Pearson similarity needs to compute the average rating of a user across all movies they rated, we have to take a slightly different approach to preparing the data than with the other similarity algorithms that are computed purely on the ratings that intersect with the other user.The algo.similarity.asVector converts our pairs of movies and ratings into a list of maps containing categories and weights. We can then pass those lists into the function, making sure that we pass the vectorType: maps” parameter so that the function knows what format is being passed.If we have two vectors of values of equal length we can pass those in directly without doing this data conversion and without the extra parameter:RETURN algo.similarity.pearson([1,2,3], [4,5,6])There is also a procedure that can be used to compute similarities between larger datasets.Cypher Projection SupportSpeaking of similarity procedures…we’ve added Cypher projection support to the Pearson Similarity, Cosine Similarity, and Euclidean Distance procedures, which is useful for reducing memory usage when computing similarities with large similarity vectors.For example, on the recommendation dataset if we want to compute similarity between all pairs of users based on movie ratings, we’d need to construct vectors with a size equal to the number of movies. We have 9,125 movies in this dataset, so we’d have a vector containing 9,125 items for each user, even though most of the values in those vectors would be NaN since users tend to only rate a small number of movies.If we use a Cypher projection instead the procedure builds compressed vectors that take up much less space.Anyway, enough about the internals, let’s see how we’d use this feature on the dataset. The following query finds the most similar user to each user, assuming that they have a minimum similarity of 0.1. It then creates a SIMILAR relationship between the users:WITH  MATCH (user:User)-[rated:RATED]->(c)      RETURN id(user) AS item, id(c) AS category,              rated.rating AS weight  AS queryCALL algo.similarity.pearson(query, {  graph: cypher, topK: 1, similarityCutoff: 0.1, write:true})YIELD nodes, similarityPairs, write, writeRelationshipType, writeProperty, min, max, mean, stdDev, p95RETURN nodes, similarityPairs, write, writeRelationshipType, writeProperty, min, max, mean, p95The following graph from the Neo4j browser shows us the additional relationship that have been added:ArticleRankArticleRank is a slight variation on the popular PageRank algorithm. It reduces the bias that PageRank has towards assigning higher scores to nodes with relationships from nodes that have few outgoing relationships.To find the score for a given node, PageRank counts up all the incoming scores and divides them by the number of outgoing links for each given incoming page. ArticleRank assigns a score by counting up all the incoming scores, and divides them by the average number of outgoing links plus the number of outgoing links for each given incoming pageThis algorithm is often preferred for doing analysis on citation networks, and Tomaz Bratanic has written a blog post showing how to use this algorithm on the Stanford High-energy physics theory citation network.He also contrasts the results with those given by the PageRank algorithm.Timothy Holdsworth worked on this algorithm so thanks to Timothy for this useful addition to the library!Bug FixesThis release also contains a fix for an ArrayOutOfBounds exception that sometimes happened when loading self relationships.We’ve also done some performance optimisations on the Louvain algorithm, and have observed a speed up of 7–8 times on test datasets.We hope you enjoy this release, and if you have any questions or suggestions please send us an email to devrel@neo4j.comFree download: O’Reilly Graph Algorithms on Apache Spark and Neo4j”;Feb 1, 2019;[]
https://medium.com/neo4j/discover-auradb-free-week-26-goodreads-books-and-recommendations-54fb47e3f201;Michael HungerFollowMay 23, 2022·8 min readDiscover AuraDB Free — Week 26 Goodreads Books and RecommendationsThis week, we’re exploring book recommendation data as a graph. And what better source for that than Goodreads?If you’d rather watch the recording of our livestream, enjoy it here and below. Otherwise, keep on reading.The Data SetI found a well-sized dataset on Kaggle that has the following files:books.csv (10k books and authors)tags.csv (230k tags)book_tags.csv (1M tags for books)ratings.csv (53k ratings)to_read.csv (53k to read intentions)This should work within our free database limits on AuraDB, except for the tags.Frankly, those tags are also a mess. So many have been used only a few times, so we can limit our data to those that have at least 100 uses, which leaves us with 2,081 tags.Often I use xsv as a nice command line tool for CSV processing.Here we join the data from the two tag-files together (by tag_id) and only keep rows where the count column has at least 3 digits (regexp).xsv join tag_id tags.csv tag_id book_tags.csv | \xsv search -s count  \d{3,}  | xsv countxsv join tag_id tags.csv tag_id book_tags.csv | \xsv search -s count  \d{3,}  | \xsv select tag_id,goodreads_book_id,count \> book_tags_reduced.csvFor the actual tags_reduced.csv file we do the same but then have to de-duplciate ourselves. We can use sort and uniq on a header-less tags file to de-duplicate the tags.Create a Neo4j AuraDB Free InstanceGo to https://dev.neo4j.com/neo4j-aura to register or log into the service (you might need to verify your email address).After clicking Create Database you can create a new Neo4j AuraDB Free instance. Select a region close to you and give it a name — e.g. Goodreads.Choose the blank database” option, as we want to import this data ourselves.On the Credentials popup, make sure to save the password somewhere safe. The default username is always neo4j.Then wait 3 to 5 minutes for your instance to be created.Afterwards you can connect (you’ll need the password) via:Import Button with Neo4j Data ImporterExplore Button with Neo4j Bloom for visual explorationQuery Button with Neo4j BrowserThe connection URL: neo4j+s://xxx.databases.neo4j.io is available and you can copy it to your credentials (you might need it later).If you want to see examples of programmatically connecting to the database, go to the Connect” tab of your instance and pick the language of your choice.Data Modeling and ImportAfter our database is running, we can use the Import button to open Data Importer.TLDRIf you don’t want to do the steps manually you can click on the three dots …​ and load the model with data zip file from here.There we add our five CSV files to the left side and start drawing our model. You can follow the video for individual steps.Create the nodes and relationships (drag from halo of the node)Name them and sometimes reverse the relationshipsSelect a CSV file for each element and map the columns from the fileRename / provide types for some fields (ratings, year, count)Select an id field for nodesThe CSV files turn increasingly green as you progress.Then hit the Run Import button to start the import, and provide the password (that you hopefully saved!)Unfortunately, not all of the data fits into our free database — but it’s only 4,000 users and 12,000 ratings that are left behind, so not too much to worry about.As the authors are only provided in a comma-separated list in our data, we can run the following statement to post-process them and split them into individual nodes.Post processing to individualize Authors:// find authors and booksMATCH (n:Author)-[:WROTE]->(b)// split author name by commaWITH b, n, split(n.author,, ) as names// turn list of names in to rows of nameUNWIND names as name// get-or-create an author with that nameMERGE (a:Author {name:name})// if its a new author node, then the previous one was a a combined authorWITH * WHERE n <> a// get rid of the combined author and its relationshipsDETACH DELETE n// create a new relationship to the bookMERGE (a)-[:WROTE]->(b)// Added 5841 labels, created 5841 nodes, deleted 4664 nodes, set 5841 properties, deleted 10000 relationships, created 13209 relationships, completed after 1152 ms.You see our free database is now pretty full — close to 100 percent of the node and relationship limits.Initial QueriesNow that we have our data in the database, click the Explore data in browser button, which takes you to our database UI.Here we can run exploratory queries on the data. The query language is Cypher, which is like SQL for graphs. It’s centered around expressing visual patterns of your data (plus the regular filters, aggregation, pagination, etc.).You can learn all about it in this short GraphAcademy course.Overviewmatch (n) return labels(n) as label, count(*)These are the counts that we have in the database now:╒══════════╤══════════╕│ label    │ count(*) │╞══════════╪══════════╡│[ Book ]  │10000     │├──────────┼──────────┤│[ Tag ]   │2081      │├──────────┼──────────┤│[ User ]  │31071     │├──────────┼──────────┤│[ Author ]│5841      │└──────────┴──────────┘Book With DataMATCH (n:Book) RETURN n LIMIT 25Prolific AuthorsMATCH (n:Author)RETURN n, size( (n)-[:WROTE]->()) as booksorder by books desc limit 20We already knew that Terry Pratchett was really prolific, but I was blown away by Stephen King’s 97 books!╒═════════════════╤═══════╕│ n.name          │ books │╞═════════════════╪═══════╡│ James Patterson │98     │├─────────────────┼───────┤│ Stephen King    │97     │├─────────────────┼───────┤│ Nora Roberts    │65     │├─────────────────┼───────┤│ Dean Koontz     │64     │├─────────────────┼───────┤│ Terry Pratchett │50     │├─────────────────┼───────┤│ Agatha Christie │43     │├─────────────────┼───────┤│ J.D. Robb       │41     │├─────────────────┼───────┤│ Neil Gaiman     │41     │├─────────────────┼───────┤│ Meg Cabot       │38     │├─────────────────┼───────┤│ Janet Evanovich │37     │└─────────────────┴───────┘TagsMATCH (t:Tag {name: sherlock-holmes })<-[r:TAGGED]-(b:Book)<-[w:WROTE]-(a)RETURN *Related TagsMATCH (t:Tag)<-[r:TAGGED]-(b:Book)-[:TAGGED]->(other:Tag)WHERE t.name CONTAINS dystop AND NOT other.name CONTAINS dystopRETURN other.name, count(*) as freqORDER BY freq desc SKIP 20 LIMIT 20Here are tags related to dystopia— you can already see that it mixes structural (ownership, intent to read, type of book) with genre tags and that the nomenclature is all over the place.╒════════════════╤══════╕│ other.name     │ freq │╞════════════════╪══════╡│ ebook          │263   │├────────────────┼──────┤│ adventure      │243   │├────────────────┼──────┤│ scifi          │241   │├────────────────┼──────┤│ audiobooks     │236   │├────────────────┼──────┤│ read-in-2014   │233   │├────────────────┼──────┤│ teen           │218   │├────────────────┼──────┤│ ebooks         │211   │├────────────────┼──────┤│ sci-fi-fantasy │205   │├────────────────┼──────┤│ read-in-2015   │199   │├────────────────┼──────┤│ my-books       │189   │└────────────────┴──────┘We can also look at our users to see how they rated books. To visualize it, we can put the rating on each relationship from the detail pane on the right.A User’s Ratingsmatch (u:User {user_id:314})-[r:RATED]->(b) return *Let’s compute some recommendations for our users 314.Recommendation — Content Based by AuthorFirst we start with content-based recommendations, usually via genre or authors. The tags are not well structured for genres, so let’s go with the authors.So we expand from the highly rated (>= 4) books from our user to their authors and the books they’ve written.Then we use those books’ average ratings to sort the results and exclude the books from recommendations that the user has already rated (no matter how) — i.e. read.MATCH (u:User {user_id:314})-[r:RATED]->(b)<-[w1:WROTE]-(author)-[w2:WROTE]->(reco)WHERE r.rating >= 4AND NOT (u)-[:RATED]->(reco)AND NOT reco.title contains HarryRETURN DISTINCT reco.title, author.name, reco.average_ratingORDER BY reco.average_rating DESC SKIP 20 LIMIT 10Here is the second page of recommendations — the first one had a few too obvious ones.One drawback is that there is no information about which books are the same (just in a different edition or language) or contained in which boxset. That’s why we excluded Harry books manually.Recommendation — Collaborative Filtering by PeersIn a collaborative filtering or peer recommendation, you try to find the people who are most similar to yourself (have expressed similar tastes) and then look at what else they rated favorably.That is the people also bought” recommendation you often see on all kinds of sites.We can return the frequency a recommended book showed up in this peer group, and use that for sorting our data, or combine/multiply it with its average rating.So our full recommendation query is only those three lines:Find peers via similar ratingFind their their highly rated booksCompute frequency of recommendationsMATCH (u:User {user_id:314})-[r:RATED]->(book)<-[r2:RATED]-(peer)-[r3:RATED]->(reco)// exclude already read booksWHERE NOT (u)-[:RATED]->(reco)// peers show similar rating behaviorAND abs(r.rating-r2.rating) <= 1// highly rated books from peersAND r3.rating >= 4// count how frequently the recommend book shows upWITH reco, count(*) as freq// find the authors for our booksMATCH (reco)<-[:WROTE]-(author)RETURN reco.title, freq, reco.average_rating, freq*reco.average_rating as score, collect(author.name) as authorsORDER BY score DESC SKIP 10 LIMIT 10Here is the result from page two, so we have enough to read for our user 314.ConclusionThis was a really fun episode! We got a lot of runway out of the data.Here are some ways you can take this exercise further:You can export your own Goodreads Data and combine it with this dataset.There are many, larger datasets of Goodreads data on Kaggle or elsewhere — e.g. from our colleague Jennifer Reif.Classification (structural, generes, behavioral) and de-duplication of Tags.Connect similar books, like translations, boxsets, etc. to exclude them from recommendations.Develop multi-score recommendations, with weights.Built an API, web, or mobile app on top of this data — for example, using GraphQL or Spring Data.;May 23, 2022;[]
https://medium.com/neo4j/how-i-built-the-apoc-user-guide-graph-app-cb9d1dae7b9c;Adam CowleyFollowMay 5, 2020·5 min readHow I Built… the APOC User Guide Graph AppThe new APOC User Guide Graph AppAt the end of last week, I tweeted about a soft-release of the new APOC User Guide Graph App. This has been one of the many Graph App based experiments that I have been working on over the past couple of months so it’s nice to have a version out in the wild.I thought I’d take some time to write up how I built the Graph App and give you an overview of some of the features.The idea behind the Graph App was to take the existing documentation and add some extra pieces of functionality to make it more interactive. At Neo4j, the majority of our docs and user guides are written with Asciidoc. As part of the build process, this builds XML, HTML and PDF files.The original plan was to somehow take the XML files, load them in and wrap them in Vue.js components but this proved to be problematic. In the end, the easier route was to load the HTML files using and extract the relevant DOM elements using JavaScript, manipulate the links and then add them to the <body> of the Graph App.Table of ContentsThe navigation on the left hand side loads in the Table of Contents using the fetch API and adds the HTML into its own virtual” DOM — basically a HTML element.const url = `${this.root}/toc.html`fetch(url)    .then(res => res.text())    .then(html => {        this.toc = document.createElement(body)        this.toc.innerHTML = html    })    .finally(() => this.loading = false)Once that was loaded, I could use querySelectorAll to find all of the links and load pass them into an array for the template to pick up.computed: {  // ...  chapters() {    if ( !this.toc ) return []return Array.from(this.toc.querySelectorAll(.chapter a))      .map(a => ({          class: a.parentElement.className,          to: { path: appendIndex(a), },          text: a.innerHTML,          children: Array.from(a.parentElement.parentElement.querySelectorAll(.section a)).map(a => ({              class: a.parentElement.className,              to: { path: appendIndex(a), },              text: a.innerHTML,          })),      }))  },}The nice thing about computed properties in Vue.js is that it makes it really easy to manipulate variables — this made the search a 5 minute job. After creating an input element bound to a v-model, I could take the value from that and use it to filter down the chapters property based on whether the term was mentioned in the chapter link or it’s child sections.chapterResults() {  if ( this.search ===  ) return this.chaptersreturn this.chapters.filter(chapter =>    chapter.text.toLowerCase().includes(this.search.toLowerCase())    || chapter.children.find(child => child.text.toLowerCase().includes(this.search.toLowerCase()))  )    .map(chapter => ({        ...chapter,        children: chapter.children.filter(section => section.text.toLowerCase().includes(this.search.toLowerCase()))    }))},Hacking the ContentThe content was a little more tricky, but I took the same approach of loading the contents of a file using fetch and manipulating the content after it was loaded into a virtual element using document.createElement.When a link on the sidebar is clicked, the path of the file is added to the hash of the page, and a Document component picks this up via Vue Router. From there, there is a bit of work to do to correct the links (for example relative links and adding a target= _blank  attribute to any external links), then adding a listener to prevent the default action and pass a navigation event to Vue Router.Running Cypher in Neo4j Browser with a single clickFor any cypher code blocks, you can click the Run in Browser button to copy the code to Neo4j Browser. This takes advantage of the Deep Link functionality available in Neo4j Desktop.Handy buttons to copy the code snippet to the clipboard or run the cypher query in Neo4j BrowserUnder the hood, this is pretty simple. The button simply takes the content from the code block, then sets window.location.href to a URL similar to the following to instruct Neo4j Desktop to open up the Neo4j Browser graph app with the arg portion (in this case, a URL encoded version of a Cypher statement) pre-populated into the browser bar.neo4j-desktop://graphapps/neo4j-browser?cmd=edit&arg=MATCH%20%28n%29%20RETURN%20count%28n%29%20AS%20countSome of the code blocks are also quite complex — for that reason it made sense to add a Copy to Clipboard button to each code element in case the user needs to copy the snippet into a project or share it with someone else.const copyToClipboard = code => {    const textarea = document.createElement(textarea)    textarea.value = cleanCode(code)    textarea.setAttribute(readonly, )    textarea.style.position = absolute    textarea.style.left = -9999px    document.body.appendChild(textarea)    textarea.select()    document.execCommand(copy)    document.body.removeChild(textarea)}Publishing to npmThe Graph App has now been published to npm. The major benefit to this is that Neo4j Desktop will now keep this up to date. If there is a change to the documentation or we decide to add some functionality, all we need to do is run npm publish to upload it to npm.Neo4j Desktop will then regularly check for updates and install them as long as the app is valid for the current version of the Neo4j Desktop API.To install the Graph App now, simply paste the following URL into the Install form at the bottom of the Graph Apps pane in Neo4j Desktop:https://registry.npmjs.org/@graphapps/apocIt’s a simple app but I hope that you find it useful. I’d love to hear your feedback, so if there is anything that you love or think is missing from the app you can catch me on Twitter or post a message on the Neo4j Community site.If you are interested in the other Graph Apps available, you can check out the Graph App Gallery or if you would like to build your own Graph App there is a developer guide at neo4j.com/developer/graph-app-development.Happy Graphing!- Adam;May 5, 2020;[]
https://medium.com/neo4j/using-the-neo4j-graph-database-and-cypher-to-solve-this-brain-teaser-why-argue-350fde86da14;Dan FlavinFollowJun 5, 2020·9 min readUsing the Neo4j Graph Database and Cypher To Solve This Brain Teaser. Why Argue?Count triangles, not triangle countPopular Mechanics How Many Triangles” post.I stumbled across this Popular Mechanics post We Spent All Day Arguing About This Triangle Brain Teaser. Can You Solve It?” while COVID-cruising the web looking for something that was not about COVID. The post focuses on counting the number of embedded triangles in the How Many Triangles” drawing.Why A Graph Database And The Cypher Query Language?Who wouldn’t see a graph structure in this diagram? Each triangle could be expressed as a graph traversal path using a graph pattern expression with the Neo4j Cypher query language. Terms such as graph traversal” and graph pattern” may make intuitive sense for those not familiar with graph databases, but how does it work in practice? Your author hopes that this post will start to address that question.No Arguments!How to count the number of embedded triangles? Start by representing our triangle as data in the Neo4j graph database. Then run a pattern based Cypher query to count the number of paths that represent a triangle and you have the answer. No arguments! I often seen new Neo4j users reach the exalted yes!” moment when they realize a Cypher query only expresses a graph pattern. Users don’t have to know every detail about what data is in the database and how it is interrelated before they can even formulate a query. The Cypher query pattern we’re going to use to count embedded triangles is generic and does not rely on knowing that we’re starting with a 3 by 3 embedded triangle scenario. This highlights the value of using an expressive query pattern in Cypher because it will work for any combination of X by Y embedded triangles.First you need a m̵o̵d̵e̵l̵. 🅓🅐🅣🅐.One of the fun things about building a data model with the Neo4j Label Property Graph (LPG) is that it can be schema-less. Similar to Cypher being expressive, each node or relationship in the graph can have a different structure (if you let it). This makes a LPG modeling exercise flexible, visually driven and is natural to the way people think. Now comes the fun part. The data for this exercise was created from the conceptional model, which then revealed the physical schema. Yes, I had a model in mind when I created the data, but the data came first and the model is the result. I did not have to define tables and columns before I could create data. Schema-less provides a concept first approach that can be extremely valuable when working on building and evolving a data model for changing business requirements.Defining Our TriangleI created the data based on the way I would draw and label the triangle diagram. First, draw the triangles and dividing horizontal lines with each intersection being a point with a reference identifier. Next, I define the direction I would trace traversing between points to identify each distinct triangle.Counting the number of triangles is done by starting at point zero, then visually counting the number of triangles by traversing the points in all combinations that form a triangle. Translating this into LPG terms:Each :Point is a Node object with an ID a property. There are 12 :Point nodes in total with ID’s 0 through 12.Each :Point is related to the next :Point in the down, or across direction. These are RELATIONSHIPS between the Nodes identified here by the :DOWN and :ACROSS arrows in the diagram. Note that the relationship arrow indicates a direction.The rest of the post takes the concept of how I drew and counted and triangles and turns it into a Neo4j graph database and the triangle counting Cypher queries.Playing AlongNo worries continuing on if you’re not familiar with the Neo4j graph database and the Cypher query language. Look here for a quick introduction to the Cypher query language. Download and install the Neo4j Desktop development environment, or use a free Neo4j Sandbox instance in the cloud if you want to play with the queries yourself. There’s a github repo with all the content here if you’re not the cut-and-paste type.Create the DataExecute the following Cypher commands in the Neo4j Browser to create the graph that represents the triangle drawing. There’s a parametrized general solution for creating X by Y triangles (or just grids) in the github repo for this post.// Manually create triangle graph with 3 triangles for 3 levelsMATCH (n) DETACH DELETE n  // DELETE existing data// CREATE :Nodes (triangle intersections)CREATE  (zero:Point {pID: 0}), (one:Point {pID: 1}),     (two:Point {pID: 2}), (three:Point {pID: 3}),   (four:Point {pID: 4}), (five:Point {pID: 5}),    (six:Point {pID: 6}), (seven:Point {pID: 7}),   (eight:Point {pID: 8}), (nine:Point {pID: 9}),    (ten:Point {pID: 10}),  (eleven:Point {pID: 11}), (twelve:Point {pID: 12}) // CREATE :RELATIONSHIPS (lines between intersections)CREATE (zero)-[:DOWN]->(one),   (zero)-[:DOWN]->(two),   (zero)-[:DOWN]->(three),   (zero)-[:DOWN]->(four)CREATE (one)-[:ACROSS]->(two),  (one)-[:DOWN]->(five)CREATE (two)-[:ACROSS]->(three), (two)-[:DOWN]->(six)CREATE (three)-[:ACROSS]->(four), (three)-[:DOWN]->(seven)CREATE (four)-[:DOWN]->(eight)CREATE (five)-[:ACROSS]->(six), (five)-[:DOWN]->(nine)CREATE (six)-[:ACROSS]->(seven), (six)-[:DOWN]->(ten)CREATE (seven)-[:ACROSS]->(eight), (seven)-[:DOWN]->(eleven)CREATE (eight)-[:DOWN]->(twelve)CREATE (nine)-[:ACROSS]->(ten)CREATE (ten)-[:ACROSS]->(eleven)CREATE (eleven)-[:ACROSS]->(twelve)We can see the results of the data creation step by running the simple Cypher query MATCH (n) RETURN n in the Neo4j Browser then arranging to match our diagram. In LPG terms, the blue circle is a Node, with the label :Point. Each point has a property for an identifier, values 0–12 (the property name is pID ) which are connected to other points by the :DOWNand :ACROSS RELATIONSHIPS.Our 3x3 triangle graph with intersection node id’s and the :DOWN, and :ACROSS relationshipsThe graph schema resulting from the data load can be seen by running the CALL db.schema.visualization() function in the Neo4j Browser .Label Property Graph Schema For Our Triangle GraphCounting TrianglesTime to count triangles by running this Cypher query:MATCH (top:Point) WHERE NOT ( (top:Point)<--(:Point) )  MATCH path=(top)-[:DOWN*]->(:Point)-[:ACROSS*]->(:Point)<-[:DOWN*]-(top)RETURN count(path) AS triangleCountWhich returns the final answer (it better be 18!).18, not 42What Did This Query Do??? Breaking Down the Cypher QueryThe query consists of two MATCH statements. The first finds the node that represents the top of the triangle, a.k.a. the top” node of the triangle, which is the :Point labeled node with the property pID: 0. The second traverses the graph starting at the top node and returns the paths that match the Cypher pattern for a triangle. Cypher allows the chaining of individual statements together and referencing results along the execution path. A more complete description of this functionality in the Neo4j documentation.First MATCH statement - find the top of the triangleMATCH (top:Point) WHERE NOT ( (top:Point)<--(:Point) )The first Cypher queryMATCH statement (analogous to a SQL select) finds the top” node of the triangle using a pattern without knowing it has a pID property value of 0.The pattern in the WHERE clause, WHERE NOT ( (top)<--(:Point) ) represents all :Point nodes that do not have incoming relationships. There is only one node in this database that has no incoming relationships. Look at the Our 3x3 triangle graph…” image above and notice the [:DOWN] relationships and their direction from (:Point {pID: 0}) to visualize what this query pattern represents. Where going to assign query variable top to this node to reference in the next query.It would easiest to get the node representing the top of the triangle by explicitly querying for the :Point node with pID: 0 with the Cypher statement MATCH (top:Point { pID: 0 } )... or MATCH (top:Point) WHERE top.Id = 0.... Hardcoding anything is an anti-pattern for pattern queries IMHO.Second MATCH statement - traverse triangle patternsMATCH path=(top)-[:DOWN*]->(:Point)-[:ACROSS*]->(:Point)<-[:DOWN*]-(top)RETURN count(path) AS triangleCountThe next MATCH Cypher query returns all the closed triangle paths starting at the top node and produces a row for every triangle matching path. The individual paths are stored in the path variable. Return the count of the number of paths using the count() function, and you have the triangle count were looking for.This Cypher query pattern is simply a translation of how to trace triangles on a piece of paper. Or in graph concept terms find all the paths through the graph that traverse downward from the top point to the next point, then traverse right, then traverse back to the top”. Repeat this pattern until you’ve covered all the traversal possibilities you can find. A visual sequence for the first traversal and Cypher query patterns is below. Remember the variable top is the node (:Point {pID: 0}) found with our first MATCH statement.Traverse the [:DOWN] relationship directionally indicated by ->from the topnode. The first traversal is: (:Point {pID: 0})-[:DOWN]->(:Point {:pID:1})Traverse the [:ACROSS] relationship directionally indicated by -> from the current (:Point)node. This part of the traversal is (:Point {pID: 1})-[:ACROSS]->(:Point {pID:2})Traverse the [:DOWN]relationship in the -> direction, which in this part of the traversal specified by Cypher pattern is (:Point {pID: 2})<-[:DOWN]-(:Point {pID:0}). Where (:Point {pID: 0}) is the top Cypher query variable.Repeat until there are no more unique paths to traverse. This is done through an unbounded number of traversal hops by using the * modifier on the [:DOWN*] and [:ACROSS*] relationship pattern.5. Return the count of the number of paths (triangles)What triangle patterns did we return?Getting the right number of 18 for the count is all fine, but how do we know it’s the right 18? Did the query identify all the triangles it should, or was this just an accident? All we have to do is modify our counting Cypher query to show us what it is counting by using the Cyphernodes() and length() functions to see what triangle paths are returned.MATCH (top:Point) WHERE NOT ( (top:Point)<--(:Point) )  MATCH path=(top)-[:DOWN*]->(:Point)-[:ACROSS*]->(:Point)<-[:DOWN*]-(top)RETURN nodes(path) AS paths, length(path) AS nodeCntORDER BY nodeCnt ASCPaths through the nodes and their node countThe first row in the text output shows the nodes in the path for the first embedded triangle, which would match this Cypher pattern:(:Point {pID: 0})-[:DOWN]->(:Point {pID: 1})-[:ACROSS]->(:Point {pID:2})<-[:DOWN]-(:Point {pID: 0})I will leave the proof up to the interested reader to verify that all the possible triangle paths are represented. I always hated it when my professors would use that phrase.It’s a graph. I want visuals!One last fun bit would be to take each of the paths that represent an embedded triangle and create triangle data for each. Fortunately there’s an apoc Neo4j plug-in procedure for that. apocstands for Awesome Procedures On Cypher. There are an incredible number apoc’s that address too many functional areas to begin to describe here. Look at the introduction to apocs in the Developer Guide and the Neo4j documentation if you’re using Neo4j and are not aware of them. You can even write your own plugins for your own unique requirements. How flexible is that?We use the same query pattern we’ve used all along and send the resulting paths to the apoc.refactor.cloneSubgraphFromPaths() function that creates nodes and relationships for each path.MATCH (n) SET n.origTri = TRUE // Flag source nodesMATCH (top:Point) WHERE NOT ( (top:Point)<--(:Point) ) MATCH path=(top)-[:DOWN*]->(:Point)-[:ACROSS*]->(:Point)<-[:DOWN*]-(top)WITH pathCALL apoc.refactor.cloneSubgraphFromPaths([path], {skipProperties:[‘origTri’]}) YIELD input, output, errorRETURN input, output, errorReturn the new nodes and relationships without the original data by running:MATCH (n:Point) WHERE NOT exists(n.origTri) RETURN n… and there you have a visualization for each triangle path traversed.What about counting triangles versus triangle count?The subtitle of this post is Count triangles, not triangle count”. Same words, different meaning. There is a Neo4j Graph Data Science Library (GDSL) community detection algorithm named triangleCount that would not apply here. The triangleCount algorithm is commonly used for feature engineering or social network analysis. The triangleCount 3-clique analysis does not give anything of relevance because this graph is highly symmetrical. The GDSL and how to apply its many graph algorithms are the topics of many other posts!;Jun 5, 2020;[]
https://medium.com/neo4j/create-a-data-marvel-part-5-writing-the-domain-classes-27a39ab0666a;Jennifer ReifFollowJan 9, 2019·8 min readCreate a Data Marvel — Part 5: Writing the Domain Classes*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)Picking up from where we left off before the holiday season in Part 4 of this Marvel tech series, this post will show the code for our Marvel comic entities. As you may recall, our data model is structured with 6 entities, as shown in the image below.At the end of the last post, we created the Spring Data Neo4j application using the start.spring.io site, opened the project in IntelliJ, and delved into the project structure. Now, we are ready to write some code, starting with our domain classes!Domain classesJust as we did when we loaded the data from the Marvel API, we will start with the domain entity classes. They are a smaller and simpler entrypoint to the rest, and the values are more familiar to even non-comic-book fans.The Character classWe will create our domain class for characters in IntelliJ by right-mouse clicking on the demo folder (filepath: src->main->java->com->example->demo), mousing over the New option, and choosing the Java Class option.Fill in the name you want for your class (in this case, Character), click OK, and IntelliJ generates some bare-bones structure for us, so we can take it from there.One brief note on class names before we move on. Take care when naming your classes that they do not conflict with language keywords and types. For instance, the class Character caused some confusion with later code because there is also a Java character type. We noted the problem and easily avoided it, but it is something to keep in mind when you’re in full domain definition mode”. If I would have given this a bit more thought before our project, I probably would have chosen something like ‘ComicCharacter’ instead.The generated code includes just an outline of our class structure. We have the package path defined at the top for us and the (empty) class declaration beneath that.Now we start adding some definition for our Marvel use case. The full code for this class is below. We will explain the syntax in the following paragraphs.package com.example.demoimport …@Data@NoArgsConstructor@RequiredArgsConstructor@NodeEntitypublic class Character {  @Id  @GeneratedValue  private Long neoId  @NonNull  private Long id  @NonNull  private String name, description, resourceURI, thumbnail}We first want to add annotations for Lombok to shortcut some boilerplate like our getters/setters and constructors. The @Data annotation handles much of that boilerplate for the standard POJOs, supplying the getters, setters, and equals+hashcode+toString methods. Annotations @NoArgsConstructor and @RequiredArgsConstructor generate a constructor with no arguments and a constructor with one argument for each field required to be @NonNull, respectively. We also need to annotate with @NodeEntity to let Spring Data Neo4j (SDN) know that this is a domain class for a node in our graph database.Within the class declaration, we start defining our member variables/fields for the database and the Marvel data. The first id field (neoId) is the internal id that Neo4j assigns to each of its nodes and relationships when it is stored. It is rarely used or referenced, but it is part of the entity in the database, so we assign the @Id and @GeneratedValue annotations to it since we do not handle the values.The next id field (id) is the identifier from the Marvel API. This value allows us to retrieve any additional info from the API or reference an entity without translating for custom ids. We add the @NonNull annotation to ensure that the field always has a value and will not accept null. The last fields are String values from the API that we felt were the most meaningful to our application. The @NonNull annotation also applies here to ensure values are not missing.The repository interfaceNext, we will need to create a repository for Character objects for the data access layer to the database. Spring Data is designed with the flexibility to interface with different kinds of data stores, so the interaction with Neo4j should be similar to many of Spring Data’s other interface projects.Just as we did to create the Character class, we can right-click on the demo folder in the project structure, choose New, then pick Java Class. When the box appears to type in the name, we will use CharacterRepo, then choose Interface as the Kind, rather than the Class type.Click OK, and the interface should appear with some skeleton code generated. Again, the package path is defined for us, as well as the interface declaration.The only thing we need to add here is to make our interface extend the Neo4jRepository interface, as shown below.package com.example.demoimport org.springframework.data.neo4j.repository.Neo4jRepositorypublic interface CharacterRepo    extends Neo4jRepository<Character, Long> {}The Neo4jRepository interface provides Neo4j-specific implementation details on top of several extended Spring repositories as the foundation. Neo4jRepository requires two types to be specified — our class type and its id type. Once we add our Character and Long values here, we have finished our required code for this interface.* Note: If we wanted to run some queries to return Character data specifically or view a character’s relationships, we could define methods and other details in this interface. However, for simplicity, we chose to define all those methods through one entity — the ComicIssue. Since we decided in an earlier step that our project only cared about data as it was related to the ComicIssue and made it the center of our graph data model, it made sense to focus our methods there.The controller classFinally, we need one more piece to complete our Character classes. We have our entity defined and our data access interface written. The controller acts as the messenger between the data layer and the user interface to accept requests from the user and send back responses. This is where the code for logic and data manipulation is typically placed. It coordinates different responses based on the kind of input it receives.For our character controller, we add a new Java class like we did with the domain class (right-click on demo folder, choose New->Java Class). Type in the name CharacterController, keep the Kind value as Class, and click OK.We will need to add a couple of annotations, as well as some methods to this class. The completed class code is below, and the explanation will follow.package com.example.demoimport ….@RestController@RequestMapping(/characters”)public class CharacterController {  private final CharacterRepo repo  public CharacterController(CharacterRepo repo) {      this.repo = repo  }  @GetMapping  public Iterable<Character> getAllCharacters() {     return repo.findAll()  }}The @RestController annotation combines the @Controller and @ResponseBody annotations, eliminating the need to put @ResponseBody on each request handling method. Our @RequestMapping annotation is used to map requests to a controller method for a certain path (in this case, /characters).Within the class declaration, we begin by creating a local member variable for our CharacterRepo, and the next line, our constructor, injects that repository into our controller so that we can access the data layer. The @GetMapping annotation tells us this will be a GET method and precedes the method declaration for getAllCharacters() that accesses the repository, executes the provided findAll() method, and returns multiple Character entities (Iterable of type Character).Wrapping upFortunately, this is all the code we need at this point to define our Character-related classes. This would be enough to hit the /characters endpoint (using the curl command or something similar) and return all the Character entities in our database.What I LearnedI had built similar applications with Spring in the past, so this was pretty straightforward and consistent with my past experiences. Spring handles much of the boilerplate and nuances, so I could focus on the data and representation, rather than the setup code to get from the database to the user. It didn’t take too much additional research or effort to integrate with Neo4j. Helpful documentation and examples also shortened the learning curve, as well.As always, the highlights of my learning experience in this step are listed below. :)The built-in capabilities of the Spring ecosystem help reduce boilerplate code and nearly eliminate the most rudimentary (and often boring) parts of the codebase.Spring Data provides a highly-flexible and simple interface to plug into a variety of data sources. It is designed to work seamlessly, whether you are working with relational, NoSQL, or graph databases. Consistent syntax helped me focus on the pieces of the code that were specific to Neo4j and the graph data model.Coding the Character-related classes first gave me an easier on-ramp because the code was simple and nearly mirrored previous applications with relational backends. Starting with a small piece of the application and not focusing (yet!) on the relationship component of the graph model allowed me to easily consume and better internalize any differences in smaller chunks.Next StepsEven with the little bit of code that we wrote today, we can now retrieve Character entities from Neo4j and display them (even if it is only in JSON format). :) In upcoming posts, we will continue adding pieces of code until we can retrieve and review all of our entities through a prettier interface!ResourcesFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j GuideProject Lombok docsPrevious parts of this blog series: Part 1, Part 2, Part 3, Part 4;Jan 9, 2019;[]
https://medium.com/neo4j/create-a-data-marvel-part-8-controlling-and-servicing-our-comic-endpoints-4dd08b81e0e;Jennifer ReifFollowFeb 14, 2019·10 min readCreate a Data Marvel — Part 8: Controlling and Servicing our Comic Endpoints*Update*: All parts of this series are published and related content available.Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7, Part 8, Part 9, Part 10Completed Github project (+related content)We will continue developing our ComicIssue classes for passing the ComicIssue objects, along with any entities related to them. This post will cover the ComicIssue controller and service classes for handling requests and shaping any results. As before, previous blog posts (Parts 1–7) have built upon information and code leading up to this. For review of code for the ComicIssue domain, repository, and controller classes, you can check out Part 7 of this series.Let’s go ahead and dive in!Controller vs. Service classesAs we have previously discussed, the controller class is the intermediary between the data layer and the user interface to accept requests from the user and send back responses. This is where the code for logic and data manipulation is typically placed.One minor distinction for our ComicIssue is that we will use the controller to outline and map our ComicIssue endpoints, then use a service class to handle logic and formatting of the data for us. This type of architecture (creating an external and internal API) is mostly personal preference for division of labor and easier-to-read code. Hopefully, you will see why shortly. If we included all the upcoming code in one class, the waters could get murky very quickly. Creating a separate ‘service layer’ class adds passthroughs to a couple of operations, but it also keeps our controller class more readable and maintainable by placing the rather substantial data assembly/formatting in its own class, along with those remaining data access (repository) calls.The controller classpackage com.thehecklers.sdnmarvelcomics.comicissueimport …@AllArgsConstructor@Controller@RequestMapping(/comicissues”)public class ComicIssueController {  private final ComicIssueService issueService  @GetMapping(/findbyname”)  @ResponseBody  public ComicIssue findByName(@RequestParam String name) {    return issueService.findByName(name)  }  @GetMapping(/findbynamelike”)  @ResponseBody  public Iterable<ComicIssue> findByNameLike(@RequestParam String name) {    return issueService.findByNameLike(name)  }  @GetMapping(/buildgraph”)  @ResponseBody  public Map<String, Object> buildgraph(@RequestParam(required = false) Integer limit) {    return issueService.graph(limit == null ? 100 : limit)  }  @GetMapping(/graph”)  public String graph(@RequestParam(required = false) Integer limit, Model model) {    model.addAttribute(buildgraph(limit))    return issuesgraph”  }}In the code above, we start our class with an @AllArgsConstructor Lombok annotation to create a constructor with one parameter for each field in the class, in this case, the ComicIssueService bean we want Spring Boot to infect/autowire for our use. The next annotation for @Controller is different from our other entity controllers because we do not want to return JSON for every method in the ComicIssueController. Remember that the @RestController annotation we have been using is a combination of @Controller and @ResponseBody, the latter of which defaults to a JSON-format return. Instead, we want to specify which methods should return JSON and which could return an html page (through a template engine). Finally, our last annotation sets up the general endpoint for all of our ComicIssue calls (/comicissues”).Our class body starts by injecting our service class, then defines our methods. The findByName() and findByNameLike() methods are first. These are pretty straightforward, as we need to set up the endpoints for each using the @GetMapping annotation and then annotate with @ResponseBody to notify Spring that these methods will simply return JSON as the response.The findByName() method is expected to only return a single result, and we need to pass in the searched name as a parameter. The method will call the service class method with the name parameter as the argument.The findByNameLike() is handling a fuzzy search, so it could return several possible matching comic issues. It will expect an Iterable of ComicIssues in return and also pass the searched name as a parameter. Just like our findByName() method, this method calls the service class method and passes the name to it.Our third method is buildgraph(). This method also uses the same annotations as our previous methods for mapping the endpoint and defining our JSON return. However, this method definition and call is a bit different. The code is repeated below for reference alongside the explanation.@GetMapping(/buildgraph”)@ResponseBodypublic Map<String, Object> buildgraph(@RequestParam(required = false) Integer limit) {  return issueService.graph(limit == null ? 100 : limit)}First, we are expecting a Map<> to return from this method because it formats our data to be displayed for our visualization. D3 expects a map of nodes and relationships, so that is what must return here. Getting the data into that map format is what our service class will do. We will look at that code in a bit.We are also passing an integer parameter called limit to this method. This will allow us to specify how much of the graph we want to return in our visualization. If we have a very large graph, rendering a visualization in our webpage could overload the page or cause performance impact. Depending on user preferences, we can pass in an arbitrary number to limit the volume of the return.Just like our previous methods, buildgraph() will call the service class method, but it passes something more interesting in the parameter (limit==null ? 100 : limit). This bit of code is a ternary operator that checks if limit is equal to null. If it is, it uses the default value of 100. Otherwise, it uses the passed-in value. If the user forgets to pass in a value (or doesn’t need a specific amount returned), this ensures that the number of nodes and relationships returned to the visualization will not exceed 100. It will avoid an accidental browser overload or poor performance.The last method is graph(). This code is what will call the template engine that searches for an html page and loads it to the browser, if it finds one. We have restated the method code below to use as reference with the following explanation.@GetMapping(/graph”)public String graph(@RequestParam(required = false) Integer limit, Model model) {  model.addAttribute(buildgraph(limit))  return issuesgraph”}Our usual @GetMapping annotation sets up the endpoint for calling this method, and we can then define it. We are expecting a return type of String because our template engine will use the returned string to search for an html file with that same name to serve up to our browser. Our parameters for the method include the integer limit (again, for the visualization) and a model object that can supply attributes for rendering pages. In our case, we call the addAttribute() method on the model and pass our buildgraph() method with the limit parameter to convert our data to the expected visualization format, then add that as an attribute to the model for rendering. Finally, the method returns the issuesgraph String, which will cause the template engine to look for an issuesgraph.html file to match it. We will show the code for the html file later.Now that we have our Controller class completed and all of our endpoints and methods defined, we can move on to our service class and show how we handle additional logic there.The service classThis class has quite a bit of code, so we will break up the blocks into a couple of chunks for easier consumption. Let us start with the annotations, class declaration, and a couple of our simpler methods.package com.thehecklers.sdnmarvelcomics.comicissueimport …@AllArgsConstructor@Servicepublic class ComicIssueService {  private final ComicIssueRepo issueRepo  @Transactional(readOnly = true)  public Iterable<ComicIssue> findAllComicIssues() {    return issueRepo.findAll()  }  @Transactional(readOnly = true)  public ComicIssue findByName(String name) {    return issueRepo.findByName(name)  }  @Transactional(readOnly = true)  public Iterable<ComicIssue> findByNameLike(String name) {    return issueRepo.findByNameLike(name)  }  @Transactional(readOnly = true)  public Map<String, Object> graph(Integer limit) {    return toD3Format(issueRepo.graph(limit))  }  ...}In the above segment, we use the @AllArgsConstructor to have Lombok include a constructor with all member variables as parameters. The @Service is an alias for @Component, which tells Spring to create a bean and make it available to the application for use. We use @Service instead of @Component as an indication that this is a stateless interface (a la DDD) that serves as a foundation for our internal API.Within our class body, we inject our ComicIssueRepo into our service, allowing the service to call the repository methods. Next, we have 4 methods using the @Transactional annotation. This annotation sets boundaries for how much will run in a single transaction. We could run multiple calls to Neo4j within a transaction, but we want to separate each method into its own unit of work. We also specify that data in this method should be read-only and should not have any write transactions.Our first method is findAllComicIssues(). This method simply calls the repo and executes the built-in findAll() method to return all ComicIssues in the database, using the same pattern from our other entity controller classes. The next two methods are for findByName() and findByNameLike(). These methods are straightforward and call the respective method from the issueRepo and both pass in the name value as a parameter.The last method in this block is for graph(). It expects a map returned with the nodes and relationships for the visualization and expects the limit parameter. The return call is the most interesting piece. First, we call the issueRepo and associated method and pass in our limit value. However, in order to get our data into the format the D3 visualization expects (Map<>), we also need to transform and manipulate those results. That is what the toD3Format() method will do, and we will look at that code shortly. We wrap the issueRepo.graph(limit) call within the toD3Format() method, and we should end up with our Map<> structure for displaying the graph visualization on our webpage.Now we can look at what the toD3Format() method does to transform the data into a Map<>.private Map<String, Object> toD3Format(Iterable<ComicIssue> issues) {List<Map<String, Object>> nodes = new ArrayList<>()List<Map<String, Object>> rels = new ArrayList<>()int i = 0Iterator<ComicIssue> result = issues.iterator()while (result.hasNext()) {  ComicIssue issue = result.next()  nodes.add(map(name”, issue.getName(), label”, issue”))  int target = i  i++  for (Character character : issue.getCharacters()) {    Map<String, Object> comicChar = map(name”, character.getName(), label”, character”)    int source = nodes.indexOf(comicChar)    if (source == -1) {      nodes.add(comicChar)      source = i++    }    rels.add(map(source”, source, target”, target))  }  for (Creator creator : issue.getCreators()) {    //same code block as for Characters  }  for (Series series : issue.getSeries()) {    ...  }  for (Story story : issue.getStories()) {    ...  }  for (Event event : issue.getEvents()) {    ...  }}return map(nodes”, nodes, links”, rels)}private Map<String, Object> map(String key1, Object value1, String key2, Object value2) {    Map<String, Object> result = new HashMap<String, Object>(2)    result.put(key1, value1)    result.put(key2, value2)    return result}I have condensed the code a bit to make it easier to read, but the same code block within the for (Character) loop is the same for each entity. The toD3Format() method will return a Map<> type, which is what d3 needs to render our visualization. Our Iterable<ComicIssue> is passed into the method, so we can iterate over the list and separate our objects into nodes and relationships.We begin the method body by creating two new ArrayLists — one for nodes and one for relationships (rels). Then, we set up an iterator to loop through each ComicIssue object and grab the next comic (ComicIssue issue = result.next()). The next line adds two values to our nodes map for the issue name and the label on the issue (which is :ComicIssue). It does this by passing the name and label to the map() method that creates a HashMap with two keys and two values, then adding the resultant Map<> to the list of mapped nodes.Now, for each ComicIssue entity, we have sub-entities for each Character, Creator, Event, Series, or Story related to that comic. We will need to loop through each of those lists and map the start and end node of the relationship, as well. This bit of code is repeated for clarity.int target = ii++for (Character character : issue.getCharacters()) {   Map<String, Object> comicChar = map(name”, character.getName(), label”, character”)  int source = nodes.indexOf(comicChar)  if (source == -1) {    nodes.add(comicChar)    source = i++  }  rels.add(map(source”, source, target”, target))}We start with our target node, which is the current ComicIssue node that is on one side of the relationship. For each Character related to the particular ComicIssue, we call the map() method to create another HashMap from the name property and label of each Character. Then, we pull the the index of that entity in the list and assign that to our source variable (source = nodes.indexOf(comicChar)). At the end of the block, we also call the map() method to format the source and target values into our expected HashMap and add them to the relationship map. Now, each of the ArrayLists we initially set up (nodes and rels) will contain a list of indexes for the related objects and their connections.We loop through the same logic for the rest of our entities (Creator, Event, Series, Story) that we did for Character, adding each of the nodes and relationships for these objects to the arrays.Now we have a functional application that should return ComicIssues from our endpoints, along with the related Character, Creator, Event, Series, and Story entities! We can tap some of the /comicissues endpoints to ensure everything is working. Our last step will be to create our html page for viewing the data in a webpage and rendering our visualization!What I LearnedThis step was quite involved, as we starting knitting together all of the entities to return with a particular ComicIssue. The toughest part was probably setting up the endpoints, template engine structure, and d3 formatting for the visualization. I’ve outlined my challenges in the points below.D3 looks complicated at first, but it actually expects a straightforward format of maps. I was able to use much of the existing code from some of our sample projects, and then modify it for our use case. The complexity was in the number of entities that we have for our Marvel Comics data.The separation of Controller and Service was very confusing at first. I wasn’t sure why it was needed or what purpose it served. However, it became clear that I didn’t want to include all of the code from both classes in a single file (would’ve ended up around 140 lines of code)! Separating endpoints from the formatting and mapping logic of the d3 code made it much easier for me to sift through code and change things. Separating concerns into two classes (external-facing controller API and internal service API) made for cleaner code divisions and an API that was easier to reason about.Next StepsWe are so close to have a full-stack application completed. Our next post will place the final (pretty!) puzzle piece into place to add the html page and allow us to interact with the data in a visual and friendly manner. Hope to see you then!ResourcesFollow the duo on Twitter to see what’s coming: @mkheck and @jmhreifDownload Neo4jSpring Data Neo4j docsSpring Data Neo4j GuideProject Lombok docsPrevious parts of this blog series: Part 1, Part 2, Part 3, Part 4, Part 5, Part 6, Part 7;Feb 14, 2019;[]
https://medium.com/neo4j/community-detection-in-social-networks-with-neo4j-and-netscan-d2f90074dcbb;Vitor HortaFollowAug 29, 2018·3 min readCommunity detection in social networks with Neo4j and NetSCANIf graphs and social networks are everywhere, communities are too. As people have a great tendency to form groups, investigating these groups can be very useful.For example, if you are a developer then you are probably making use of several open source packages and you’re definitely getting a lot of help in Q&A forums and other websites like this one.Perhaps you are the one making a great difference to someone else around the world and… aren’t you curious to know who they are or what are they creating with your kind collaborations?Community detection methods can help us finding those people and a lot of applications can benefit from this. Some good examples are recommendation systems, question routing, viral marketing and many many others.But this is not a trivial task, specially in big and dense networks. To show this, let’s take a look at a network modeled from StackOverflow data and constructed in the Neo4j.Stackoverflow network example in Neo4j. The blue nodes represent users and the relationships represent answers given by the source node to the target node.In a network like this, nodes represent users and the edges are answers given by the source node to the target node. It’s reasonable to say that in this example we have two communities, a node acting like a bridge between these groups and two influential nodes that have most of the connections.So you can ask me: what’s the big deal? One could detect this instantly by looking at the figure!”. And you’re right, so let’s turn things more complicated.Gephi visualization of the entire StackOverflow network.How about now?As you can see it’s not trivial task. It’s clear that, if we want to detect communities and find influential people in such a huge network, we need automation. In order to tackle this problem we have recently developed the NetSCAN algorithm. NetSCAN is a density-based method for detecting communities in social networks and finding influential people. It can also detect the semantic meaning of groups, which you’ll see, it’s great!The algorithm was implemented for Neo4j and the execution is the simplest one. All you have to do is to run the procedure as a cypher query!CALL netscan.find_communities(User,ANSWERED,userId,weight, HIGHER_BETTER, eps, minPts, radius)The installation instructions and parameter definitions are available on github and all the details about the algorithm can be found in the published paper. But for now, let’s just take a look at the algorithm in action and some results.Two communities found by NetSCAN. The first is a python community (red) with one influential node (green) and the second is a c++ community (red) with two influential nodes (green).As we can see, even for huge networks NetSCAN can detect communities, find influential people and also identify their topics of interests. And there are many more interesting things, such as the possibility to find developers participating in multiple communities of different subject, characterizing people with multidisciplinary skills.So again, if you want to use this in your own network and Neo4j graphs I’m sure you’ll find many interesting possibilities. Give it a try, it’s very simple to install and to use. Also feel free to give feedback in this post, github or email. I’ll be happy to help you discover communities. Maybe we’ll find out that we are part of one!;Aug 29, 2018;[]
https://medium.com/neo4j/introducing-neomap-a-neo4j-desktop-application-for-spatial-data-3e14aad59db2;Estelle ScifoFollowNov 16, 2019·4 min readIntroducing Neomap, a Neo4j Desktop application for spatial dataNeo4j is one of the, if not THE, most famous graph database. It comes with Neo4j Browser that includes a powerful graph visualization tool. However, this tool is not well suited for nodes with geographical attributes that we prefer visualizing on a map. Neomap was born to address this issue.neomap 0.3.1 openingInstallationIf you haven’t already, you will have to install Neo4j Desktop. It’s an awesome application from which you can manage your local and remote graphs, installed plugins and, most important for us today, applications. Installation instructions for Neo4j desktop depending on your OS can be found on the Neo4j website.install.graphapp.ioOnce Neo4j Desktop is installed and running, we can move on to the neomap installation.You can now also find neomap on the Neo4j Graph App Gallery install.graphapp.io1. First step is to visit https://github.com/stellasia/neomap/releases and download the last release’ .tgz: neomap-<version>.tgz (neomap-0.3.1.tgz at the time of writing)2. In Neo4j Desktop and go to Graph Applications” view:3. Drag and drop the tarball you downloaded earlier below Install Graph Application”If you have an older version you might want to remove that one.After having agreed to trust this new application, it will be installed in Neo4j Desktop and available to be added in any of your projects.4. In a given project, click to Add Application” (it should be next to Neo4j Browser” which is installed by default)Add a new application5. In the popup window, select neomap”.+Add neomapWe’re done! Neomap is now available for all the graphs in that project. Let’s discover how to use it.UsageLet’s learn about the functionalities by going through the interface. All configurations are done in the left panel, where you can create the layers that will be displayed on the right side map.Two main options are to be considered:Layer typeThe simple modeIn the simple mode, you choose the node labels that will be shown on the map together with the properties to be used as latitude/longitude and the tooltip.You can also configure the LIMIT of rows to be fetched.NOTE: If you display too many markers, the rendering performance will slow down.If you need more configuration, like including WHERE clause, you will have to use the advanced mode.The advanced modeThere, you can write the cypher query you need. The only constraint is that it must RETURN at least the following columns (all the other will be ignored):- latitude- longitude- tooltip (optional): the data to display on marker hover in case of marker rendering (see below).Layer type choiceNote: Currently the Neo4j spatial Point type is not yet supported. If you want to use it you can use a Cypher query like this in an Advanced Layer.MATCH (n:Location) RETURN n.location.latitude as latitude, n.location.latitude as latitude, n.name as tooltipMap renderingMarkersEach node will be shown as a pin marker. Some options are specific to marker layers:- Color: marker color- Tooltip (in simple layer mode): the property to use in popups.HeatmapA heatmap will be created based on the nodes distribution at the location.Options specific to heatmap layers are:- Radius: radius for heatmap points (default: 30) (source)Map rendering choiceYou can add as many layers as you want, mixing the layer type and the rendering at your convenience.Example rendering with two layers: one heatmap layer and one marker layer.On the map top right corner, you can list all layers and hide/show them if you need to. If a layer is not needed anymore, you can delete it permanently by clicking the red Delete” button in the layer list from the left side bar.Check out the video on YouTube for a live demo:Live Demo Video for neomapMore information and documentation about future versions will be made available on the project wiki: https://github.com/stellasia/neomap/wikiNext stepsneomap was started during Global Graph Hack 2019 (a one month hackathon) but it is still an evolving project. For instance, I plan to implement support for Neo4j’s spatial types in the very near future.If you like the project and would like to contribute, I welcome all kind of contributions, for instance:- Help with fixing the open issues in GitHub (including both bugs and planned features implementation)- Report bugs- Suggest new features that would be helpful to youAnd in any case, don’t hesitate to star the project on GitHub if you think it’s a useful tool!https://github.com/stellasia/neomap/Thanks for reading and enjoy mapping your graph data!;Nov 16, 2019;[]
https://medium.com/neo4j/high-fidelity-load-preview-now-available-for-neo4j-data-importer-2478bb6abed0;Junxiang ChenFollowAug 23, 2022·5 min readHigh-Fidelity Load Preview — Now Available for Neo4j Data ImporterData Importer’s latest release 0.5.0 brings exciting improvements that make graph modeling and mapping smoother through a visual preview.Photo by DeepMind on UnsplashIn our previous release, we introduced the Preview” feature that helps users visualize graph data before committing data to their database.Importing Your Data Into Neo4j Just Got Even EasierEarlier this year, we introduced a new tool, Data Importer, to help users easily import their flat file data into…medium.comOur initial approach was to scan the first few rows of relationship files, and then create source and target nodes along with the corresponding relationship on each row. It’s straightforward and trivial to demonstrate how data is connected.However, this approach has several drawbacks as node files are not scanned, thereby limiting the fidelity. Here are a few limitations we had to work through:Relationship rows don’t usually contain node properties information, so columns mapped to IDs in the relationships are usually the only available node properties.The graph preview doesn’t always reveal the actual graph structure since relationships and their linked nodes are not guaranteed to exist. For example, if an associated node file is empty, the preview approach still loads corresponding nodes and relationships from relationship files.Unconnected nodes cannot be previewed because there are no corresponding relationship file mappings.In the latest release, we created an enhanced approach to substantially improve the preview fidelity. You can preview the graph at any stage of the modeling/mapping process. The improved preview process is as follows:At the beginning of the preview, the graph model is validated. All entities that are not fully mapped are disregarded in the subsequent steps.For each fully mapped relationship, the mapped files are scanned. Then the first few rows of relationship data are buffered.For each fully mapped node, the first few rows of the mapped files are scanned and compared with the buffered relationship. Only rows with from/to ID values that appeared in the buffered relationships are loaded. In this way, the preview approach maximizes the graph’s connectivity rather than creating many isolated nodes in the preview.After adding the nodes, relationships in the buffer are attached to the loaded nodes.Finally, disconnected nodes are loaded, either isolated nodes in the model or as ones that don’t yet have a fully mapped relationship. In addition, nodes with no matched rows found in step three will be treated as solo nodes and will be loaded as well.Using the enhanced workflow drastically improves the preview accuracy. All steps above utilize the same transaction instance of neo4j-driver. This way the whole data loading process is under an identical context, which can be deduplicated and constructed as a connected graph. Once finished, the operation will be rolled back so that the data is not committed to the database.Here’s an example to demonstrate how we did it.If you want to follow these steps with your own Neo4j instance, you can create a free, empty Neo4j AuraDB database and click the Import” button to use Data Importer.Neo4j Aura - Fully Managed Cloud SolutionAuraDB Free For small development projects, learning, experimentation and prototyping. Start Free AuraDB Professional…neo4j.comSuppose the user is utilizing Data Importer to model the Northwind graph (zip file to load as model and data), and some graph entities, like the HAS_CATEGORY relationship and the Order node, have not yet been fully mapped. In the Data Importer graph modeling canvas, these graph entities with incomplete mappings are drawn with dashed outlines as shown below.Northwind graph with incomplete mappingsTo better visualize the preview process, the flowchart below demonstrates how Data Importer previews the Northwind graph example.Load preview flowchart based on Northwind graphAssume we’re going to scan the first three relationship rows for each relationship file and the first six node rows for each node file and pick up to three matched rows in the preview. Using our new workflow, here’s how it would go:The graph model will be filtered. Only Category, Product and Supplier nodes and SUPPLIED_BY relationship will be taken into account.The corresponding file products.csv mapped in SUPPLIED_BY relationship will be scanned and the first 3 rows, which contain columns mapped to IDs of Product and Supplier nodes are buffered.Product and Supplier nodes will be loaded by scanning corresponding mapped files products.csv and suppliers.csv for the first 6 rows. productID in rows with value 1, 2, or 3 will be cherry-picked from products.csv, and supplierID in rows with value 1 will be cherry-picked from suppliers.csv.SUPPLIED_BY relationship will be loaded by buffered data from products.csv in step two. As a result, Product and Supplier nodes are connected.The Category node doesn’t have any fully mapped relationships, so it will be loaded by picking all the first three rows without comparison with buffered relationship data.Graph data is built via the transaction instance, and it will be fetched and processed for graph visualization. Then the transaction will be rolled back.The preview result in this example is shown below:Load preview graph visualization based on Northwind graph, loading up to three relationship rows for each relationship, scanning up to six node rows for each node file, and picking up to three node rows for each nodeIn practice, we actually preview up to 300 relationships in total and scan up to 50,000 rows in each node file to preview at accurately as possible. This is more time-consuming than our the previous approach, so we introduced a progress bar to indicate the progress of data loading while you’re waiting for the result. We think it’s worth the wait for Preview’s improved accuracy.Preview complete Northwind graph dataImproving the preview logic to more faithfully simulate the load process makes errors a lot easier to catch before committing to a load.Additionally, we now support the previewing of properties. Through these preview improvements, we’re making it easier than ever for users to catch any mapping problems with the highest fidelity preview we’ve ever made.Please try out the new feature and let us know if it is helpful for you. You can submit feedback and feature requests here. So far, we’ve gotten really great feedback and suggestions.Data-Importer | Neo4jneo4j-aura.canny.ioTry it out now in your Neo4j Aura console.;Aug 23, 2022;[]
https://medium.com/neo4j/explore-new-worlds-adding-plugins-to-neo4j-26e6a8e5d37e;Jennifer ReifFollowJun 6, 2018·8 min readStar Trek — U.S.S. EnterpriseExplore New Worlds — Adding Plugins to Neo4j*updated 10/09/19As always, there are always a flurry of changes to Neo4j and some of its tools. To coincide with each release, we usually also have updates completed for our extension libraries.With all this in mind, I thought I would quickly show how to install 2 plugins from our list (APOC and graph algorithms). I hope to add to your list of skills and capabilities and open up the possibilities of utilizing Neo4j. Let’s get started!Graph AlgorithmsI want to kick off the post with this one because it is really fun and visual. This algorithms library allows you to explore your data and extract more meaningful insights in less time than traditional reports, queries, human review, etc. Currently, Neo4j’s graph algorithms library contains all the typically-used and desired algorithms, such as centrality, community detection, pathfinding, and many more.With the latest release, we also added the approximate nearest neighborsand a few other improvements, including relationship aggregation and running on multiple relationships. For those new to the algorithm space, our new docs give a great explanation of any recent additions, as well as some examples.This stuff sounds pretty awesome, right? So now let’s get Neo4j Desktop set up so you can start right in working with graph algorithms and testing them out.Open Neo4j Desktop. If you don’t have this application on your machine, you can download it from Neo4j’s website and follow the instructions to get it set up.Choose or Create a project to work within.There are two different methods to install the Graph Algorithms plugin from here. I will list both options here, and you can choose which one you prefer.A. If you want to install at the project level (i.e. all of your databases within the project will include the plugin), then you can pick your project and scroll down to the bottom of the right pane. At the bottom of the right pane, there is a box with Add Plugin. Click that, then choose the plugin you want to add (graph algorithms, in this case), then click Install on the popup message. You should then see it show up in your project pane in Desktop once you close the plugin modal.B. If you want to install at the database level (i.e. you don’t want all of your databases in the project to have the plugin), then you can pick your project and click Manage on the database where you want to install it. Under the plugins tab, you will should see options for a few plugins. Click Install for the Graph Algorithms.*Note: if your database is running, it may shut down and restart, or you may have to do that before the plugin can be used.Once the installation is complete, you are ready to start running algorithms! You can load your own dataset and use Neo4j’s algorithms to bring out connections, or if you’re not sure how to get started, you can use a few of our sandbox guides.My personal favorite guide is the Graph of Thrones example, where we look at the connections in the Game of Thrones book series to see which characters are the most influential, which characters tend to group into certain communities, and which characters are the most connected to others!To access that sandbox guide, you can choose your project and database where you installed the algorithms, then click Open Browser and type :play data_science in the command line and click Play. The guide will walk you through the rest! The image below shows a visualization done on the Graph of Thrones data set in Neo4j.You can also get quick help on any of the algorithm syntaxes with call algo.list(algorithm-name)APOCThis library was created by one of my colleagues as a way to provide additional procedures and functions that developers often need, but not yet baked into the product. The variety is amazing, and it is well-supported. Say goodbye to writing boring, repetitive functions over and over. It is also incredibly easy to run these procedures from the Neo4j Browser command line.Where have these been all my life? :) Let’s get them set up and dive in!You can follow the same first two steps above that we had for the Graph Algorithms plugin by opening Neo4j Desktop and choosing the project you want to work within. I’ve copied options A and B for installing the APOC plugin here so that I can provide relevant screenshots.A. If you want to install at the project level (i.e. all of your databases within the project will include the plugin), then you can pick your project and scroll down to the bottom of the right pane. At the bottom of the right pane, there is a box with Add Plugin. Click that, then choose the plugin you want to add (APOC, in this case), then click Install on the popup message. You should then see it show up in your project pane in Desktop once you close the plugin modal.B. If you want to install at the database level (i.e. you don’t want all of your databases in the project to have the plugin), then you can pick your project and click Manage on the database where you want to install it. Under the plugins tab, you will should see options for a few plugins. Click Install for APOC.*Note: if your database is running, it may shut it down and restart, or you may have to do that before the plugin can be used.To start working with APOC, you can start any of our sandbox guides or create a query with a procedure or function. Some of the ones that I’ve been working with recently are listed below. More info and details of all the available procedures can be found in the APOC documentation on GitHub.APOC has a built-in help procedure that gives you a short description and syntax for all procedures and functions, you can run it with:call apoc.help(keyword)apoc.date.format(dateForConversion, [timeUnit], [format]) — convert an epoch time-value to a desired format. Can be useful for outputting to reports, showing on a web screen, including it in a URL as a parameter, and hundreds of other things.apoc.load.json(url) — load data from a URL and use Cypher statements to create or update data in Neo4j database. Excellent for calling an API and dumping retrieved data into Neo4j.apoc.periodic.iterate(query1, query2, {param1: value1}) — used as a sort of batch loader. Can pull a list of results in the first query, then execute another query on each of those query1 results to update each one or retrieve other data for it. Can set parameters for batch size, variables, retry number, etc.Other Plugins and Graph AppsThere are many more plugins and graph applications you can add to interact with Neo4j in various ways. To check out the full list and to install any of them, you can go to the Graph Applications tab on Neo4j Desktop, as shown below.You should see a list of applications you already have access to, as well as a section near the bottom to install other graph apps. Here, you can enter the url to pull in the graph app, or you can also discover all the applications we have available. To do that, click the Discover more graph apps link in blue at the bottom, and it will display a webpage with all the graph apps!Just as an example, we’re going to install the Graph Algorithms Playground (NEuler), which is a tool to try out and visualize the graph algorithms on top of data sets. Several default data sets are provided for loading, or users can import their own data to Neo4j and use this to explore algorithms with their data.So, on the graph app library page, we will choose Install on the Graph Algorithms Playground and go back to Neo4j Desktop.Now that you’ve installed the application, you need to add it to whichever project you are working in. Simply pick your project, then click on the Add Application box at the top of the right pane and choose Add on the Graph Algorithms Playground application, then click Ok.Between the plugins and graph apps, you can cover a lot of ground and different use cases. There is a lot of documentation, and we are always willing to help with questions or confusion via Community Site, Slack, StackOverflow, or email. Happy coding!ResourcesDeveloper Getting Started GuidesAbout: APOC, Graph algorithmsDocumentation: APOC, Graph algorithmsNeo4j SandboxDownload Neo4j;Jun 6, 2018;[]
https://medium.com/neo4j/summer-of-nodes-week-3-whodunit-5a511cac6e13;Ljubica LazarevicFollowAug 17, 2020·3 min read*Now updated with hints and solutions!*Summer of Nodes: Week 3 — Whodunit — Murder MysteryMissed the live stream? Catch up here!Missed the hints stream? Catch up here!Missed the solutions stream? Catch up here!Series playlistHello everybody!Summer of Nodes 2020 is now over. If you’ve not had a chance to look at the challenges, you can always have a go at your leisure:The socially distanced barbecueThe online day outThe whodunit (this post)Exploring the areaThis week’s theme — whodunit?Another popular activity we like to do during our days off are things that make us think, solving problems, and the like. Board games, murder mysteries are popular as well.This week’s Summer of Nodes challenge is going to take the excellent Knight Lab’s The SQL Murder Mystery”, with a graph spin, based on the data from their repository. We are going to be doing some sleuthing based on that data to figure out who the suspect is.These challenges should take approximately an hour each, depending on experience.Don’t forget, on Thursday we’ll be providing some hints and tips, just in case you need them.Beginner’s challenge — find the suspectsIntroductionWe would like you to use your sleuthing skills along with your graph prowess to figure out who the suspects are! You may use Neo4j Browser and/or Neo4j Bloom to solve this mystery, whatever works best for you.Beginners challenge:Find out the suspect and their accomplice.You will be:Loading the data based on this scriptYou’ll want to start with reading the crime scene report from Jan 15th, 2018 that took place in GraphvilleTell us who committed the crime and who else was involved!You will probably find a copy of the data model helpful for your enquiriesSome helpful resourcesIf you’ve not already done so, and are completely new to Cypher, you can learn more in the developer guideYou can get the specific on Cypher syntax hereThe Cypher reference card may also prove usefulI am completely new to Neo4j, and Sandbox, and I don’t know how to get started. Help!Here’s a short video showing you the ropes!Can I have a hint please?Of course! Check out this part of the hints stream from this week.SolutionsCheck out the solution stream here.Experienced challenge — a call for witnessesIntroductionSo it turns out things aren’t quite what the seem in Graphville… we want you to use graph algorithms to find out further potential witnesses to speak to. Whilst this is bit of a fluke in the data, it’s a great opportunity to try out the Graph Data Science (GDS) library.Experienced challenge: Find the other witnessesYou will be given hints for some witnesses in the challenge, however there may be some more we’d like to speak to. We would like you to have a go using the Weakly Connected Components (WCC) algorithm to try and identify these potential witnesses to interview.You’ll need to solve the murder first, and then use that information to help identify the right community.Why not do a bit of digging. What surprising discovery do you find along the way?What are we looking for?Use just Facebook check in events for WCC and/or the Gym check-in for the specific witness date!The names of the suspectsThe names of the other potential witnesses to speak toThe dataPlease use the load scripts as per the beginner’s challengeCan I have a hint please?Of course! Check out this part of the hints stream from this week.SolutionsCheck out the solution stream here.;Aug 17, 2020;[]
https://medium.com/neo4j/knowledge-graph-based-chatbot-with-gpt-3-and-neo4j-c4ebbd325ed;Tomaz BratanicFollowMar 8·11 min readKnowledge Graph-Based Chatbot With GPT-3 and Neo4jLearn how to develop a chatbot that provides answers based on data stored in a knowledge graph.ChatGPT has changed how I, and probably most of you, look at AI and chatbots. We can use chatbots to help us find information, construct creative works, and more.However, one problem with ChatGPT and similar chatbots is that they can hallucinate and return great-sounding — yet wildly inaccurate — results. The problem is that these large language models (LLM) are inherently black boxes, so it is hard to fix and retrain models to reduce hallucinations. Consequently, it might not be a good idea to depend on answers from ChatGPT if mission-critical tasks or lives are at stake.On the other hand, there is tremendous value in having the ability to interact with chatbots and use them as an interface for various applications.So I wanted to learn more about chatbots, and luckily Sixing Huang gave me a crash course on different ways of implementing a chatbot. I was especially intrigued by the knowledge graph-based approach to chatbots, where the chatbot returns answers based on information and facts stored in the knowledge graph.Using a knowledge graph as a storage object for answers gives you explicit and complete control over the answers provided by the chatbot and allows you to avoid hallucinations. Additionally, Sixing has already written about and shared the code to implement a knowledge graph-based chatbot, which meant I could borrow some existing ideas and wouldn’t have to start from scratch.My idea was to develop a chatbot that could be used to explore, analyze, and understand news articles.Chatbot interface. Image by the author.But first, I had to construct a knowledge graph based on news articles. Luckily, I have used and written about the information extraction pipeline numerous times, so I didn’t have to lose time doing that. Next, it was time to implement my first chatbot. It turned out that creating a knowledge graph-based chatbot is as easy as a walk in the park thanks to GPT-3. I constructed the following chatbot architecture.Knowledge graph based chatbot architecture. Image by the author.The user talks to a Chatbot on a simple Streamlit application. When the user inputs their question, the question gets sent to the OpenAI GPT-3 endpoint with a request to turn it into a Cypher statement. The OpenAI endpoint returns a Cypher statement, which is then used to retrieve the information from the knowledge graph stored in Neo4j. The retrieved data from the knowledge graph is then used to construct the answer to the user’s question. Additionally, I have added the option to summarize articles using the GPT-3 endpoint, which will be demonstrated later.All the code is available on GitHub.Constructing a knowledge graphIn order to be able to retrieve information from the knowledge graph, we first have to populate it. As mentioned, the idea is to construct a knowledge graph of news articles. Therefore, we need to find a source of quality and accurate news articles. For the purpose of this demonstration, I have used the latest 1000 articles available as a Kaggle repository. The articles are available under the CC BY-NC 4.0 license.We won’t be delving into details about the information extraction pipeline, as I have already written about this subject many times.Information extraction pipeline with SpaCy and REBELBiomedical information extraction pipelineExtracting information from newsFor the most part, the idea behind the information extraction pipeline is to extract structured information about mentioned entities and relationships from unstructured text.Information extraction pipeline. Image by the author.In this example, the information extraction pipeline would identify entities John Snow and NASA in the text. Additionally, most named entity recognition models can infer the entity type, meaning that it deduces whether the mentioned entity is a person, organization, or other.In the next step, a relationship extraction model is used to detect any structured relationships between entities. The text in the above image clearly signifies the working relationship between John Snow and NASA, which can be represented as an EMPLOYEE relationship.Interestingly, we could use GPT-3 to extract structured information from text. A project GraphGPT provides a simple prompt that can be used to generate structured data based on an input text.Using GraphGPT prompt to extract structured information from text. Image by the author.GPT-3 does a decent job of extracting relevant information from the text. It also knows that Boris in the second sentence references Boris Johnson, which is excellent. However, it does not recognize that the UK and the United Kingdom reference the same real-world entity. Entity disambiguation is a vital part of any information extraction pipeline. One approach could be to use an entity-linking strategy to map entities to a target knowledge base. Frequently, Wikipedia is used as a target knowledge base.Asking GPT-3 to link extracted entities to Wikidata. Image by the author.It is fantastic what we can achieve with a simple prompt using GPT-3 endpoint. Immediately, you can notice that both UK and United Kindom map to the same Q145 id, which can be used for entity disambiguation. On the other hand, Boris Johnson is linked to Q1446 in both instances. All that would be great, however, the id Q1446 refers to a roman emperor Caracalla.Wikidata entry for ID Q1446. Image by the author.While GPT-3 is excellent at following prompts, it tends to hallucinate external information like WikiData ids. While we might provide a good prompt for entity disambiguation in a single paragraph, it is hard to construct a good way to disambiguate entities between various texts without entity linking.We could develop our information extraction pipeline that deals with relation extraction and entity linking. I implemented such a pipeline two years ago. However, since two years is a lot in the field of NLP, we might find a solution that provides better accuracy.To avoid developing a custom information pipeline, we will use a Diffbot NLP endpoint. The Diffbot NLP endpoint extracts relationships and provides entity linking out of the box. Additionally, it offers both paragraph and entity-level sentiments, which significantly expand the set of questions we can ask our chatbot, as we can ask it about positive or negative news regarding particular people or entities.The code to run the information extraction pipeline using the Diffbot endpoint is available as a Jupyter notebook. For this demonstration, you don’t need to run it, as I have stored the output of the information extraction pipeline in the project’s data folder. However, if you want to test it on other datasets and evaluate how it performs, do give it a try.Now that the new articles have been processed, we can import the output of the information extraction pipeline into a graph database. In this example, we will be using Neo4j. The GitHub repository is set up to run as two docker services, one for Neo4j and the other for the Streamlit application, so you don’t have to install Neo4j on your own.You can either run the seed_database.sh script or execute the Import notebook to populate the graph database with news articles. The graph schema of the populated knowledge graph about the news is the following:Schema of the populated knowledge graph. Image by the author.The knowledge graph contains Article nodes containing information about the article’s web title, body content or text, and sentiment. In addition, the articles can mention one or multiple Entity nodes. The Entity nodes contain the URL property, which is the output of the entity-linking process, along with their id and type.Interestingly, the relationships between entities are not represented as connections in a graph but rather as separate Relationship nodes. The idea behind this graph modeling decision is that we want to track the text where the extracted relationships originate from. As we know, no NLP pipeline is perfect. Therefore it is essential to have the ability to verify if a relationship is accurately extracted by manually examining the originating text. In a labeled property graph database like Neo4j, we cannot have a connection pointing to another connection. Consequently, we model the extracted relationships between entities as an intermediate node.Using a GPT-3 model to generate Cypher statementsWe have already learned that GPT-3 does a great job of following orders given in a prompt. Additionally, Sixing Huang has already written about how easy it is to train the GPT-3 model to generate Cypher statements. The idea is to give the model a few examples and then let it generate a Cypher statement given the new user input. Specifically, I have prepared the following Cypher examples to train the GPT-3 model.Unfortunately, the GPT-3 endpoint has no concept of context, so we need to send the training examples along with every user input. I wonder what the ChatGPT API endpoint will look like, as ChatGPT has a concept of the context of a dialogue and how it will affect end-user applications.GPT-3 request to generate Cypher statements. Image by the author.As shown in this image, every request to the GPT-3 endpoint starts with training examples. Interestingly, we don’t have to tell the model it should generate Cypher statements or anything — we just provide training examples along with a user prompt, and the model generates Cypher statements.Chatbot implementationNow that we have prepared all the pieces of the puzzle, we can combine them in a chatbot application. I have used a Streamlit application — specifically streamlit-chat — to implement the user interface for the chatbot. I like the Streamlit application as it keeps things simple, and I can use Python to develop the user interface while avoiding any meddling with CSS.The application uses the following Python code to generate the chatbot responses.# Make a request to GPT-3 endpointcompletions = openai.Completion.create(      engine= text-davinci-003 ,      # Construct the prompt using the training examples      # combined with user input      prompt=examples +  \n#  + prompt,      max_tokens=1000,      n=1,      stop=None,      temperature=0.5,  )  # Extract Cypher query from GPT-3 response  cypher_query = completions.choices[0].text  # Use the Cypher query to read the knowledge graph  message = read_query(cypher_query)  return message, cypher_queryAs mentioned, all the code is available on GitHub if you are interested in more details. The repository also includes instructions to run the chatbot application.Let’s now try the chatbot and see how well it behaves. We can start by using an example from the training set of questions. The question is: What are the latest positive news?”Image by the author.The generated Cypher query is available on the right side of the chatbot user interface to allow for easy evaluation of generated Cypher statements. The question is in the training set, and therefore, the generated Cypher statement is identical to the example we provided.Next, we can try a variation of a question that is outside the training set. However, similar examples are provided, and GPT-3 needs to combine information from two examples to generate the Cypher statement.Image by the author.Given that we provided only 11 training examples, I am impressed with how well GPT-3 can generalize and construct appropriate Cypher statements. On the other hand, I am quite pleased with how easy it is to drill down the information provided in previous answers. It makes investigative work more fun and more accessible as you can use natural language to explore data instead of having to write Cypher statements.We can follow up and ask the chatbot about the information we have stored about Emla Fitzsimons in the knowledge graph.Image by the author.The chatbot provides information about the extracted relationships which involve the particular person. In this example, we know that Emla is an employee of the Centre for Longitudinal studies, works with Marcos Vera-Hernandez, and is interested in economics. This information was extracted using the Diffbot NLP endpoint and stored in the knowledge graph.We can ask the chatbot if there are any more news about Emla.Image by the author.It seems Emla is mentioned only in one article. I thought it would be cool to add an option to summarize news articles using the GPT-3 endpoint. As GPT-3 model follows orders quite well, you only need to ask it to summarize text, which does the job.# Make a request to GPT-3 endpointcompletions = openai.Completion.create(    engine= text-davinci-003 ,    # Prefix the prompt with a request to provide a summary    prompt= Summarize the following article: \n  + prompt,    max_tokens=256,    n=1,    stop=None,    temperature=0.5,)message = completions.choices[0].textreturn message, NoneI have added a simple exception in the code. If the user input contains summar, then we assume the task is to provide a summary of the given article.Image by the author.Since the knowledge graph contains both article and entity-level sentiment, we can search for any entities with positive or negative sentiment. For example, we can search for organizations that have been mentioned positively in the news.Image by the author.And similar to before, we can ask the chatbot follow-up questions and drill down on the information we are interested in.Image by the author.SummaryI wanted to create a project that uses natural language to explore and analyze knowledge graphs for a long time. However, the barrier to entry was too high for me as I am not a machine learning expert, and developing and training a custom model that generates Cypher statements based on user inputs was too big of a task for me.And frankly, until I joined the ChatGPT hype, I wasn’t genuinely aware of how incredible the underlying technology is and how well it works. For example, we only provided 10 training examples, and the chatbot behaves like it has worked with the given graph schema for the past five years.Hopefully, this article will encourage you to implement your own chatbots and use them to make knowledge graphs and other technologies more accessible!The code is available as a GitHub repository.;Mar 8, 2023;[]
